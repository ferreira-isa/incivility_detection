,thread_id,email_id,ahlaam_preprocessed_text,original_text,email_classification,author_name,author_email,author_role,is_first_author_thread,nr_characters,ratio_words_email_thread,position_comment_thread,is_last_comment,time_start_to_email,time_email_to_end,time_previous_to_email,time_email_to_next
0,152625,157636,"Hi,Thanks for the details.Initially i was sceptical of rst & once instead of hitting the fly, hit ""make htmldocs"" on the keyboard :), and the opinion about it was changed. It was easy to navigate through various docs & the realized that various topics (& many) were present (yes, it was there earlier also, but had to dive inside Documentation & search, while viewing the top level index.html made them standout). It was like earlier you had to go after docs, but now it was docs coming after you, that is my opinion.Later while fighting with memory-barriers.txt, felt that it might be good for it as well as to be in that company. And the readability as a text is not hurt as well.It was thought that rst conversion could be done quickly, but sincethis was my first attempt with rst, had to put some effort to get anot so bad output, even if this patch dies, i am happy to have learn trst conversion to some extent. When one of the author of the original document objected, i felt it is better to back off. But if there is a consensus, i will proceed.","Hi,  On Thu, Jan 04, 2018 at 11:27:55AM +0100, Markus Heiser wrote:   Thanks for the details.   Initially i was sceptical of rst & once instead of hitting the fly, hit make htmldocs"" on the keyboard :), and the opinion about it was changed. It was easy to navigate through various docs & the realized that various topics (& many) were present (yes, it was there earlier also, but had to dive inside Documentation & search, while viewing the toplevel index.html made them standout). It was like earlier you had to go after docs, but now it was docs coming after you, that is my opinion.  Later while fighting with memory-barriers.txt, felt that it might be good for it as well as to be in that company.  And the readability as a text is not hurt as well.  It was thought that rst conversion could be done quickly, but since this was my first attempt with rst, had to put some effort to get a not so bad output, even if this patch dies, i am happy to have learnt rst conversion to some extent.   When one of the author of the original document objected, i felt it is better to backoff. But if there is a consensus, i will proceed.  afzal""",not_technical,afzal mohammed,afzal.mohd.ma@gmail.com,0,1,1052,1.0,0.875,0,0.5,0.0,0.0,0.0
1,165231,165261,"According to my comment on the other thread, this stands true in case the child is managed by runtime PM as well.Otherwise this looks good to me. How about adding an additional patch on top taking into account the ignore_children flag and folding that into the series, kind of as you also suggested?My point is, we might as well take the opportunity to fix this right away, don't you think?","On 12 January 2018 at 14:12, Rafael J. Wysocki <rjw@rjwysocki.net> wrote:  According to my comment on the other thread, this stands true in case the child is managed by runtime PM as well.  Otherwise this looks good to me.   How about adding an additional patch on top taking into account the ignore_children flag and folding that into the series, kind of as you also suggested?  My point is, we might as well take the opportunity to fix this right away, don't you think?  [...]  Kind regards Uffe",not_technical,Ulf Hansson,ulf.hansson@linaro.org,1,0,390,1.0,0.25,0,0.0,1.0,0.0,0.0
2,167856,174594,"No problem, at the beginning, I only wanted to enable the strict. Doing this involves that I have to remove pinctrl nodes for the pins which are going to be request through the gpiolib to avoid conflicts. These pinswere configured with bias-pull-up. That's why I try to add the bias support.Thanks for the detailed answer about what you have in mind.Well, yes and not! As a consequence of enabling strict mode, I have tofind another way to configure the pins. Yes, I have noticed this issue. Right, I have spotted some drivers to fix.I will try to handle the ones related to the platforms I am using.","On Thu, Jan 18, 2018 at 11:16:44AM +0100, Linus Walleij wrote:  No problem, at the beginning, I only wanted to enable the strict. Doing this involves that I have to remove pinctrl nodes for the pins which are going to be request through the gpiolib to avoid conflicts. These pins were configured with bias-pull-up. That's why I try to add the bias support.   Thanks for the detailed answer about what you have in mind.   Well, yes and not! As a consequence of enabling strict mode, I have to find another way to configure the pins.   Yes, I have noticed this issue.   Right, I have spotted some drivers to fix.   I will try to handle the ones related to the platforms I am using.  Regards  Ludovic",not_technical,Ludovic Desroches,ludovic.desroches@microchip.com,1,1,600,1.0,0.46153846153846156,0,0.15384615384615385,0.7692307692307693,0.0,0.0
3,168169,173259,"Thanks a lot for all this work! It was long overdue and it is nice to see the project finally getting to an end, after passing into so many hands! I am not sure I understand the purpose of this level here. As far as I understand, you only have per-engine control whether you want to enable CG or not. What you call BLCG and SLCG levels just mean ""don't use the boot values, but rather use our values (taken from nvidia)"". Now, here comes the nasty part: NVIDIA only ever validated the boot values (I guess they are extremely safe ones), and the optimised values (the ones coming from your patch 2, 3, and 4 along with the level 3. I think introducing a single parameter that controls both CG, PG, and automatic reclocking would be safer. For CG and PG, it would be aall-or-nothing (either boot values, or everything like nvidia).This message is a bit odd, whether we keep the notion of levels or not. Can you get rid of the mention of powergating given that this is notpart of this patchset?If you agree with having a single enable bit for CG, then a simple:""Clockgating status: (boot | optimized)"" would work perfectly.All this time, I thought these parameters were for power gating... I also did not expect that clock gating had to be disabled before we could program them. Great find! Why introduce this? I can't find references to it in this patch (outside of the function below) or in the following patches.As you even export this function, it looks like you used to use thisfunction in an earlier revision of this series.Aside from all these nitpicks, the approach is quite self contained andI like the following patches. Well done!Once we settle on the configuration parameter, I can give you.","On 16/01/18 00:06, Lyude Paul wrote:  Thanks a lot for all this work! It was long overdue and it is nice to see the project finally getting to an end, after passing into so many hands!   I am not sure I understand the purpose of this level here. As far as I understand, you only have per-engine control whether you want to enable CG or not. What you call BLCG and SLCG levels just mean don't use the boot values, but rather use our values (taken from nvidia)"".  Now, here comes the nasty part: NVIDIA only ever validated the boot values (I guess they are extremely safe ones), and the optimised values (the ones coming from your patch 2, 3, and 4 along with the level 3.  I think introducing a single parameter that controls both CG, PG, and automatic reclocking would be safer. For CG and PG, it would be a all-or-nothing (either boot values, or everything like nvidia).   This message is a bit odd, whether we keep the notion of levels or not.  Can you get rid of the mention of powergating given that this is not part of this patchset?  If you agree with having a single enable bit for CG, then a simple: ""Clockgating status: (boot|optimized)"" would work perfectly.   All this time, I thought these parameters were for power gating... I also did not expect that clock gating had to be disabled before we could program them.  Great find!   Why introduce gk104_therm_new_? I can't find references to it in this patch (outside of the function below) or in the following patches.  As you even export this function, it looks like you used to use this function in an earlier revision of this series.  Aside from all these nitpicks, the approach is quite self contained and I like the following patches. Well done!  Once we settle on the configuration parameter, I can give you my R-b :)  Martin """,not_technical,Martin Peres,martin.peres@free.fr,0,0,1700,1.0,1.0,1,1.0,0.0,0.6,0.0
4,169133,179802,"Sorry about being late, just returned home and am trying to get all the backlogs under control. I remember the PPP standard is a bit cloudy about the possible issue, but the latter indeed exists (the PPP state machine was written directly to STD-51). There is related (more visible in practice, though we aren't affected) issue of ""active"" vs ""passive"" mode (hdlc_ppp.c is ""active"",and two ""passives"" wouldn't negotiate at all). Anyway the problem is real (though not very visible in practice, especially on relatively modern links rather than 300 or 1200 bps dialupconnections) and should be fixed. Looking at the patch, my first impression is it makes the code differ from STD-51 a little bit. On the other hand, perhaps applying it as is and forgetting about the issue is the way to go. Ideally, I think the negotiation failure should end up (optionally, inaddition to the current behavior) in some configurable sleep, then the negotiation should restart. If it's worth the effort at this point,I don't know. Perhaps I could look at this later, but no promises (this requires pulling on and setting up some legacy hardware). Anyway, since the patch is safe and can solve an existing problem.","Denis Du <dudenis2000@yahoo.ca> writes:   Sorry about being late, just returned home and am trying to get all the backlogs under control.  I remember the PPP standard is a bit cloudy about the possible issue, but the latter indeed exists (the PPP state machine was written directly to STD-51). There is related (more visible in practice, though we aren't affected) issue of active"" vs ""passive"" mode (hdlc_ppp.c is ""active"", and two ""passives"" wouldn't negotiate at all).  Anyway the problem is real (though not very visible in practice, especially on relatively modern links rather than 300 or 1200 bps dialup connections) and should be fixed. Looking at the patch, my first impression is it makes the code differ from STD-51 a little bit. On the other hand, perhaps applying it as is and forgetting about the issue is the way to go.  Ideally, I think the negotiation failure should end up (optionally, in addition to the current behavior) in some configurable sleep, then the negotiation should restart. If it's worth the effort at this point, I don't know.  Perhaps I could look at this later, but no promises (this requires pulling on and setting up some legacy hardware).  Anyway, since the patch is safe and can solve an existing problem:  Acked-by: Krzysztof Halasa <khc@pm.waw.pl> --  Krzysztof Halasa""",not_technical,Krzysztof Halasa,khc@pm.waw.pl,1,0,1194,0.8245033112582781,0.5,0,0.2972972972972973,0.6756756756756757,0.10810810810810811,0.24324324324324326
5,169133,193114,"How  do you think my patch? As you see, he think my patch is ok to be accepted. But if you have a better idea to fix it, I am glad to see it. Anyway, this issue have to be fixed. Denis DU Denis Du writes:Sorry about being late, just returned home and am trying to get all the backlogs under control. I remember the PPP standard is a bit cloudy about the possible issue, but the latter indeed exists (the PPP state machine was written directlyto STD-51). There is related (more visible in practice, though we aren't affected) issue of ""active"" vs ""passive"" mode (hdlc_ppp.c is ""active"",and two ""passives"" wouldn't negotiate at all). Anyway the problem is real (though not very visible in practice, especially on relatively modern links rather than 300 or 1200 bps dialup connections) and should be fixed. Looking at the patch, my first impression is it makes the code differ from STD-51 a little bit. On the other hand, perhaps applying it as is and forgetting about the issue is the way to go.Ideally, I think the negotiation failure should end up (optionally, in addition to the current behavior) in some configurable sleep, then the negotiation should restart. If it's worth the effort at this point,I don't know. Perhaps I could look at this later, but no promises (this requires pulling on and setting up some legacy hardware). Anyway, since the patch is safe and can solve an existing problem.","  Hi, David:  How  do you think my patch?  As you see, Krzysztof  think my patch is ok to be accepted.  But if you have a better idea to fix it,I am glad to see it. Anyway, this issue have to be fixed.    Denis DU     On Sunday, January 28, 2018, 9:34:15 AM EST, Krzysztof Halasa <khc@pm.waw.pl> wrote:       Denis Du <dudenis2000@yahoo.ca> writes:     Sorry about being late, just returned home and am trying to get all the backlogs under control.  I remember the PPP standard is a bit cloudy about the possible issue, but the latter indeed exists (the PPP state machine was written directly to STD-51). There is related (more visible in practice, though we aren't affected) issue of active"" vs ""passive"" mode (hdlc_ppp.c is ""active"", and two ""passives"" wouldn't negotiate at all).  Anyway the problem is real (though not very visible in practice, especially on relatively modern links rather than 300 or 1200 bps dialup connections) and should be fixed. Looking at the patch, my first impression is it makes the code differ from STD-51 a little bit. On the other hand, perhaps applying it as is and forgetting about the issue is the way to go.  Ideally, I think the negotiation failure should end up (optionally, in addition to the current behavior) in some configurable sleep, then the negotiation should restart. If it's worth the effort at this point, I don't know.  Perhaps I could look at this later, but no promises (this requires pulling on and setting up some legacy hardware).  Anyway, since the patch is safe and can solve an existing problem:  Acked-by: Krzysztof Halasa <khc@pm.waw.pl> --  Krzysztof Halasa""",not_technical,Denis Du,dudenis2000@yahoo.ca,0,1,1398,1.0,0.6,0,0.5405405405405406,0.43243243243243246,0.24324324324324326,0.0
6,170193,188745,"Thanks for the review and apologies for the delay. Replies inlined below. ]okok, this is all new stuff to me ... I suppose I should do it also for all the other new files I create But what is the license for the documentation? It's not code, so GPL seems wrong. Creative commons? I just noticed a patch for checkpatch.pl about SPDX and asked the same question there. I'll move it to the Use section.[...]I will add a reference to the self test file. In practice it can also work as example.ok, the example route should be more explicative.--thanks again for the review.","Thanks for the review and apologies for the delay. Replies inlined below.  On 30/01/18 19:08, Jonathan Corbet wrote:  [...]   ok   ok, this is all new stuff to me ... I suppose I should do it also for all the other new files I create  But what is the license for the documentation? It's not code, so GPL seems wrong. Creative commons?  I just noticed a patch for checkpatch.pl about SPDX and asked the same question there.  https://lkml.org/lkml/2018/2/2/365   [...]   ok   [...]   I'll move it to the Use section.  [...]   I will add a reference to the selftest file. In practice it can also work as example.   ok, the example route should be more explicative.  -- thanks again for the review, igor",not_technical,Igor Stoppa,igor.stoppa@huawei.com,0,1,569,0.5324675324675324,0.6,0,0.23076923076923078,0.7692307692307693,0.0,0.0
7,180488,180837,"wrote: Then, I am wondering why we are holding mmap_sem when calling migrate_pages() in existing code. Sorry, I missed that. If mmap_sem is not needed for migrate_pages(), please ignore this patch.","  Michal Hocko wrote:  Then, I am wondering why we are holding mmap_sem when calling migrate_pages() in existing code. http://elixir.free-electrons.com/linux/latest/source/mm/migrate.c#L1576   Sorry, I missed that. If mmap_sem is not needed for migrate_pages(), please ignore this patch.    --  Best Regards, Yan Zi  ",not_technical,Zi Yan,zi.yan@cs.rutgers.edu,0,0,197,1.0,0.36363636363636365,0,0.0,0.0,0.0,0.0
8,183468,183472,Thanks for testing this and letting me know.,"On Tue, Feb 27, 2018 at 02:17:15AM +0000, Harsh Shandilya wrote:  Thanks for testing this and letting me know.  greg k-h",not_technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,1,44,1.0,0.9047619047619048,0,0.0,0.0,0.0,0.0
9,207400,207934,"Cool. I would say this is done right. How about writing an i2c bus driver which sits directly on top ofanother i2c bus? Basically a one port i2c mux. The current mux code does not seem to directly allow it, since itcalls i2c_transfer() directly on the parent, where as you want it to call your own i2c_transfer function. But maybe you could expended the core mux code to allow the i2c_mux_core structure to contain a transfer function?","Hi Tim  Cool. I would say this is done right.   How about writing an i2c bus driver which sits directly on top of another i2c bus? Basically a one port i2c mux.  The current mux code does not seem to directly allow it, since it calls i2c_transfer() directly on the parent, where as you want it to call your own i2c_transfer function. But maybe you could expended the core mux code to allow the i2c_mux_core structure to contain a transfer function?        Andrew",not_technical,Andrew Lunn,andrew@lunn.ch,1,0,435,0.7016129032258065,0.6470588235294118,0,0.0,0.0,0.0,0.0
10,207400,208102,"Hi Dmitry - thanks for the review!okok - will have this in oops, did not mean to submit that it was for original debugging and not needed - will remove Yes, that makes sense. I'll propose something like the following in v2: right - thanks! I am using request_threaded_irq with thread_fn with thread_fn (vs handler).Do you mean why use a work procedure? I guess I don't need that andcan call input_report_key directly from the irq. ok can you point me to an example dts/driver?","On Tue, Feb 27, 2018 at 8:54 PM, Dmitry Torokhov <dmitry.torokhov@gmail.com> wrote:  Hi Dmitry - thanks for the review!   ok   ok - will have this in v2:  // SPDX-License-Identifier: GPL-2.0 // // Copyright (C) 2018 Gateworks Corporation // // This driver dispatches Linux input events for GSC interrupt events //    oops, did not mean to submit that   it was for original debugging and not needed - will remove   Yes, that makes sense. I'll propose something like the following in v2:  gsc_input {    compatible = gw,gsc-input"",     button {       label = ""user pushbutton"",       linux,code = <256>,       interrupts = <0>    },     key-erased {       label = ""key erased"",       linux,code = <257>,       interrupts = <1>    },     ... },    right - thanks!   I am using request_threaded_irq with thread_fn with thread_fn (vs handler).  Do you mean why use a work procedure? I guess I don't need that and can call input_report_key directly from the irq.   ok   can you point me to an example dts/driver?  Tim """,not_technical,Tim Harvey,tharvey@gateworks.com,1,1,476,0.7983870967741935,0.7647058823529411,0,0.0,0.0,0.0,0.0
11,207400,208141,"No, at this point it requires both I2C and OF. I may add platform data to support an older non-device-tree family of boards but it still would require I2C. I will remove the || COMPILE_TEST Thanks for catching that.","On Tue, Feb 27, 2018 at 6:00 PM, Randy Dunlap <rdunlap@infradead.org> wrote:  Randy,  No, at this point it requires both I2C and OF. I may add platform data to support an older non-device-tree family of boards but it still would require I2C.  I will remove the || COMPILE_TEST  Thanks for catching that.  Tim",not_technical,Tim Harvey,tharvey@gateworks.com,1,1,215,0.3467741935483871,0.8235294117647058,0,0.0,0.0,0.0,0.0
12,207400,208159,"Thanks for the review! oops - left that in by mistake.It has 16x ADC channels where some can be temperatures and others can be voltage inputs (based on device tree). understood - a much cleaner pattern right - removed yikes - thanks for catching that ok yes, that static arrays are not very forward-thinking and yes my arrays are not consistent. I'll convert to dynamically allocating the channels for v2right - certainly an issue will do will add validation ok Do you mean stuffing a u32 into a u8? will fix will fix will do could also return -EINVAL but not with the args I'm passing in so I'll change it to this.","On Tue, Feb 27, 2018 at 6:05 PM, Guenter Roeck <linux@roeck-us.net> wrote:  Guenter,  Thanks for the review!   oops - left that in by mistake.   It has 16x ADC channels where some can be temperatures and others can be voltage inputs (based on device tree).   understood - a much cleaner pattern   right - removed   yikes - thanks for catching that   ok   yes, that static arrays are not very forward-thinking and yes my arrays are not consistent. I'll convert to dynamically allocating the channels for v2   right - certainly an issue   will do   will add validation   ok   Do you mean stuffing a u32 into a u8?   will fix   will fix   will do   could also return -EINVAL but not with the args I'm passing in so I'll change it to: return PTR_ERR_OR_ZERO(hwmon),  Thanks!  Tim",not_technical,Tim Harvey,tharvey@gateworks.com,1,1,615,1.0,0.9411764705882353,0,0.0,0.0,0.0,0.0
13,216128,216133,"Sorry for being dense. What tree is this against? I can't find mention of amdcz in Linus's tree nor linux-next. Where does get used where isn't defined? (i.e. why is the #ifdef needed here?) Otherwise, sure, sounds good. :)","On Wed, Mar 14, 2018 at 2:44 PM, Daniel Kurtz <djkurtz@chromium.org> wrote:  Sorry for being dense. What tree is this against? I can't find mention of amdcz in Linus's tree nor linux-next.   Where does serial8250_skip_old_ports get used where CONFIG_SERIAL_8250 isn't defined? (i.e. why is the #ifdef needed here?)   Otherwise, sure, sounds good. :)  -Kees  --  Kees Cook Pixel Security",not_technical,Kees Cook,keescook@chromium.org,1,0,223,1.0,0.45454545454545453,0,0.0,1.0,0.0,0.0
14,216128,216130,Can you possibly send the entire series again and CC all patches to linux-acpi and fix the kbuild warnings if the are relevant for that matter? Thanks!,"On Wednesday, March 14, 2018 10:44:37 PM CET Daniel Kurtz wrote:  Can you possibly send the entire series again and CC all patches to linux-acpi and fix the kbuild warnings if the are relevant for that matter?  Thanks!",not_technical,Rafael J. Wysocki,rjw@rjwysocki.net,1,0,151,0.5272727272727272,1.0,1,1.0,0.0,0.25,0.0
15,222860,230472,"Can you add a commit message explaining why you add a specific def config for this board. FYI, previously, the same def config was used for boards.You will also need to resync with the last master branch regarding defconfig content.","Hi Yannick  On 03/02/2018 04:44 PM, yannick fertre wrote:  Can you add a commit message explaining why you add a specific defconfig  for this board. FYI, previously, the same defconfig was used for all  STM32F7 boards (ie /stm32f746-disco_defconfig).  You will also need to resync with the last master branch regarding  defconfig content.  Thanks  Patrice ",not_technical,Patrice CHOTARD,patrice.chotard@st.com,0,0,232,0.5,0.75,0,1.0,0.0,0.6,0.0
16,230843,231058,"This is indeed guaranteed. For FTRACE use case. If it's being called from FTRACE in run time, this would mean there were long calls in this module section, which inturn means, get_module_plt() was called at least once for this module and this section. This doesn't hold in general, though.In any case, if you insist, I can try to rework the whole stuff implementing module_finalize().","Hi Ard!  On 13/03/18 18:18, Ard Biesheuvel wrote:  This is indeed guaranteed. For FTRACE use case. If it's being called from FTRACE in run time, this would mean there were long calls in this module section, which in turn means, get_module_plt() was called at least once for this module and this section.  This doesn't hold in general, though.  In any case, if you insist, I can try to rework the whole stuff implementing module_finalize().  --  Best regards, Alexander Sverdlin.",not_technical,Alexander Sverdlin,alexander.sverdlin@nokia.com,1,1,384,1.0,0.4782608695652174,0,0.0,1.0,0.0,0.0
17,231231,232204,So make it fit by returning an unsigned int. Good mailing practices for 400: avoid top-posting and trim the reply.,"On Thu, Mar 15, 2018 at 12:46:05AM +0100, Maciej S. Szmigiero wrote:  So make it fit by returning an unsigned int.  --  Regards/Gruss,     Boris.  Good mailing practices for 400: avoid top-posting and trim the reply.",not_technical,Borislav Petkov,bp@alien8.de,1,0,114,1.0,0.5714285714285714,0,1.0,0.0,0.0,0.0
18,239101,240572,"Thank you so much for many style, formatting and other issues fixes and also forintegration of 'check_at_most_once' patch, it saved me several review iterations. Regarding free of sg in two error paths, you were correct.I fixed it by placing several error labels to differentiate each handling. I also noted that was not released properly, this is also fixed. following is a diff of my fix based on your modifications.(I can send it in a patch format, but it doesn't include a fix for Eric Biggers comments) I took your modifications and working upon it.","   Thank you so much for many style, formatting and other issues fixes and also for integration of 'check_at_most_once' patch, it saved me several review iterations.   Regarding free of sg in two error paths, you were correct. I fixed it by placing several error labels to differentiate each handling. I also noted that reqdata_arr[b].req was not released properly, this is also fixed. following is a diff of my fix based on your modifications. (I can send it in a patch format, but it doesn't include a fix for Eric Biggers comments)   @@ -573,10 +573,9 @@ static void verity_verify_io(struct dm_verity_io *io)                         verity_bv_skip_block(v, io, &io->iter),                         continue,                 } -                 reqdata_arr[b].req = ahash_request_alloc(v->tfm, GFP_NOIO),                 if (unlikely(reqdata_arr[b].req == NULL)) -                       goto err_memfree, +                       goto err_mem_req,                 ahash_request_set_tfm(reqdata_arr[b].req, v->tfm),                   /* +1 for the salt buffer */ @@ -586,7 +585,7 @@ static void verity_verify_io(struct dm_verity_io *io)                                    GFP_NOIO),                 if (!sg) {                         DMERR_LIMIT(%s: kmalloc_array failed"", __func__), -                       goto err_memfree, +                       goto err_mem_sg,                 }                 sg_init_table(sg, num_of_buffs),                 // FIXME: if we 'err_memfree' (or continue,) below how does this sg get kfree()'d? @@ -595,7 +594,7 @@ static void verity_verify_io(struct dm_verity_io *io)                                           reqdata_arr[b].want_digest,                                           &reqdata_arr[b].fec_io, &is_zero),                 if (unlikely(r < 0)) -                       goto err_memfree, +                       goto err_mem,                   if (is_zero) {                         /* @@ -605,7 +604,7 @@ static void verity_verify_io(struct dm_verity_io *io)                         r = verity_for_bv_block(v, io, &io->iter,                                                 verity_bv_zero),                         if (unlikely(r < 0)) -                               goto err_memfree, +                               goto err_mem,                         verity_cb_complete(iodata, r),                         continue,                 } @@ -644,7 +643,11 @@ static void verity_verify_io(struct dm_verity_io *io)         }         return,   -err_memfree: +err_mem: +       kfree(sg), +err_mem_sg: +       ahash_request_free(reqdata_arr[b].req), +err_mem_req:         /*          * reduce expected requests by the number of unsent          * requests, -1 accounting for the current block         atomic_sub(blocks - b - 1, &iodata->expected_reqs),         verity_cb_complete(iodata, -EIO),   I took your modifications and working upon it.  """,not_technical,Unkown Name,yael.chemla@foss.arm.com,0,0,554,0.9819819819819819,0.9285714285714286,0,0.06666666666666667,0.9333333333333333,0.0,0.9333333333333333
19,245912,245953,"The driver is looking good! It looks like you've done some kind of review that we weren't allowed to see, which is a double edged sword - I might be asking about things that you've already spoken about with someone else. I'm only just learning about PECI, but I do have some general comments below. I think just saying ASPEED PECI support is enough. That way if the next ASPEED SoC happens to have PECI we don't need to update all of the help text :) Nit: we use ASPEED instead of AST in the upstream kernel to distingush from the aspeed sdk drivers. If you feel strongly about this then I won't insist you change. I know these come from the ASPEED sdk driver. Do we need them all? Could the above use regmap_read_poll_timeout instead? That looks like an endian swap. Can we do something like this? Having #defines is frowned upon. I think print_hex_dump_debug will dowhat you want here.I find this hard to read. Use a few more lines to make it clear what your code is doing. Actually, the entire for loop is cryptic. I understand what it's doing now. Can you rework it to make it more readable? You follow a similar pattern above in the write case. Given the regmap_read is always going to be a memory read on the aspeed, I can't think of a situation where the read will fail. On that note, is there a reason you are using regmap and not just accessing the hardware directly? regmap imposes a number of pointer lookups and tests each time you do a read or write. Again, a memory mapped read won't fail. How about we check that the regmap is working once in your _probe() function, and assume it will continue working from there (or remove the regmap abstraction all together). All of this code is for debugging only. Do you want to put it behind some kind of conditional? We have a framework for doing clocks in the kernel. Would it make sense to write a driver for this clock and add it to this? The property is optional so I suggest we don't print a message if it's not present. We certainly don't want to print a message saying ""invalid"". The same comment applies to the other optional properties below. Can we probe in parallel? If not, putting a sleep in the _probe will hold up the rest of drivers from being able to do anything, and hold up boot.If you decide that you do need to probe here, please add a comment.(This is the wait for the clock to be stable?) This interrupt is only for the peci device. Why is it marked as shared?","Hello Jae,  On 11 April 2018 at 04:02, Jae Hyun Yoo <jae.hyun.yoo@linux.intel.com> wrote:  The driver is looking good!  It looks like you've done some kind of review that we weren't allowed to see, which is a double edged sword - I might be asking about things that you've already spoken about with someone else.  I'm only just learning about PECI, but I do have some general comments below.   I think just saying ASPEED PECI support is enough. That way if the next ASPEED SoC happens to have PECI we don't need to update all of the help text :)   Nit: we use ASPEED instead of AST in the upstream kernel to distingush from the aspeed sdk drivers. If you feel strongly about this then I won't insist you change.   I know these come from the ASPEED sdk driver. Do we need them all?   Could the above use regmap_read_poll_timeout instead?   That looks like an endian swap. Can we do something like this?   regmap_write(map, reg, cpu_to_be32p((void *)msg->tx_buff))   Having #defines is frowned upon. I think print_hex_dump_debug will do what you want here.   I find this hard to read. Use a few more lines to make it clear what your code is doing.  Actually, the entire for loop is cryptic. I understand what it's doing now. Can you rework it to make it more readable? You follow a similar pattern above in the write case.   Given the regmap_read is always going to be a memory read on the aspeed, I can't think of a situation where the read will fail.  On that note, is there a reason you are using regmap and not just accessing the hardware directly? regmap imposes a number of pointer lookups and tests each time you do a read or write.   Again, a memory mapped read won't fail. How about we check that the regmap is working once in your _probe() function, and assume it will continue working from there (or remove the regmap abstraction all together).   All of this code is for debugging only. Do you want to put it behind some kind of conditional?   We have a framework for doing clocks in the kernel. Would it make sense to write a driver for this clock and add it to drivers/clk/clk-aspeed.c?   The property is optional so I suggest we don't print a message if it's not present. We certainly don't want to print a message saying invalid"".  The same comment applies to the other optional properties below.   Can we probe in parallel? If not, putting a sleep in the _probe will hold up the rest of drivers from being able to do anything, and hold up boot.  If you decide that you do need to probe here, please add a comment. (This is the wait for the clock to be stable?)   This interrupt is only for the peci device. Why is it marked as shared? """,not_technical,Joel Stanley,joel@jms.id.au,1,0,2439,1.0,0.24074074074074073,0,0.0,1.0,0.0,0.0
20,258997,259513,I'm really sorry for this. could you please illustrate me what the kconfig & warning is?I didn't get such warnings from 0-day.,"Hi, Linus,  On 三, 2018-04-11 at 17:01 -0700, Linus Torvalds wrote:  I'm really sorry for this. could you please illustrate me what the kconfig & warning is? I didn't get such warnings from 0-day.  thanks, rui",not_technical,Zhang Rui,rui.zhang@intel.com,1,1,126,0.4745762711864407,0.12,0,0.0,0.75,0.0,0.0
21,258997,260193,I'm not the one that added this switch statement (it has been there since 2011) and I would be happy to remove it.  However could we please defer this to v4.17 and merge the current set of Exynos thermal fixes/cleanups (they simplify the driver a lot and make ground for future changes)?,"On Friday, April 13, 2018 11:19:40 AM Daniel Lezcano wrote:  I'm not the one that added this switch statement (it has been there since 2011) and I would be happy to remove it.  However could we please defer this to v4.17 and merge the current set of Exynos thermal fixes/cleanups (they simplify the driver a lot and make ground for future changes)?  Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",not_technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,287,1.0,0.56,0,0.5,0.25,0.0,0.0
22,259276,259489,"Thanks for your reply :) I admit I am not familiar with this driver. I did not know this driver is only loaded during system boot-up time, I thought this driver can be loaded as a kernel module (like many drivers) after system booting. After knowing this, I admit my patch is not proper, sorry...","  On 2018/4/12 10:21, arvindY wrote:  Hello, Arvind. Thanks for your reply :)  I admit I am not familiar with this driver. I did not know this driver is only loaded during system boot-up time, I thought this driver can be loaded as a kernel module (like many  drivers) after system booting. After knowing this, I admit my patch is not proper, sorry...   Best wishes, Jia-Ju Bai",not_technical,Jia-Ju Bai,baijiaju1990@gmail.com,0,1,296,0.7619047619047619,1.0,1,0.0,0.0,0.0,0.0
23,266017,266029,"Well, I am not sure. Could you please give me hints, how to debug this further? Is there some debug flag? I am only aware of the Ftrace framework, but in my experience it also skews the timings quite a bit, so might not be the best choice.","Dear Takashi,   On 04/23/18 14:21, Takashi Iwai wrote:  Well, I am not sure. Could you please give me hints, how to debug this  further? Is there some debug flag?  I am only aware of the Ftrace framework, but in my experience it also  skews the timings quite a bit, so might not be the best choice.   Kind regards,  Paul  ",not_technical,Paul Menzel,pmenzel+alsa-devel@molgen.mpg.de,0,1,239,1.0,0.3,0,0.0,1.0,0.0,0.0
24,266279,266331,I see I've missed some obvious things that you've pointed out here. I'll mark these warnings as False Positives and take your points into account for the analysis of the rest of the Spectre issues reported by Smatch.Sorry for the noise and thanks for the feedback.,"  On 04/23/2018 01:24 PM, Mauro Carvalho Chehab wrote: I see I've missed some obvious things that you've pointed out here. I'll  mark these warnings as False Positives and take your points into account  for the analysis of the rest of the Spectre issues reported by Smatch.  Sorry for the noise and thanks for the feedback.  Thanks -- Gustavo",not_technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,264,0.14613180515759314,0.35,0,0.0,1.0,0.0,0.0
25,266279,266334,"Hi, Please, drop this series. Further analysis is required as it seems all these are False Positives. Sorry for the noise.","Hi,  Please, drop this series. Further analysis is required as it seems all  these are False Positives.  Sorry for the noise.  Thanks -- Gustavo  On 04/23/2018 12:37 PM, Gustavo A. R. Silva wrote:",not_technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,122,0.07449856733524356,0.375,0,0.0,1.0,0.0,0.0
26,266279,267299,"Thanks for a comprehensive explanation about that. It now makes more sense to me. Yeah, better to apply a fix to avoid the issue with VIDIOC_ENUM_FMT. Btw, on almost all media drivers, the implementation for enumerating the supported formats are the same (and we have a few other VIDOC_ENUM_fooioctls that usually do similar stuff): the V4L2 core calls a driver, with looks into an array, returning the results to the core. So, a fix like that should likely go to almost all media drivers (there are a lot of them!), and, for every new one, to take care to avoid introducing it again during patch review process.So, I'm wondering if are there any way to mitigate it inside the core itself, instead of doing it on every driver. Ok, a ""poor man"" approach would be to pass the array directly to the core and let the implementation there to implement the array fetch logic, calling this there, but I wonder if are there any other way that won't require too much code churn.","Em Tue, 24 Apr 2018 12:36:09 +0200 Peter Zijlstra <peterz@infradead.org> escreveu:    Peter,  Thanks for a comprehensive explanation about that. It now makes more sense to me.  Yeah, better to apply a fix to avoid the issue with VIDIOC_ENUM_FMT.   Btw, on almost all media drivers, the implementation for enumerating the supported formats are the same (and we have a few other VIDOC_ENUM_foo ioctls that usually do similar stuff): the V4L2 core calls a driver, with looks into an array, returning the results to the core.  So, a fix like that should likely go to almost all media drivers (there are a lot of them!), and, for every new one, to take care to avoid introducing it again during patch review process.  So, I'm wondering if are there any way to mitigate it inside the  core itself, instead of doing it on every driver, e. g. changing v4l_enum_fmt() implementation at v4l2-ioctl.  Ok, a poor man"" approach would be to pass the array directly to the core and let the implementation there to implement the array fetch logic, calling array_index_nospec() there, but I wonder if are there any other way that won't require too much code churn.   Thanks, Mauro""",not_technical,Mauro Carvalho Chehab,mchehab@kernel.org,1,0,969,0.5816618911174785,0.55,0,0.037037037037037035,0.9629629629629629,0.0,0.0
27,266918,267111,"please don't submit such a huge number of patches all at one time. Also, please fix the indentation of the functions whose arguments span multiple lines as has been pointed out to you in patch feedback. Finally, make this a true patch series.  It is so much easier for maintainers to work with a set of changes all doing the same thing if you make them a proper patch series with an appropriate ""[PATCH 0/N] ...""header posting. Thank you."," Luc please don't submit such a huge number of patches all at one time.  Also, please fix the indentation of the functions whose arguments span multiple lines as has been pointed out to you in patch feedback.  Finally, make this a true patch series.  It is so much easier for maintainers to work with a set of changes all doing the same thing if you make them a proper patch series with an appropriate [PATCH 0/N] ..."" header posting.  Thank you.""",not_technical,David Miller,davem@davemloft.net,1,0,438,0.4444444444444444,0.6666666666666666,0,0.0,1.0,0.0,1.0
28,289026,291846,"One of the basic questions/concerns I have is accounting for surplus huge pages in the default memory resource controller.  The existing huegtlb resource controller already takes hugetlbfs huge pages into account, including surplus pages.  This series would allow surplus pages to be accounted for in the default  memory controller, or the hugetlb controller or both. I understand that current mechanisms do not meet the needs of the aboveuse case.  The question is whether this is an appropriate way to approach the issue.  My cgroup experience and knowledge is extremely limited, but it does not appear that any other resource can be controlled by multiple controllers.  Therefore, I am concerned that this may be going against basic cgroup design philosophy.It would be good to get comments from people more cgroup knowledgeable, and especially from those involved in the decision to do separate hugetlb control","On 05/17/2018 09:27 PM, TSUKADA Koutaro wrote:  One of the basic questions/concerns I have is accounting for surplus huge pages in the default memory resource controller.  The existing huegtlb resource controller already takes hugetlbfs huge pages into account, including surplus pages.  This series would allow surplus pages to be accounted for in the default  memory controller, or the hugetlb controller or both.  I understand that current mechanisms do not meet the needs of the above use case.  The question is whether this is an appropriate way to approach the issue.  My cgroup experience and knowledge is extremely limited, but it does not appear that any other resource can be controlled by multiple controllers.  Therefore, I am concerned that this may be going against basic cgroup design philosophy.  It would be good to get comments from people more cgroup knowledgeable, and especially from those involved in the decision to do separate hugetlb control.  --  Mike Kravetz ",not_technical,Mike Kravetz,mike.kravetz@oracle.com,1,0,914,0.6086956521739131,0.5,0,0.5,0.5,0.0,0.0
29,289026,292549,"I am sorry that I didn't join the discussion for the previous version but time just didn't allow that. So sorry if I am repeating something already sorted out. There was a deliberate decision to keep hugetlb and ""normal"" memory cgroup controllers separate. Mostly because hugetlb memory is an artificial memory subsystem on its own and it doesn't fit into the rest of memcg accounted memory very well. I believe we want to keep that status quo. Well such a use case requires an explicit configuration already. Either by using special wrappers or modifying the code. So I would argue that you have quite a good knowlege of the setup. If you need a greater flexibility then just do not use hugetlb at all and rely on THP.[...]I do not really think this is a good idea. We really do not want to make the current hugetlb code more complex than it is already. The current hugetlb cgroup controller is simple and works at least somehow. I would not add more on top unless there is a really strong usecase behind. Please make sure to describe such a usecase in details before we evenstart considering the code. Well, then I would argue that you shouldn't use 64kB pages for your setup or allow THP for smaller sizes. Really hugetlb pages are by no means a substitute here.--","On Fri 18-05-18 13:27:27, TSUKADA Koutaro wrote:  I am sorry that I didn't join the discussion for the previous version but time just didn't allow that. So sorry if I am repeating something already sorted out.   There was a deliberate decision to keep hugetlb and normal"" memory cgroup controllers separate. Mostly because hugetlb memory is an artificial memory subsystem on its own and it doesn't fit into the rest of memcg accounted memory very well. I believe we want to keep that status quo.   Well such a usecase requires an explicit configuration already. Either by using special wrappers or modifying the code. So I would argue that you have quite a good knowlege of the setup. If you need a greater flexibility then just do not use hugetlb at all and rely on THP. [...]   I do not really think this is a good idea. We really do not want to make the current hugetlb code more complex than it is already. The current hugetlb cgroup controller is simple and works at least somehow. I would not add more on top unless there is a _really_ strong usecase behind. Please make sure to describe such a usecase in details before we even start considering the code.   Well, then I would argue that you shouldn't use 64kB pages for your setup or allow THP for smaller sizes. Really hugetlb pages are by no means a substitute here. --  Michal Hocko SUSE Labs""",not_technical,Michal Hocko,mhocko@kernel.org,1,0,1267,1.0,0.6071428571428571,0,0.6666666666666666,0.3333333333333333,0.0,0.0
30,289026,294078,"I apologize for having confused.The hugetlb pages obtained from the pool do not waste the buddy pool. On the other hand, surplus hugetlb pages waste the buddy pool. Due to this difference in property, I thought it could be distinguished.Although my memcg knowledge is extremely limited, memcg is accounting forvarious kinds of pages obtained from the buddy pool by the task belonging to it. I would like to argue that surplus hugepage has specificity interms of obtaining from the buddy pool, and that it is specially permitted charge requirements for memcg.It seems very strange that charge hugetlb page to memcg, but essentiallyit only charges the usage of the compound page obtained from the buddy pool,and even if that page is used as hugetlb page after that, memcg is not interested in that. I will completely apologize if my way of thinking is wrong. It would be greatly appreciated if you could mention why we can not charge surplus huge pages to memcg. I could not understand the intention of this question, sorry. When resize the pool, I think that the number of surplus huge pages in use does not change. Could you explain what you were concerned about?","On 2018/05/23 3:54, Michal Hocko wrote:  I apologize for having confused.  The hugetlb pages obtained from the pool do not waste the buddy pool. On the other hand, surplus hugetlb pages waste the buddy pool. Due to this difference in property, I thought it could be distinguished.  Although my memcg knowledge is extremely limited, memcg is accounting for various kinds of pages obtained from the buddy pool by the task belonging to it. I would like to argue that surplus hugepage has specificity in terms of obtaining from the buddy pool, and that it is specially permitted charge requirements for memcg.  It seems very strange that charge hugetlb page to memcg, but essentially it only charges the usage of the compound page obtained from the buddy pool, and even if that page is used as hugetlb page after that, memcg is not interested in that.  I will completely apologize if my way of thinking is wrong. It would be greatly appreciated if you could mention why we can not charge surplus hugepages to memcg.   I could not understand the intention of this question, sorry. When resize the pool, I think that the number of surplus hugepages in use does not change. Could you explain what you were concerned about?  --  Thanks, Tsukada",not_technical,TSUKADA Koutaro,tsukada@ascade.co.jp,0,1,1163,0.857707509881423,0.75,0,1.0,0.0,0.0,0.0
31,289026,271213,"I do not see anything like that. adjust_pool_surplus is simply and As you said, my patch did not consider handling when manipulating the pool. And even if that handling is done well, it will not be a valid reason to charge surplus huge page to memcg. I understood the concept of memcg. As you said, it must be an alien. Thanks to the interaction up to here, I understood that my solution is inappropriate. I will look for another way.Thank you for your kind explanation.","On 2018/05/24 22:24, Michal Hocko wrote [...]> I do not see anything like that. adjust_pool_surplus is simply and  As you said, my patch did not consider handling when manipulating the pool. And even if that handling is done well, it will not be a valid reason to charge surplus hugepage to memcg.  [...]  I understood the concept of memcg.  [...]  As you said, it must be an alien. Thanks to the interaction up to here, I understood that my solution is inappropriate. I will look for another way.  Thank you for your kind explanation.  --  Thanks, Tsukada",not_technical,TSUKADA Koutaro,tsukada@ascade.co.jp,0,1,470,0.3794466403162055,0.9642857142857143,0,1.0,0.0,0.0,0.0
32,289431,289688,"I'm not sure I understand what you intend here. If __sync_blockdev fails, then the error should have already been marked in (via patch #6). We wouldn't want to record that again at syncfs time.Note that __sync_blockdev will return errors based on the legacy flags.We really do need to record it in the superblock as soon as possible after an error occurs. If we want to allow userland to eventually beable to scrape this value out of the kernel (as we discussed at LSF/MM)then we can't assume that it'll be doing any sort of syncfs call before hand.The main reason to push this down into the filesystems is to allow them control over whether to report errors at syncfs time via the super blocker rseq_t or not. If we don't really care about allowing this to be an opt-in thing, then we could just take the patch that I sent on April 17th write back errors and report them to syncfs We'd also want patch #6 from this series, I think, but that's more or less enough to implement this over all filesystems, assuming they use mapping_set_error to record writeback errors. I'm fine with either approach.","On Fri, 2018-05-18 at 08:22 -0700, Matthew Wilcox wrote:  I'm not sure I understand what you intend here. If __sync_blockdev fails, then the error should have already been marked in sb->s_wb_err (via patch #6). We wouldn't want to record that again at syncfs time. Note that __sync_blockdev will return errors based on the legacy AS_EIO/AS_ENOSPC flags.  We really do need to record it in the superblock as soon as possible after an error occurs. If we want to allow userland to eventually be able to scrape this value out of the kernel (as we discussed at LSF/MM) then we can't assume that it'll be doing any sort of syncfs call beforehand.  The main reason to push this down into the filesystems is to allow them control over whether to report errors at syncfs time via the superblock errseq_t or not. If we don't really care about allowing this to be an opt-in thing, then we could just take the patch that I sent on April 17th:      [PATCH] fs: track per-sb writeback errors and report them to syncfs  We'd also want patch #6 from this series, I think, but that's more or less enough to implement this over all filesystems, assuming they use mapping_set_error to record writeback errors. I'm fine with either approach. ",not_technical,Jeff Layton,jlayton@kernel.org,1,1,1098,1.0,0.8333333333333334,0,0.0,1.0,0.0,0.0
33,306145,317171,"Please don't top post. And wrap your lines at around 75 characters Look closely at the two implementations. Look at whatmmd_phy_indirect() does. I think these are identical. So don't add your own helper, please use the core code.","On Tue, Jun 19, 2018 at 02:23:41PM +0000, Onnasch, Alexander (EXT) wrote:  Hi Alexander  Please don't top post. And wrap your lines at around 75 characters      Look closely at the two implementations. Look at what mmd_phy_indirect() does. I _think_ these are identical. So don't add your own helper, please use the core code.       Andrew",not_technical,Andrew Lunn,andrew@lunn.ch,1,0,229,1.0,0.6666666666666666,0,0.24074074074074073,0.7407407407407407,0.0,0.7407407407407407
34,307410,307390,"Oops, sorry, I double posted patch 5. Please disregard the second one.","Oops, sorry, I double posted patch 5. Please disregard the second one.  Logan  On 08/06/18 06:08 PM, Logan Gunthorpe wrote:",not_technical,Logan Gunthorpe,logang@deltatee.com,1,1,70,0.06866952789699571,0.3548387096774194,0,0.0,1.0,0.0,0.024390243902439025
35,307410,311549,"Well, clients not checking the error code made this harder to debug for sure, but removing the error code is a side effect and not what is happening here (in fact someone should probably still go back and add error checking because these functions can still return errors but that's not really something I have time to do). After the next couple patches, the clients will use this change to detect that there are no port numbers and handle things similarly to the way they did before they were broken by the multiport changes.This is the opposite of what I've ever heard before. Having a commit message that explains what led up to this commit is a good thing and allows people debugging in the future to better understand the decisions made. People debugging commits will never find the 0/X cover letter which is just intended to introduce the series to reviewers and describe changes if the series is posted multiple times. No this is not a feature request. This is fixing a regression that broke previously working code in the only sensible way I can come up with. If you have a better way to fix this, I'd be glad to hear it. But this should *not* be treated as a feature request.","  On 12/06/18 09:59 AM, Jon Mason wrote:  Well, clients not checking the error code made this harder to debug for sure, but removing the error code is a side effect and not what is happening here (in fact someone should probably still go back and add error checking because these functions can still return errors but that's not really something I have time to do). After the next couple patches, the clients will use this change to detect that there are no port numbers and handle things similarly to the way they did before they were broken by the multiport changes.   This is the opposite of what I've ever heard before. Having a commit message that explains what led up to this commit is a good thing and allows people debugging in the future to better understand the decisions made. People debugging commits will never find the 0/X cover letter which is just intended to introduce the series to reviewers and describe changes if the series is posted multiple times.   No this is not a feature request. This is fixing a regression that broke previously working code in the only sensible way I can come up with. If you have a better way to fix this, I'd be glad to hear it. But this should *not* be treated as a feature request.  Logan",not_technical,Logan Gunthorpe,logang@deltatee.com,1,1,1184,1.0,0.5483870967741935,0,0.07317073170731707,0.926829268292683,0.0,0.07317073170731707
36,314071,314224,"The commit description is not quite correct.  What the flag does is allow a discard request for those block devices which do not have the flag.I will note that the  flag is a bit controversial in linux-fsdevel.  I have a similar patch in the VFS inGoogle's internal data center kernel, as well as an internal patch which implements support for this flag in ext4.  However, the patches are out of tree, because pretty much all of the file system developers who work for enterpise distributions were against this functionality. I know of one other major cloud provider (in China) using the functionality as an out-of-tree patch, but with no one else speakingin favor of it, and everyone else NAK'ing the patch and enterprise distro's saying they would revert the patch in their distro kernels,the compromise we came to was that the code point for NO_HIDE_STALE_FL would be reserved so that users of the out-of-tree patches wouldn't collide with future fallocate flags, and I would stop trying to push the patches upstream.I have no idea how Darrick was able to get commit upstream, but I guess it was less controversial for block devices than for file systems.So I'm certainly in favor of this patch landing in mainline, but you should be aware that there may be some opposition to it.","On Fri, Jun 15, 2018 at 06:51:08PM +0800, Sig Shen wrote:  The commit description is not quite correct.  What the NO_HIDE_STALE flag does is allow a discard request for those block devices which do not have the DISCARD_ZEROES_DATA flag.  I will note that the FALLOC_FL_NO_HIDE_STALE flag is a bit controversial in linux-fsdevel.  I have a similar patch in the VFS in Google's internal data center kernel, as well as an internal patch which implements support for this flag in ext4.  However, the patches are out of tree, because pretty much all of the file system developers who work for enterpise distributions were against this functionality.  I know of one other major cloud provider (in China) using the functionality as an out-of-tree patch, but with no one else speaking in favor of it, and everyone else NAK'ing the patch and enterprise distro's saying they would revert the patch in their distro kernels, the compromise we came to was that the code point for NO_HIDE_STALE_FL would be reserved so that users of the out-of-tree patches wouldn't collide with future fallocate flags, and I would stop trying to push the patches upstream.  I have no idea how Darrick was able to get commit 25f4c41415e5 upstream, but I guess it was less controversial for block devices than for file systems.  So I'm certainly in favor of this patch landing in mainline, but you should be aware that there may be some opposition to it.  Cheers,  					- Ted",not_technical,Theodore Y. Ts'o,tytso@mit.edu,1,0,1283,1.0,1.0,1,0.0,0.0,0.0,0.0
37,331266,331339,I can't take patches without any change log text at all :(,"On Tue, Jul 03, 2018 at 05:04:10PM +1000, Andrew Jeffery wrote:  I can't take patches without any changelog text at all :(",not_technical,Greg KH,gregkh@linuxfoundation.org,1,0,58,0.09090909090909091,0.375,0,0.0,0.0,0.0,0.0
38,346223,347267,"As far as I can tell, the above is the whole reason for the patch set,yes?  To avoid confusing users.Is that sufficient?  Can we instead simplify their lives by providing better documentation or informative printks or better Kconfig text, etc?And who *are* the people who are performing this configuration?  Random system administrators?  Linux distro engineers?  If the latter then they presumably aren't easily confused! In other words, I'm trying to understand how much benefit this patch set will provide to our users as a whole.","On Wed, 18 Jul 2018 10:49:44 +0800 Baoquan He <bhe@redhat.com> wrote:   As far as I can tell, the above is the whole reason for the patchset, yes?  To avoid confusing users.  Is that sufficient?  Can we instead simplify their lives by providing better documentation or informative printks or better Kconfig text, etc?  And who *are* the people who are performing this configuration?  Random system administrators?  Linux distro engineers?  If the latter then they presumably aren't easily confused!  In other words, I'm trying to understand how much benefit this patchset will provide to our users as a whole.",not_technical,Andrew Morton,akpm@linux-foundation.org,1,0,533,1.0,0.3333333333333333,0,0.0,0.875,0.0,0.0
39,347183,348381,"Hopefully I'm not missing anything here, but this doesn't really make any sense. I'm not sure I explained myself as well as I thought I did. To behonest, I had to double check this about literally 20 times to make sure I was actually understanding this issue correctly. Turns out I was missing a couple of parts, so I'm going to try again at explaining this using a diagram that shows the various threads running concurrently phew. that took a LONG time to come up with. Anyway-that's why your explanation doesn't make sense: the deadlock is happening because we're calling pm_runtime_get_sync(). If we were to make that call conditional, all that would meanis that we wouldn't grab any runtime power reference and the GPU would immediately suspend once the atomic commit finished, as the suspend request inThread 5 would finally get unblocked and thus----suspend. Hopefully I explained that better this time, I'll definitely make sure to actually include that diagram in the patch. As for whether or not this patch is even the right solution, I will need to confirm that tommorrow (if you don't think it is still, please feel free to say so!) because it's getting late here.","On Thu, 2018-07-19 at 09:49 +0200, Lukas Wunner wrote:  Hopefully I'm not missing anything here, but this doesn't really make any sense. I'm not sure I explained myself as well as I thought I did. To be honest, I had to double check this about literally 20 times to make sure I was actually understanding this issue correctly. Turns out I was missing a couple of parts, so I'm going to try again at explaining this using a diagram that shows the various threads running concurrently  START: Driver load        |        |        |           Thread 1         ----- output_poll_execute()                        |            drm_helper_hpd_irq_event()                        |                        |  Schedules →            Thread 2                        ----------------- nouveau_display_hpd_work()                     Finish                          |                                            pm_runtime_get_sync() <--- keeps GPU alive                                                     |                                                    ...                                                     |                        ------------------------------                 <still Thread 2>              drm_helper_hpd_irq_event()                        |                        |         Schedules →                        ------------------------------                        |                         Thread 3 Drop last RPM ref -v   |                   output_poll_execute()              pm_runtime_put_sync()                  |                        |             drm_dp_mst_topology_mgr_set_mst()                     Finish                          |                                                     |                                ← Schedules          |                        -----------------------------|                        |                            |                     Thread 4                        |                        |                            |           drm_dp_mst_link_probe_work()  drm_dp_mst_wait_tx_reply() <-- these wait on eachother                        |                            |                     this is normal                        |    Sideband transactions   |                        |    happen here, this is    |                        |    where timeouts happen   |                        |                            |                        |      5 seconds later...    | autosuspend delay kicks in                        |            ...             |                        |                        |                            |                     Thread 5                        |  Communication + timeouts  |                 pm_runtime_work                        |  are still happening here  |                        |                        |                            |           nouveau_pmops_runtime_suspend()                        |                            |                        |                        |  Success! We can enable    |                        |                        |        displays now!       |            drm_kms_helper_poll_disable()                        |                            |                        |                        |                 *Atomic commit begins*              |                        |                            |     <-------------     |                        | More sideband transactions |       Waiting on       |                        |          ......            |  output_poll_execute() |                        |                            |     <-------------     |                        |                  pm_runtime_get_sync()              |                        |                            |                        |                        |   -------------------->    |     ------------->     |                        |        Waiting on          |       Waiting on       |                        |     output_poll_exec()     |    suspend requests    |                        |   -------------------->    |     ------------->     |                        |                            |                        |                        ----------------------------------------------------->|                                                                          DEADLOCK  phew. that took a LONG time to come up with.  Anyway-that's why your explanation doesn't make sense: the deadlock is happening because we're calling pm_runtime_get_sync(). If we were to make that call conditional (e.g. drm_kms_helper_is_poll_worker()), all that would mean is that we wouldn't grab any runtime power reference and the GPU would immediately suspend once the atomic commit finished, as the suspend request in Thread 5 would finally get unblocked and thus----suspend.  Hopefully I explained that better this time, I'll definitely make sure to actually include that diagram in the patch. As for whether or not this patch is even the right solution, I will need to confirm that tommorrow (if you don't think it is still, please feel free to say so!) because it's getting late here.  Cheers! 	Lyude  --  Cheers, 	Lyude Paul",not_technical,Lyude Paul,lyude@redhat.com,1,1,1175,0.5435779816513762,0.6,0,0.05555555555555555,0.9444444444444444,0.0,0.0
40,347183,349692,"First of all, I was mistaken when I wrote above that a check for! worker would solve the problem.  Sorry! It doesn't because the call to this is not happening in it but in work. Looking once more at the three stack traces you've provided, we've got:- For the moment we can ignore the first task, i.e. execute,and focus on the latter two.As said I'm unfamiliar with MST but browsing through topology.cI notice that it is the ->work element and is queued on HPD.  I further notice that the work item is flushed on And before the work item is flushed, the HPD source is quiesced.So it looks like work can only ever run while the GPU is runtime resumed, it never runs while the GPU is runtime suspended.  This means that you don't have to acquire any runtime PM references in or below work. Au contraire, you must not acquire any because it will deadlock while the GPU is runtime suspending.  If there are functions which are called from work as well as from other contexts,and those other contexts need a runtime PM ref to be acquired, you need to acquire the runtime PM ref conditionally on not being work (using the current_work() technique). Alternatively, move acquisition of the runtime PM ref further up in the call chain to those other contexts. Right, that seems to be a bug suspend:If a display is plugged in while the GPU is about to runtime suspend,the display may be lit up by execute but the GPU will then nevertheless be powered off. I guess after calling disable we should re-check if a crtc has been activated.  This should have bumped the runtime PMrefcount and have_disp_power_ref should be true.  In that case, the suspend should return -EBUSY to abort the runtime_suspend.The same check seems necessary after flushing it:If the work item lit up a new display, all previous suspend steps needto be unwound and -EBUSY needs to be returned to the PM core. Communication with an MST hub exceeding the autosuspend timeout is just one scenario where this bug manifests itself. BTW, disable seems to be called twice in the runtime_suspend code path, once in this and a second time in this.A stupid question, I notice that it calls this? Why isn't that?","On Thu, Jul 19, 2018 at 08:08:15PM -0400, Lyude Paul wrote:  First of all, I was mistaken when I wrote above that a check for !drm_kms_helper_is_poll_worker() would solve the problem.  Sorry! It doesn't because the call to pm_runtime_get_sync() is not happening in output_poll_execute() but in drm_dp_mst_link_probe_work().  Looking once more at the three stack traces you've provided, we've got: - output_poll_execute() stuck waiting for fb_helper->lock   which is held by drm_dp_mst_link_probe_work() - rpm_suspend() stuck waiting for output_poll_execute() to finish - drm_dp_mst_link_probe_work() stuck waiting in rpm_resume()  For the moment we can ignore the first task, i.e. output_poll_execute(), and focus on the latter two.  As said I'm unfamiliar with MST but browsing through drm_dp_mst_topology.c I notice that drm_dp_mst_link_probe_work() is the ->work element in drm_dp_mst_topology_mgr() and is queued on HPD.  I further notice that the work item is flushed on ->runtime_suspend:  nouveau_pmops_runtime_suspend()   nouveau_do_suspend()     nouveau_display_suspend()       nouveau_display_fini()         disp->fini() == nv50_display_fini() 	  nv50_mstm_fini() 	    drm_dp_mst_topology_mgr_suspend() 	      flush_work(&mgr->work),  And before the work item is flushed, the HPD source is quiesced.  So it looks like drm_dp_mst_link_probe_work() can only ever run while the GPU is runtime resumed, it never runs while the GPU is runtime suspended.  This means that you don't have to acquire any runtime PM references in or below drm_dp_mst_link_probe_work(). Au contraire, you must not acquire any because it will deadlock while the GPU is runtime suspending.  If there are functions which are called from drm_dp_mst_link_probe_work() as well as from other contexts, and those other contexts need a runtime PM ref to be acquired, you need to acquire the runtime PM ref conditionally on not being drm_dp_mst_link_probe_work() (using the current_work() technique).  Alternatively, move acquisition of the runtime PM ref further up in the call chain to those other contexts.    Right, that seems to be a bug nouveau_pmops_runtime_suspend():  If a display is plugged in while the GPU is about to runtime suspend, the display may be lit up by output_poll_execute() but the GPU will then nevertheless be powered off.  I guess after calling drm_kms_helper_poll_disable() we should re-check if a crtc has been activated.  This should have bumped the runtime PM refcount and have_disp_power_ref should be true.  In that case, the nouveau_pmops_runtime_suspend() should return -EBUSY to abort the runtime_suspend.  The same check seems necessary after flushing drm_dp_mst_link_probe_work(): If the work item lit up a new display, all previous suspend steps need to be unwound and -EBUSY needs to be returned to the PM core.  Communication with an MST hub exceeding the autosuspend timeout is just one scenario where this bug manifests itself.  BTW, drm_kms_helper_poll_disable() seems to be called twice in the runtime_suspend code path, once in nouveau_pmops_runtime_suspend() and a second time in nouveau_display_fini().  A stupid question, I notice that nv50_display_fini() calls nv50_mstm_fini() only if encoder_type != DRM_MODE_ENCODER_DPMST.  Why isn't that == ?  Thanks,  Lukas",not_technical,Lukas Wunner,lukas@wunner.de,1,0,2162,1.0,0.8,0,0.1111111111111111,0.8333333333333334,0.05555555555555555,0.1111111111111111
41,365796,367141,"Just a blind shot, without going into details - could you please check if led-sources property documented in the common LED bindings couldn't help here?","Hi Dan,  On 08/08/2018 11:04 PM, Dan Murphy wrote:  Just a blind shot, without going into details - could you please check if led-sources property documented in the common LED bindings couldn't help here?  --  Best regards, Jacek Anaszewski",not_technical,Jacek Anaszewski,jacek.anaszewski@gmail.com,1,0,152,0.1414141414141414,0.48,0,0.16666666666666666,0.8333333333333334,0.0,0.0
42,365796,367690,Hi! This is better than my proposal. Thanks!,"Hi!   This is better than my proposal. Thanks! 								Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",not_technical,Pavel Machek,pavel@ucw.cz,1,0,44,0.05555555555555555,0.8,0,0.16666666666666666,0.8333333333333334,0.0,0.0
43,368515,368789,"I welcome this feature, been wanting it for some time now. There is simply not enough support in maps or smaps to get this information. This is important to improve code and data layouts.I would like to see the following changes to your proposal. That would allow two things. In some measurements, you may just care about the distribution of accesses across page sizes. No need to use double the buffer space to save the address you will not use. Layout is important for code as well, in fact, that's what most peoplewant first. Having a CODE_PAGE_SIZE is therefore useful. I am happy adding it on top on your proposal. Note that it would not have to be tied to PEBS unlike this.Thanks.","On Fri, Aug 10, 2018 at 6:37 AM <kan.liang@linux.intel.com> wrote: I welcome this feature, been wanting it for some time now. There is simply not enough support in /proc/PID/maps or smaps to get this information. This is important to improve code and data layouts.  I would like to see the following changes to your proposal:    - call it PERF_SAMPLE_DATA_PAGE_SIZE  That would allow two things:    1 - not tied to PERF_SAMPLE_ADDR    2 - Allow PERF_SAMPLE_CODE_PAGE_SIZE to be added  In some measurements, you may just care about the distribution of accesses across page sizes. No need to use double the buffer space to save the address you will not use.  Layout is important for code as well, in fact, that's what most people want first. Having a CODE_PAGE_SIZE is therefore useful. I am happy adding it on top on your proposal. Note that PERF_SAMPLE_CODE_PAGE_SIZE would not have to be tied to PEBS unlike DATA_PAGE_SIZE.  Thanks. ",not_technical,Stephane Eranian,eranian@google.com,0,0,686,1.0,1.0,1,0.0,0.0,0.0,0.0
44,374095,374559,Please use your real name. 1st Signed-off-by and patch author should match. Good find! I'll queue this for the next fixes-pull-request.,"Lui Song,  Am Montag, 20. August 2018, 08:09:05 CEST schrieb Liu Song:  Please use your real name. 1st Signed-off-by and patch author should match.    Good find! I'll queue this for the next fixes-pull-request.  Thanks, //richard",not_technical,Richard Weinberger,richard@nod.at,1,0,135,1.0,1.0,1,0.0,0.0,0.0,0.0
45,377230,377461,"Interesting - I don't see the grant head reservation code in any of my performance benchmark profiling, even when running at over a million transactions on a 2-socket 32-core 64-thread skylakesystem. I see other places in the transaction subsystem that are hot (e.g the CIL context lock), but not the space reservations. My initial suspect is that you have a tiny log on your test file system, so it's permanently out of space and so always hitting the slow path. Can you tell us what the storage is and it's configuration? At minimum, I need to see the output of the xfs_infocommand on your test filesystem. Fixing this may simply be using alarger log on your benchmark systems. FWIW, can you post the actual profile you are seeing in the commit message? That helps us identify similar problems in the future, andit lets us know what paths are leading to the transaction reservation contention. i.e. this may not even be a problem with the transaction reservation code itself. How does this impact on the strict FIFO queue behaviour the grantqueues currently have? The current code only wakes up enough waiters to consume the newly available space and it queues new waiters to the tail of the queue. If there ever is a spurious wakeup then the waiter that was woken from the head remains there until the next wakeup comes in. This is intentional - spurious wakeups are rare enough we can ignore them because a) this is the slow path, and b) correctness is far more important that performance in this path. The fast path is already lockless, and we've already given up peformance if we reach this slow path. hence we only care about correctness in this path, not performance optimisation.AFAICT the patch changes the spurious wakeup behaviour - it requeues tasks to the tail of the queue if there wasn't space available when they are woken, rather than leaving them as them at the head.  They now have to wait for all the other reservations to make progress.This breaks the guarantees of ordered forward progress the grant queue provides permanent transaction reservations and hence opens us up to log space deadlocks because those transactions can't move their objects forward in the log to free up space in the log...Also, I note that wake_q_add() assumes that the wake queue is a local stack object and so not subject to concurrency - it explicitly states this in the code. That's not the case here - the wake queue is part of the grant head, and so is subject to extreme concurrency that is tempered by a spin lock.  Does the wake_q code work correctly (e.g. have all the necessary memory barriers, etc) whenit's not a local stack object and instead protected from concurrency by a spin lock? At minimum, the wake_q infrastructure comments and documentation need updating to accommodate this new use case that wake queues are being used for.This doesn't generally doesn't happen because the space accounting tends to prevent multiple wakeups. i.e. we only wake the tasks we have reservation space for, and log space being made available tends to arrive in discrete chunks (because IO is slow!) such that that pending wakeups have already been processed before the next chunk of available space comes in....Yes, but they are very rare and we don't really care about this in the slow path. If you see lots of them, it's typically a sign of an inappropriately configured filesystem for the workload being run. On a correctly configured system, we should almost never use this slowpath....I'm betting that you'll get that and a whole lot more simply byincreasing the log size and not running the slow path at all. Where's the hunk context in your headers? You must be using anon-standard git option here. Linux kernel specific includes go in this not individual files. Why do you need to delete the ticket from the queue here? This leads to landmines and incorrect non-FIFO behaviour....... here. This is a potential list corruption landmine because this function now has unbalanced list add and removal contexts. IOWs, we can't restart this loop without first having guaranteed the ticket is not already on the ticket queue. You need to document constraints like this in comments and explain what code needs to guarantee those constraints are met. [Because, as I noted at the end, you got thiswrong for xlog_grant_head_wake_all()] To maintian FIFO behaviour, the ticket needs to be left at the head of the grant head wait queue until it has space available to make progress, not get removed and requeued to the tail. Spurious wakeups are irrelevant here - forwards progress (i.e. correctness) requires FIFO ticket ordering behaviour be maintained.This push is needed to make the necessary space we are waiting on available in the log. Hence leaving it out of the loop you put below will cause the journal to get stuck in the spurious wakeuploop below and be unable to make progress. This will lead to filesystem hangs.That's a new nested loop. Please implement it as a loop. This is buggy  - i will lead to hangs if the filesystem is shutdown and there is a spurious wakeup that triggers this to go back to sleep.The shutdown check needs to break the sleep loop. That's racy. You can't drop the spin lock betweenxlog_grant_head_wake() and xlog_grant_head_wait(), because free_bytes is only valid while while the spinlock is held.  Same for the ""wake_all"" variable you added. i..e. while waking up the waiters, we could have run out of space again and had more tasks queued, or had the AIL tail move and now have space available. Either way, we can do the wrong thing because we dropped the lockand free_bytes and wake_all are now stale and potentially incorrect.That's another landmine. Just define the wakeq in the context where it is used rather than use a function wide variable that requires reinitialisation. Ok, what about wake all? You didn't convert that to use wake queues, and so that won't remove tickets for the granthead waiter list, and so those tasks will never get out of the new inner loop you added to xlog_grant_head_wait(). That means filesystem shutdowns will just hang the filesystem and leave it unmountable. Did you run this through fstests?","On Thu, Aug 23, 2018 at 12:26:10PM -0400, Waiman Long wrote:  Interesting - I don't see the grant head reservation code in any of my performance benchmark profiling, even when running at over a million transactions/s on a 2-socket 32-core 64-thread skylake system. I see other places in the transaction subsystem that are hot (e.g the CIL context lock), but not the space reservations.  My initial suspect is that you have a tiny log on your test filesystem, so it's permanently out of space and so always hitting the slow path. Can you tell us what the storage is and it's configuration? At minimum, I need to see the output of the xfs_info command on your test filesystem. Fixing this may simply be using a larger log on your benchmark systems.  FWIW, can you post the actual profile you are seeing in the commit message? That helps us identify similar problems in the future, and it lets us know what paths are leading to the transaction reservation contention. i.e. this may not even be a problem with the transaction reservation code itself.   How does this impact on the strict FIFO queue behaviour the grant queues currently have? The current code only wakes up enough waiters to consume the newly available space and it queues new waiters to the tail of the queue. If there ever is a spurious wakeup then the waiter that was woken from the head remains there until the next wakeup comes in. This is intentional - spurious wakeups are rare enough we can ignore them because a) this is the slow path, and b) correctness is far more important that performance in this path. The fast path is already lockless, and we've already given up peformance if we reach this slow path. hence we only care about correctness in this path, not performance optimisation.  AFAICT the patch changes the spurious wakeup behaviour - it requeues tasks to the tail of the queue if there wasn't space available when they are woken, rather than leaving them as them at the head.  They now have to wait for all the other reservations to make progress. This breaks the guarantees of ordered forward progress the grant queue provides permanent transaction reservations and hence opens us up to log space deadlocks because those transactions can't move their objects forward in the log to free up space in the log...  Also, I note that wake_q_add() assumes that the wake queue is a local stack object and so not subject to concurrency - it explicitly states this in the code. That's not the case here - the wake queue is part of the grant head, and so is subject to extreme concurrency that is tempered by a spin lock.  Does the wake_q code work correctly (e.g. have all the necessary memory barriers, etc) when it's not a local stack object and instead protected from concurrency by a spin lock? At minimum, the wake_q infrastructure comments and documentation need updating to accommodate this new use case that wake queues are being used for.   This doesn't generally doesn't happen because the space accounting tends to prevent multiple wakeups. i.e. we only wake the tasks we have reservation space for, and log space being made available tends to arrive in discrete chunks (because IO is slow!) such that that pending wakeups have already been processed before the next chunk of available space comes in....   Yes, but they are very rare and we don't really care about this in the slow path. If you see lots of them, it's typically a sign of an inappropriately configured filesystem for the workload being run. On a correctly configured system, we should almost never use this slow path....   I'm betting that you'll get that and a whole lot more simply by increasing the log size and not running the slow path at all.   Where's the hunk context in your headers? You must be using a non-standard git option here.   Linux kernel specific includes go in fs/xfs/xfs_linux.h, not individual files.   Why do you need to delete the ticket from the queue here? This leads to landmines and incorrect non-FIFO behaviour...   .... here. This is a potential list corruption landmine because this function now has unbalanced list add and removal contexts. IOWs, we can't restart this loop without first having guaranteed the ticket is not already on the ticket queue. You need to document constraints like this in comments and explain what code needs to guarantee those constraints are met. [Because, as I noted at the end, you got this wrong for xlog_grant_head_wake_all()]  To maintian FIFO behaviour, the ticket needs to be left at the head of the grant head wait queue until it has space available to make progress, not get removed and requeued to the tail. Spurious wake ups are irrelevant here - forwards progress (i.e. correctness) requires FIFO ticket ordering behaviour be maintained.   This push is needed to make the necessary space we are waiting on available in the log. Hence leaving it out of the loop you put below will cause the journal to get stuck in the spurious wakeup loop below and be unable to make progress. This will lead to filesystem hangs.   That's a new nested loop. Please implement it as a loop.   This is buggy  - i will lead to hangs if the filesystem is shut down and there is a spurious wakeup that triggers this to go back to sleep. The shutdown check needs to break the sleep loop.   That's racy. You can't drop the spin lock between xlog_grant_head_wake() and xlog_grant_head_wait(), because free_bytes is only valid while while the spinlock is held.  Same for the wake_all"" variable you added. i..e. while waking up the waiters, we could have run out of space again and had more tasks queued, or had the AIL tail move and now have space available. Either way, we can do the wrong thing because we dropped the lock and free_bytes and wake_all are now stale and potentially incorrect.   That's another landmine. Just define the wakeq in the context where it is used rather than use a function wide variable that requires reinitialisation.   Ok, what about xlog_grant_head_wake_all()? You didn't convert that to use wake queues, and so that won't remove tickets for the grant head waiter list, and so those tasks will never get out of the new inner loop you added to xlog_grant_head_wait(). That means filesystem shutdowns will just hang the filesystem and leave it unmountable. Did you run this through fstests?  Cheers,  Dave --  Dave Chinner david@fromorbit.com""",not_technical,Dave Chinner,david@fromorbit.com,0,0,6166,1.0,0.6666666666666666,0,0.0,0.6666666666666666,0.0,0.0
46,377230,378511,"Thanks for your detailed review of the patch. I now have a betterunderstanding of what should and shouldn't be done. I have sent out amore conservative v2 patchset which, hopefully, can address the concerns that you raised","On 08/24/2018 05:54 PM, Waiman Long wrote:  Thanks for your detailed review of the patch. I now have a better understanding of what should and shouldn't be done. I have sent out a more conservative v2 patchset which, hopefully, can address the concerns that you raised.  Cheers, Longman",not_technical,Waiman Long,longman@redhat.com,1,1,222,0.03550295857988166,1.0,1,1.0,0.0,0.3333333333333333,0.0
47,378505,379417,"Can you please re-run and report the results for each patch on the ram disk setup? And, please, include the mkfs.xfs or xfs_info output for the ramdisk filesystem so I can see /exactly/ how much concurrency the filesystems are providing to the benchmark you are running. 50GB is tiny for XFS. Personally, I've been using ~1PB filesystems(*) for the performance testing I've been doing recently...","On Mon, Aug 27, 2018 at 11:34:13AM -0400, Waiman Long wrote:  Can you please re-run and report the results for each patch on the ramdisk setup? And, please, include the mkfs.xfs or xfs_info output for the ramdisk filesystem so I can see /exactly/ how much concurrency the filesystems are providing to the benchmark you are running.   50GB is tiny for XFS. Personally, I've been using ~1PB filesystems(*) for the performance testing I've been doing recently...  Cheers,  Dave.  (*) Yes, petabytes. Sparse image files on really fast SSDs are a wonderful thing. --  Dave Chinner david@fromorbit.com",not_technical,Dave Chinner,david@fromorbit.com,0,0,396,0.8850574712643678,1.0,1,1.0,0.0,0.0,0.0
48,379138,379657,"I like the idea and I think it's good direction to go, but could you please share some from perf stat or whatever you used to meassure the new performance?","On Mon, Aug 27, 2018 at 08:03:21PM +0300, Alexey Budankov wrote:  I like the idea and I think it's good direction to go, but could you please share some from perf stat or whatever you used to meassure the new performance?  thanks, jirka",not_technical,Jiri Olsa,jolsa@redhat.com,1,0,155,1.0,0.4,0,0.0,0.0,0.0,0.0
49,390394,390741,"Sorry but I don't like imposing a run-time check on everybody when stack-based requests are the odd ones out.  If we're going to make this a run-time check (I'd much prefer a compile-time check, but I understand that this may involve too much churn), then please do it for stack-based request users only.","On Thu, Sep 06, 2018 at 03:58:52PM -0700, Kees Cook wrote:  Sorry but I don't like imposing a run-time check on everybody when stack-based requests are the odd ones out.  If we're going to make this a run-time check (I'd much prefer a compile-time check, but I understand that this may involve too much churn), then please do it for stack-based request users only.  Thanks, --  Email: Herbert Xu <herbert@gondor.apana.org.au> Home Page: http://gondor.apana.org.au/~herbert/ PGP Key: http://gondor.apana.org.au/~herbert/pubkey.txt",not_technical,Herbert Xu,herbert@gondor.apana.org.au,1,0,304,1.0,0.5,0,0.0,1.0,0.0,0.0
50,391895,394633,"I am not subscribed to LKML, please keep me CC'd on replies. I tried a simple test with several VMs (in my initial test, I have 48 idle 1-cpu 512-mb VMs and 2 idle 2-cpu, 2-gb VMs) using libvirt, none pinned to any CPUs. When I tried to set all of the top-level libvirt cpuc groups' to be co-scheduled the machine hangs. This is using cosched_max_level=1.There are several moving parts there, so I tried narrowing it down, by only coscheduling one VM, and thing seemed fine One thing that is not entirely obvious to me (but might be completely intentional) is that since by default the top-level libvirt cpu cgroups are empty. cat tasks the result of this should be a no-op, right? [This becomes relevant below] Specifically, all of the threads of qemu are in sub-cgroups, which do not indicate they are co-scheduling: When I then try to coschedule the second VM, the machine hangs. On the console, I see the same backtraces I see when I try to set all of the VMs to be coscheduled: am happy to do any further debugging I can do, or try patches on top of those posted on the mailing list.","[ I am not subscribed to LKML, please keep me CC'd on replies ]  On 07.09.2018 [23:39:47 +0200], Jan H. Schnherr wrote:  I tried a simple test with several VMs (in my initial test, I have 48 idle 1-cpu 512-mb VMs and 2 idle 2-cpu, 2-gb VMs) using libvirt, none pinned to any CPUs. When I tried to set all of the top-level libvirt cpu cgroups' to be co-scheduled (/bin/echo 1 > /sys/fs/cgroup/cpu/machine/<VM-x>.libvirt-qemu/cpu.scheduled), the machine hangs. This is using cosched_max_level=1.  There are several moving parts there, so I tried narrowing it down, by only coscheduling one VM, and thing seemed fine:  /sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# echo 1 > cpu.scheduled  /sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# cat cpu.scheduled  1  One thing that is not entirely obvious to me (but might be completely intentional) is that since by default the top-level libvirt cpu cgroups are empty:  /sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# cat tasks   the result of this should be a no-op, right? [This becomes relevant below] Specifically, all of the threads of qemu are in sub-cgroups, which do not indicate they are co-scheduling:  /sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# cat emulator/cpu.scheduled  0 /sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# cat vcpu0/cpu.scheduled  0  When I then try to coschedule the second VM, the machine hangs.  /sys/fs/cgroup/cpu/machine/<VM-2>.libvirt-qemu# echo 1 > cpu.scheduled  Timeout, server <HOST> not responding.  On the console, I see the same backtraces I see when I try to set all of the VMs to be coscheduled:  [  144.494091] watchdog: BUG: soft lockup - CPU#87 stuck for 22s! [CPU 0/KVM:25344] [  144.507629] Modules linked in: act_police cls_basic ebtable_filter ebtables ip6table_filter iptable_filter nbd ip6table_raw ip6_tables xt_CT iptable_raw ip_tables s [  144.578858]  xxhash raid10 raid0 multipath linear raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor ses raid6_pq enclosure libcrc32c raid1 scsi [  144.599227] CPU: 87 PID: 25344 Comm: CPU 0/KVM Tainted: G           O      4.19.0-rc2-amazon-cosched+ #1 [  144.608819] Hardware name: Dell Inc. PowerEdge R640/0W23H8, BIOS 1.4.9 06/29/2018 [  144.616403] RIP: 0010:smp_call_function_single+0xa7/0xd0 [  144.621818] Code: 01 48 89 d1 48 89 f2 4c 89 c6 e8 64 fe ff ff c9 c3 48 89 d1 48 89 f2 48 89 e6 e8 54 fe ff ff 8b 54 24 18 83 e2 01 74 0b f3 90 <8b> 54 24 18 83 e25 [  144.640703] RSP: 0018:ffffb2a4a75abb40 EFLAGS: 00000202 ORIG_RAX: ffffffffffffff13 [  144.648390] RAX: 0000000000000000 RBX: 0000000000000057 RCX: 0000000000000000 [  144.655607] RDX: 0000000000000001 RSI: 00000000000000fb RDI: 0000000000000202 [  144.662826] RBP: ffffb2a4a75abb60 R08: 0000000000000000 R09: 0000000000000f39 [  144.670073] R10: 0000000000000000 R11: 0000000000000000 R12: ffff8a9c03fc8000 [  144.677301] R13: ffff8ab4589dc100 R14: 0000000000000057 R15: 0000000000000000 [  144.684519] FS:  00007f51cd41a700(0000) GS:ffff8ab45fac0000(0000) knlGS:0000000000000000 [  144.692710] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [  144.698542] CR2: 000000c4203c0000 CR3: 000000178a97e005 CR4: 00000000007626e0 [  144.705771] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [  144.712989] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 [  144.720215] PKRU: 55555554 [  144.723016] Call Trace: [  144.725553]  ? vmx_sched_in+0xc0/0xc0 [kvm_intel] [  144.730341]  vmx_vcpu_load+0x244/0x310 [kvm_intel] [  144.735220]  ? __switch_to_asm+0x40/0x70 [  144.739231]  ? __switch_to_asm+0x34/0x70 [  144.743235]  ? __switch_to_asm+0x40/0x70 [  144.747240]  ? __switch_to_asm+0x34/0x70 [  144.751243]  ? __switch_to_asm+0x40/0x70 [  144.755246]  ? __switch_to_asm+0x34/0x70 [  144.759250]  ? __switch_to_asm+0x40/0x70 [  144.763272]  ? __switch_to_asm+0x34/0x70 [  144.767284]  ? __switch_to_asm+0x40/0x70 [  144.771296]  ? __switch_to_asm+0x34/0x70 [  144.775299]  ? __switch_to_asm+0x40/0x70 [  144.779313]  ? __switch_to_asm+0x34/0x70 [  144.783317]  ? __switch_to_asm+0x40/0x70 [  144.787338]  kvm_arch_vcpu_load+0x40/0x270 [kvm] [  144.792056]  finish_task_switch+0xe2/0x260 [  144.796238]  __schedule+0x316/0x890 [  144.799810]  schedule+0x32/0x80 [  144.803039]  kvm_vcpu_block+0x7a/0x2e0 [kvm] [  144.807399]  kvm_arch_vcpu_ioctl_run+0x1a7/0x1990 [kvm] [  144.812705]  ? futex_wake+0x84/0x150 [  144.816368]  kvm_vcpu_ioctl+0x3ab/0x5d0 [kvm] [  144.820810]  ? wake_up_q+0x70/0x70 [  144.824311]  do_vfs_ioctl+0x92/0x600 [  144.827985]  ? syscall_trace_enter+0x1ac/0x290 [  144.832517]  ksys_ioctl+0x60/0x90 [  144.835913]  ? exit_to_usermode_loop+0xa6/0xc2 [  144.840436]  __x64_sys_ioctl+0x16/0x20 [  144.844267]  do_syscall_64+0x55/0x110 [  144.848012]  entry_SYSCALL_64_after_hwframe+0x44/0xa9 [  144.853160] RIP: 0033:0x7f51cf82bea7 [  144.856816] Code: 44 00 00 48 8b 05 e1 cf 2c 00 64 c7 00 26 00 00 00 48 c7 c0 ff ff ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 b8 10 00 00 00 0f 05 <48> 3d 01 f0 ff ff8 [  144.875752] RSP: 002b:00007f51cd419a18 EFLAGS: 00000246 ORIG_RAX: 0000000000000010  I am happy to do any further debugging I can do, or try patches on top of those posted on the mailing list.  Thanks, Nish",not_technical,Nishanth Aravamudan,naravamudan@digitalocean.com,0,0,1088,0.1336432306798373,0.5526315789473685,0,0.04597701149425287,0.9540229885057471,0.011494252873563218,0.0
51,431048,432750,"There seems to be a disconnect between what I am trying to communicate and what I perceive you to have understood. I'll add comments below to try to make more clear what I'm trying to say.But first a general statement.  I understand that the intent of the patch wording is to allow use of email addresses in the tags of a patch submittal or git commit without being an unacceptable behavior.  I do not think that the words in the patch accomplish that goal.The patch says ""Publishing ... electronic address not ordinarily collected by the project, without explicit permission"".  (I think it is fair to abstract here with ""..."".)  This phrase specifies which email addresses can be published.  It does not specify in what casesthe email address can be published.  The desired goal is to be able to publish email addresses in patch and commit tags. Which email addresses are allowed to be published?  (This is the point of my original comment.)  To me, the patch wording is describing how I can determine whether I can put a specific email address in a tag in a patch that I submit or commit.  I can put an email address in a tag _if_ it is ""ordinarily collected by the project"".This then leads my mental process down the path of the disclosures (from all of the companies that I do business with) that tell me what they are going to do with my personal information, such as my address.  (They usually plan to share it with the world for their financial benefit.) In that context, my personal information is not _public_, but it is_ordinarily collected_ by the company.  I hope this provides some insight into what I am reading into ""ordinarily collected by the project"". My original comment was trying to provide the concept behind a way tocreate an alternate wording in the patch to define ""which email addresses"". Where are email addresses allowed to be published?  I do not understand the patch wording to address this at all. Trying to understand how you are understanding my comment vs what I intended to communicate, it seems to me that you are focused on the ""where allowed"" and I am focused on the ""which email addresses"". More clear?  Or am I still not communicating well enough? Permission vs exclusion is orthogonal to my comments. ""building linux"" is not the patch wording.  ""ordinarily collected by the project"" is a much broader universe. A very simplistic definition of public _could_ be:- Visible on a project mail list that any one can subscribe to- Visible on a project mail list whose archive is available via the public internet- Visible on an interactive communication (""chat"") platform thatis open to the public internet- Published on a web page intended for public access (for example this could cover opt-in conference attendee lists and emails that conference presenters voluntarily place in their slides).- (I am guessing the above covers 97% or more of possible publicsources, but maybe there are some more common sources.) I'm sure that the professionals that deal with information privacy could provide better wording for the above list.  I am but an amateur in that field. Anything else collected by the project would not be considered public. For example, an email address provided in an email sent to me and not copied to any mail list would not be public.","On 10/16/18 19:41, James Bottomley wrote:  There seems to be a disconnect between what I am trying to communicate and what I perceive you to have understood.  I'll add comments below to try to make more clear what I'm trying to say.  But first a general statement.  I understand that the intent of the patch wording is to allow use of email addresses in the tags of a patch submittal or git commit without being an unacceptable behavior.  I do not think that the words in the patch accomplish that goal.    The patch says Publishing ... electronic address not ordinarily collected by the project, without explicit permission"".  (I think it is fair to abstract here with ""..."".)  This phrase specifies which email addresses can be published.  It does not specify in what cases the email address can be published.  The desired goal is to be able to publish email addresses in patch and commit tags.  Which email addresses are allowed to be published?  (This is the point of my original comment.)  To me, the patch wording is describing how I can determine whether I can put a specific email address in a tag in a patch that I submit or commit.  I can put an email address in a tag _if_ it is ""ordinarily collected by the project"".  This then leads my mental process down the path of the disclosures (from all of the companies that I do business with) that tell me what they are going to do with my personal information, such as my address.  (They usually plan to share it with the world for their financial benefit.) In that context, my personal information is not _public_, but it is _ordinarily collected_ by the company.  I hope this provides some insight into what I am reading into ""ordinarily collected by the project"".  My original comment was trying to provide the concept behind a way to create an alternate wording in the patch to define ""which email addresses"".  Where are email addresses allowed to be published?  I do not understand the patch wording to address this at all.  Trying to understand how you are understanding my comment vs what I intended to communicate, it seems to me that you are focused on the ""where allowed"" and I am focused on the ""which email addresses"".  More clear?  Or am I still not communicating well enough?    Permission vs exclusion is orthogonal to my comments.  ""building linux"" is not the patch wording.  ""ordinarily collected by the project"" is a much broader universe.  A very simplistic definition of public _could_ be:    - Visible on a project mail list that any one can subscribe to   - Visible on a project mail list whose archive is available via     the public internet   - Visible on an interactive communication (""chat"") platform that     is open to the public internet   - Published on a web page intended for public access (for example     this could cover opt-in conference attendee lists and emails     that conference presenters voluntarily place in their slides).   - (I am guessing the above covers 97% or more of possible public     sources, but maybe there are some more common sources.)  I'm sure that the professionals that deal with information privacy could provide better wording for the above list.  I am but an amateur in that field.  Anything else collected by the project would not be considered public. For example, an email address provided in an email sent to me and not copied to any mail list would not be public.  -Frank """,not_technical,Frank Rowand,frowand.list@gmail.com,1,0,3287,1.0,0.42105263157894735,0,0.25,0.5,0.0,0.0
52,498594,574206,"I would really like to get an ack from the people who have been deep into this first.  If you can get that, and preferably resubmit with a less condescending changelog, I can pick it up.","On Mon, 11 Mar 2019 11:21:10 +0100 Pavel Machek <pavel@ucw.cz> wrote:   I would really like to get an ack from the people who have been deep into this first.  If you can get that, and preferably resubmit with a less condescending changelog, I can pick it up.  Thanks,  jon",not_technical,Jonathan Corbet,corbet@lwn.net,1,0,186,0.10335917312661498,0.6363636363636364,0,0.32270916334661354,0.6772908366533864,0.0,0.0
53,521470,533953,"is this patchset still an RFC or you really want to see this merged ASAP? If I am not mistaken there is still some work in progress trying to push all the SCP stuff? Lee, personally I have some concerns. Looks like the cros_* family is increasing quickly lately and I am wondering if we are really doing well all this.To be honest, I'd like to take a deeper look before merge this, btw Ithought there was no hurry because of the RFC and I guess there are still some scp things that are missing. I might be wrong, and if that's not the case I can take a look deeper and the end of the week.","Hi Lee, Pi-Hsun,  Missatge de Lee Jones <lee.jones@linaro.org> del dia dc., 30 de gen. 2019 a les 14:07:  Pi-Hsun, is this patchset still an RFC or you really want to see this merged ASAP? If I am not mistaken there is still some work in progress trying to push all the SCP stuff?  Lee, personally I have some concerns. Looks like the cros_* family is increasing quickly lately (cros_ec, cros_pd, cros_scp, cros_ish, cros_fp ...) and I am wondering if we are really doing well all this. To be honest, I'd like to take a deeper look before merge this, btw I thought there was no hurry because of the RFC and I guess there are still some scp things that are missing. I might be wrong, and if that's not the case I can take a look deeper and the end of the week.  Best regards,  Enric  ",not_technical,Enric Balletbo Serra,eballetbo@gmail.com,1,0,589,1.0,0.75,0,1.0,0.0,0.0,0.0
54,539613,546296,"You are missing a cover letter from this patch set. Please have it inv2. Also use tag ""selftests/tpm2"" instead of having two tags in the short summaries. Now they look a bit weird.","You are missing a cover letter from this patch set. Please have it in v2. Also use tag selftests/tpm2"" instead of having two tags in the short summaries. Now they look a bit weird.  On Tue, Feb 05, 2019 at 12:28:39PM -0800, Tadeusz Struk wrote:  8<   Remove.   8<   Remove.   8<   Remove.   8<   Remove.   8<  Remove.   /Jarkko""",not_technical,Jarkko Sakkinen,jarkko.sakkinen@linux.intel.com,1,0,180,0.5909090909090909,0.6,0,0.7142857142857143,0.14285714285714285,0.7142857142857143,0.0
55,571417,571702,This is phenomenal. Thank you so much for digging into this. I'm hoping this will greatly reduce the risk of future leakage.,"On March 7, 2019 3:45:11 AM PST, Peter Zijlstra <peterz@infradead.org> wrote:  This is phenomenal. Thank you so much for digging into this. I'm hoping this will greatly reduce the risk of future leakage. --  Sent from my Android device with K-9 Mail. Please excuse my brevity.",not_technical,Unkown Name,hpa@zytor.com,0,0,124,1.0,0.36,0,0.0,0.9767441860465116,0.0,0.0
56,72369,470502,"I mean the level of a resource in IOMEM tree (the one that's printed from this). 1-st level means its parent is root and so on. If it's not a problem anymore IIUC, can we revert the change as it still breaks. for the reasons I described above? Nothing prevents - true, but that's plainly wrong from OS point of view to grab physical ranges for something without knowing what's actually behind on that platform. I think we shouldn't consider this as a valid thing to do and don't try to work around initially incorrect code.","On 26/11/2018 16:25, Boris Ostrovsky wrote:  I mean the level of a resource in IOMEM tree (the one that's printed from /proc/iomem). 1-st level means its parent is root and so on.   If it's not a problem anymore IIUC, can we revert the change as it still breaks hotplug_unpopulated=1"" for the reasons I described above?   Nothing prevents - true, but that's plainly wrong from OS point of view to grab physical ranges for something without knowing what's actually behind on that platform. I think we shouldn't consider this as a valid thing to do and don't try to workaround initially incorrect code. """,not_technical,Igor Druzhinin,igor.druzhinin@citrix.com,0,0,523,1.0,0.6363636363636364,0,0.9970760233918129,0.0,0.0,0.0
57,152625,156564,"How do you plan to handle the external references? For example, the following LWN articles has a link this file:","On Wed, Jan 03, 2018 at 03:04:36PM +0530, afzal mohammed wrote:  How do you plan to handle the external references? For example, the following LWN articles has a link this file:  	https://lwn.net/Articles/718628/  And changing the name and/or location will break that link, AFAIK.  Regards, Boqun  [...] ",not_technical,Boqun Feng,boqun.feng@gmail.com,1,0,112,0.10267857142857142,0.375,0,0.0,0.5,0.0,0.0
58,152625,156871,"IMO symlinks are mostly ending in a mess, URLs are never stable.There is an object to handle such requirements. Take a look at *intersphinx* to see how it works:  Each Sphinx HTML build creates a file named objects.inv thatcontains a mapping from object names to URIs relative to the HTML set€™s root. This means articles from external (like lwn articles) has to be recompiled. Not perfect, but a first solution. I really like them, factually valuable comments .. please express your concern so that we have a chance to move on. I think that's a pity.-","  IMO symlinks are mostly ending in a mess, URLs are never stable. There is a    https://www.kernel.org/doc/html/latest/objects.inv  to handle such requirements. Take a look at *intersphinx* :   http://www.sphinx-doc.org/en/stable/ext/intersphinx.html  to see how it works:  Each Sphinx HTML build creates a file named objects.inv that contains a mapping from object names to URIs relative to the HTML set’s root.  This means articles from external (like lwn articles) has to be recompiled. Not perfect, but a first solution.    I really like them, factually valuable comments .. please express your concern so that we have a chance to move on.   I think that's a pity.  -- Markus --",not_technical,Markus Heiser,markus.heiser@darmarit.de,0,0,552,0.49107142857142855,0.75,0,0.5,0.0,0.0,0.0
59,152625,156500,"So I hate this rst crap with a passion, so NAK from me.","On Wed, Jan 03, 2018 at 03:04:36PM +0530, afzal mohammed wrote:  So I hate this rst crap with a passion, so NAK from me.",not_technical,Peter Zijlstra,peterz@infradead.org,1,0,55,0.06696428571428571,0.25,0,0.0,0.5,0.0,0.0
60,159308,177720,"A version of this patch has been queued by Catalin. Now that the cpu feature bits are queued, I think this can be split up into two separate series for v4.16-rc1, one to tackle NOTIFY_SEI and the associated plumbing. The second for the KVM 'make SError pending' API. I didn't sign-off this patch. If you pick some bits from another version and want to credit someone else you can 'CC:' them or just mention it in the commit-message. Irrelevant-Nit: sys-regs usually have a 'SYS_' prefix, and are in instruction encoding order lower down the file.(These PSTATE PAN things are a bit odd as they were used to generate and instruction before the fancy helpers were added). Bits of this are spread between patches 5 and 6. If you put them in the other order this wouldn't happen.(but after a rebase most of this patch should disappear) So this writes an impdef ESR, because its the existing code-path in KVM. And then you overwrite it. Which is a bit odd as there is a helper to do both in one go: How come you don't use this in this?","Hi Dongjiu Geng,  On 06/01/18 16:02, Dongjiu Geng wrote:  A version of this patch has been queued by Catalin.  Now that the cpufeature bits are queued, I think this can be split up into two separate series for v4.16-rc1, one to tackle NOTIFY_SEI and the associated plumbing. The second for the KVM 'make SError pending' API.    I didn't sign-off this patch. If you pick some bits from another version and want to credit someone else you can 'CC:' them or just mention it in the commit-message.    Irrelevant-Nit: sys-regs usually have a 'SYS_' prefix, and are in instruction encoding order lower down the file.  (These PSTATE PAN things are a bit odd as they were used to generate and instruction before the fancy {read,write}_sysreg() helpers were added).    Bits of this are spread between patches 5 and 6. If you put them in the other order this wouldn't happen.  (but after a rebase most of this patch should disappear)   So this writes an impdef ESR, because its the existing code-path in KVM.    And then you overwrite it. Which is a bit odd as there is a helper to do both in one go:      How come you don't use this in kvm_arm_set_sei_esr()?    Thanks,  James",not_technical,James Morse,james.morse@arm.com,1,0,1029,1.0,0.4642857142857143,0,0.17708333333333334,0.8229166666666666,0.0,0.010416666666666666
61,161354,161783,"For do you report a speed of 2500Mbps through eth tool, or are you reporting 1000Mbps?  I don't see any code in this patch that deals with that.--RMK's Patch system broadband for 0.8mile line in suburbia: sync at 8.8Mbps down 630kbps up According to speedtest.net.","On Tue, Jan 09, 2018 at 09:59:45AM +0100, Antoine Tenart wrote:  For 2500Base-X, do you report a speed of 2500Mbps through ethtool, or are you reporting 1000Mbps?  I don't see any code in this patch that deals with that.  --  RMK's Patch system: http://www.armlinux.org.uk/developer/patches/ FTTC broadband for 0.8mile line in suburbia: sync at 8.8Mbps down 630kbps up According to speedtest.net: 8.21Mbps down 510kbps up",not_technical,Russell King - ARM Linux,linux@armlinux.org.uk,1,0,264,1.0,0.6363636363636364,0,0.0,0.0,0.0,0.0
62,165657,169349,"Can you please use a consistent name space? retpoline_ ... or such?I really don't like fiddling with that variable. That's just hackery. The variable reflects the actual enabled mitigation state of the kernel proper. That'll break once we get other mitigation variants.These newlines are there to separate stuff for readability sake. This really can be done in a cleaner way. That only needs one function and that one can take care of setting avariable in the spectre code which then influences the sysfs output. And that output should not be ""Vulnerable"" like you force with the hackabove. It actually should tell WHY it is vulnerable despite having had protection in place before the module was loaded.","On Fri, 12 Jan 2018, Andi Kleen wrote:  Can you please use a consistent name space? retpoline_ ... or such?   I really don't like fiddling with that variable. That's just hackery. The variable reflects the actual enabled mitigation state of the kernel proper.   That'll break once we get other mitigation variants.   These newlines are there to separate stuff for readability sake.   This really can be done in a cleaner way.  in linux/module.h  #ifdef RETPOLINE extern bool retpoline_module_ok(bool has_retpoline), #else static inline bool retpoline_module_ok(bool has_retpoline) { 	return true, } #endif  static void check_modinfo_retpoline(mod, info) { 	if (retpoline_module_ok(get_modinfo(info, retpoline""))) 		return, 		 	pr_warn(""%s: loading module not compiled with retpoline compiler.\n"", 		mod->name), }  That only needs one function and that one can take care of setting a variable in the spectre code which then influences the sysfs output.  And that output should not be ""Vulnerable"" like you force with the hack above. It actually should tell WHY it is vulnerable despite having had protection in place before the module was loaded.  Thanks,  	tglx""",not_technical,Thomas Gleixner,tglx@linutronix.de,1,0,704,1.0,0.4,0,1.0,0.0,1.0,0.0
63,166193,168161,I didn't get any response to a comment I've written about the pointabove during the previous patch iteration. The old code set this bit in any mode other than AC'97,"On 15.01.2018 05:21, Nicolin Chen wrote:  I didn't get any response to a comment I've written about the point above during the previous patch iteration:> The old code set this bit in any mode other than AC'97 (where the  Maciej ",not_technical,Maciej S. Szmigiero,mail@maciej.szmigiero.name,0,0,164,1.0,0.8695652173913043,0,0.0,0.0,0.0,0.0
64,168060,169219,This does not make sense vs. the documentation: This should say:And I really have to ask whether this should be named _GLOBAL_ instead of_SHARED_. Hmm?,"On Mon, 15 Jan 2018, Mathieu Desnoyers wrote:  This does not make sense vs. the documentation:   This should say:   And I really have to ask whether this should be named _GLOBAL_ instead of _SHARED_.  Hmm?  Thanks,  	tglx",not_technical,Thomas Gleixner,tglx@linutronix.de,1,0,151,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
65,168668,168727,"Again, 'boutside' protection ...Other than that.","On 01/16/2018 11:34 AM, Johannes Thumshirn wrote: Again, 'boutside' protection ...  Other than that:  Reviewed-by: Hannes Reinecke <hare@suse.com>  Cheers,  Hannes --  Dr. Hannes Reinecke		   Teamlead Storage & Networking hare@suse.de			               +49 911 74053 688 SUSE LINUX GmbH, Maxfeldstr. 5, 90409 Nürnberg GF: F. Imendörffer, J. Smithard, J. Guild, D. Upmanyu, G. Norton HRB 21284 (AG Nürnberg)",not_technical,Hannes Reinecke,hare@suse.de,1,0,48,1.0,0.7142857142857143,0,0.0,0.0,0.0,0.0
66,169133,177084,"The timer is supposed to restart the protocol again, that's how this whole thing is designed to work.I think you are making changes to the symptom rather than the true cause of the problems you are seeing. Sorry, I will not apply this until the exact issue is better understood.Thank you.","From: Denis Du <dudenis2000@yahoo.ca> Date: Tue, 16 Jan 2018 16:58:25 +0000 (UTC)   The timer is supposed to restart the protocol again, that's how this whole thing is designed to work.  I think you are making changes to the symptom rather than the true cause of the problems you are seeing.  Sorry, I will not apply this until the exact issue is better understood.  Thank you.",not_technical,David Miller,davem@davemloft.net,1,0,288,0.18543046357615894,0.2,0,0.16216216216216217,0.8108108108108109,0.16216216216216217,0.0
67,169133,204633,"I cannot apply a patch which has been corrupted by your email client like this. Please send it properly again, plain ASCII text, and no trasnformations by your email client. You should send the patch to yourself and try to apply the patch you receive, do not send to the list until you can pass the test properly. Do not use attachments to fix this problem, the patch must be inline after your commit message and signoffs. Please read Documentation for more information. Thank you.","From: Denis Du <dudenis2000@yahoo.ca> Date: Wed, 21 Feb 2018 03:35:31 +0000 (UTC)   I cannot apply a patch which has been corrupted by your email client like this.  Please send it properly again, plain ASCII text, and no trasnformations by your email client.  You should send the patch to yourself and try to apply the patch you receive, do not send to the list until you can pass the test properly.  Do not use attachments to fix this problem, the patch must be inline after your commit message and signoffs.  Please read Documentation/process/submitting-patches.rst and Documentation/process/email-clients.rsDt for more information.  Thank you.",not_technical,David Miller,davem@davemloft.net,1,0,481,0.31788079470198677,1.0,1,1.0,0.0,0.02702702702702703,0.0
68,169133,177642,"Ok, I check the source code again. It have nothing to do with the interrupts, it is related how the hdlc.c is implemented. It will do nothing, not start any new protocol and thus the timer.My case is the carrier always good, but protocol will fail due to perfect noise, and this issue was found and complained by our customers. So it is not my theory guessing, it is a real problem.","  Ok, I check the source code again. It have nothing to do with the interrupts, it is related how the hdlc.c is implemented. In drivers/net/wan/hdlc.c#L108           if (hdlc->carrier == on)         goto carrier_exit, /* no change in DCD line level */      hdlc->carrier = on,      if (!hdlc->open)         goto carrier_exit,      if (hdlc->carrier) {         netdev_info(dev, Carrier detected\n""),         hdlc_proto_start(dev),     } else {         netdev_info(dev, ""Carrier lost\n""),         hdlc_proto_stop(dev),     }  carrier_exit:     spin_unlock_irqrestore(&hdlc->state_lock, flags),     return NOTIFY_DONE,   If carrier keep no change by if (hdlc->carrier == on)         goto carrier_exit, /* no change in DCD line level */It will do nothing, not start any new protocol and thus the timer.   My case is the carrier always good, but protocol will fail due to perfect noise, and this issue was found and complained by our customers. So it is not my theory guessing, it is a real problem.       On Monday, January 22, 2018, 3:25:16 PM EST, David Miller <davem@davemloft.net> wrote:       From: Denis Du <dudenis2000@yahoo.ca> Date: Tue, 16 Jan 2018 16:58:25 +0000 (UTC)   The timer is supposed to restart the protocol again, that's how this whole thing is designed to work.  I think you are making changes to the symptom rather than the true cause of the problems you are seeing.  Sorry, I will not apply this until the exact issue is better understood.  Thank you.""",not_technical,Denis Du,dudenis2000@yahoo.ca,0,1,382,0.271523178807947,0.4,0,0.1891891891891892,0.8108108108108109,0.0,0.10810810810810811
69,170193,170197,"Please don't put plain-text files into core-api - that's a directory full of RST documents.  Your document is 99.9% RST already, better to just finish the job and tie it into the rest of the kernel docs. We might as well put the SPDX tag here, it's a new file. This is all good information, but I'd suggest it belongs more in the 0/npatch posting than here.  The introduction of *this* document should say what it actually covers.This seems like a relevant and important aspect of the API that shouldn't be buried in the middle of a section talking about random things.So one gets this far, but has no actual idea of how to do these things. Which leads me to wonder: what is this document for?  Who are you expecting to read it?You could improve things a lot by (once again) going to RST and using directives to bring in the kerneldoc comments from the source (which, I note, do exist).  But I'd suggest rethinking this document and itsaudience.  Most of the people reading it are likely wanting to learn how to *use* this API, I think it would be best to not leave them frustrated.","On Tue, 30 Jan 2018 17:14:45 +0200 Igor Stoppa <igor.stoppa@huawei.com> wrote:   Please don't put plain-text files into core-api - that's a directory full of RST documents.  Your document is 99.9% RST already, better to just finish the job and tie it into the rest of the kernel docs.   We might as well put the SPDX tag here, it's a new file.   This is all good information, but I'd suggest it belongs more in the 0/n patch posting than here.  The introduction of *this* document should say what it actually covers.   This seems like a relevant and important aspect of the API that shouldn't be buried in the middle of a section talking about random things.   So one gets this far, but has no actual idea of how to do these things. Which leads me to wonder: what is this document for?  Who are you expecting to read it?  You could improve things a lot by (once again) going to RST and using directives to bring in the kerneldoc comments from the source (which, I note, do exist).  But I'd suggest rethinking this document and its audience.  Most of the people reading it are likely wanting to learn how to *use* this API, I think it would be best to not leave them frustrated.  Thanks,  jon",not_technical,Jonathan Corbet,corbet@lwn.net,1,0,1082,1.0,0.32,0,0.0,1.0,0.0,0.07692307692307693
70,170193,188751,"Relatively significant? I do not object to your comment, but in practice i see that:- vmalloc is used relatively little- allocations do not seem to be huge- there seem to be way larger overheads in the handling of virtual pages  (see my proposal for the LFS/m summit, about collapsing struct   vm_struct and struct vmap_area) Can you please point me to this function/macro? I don't seem to be able to find it, at least not in 4.15 During hardened user copy permission check, I need to confirm if the memory range that would be exposed to userspace is a legitimate sub-range of a pmalloc allocation. So, I start with the pair (address, size) and I must end up to something I can compare it against. The idea here is to pass through struct_page and then the relatedvm_struct/vmap_area, which already has the information about the specific chunk of virtual memory. I cannot comment on your proposal because I do not know where to find the reference you made, or maybe I do not understand what you mean","  On 01/02/18 02:00, Christopher Lameter wrote:  Relatively significant? I do not object to your comment, but in practice i see that:  - vmalloc is used relatively little - allocations do not seem to be huge - there seem to be way larger overheads in the handling of virtual pages   (see my proposal for the LFS/m summit, about collapsing struct    vm_struct and struct vmap_area)    Can you please point me to this function/macro? I don't seem to be able to find it, at least not in 4.15  During hardened user copy permission check, I need to confirm if the memory range that would be exposed to userspace is a legitimate sub-range of a pmalloc allocation.   So, I start with the pair (address, size) and I must end up to something I can compare it against. The idea here is to pass through struct_page and then the related vm_struct/vmap_area, which already has the information about the specific chunk of virtual memory.  I cannot comment on your proposal because I do not know where to find the reference you made, or maybe I do not understand what you mean :-(  -- igor",not_technical,Igor Stoppa,igor.stoppa@huawei.com,0,1,998,0.8484848484848485,0.4,0,0.07692307692307693,0.8461538461538461,0.0,0.0
71,173287,173288,You can't do it this simply as it will cause deadlock due to nested locking of the buf_lock. To share the lock you will need to provide unlocked versions of the read and write functions and use those if the lock has already been taken.,"On Sat, 20 Jan 2018 21:14:48 +0530 Shreeya Patel <shreeya.patel23498@gmail.com> wrote:   You can't do it this simply as it will cause deadlock due to nested locking of the buf_lock.  To share the lock you will need to provide unlocked versions of the read and write functions and use those if the lock has already been taken.  Jonathan   _______________________________________________ devel mailing list devel@linuxdriverproject.org http://driverdev.linuxdriverproject.org/mailman/listinfo/driverdev-devel",not_technical,Jonathan Cameron,jic23@kernel.org,1,0,235,1.0,1.0,1,0.0,0.0,0.0,0.0
72,174463,174508,Ok. I've looked at your patch for way too long now and still don't see how you've shown it to be correct. Shouldn't there be a at least a comment to explain why zero is an appropriate initialization value in that case?,"On Thu, Jan 18, 2018 at 2:49 PM, Jiri Pirko <jiri@resnulli.us> wrote:  Ok. I've looked at your patch for way too long now and still don't see how you've shown it to be correct. Shouldn't there be a at least a comment to explain why zero is an appropriate initialization value in that case?        Arnd",not_technical,Arnd Bergmann,arnd@arndb.de,1,1,218,1.0,0.75,0,0.0,0.0,0.0,0.0
73,174735,174850,Are you moving checks from the core subsystem to drivers ? This looks really nonsensical and the commit message doesn't explain the rationale for that at all.,"On 01/18/2018 07:34 PM, Kamil Konieczny wrote:  Are you moving checks from the core subsystem to drivers ? This looks really nonsensical and the commit message doesn't explain the rationale for that at all.    --  Best regards, Marek Vasut",not_technical,Marek Vasut,marex@denx.de,1,0,158,0.3258426966292135,0.3888888888888889,0,0.0,1.0,0.0,0.0
74,174735,199553,"This makes no sense, cfr my comment on 5/5. Seems like if the driver doesn't implement those, the core can easily detect that and perform the necessary action. Moving the checks out of core seems like the wrong thing to do, rather you should enhance the checks in core if they're insufficient in my opinion.","On 02/15/2018 04:41 PM, Herbert Xu wrote:  This makes no sense, cfr my comment on 5/5  Seems like if the driver doesn't implement those, the core can easily detect that and perform the necessary action. Moving the checks out of core seems like the wrong thing to do, rather you should enhance the checks in core if they're insufficient in my opinion.  --  Best regards, Marek Vasut",not_technical,Marek Vasut,marex@denx.de,1,0,307,0.7078651685393258,0.6666666666666666,0,0.9642857142857143,0.0,0.0,0.0
75,174735,199600,"The core can very well check if these functions are not populated and return ENOSYS So you remove all NULL pointer checks ? Esp. in security-sensitive code? What is the impact of this non-critical path code on performance? Come on ...You can very well impose that in the core, except you don't duplicate the code.","On 02/15/2018 06:00 PM, Kamil Konieczny wrote:  The core can very well check if these functions are not populated and return ENOSYS   So you remove all NULL pointer checks ? Esp. in security-sensitive code? What is the impact of this non-critical path code on performance?  Come on ...   You can very well impose that in the core, except you don't duplicate the code.  --  Best regards, Marek Vasut",not_technical,Marek Vasut,marex@denx.de,1,0,313,0.6966292134831461,0.7777777777777778,0,0.9642857142857143,0.0,0.0,0.0
76,174735,199673,"Why you want checks for something that not exist ? Those without them will not work and will do Oops in crypto testmgr,so such drivers should not be used nor accepted in drivers/crypto Ask yourself why crypto do not check for NULL in ahash digest or other required ahash functions. Now size of crypto core is reduced.-","  On 15.02.2018 18:06, Marek Vasut wrote:  Why you want checks for something that not exist ?  Those without them will not work and will do Oops in crypto testmgr, so such drivers should not be used nor accepted in drivers/crypto  Ask yourself why crypto do not check for NULL in ahash digest or other required ahash functions.   Now size of crypto core is reduced.  --  Best regards, Kamil Konieczny Samsung R&D Institute Poland",not_technical,Kamil Konieczny,k.konieczny@partner.samsung.com,0,1,318,0.6741573033707865,0.8333333333333334,0,0.9642857142857143,0.0,0.0,0.0
77,174735,199701,"Are you suggesting that the kernel code should NOT perform NULL pointerchecks ? Are you suggesting each driver should implement every single callback available and if it is not implemented, return -ENOSYS ? This looks like a MASSIVE code duplication. You implemented the same code thrice, it surely is not reduced.","On 02/15/2018 07:06 PM, Kamil Konieczny wrote:  Are you suggesting that the kernel code should NOT perform NULL pointer checks ?  Are you suggesting each driver should implement every single callback available and if it is not implemented, return -ENOSYS ? This looks like a MASSIVE code duplication.   You implemented the same code thrice, it surely is not reduced.  --  Best regards, Marek Vasut",not_technical,Marek Vasut,marex@denx.de,1,0,314,0.6179775280898876,0.8888888888888888,0,0.9642857142857143,0.0,0.0,0.0
78,174735,200015,"You can compile kernel with generic config and at that point you have all the duplicated code stored on your machine. But this discussion is moving away from the point I was concerned about -- that this patchset_increases_ code duplication and I find this wrong. It does NOT reduce the binary size, just try compiling all the drivers in and it will make the kernel bigger.","On 02/16/2018 10:16 AM, Kamil Konieczny wrote:  You can compile kernel with generic config and at that point you have all the duplicated code stored on your machine. But this discussion is moving away from the point I was concerned about -- that this patchset _increases_ code duplication and I find this wrong.   It does NOT reduce the binary size, just try compiling all the drivers in and it will make the kernel bigger.  --  Best regards, Marek Vasut",not_technical,Marek Vasut,marex@denx.de,1,0,372,0.7865168539325843,1.0,1,1.0,0.0,0.0,0.0
79,174735,199976,"It is source code duplication. One do not load all crypto drivers at once, simple because one board has only one crypto HW (or few closely related), and if one even try, almost none of them will initialize on given hardware. E.g. on Exynos board only exynos drivers will load, on board with omap crypto only omap crypto will load. As I said above, it reduces binary size at cost of more source code in few drivers."," On 15.02.2018 19:32, Marek Vasut wrote:  It is source code duplication. One do not load all crypto drivers at once, simple because one board has only one crypto HW (or few closely related), and if one even try, almost none of them will initialize on given hardware. E.g. on Exynos board only exynos drivers will load, on board with omap crypto only omap crypto will load.    As I said above, it reduces binary size at cost of more source code in few drivers.  --  Best regards, Kamil Konieczny Samsung R&D Institute Poland",not_technical,Kamil Konieczny,k.konieczny@partner.samsung.com,0,1,414,1.0,0.9444444444444444,0,1.0,0.0,0.0,0.0
80,202437,202446,Use normal patch styles. Fix your tools before you send any more patches.,"On Sun, 2017-09-24 at 12:22 +0200, SF Markus Elfring wrote: [] []  Use normal patch styles. Fix your tools before you send any more patches.",not_technical,Joe Perches,joe@perches.com,1,0,73,0.06521739130434782,0.36363636363636365,0,0.0,1.0,0.0,0.2536231884057971
81,202437,505099,"While we do not mind cleanup patches, the way you post them (one fix per file) is really annoying and takes us too much time to review. I'll take the ""Fix a possible null pointer"" patch since it is an actual bug fix, but will reject the others, not just this driver but all of them that are currently pendingin our patchwork. Feel free to repost, but only if you organize the patch as either fixing the same type of issue for a whole subdirectory (media/usb, media/pci, etc) or fixing all issues for a single driver. Actual bug fixes (like the null pointer patch in this series) can still be posted as separate patches, but cleanups shouldn't. So in this particular case I would expect two omap_vout patches: one for the bug fix, one for the cleanups. Just so you know, I'll reject any future patch series that do not follow these rules. Just use common sense when posting these things in the future. I would also suggest that your time might be spent more productively if you would work on some more useful projects. There is more than enough to do. However, that's up to you.","Hi Markus,  On 09/24/2017 12:20 PM, SF Markus Elfring wrote:  While we do not mind cleanup patches, the way you post them (one fix per file) is really annoying and takes us too much time to review.  I'll take the Fix a possible null pointer"" patch since it is an actual bug fix, but will reject the others, not just this driver but all of them that are currently pending in our patchwork (https://patchwork.linuxtv.org).  Feel free to repost, but only if you organize the patch as either fixing the same type of issue for a whole subdirectory (media/usb, media/pci, etc) or fixing all issues for a single driver.  Actual bug fixes (like the null pointer patch in this series) can still be posted as separate patches, but cleanups shouldn't.  So in this particular case I would expect two omap_vout patches: one for the bug fix, one for the cleanups.  Just so you know, I'll reject any future patch series that do not follow these rules. Just use common sense when posting these things in the future.  I would also suggest that your time might be spent more productively if you would work on some more useful projects. There is more than enough to do. However, that's up to you.  Regards,  	Hans""",not_technical,Hans Verkuil,hverkuil@xs4all.nl,1,0,1077,1.0,0.4090909090909091,0,0.2536231884057971,0.7463768115942029,0.2536231884057971,0.0
82,202437,745390,Would you like to answer my still remaining questions in any more constructive ways?," Would you like to answer my still remaining questions in any more constructive ways?  Regards, Markus",not_technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,84,0.06521739130434782,0.7727272727272727,0,0.45652173913043476,0.5434782608695652,0.18840579710144928,0.30434782608695654
83,202437,505195,"I did that: either one patch per directory with the same type of change, or one patch per driver combining all the changes for that driver. Yes, and you were told not to do it like that again.","On 10/30/2017 11:40 AM, SF Markus Elfring wrote:  ??? I did that: either one patch per directory with the same type of change, or one patch per driver combining all the changes for that driver.   Yes, and you were told not to do it like that again.  Regards,  	Hans",not_technical,Hans Verkuil,hverkuil@xs4all.nl,1,0,192,0.18695652173913044,0.6363636363636364,0,0.2608695652173913,0.7391304347826086,0.0,0.0
84,202437,160227,Are you going to answer any of my remaining questions in a more constructive way?," Are you going to answer any of my remaining questions in a more constructive way?  Regards, Markus",not_technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,81,0.06956521739130435,0.8181818181818182,0,0.7681159420289855,0.2318840579710145,0.30434782608695654,0.17391304347826086
85,202437,191235,Do any contributors get into the mood to take another look at software updates from my selection of change possibilities in a more constructive way? Do you need any additional development resources?," Do any contributors get into the mood to take another look at software updates from my selection of change possibilities in a more constructive way?  Do you need any additional development resources?  Regards, Markus",not_technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,198,0.14782608695652175,0.8636363636363636,0,0.9420289855072463,0.050724637681159424,0.17391304347826086,0.0
86,202437,191241,"One last time: either post per-driver patches with all the cleanups for a driver in a single patch, or a per-directory patch doing the same cleanup for all drivers in that directory. I prefer the first approach, but it's up to you. We don't have the time to wade through dozens of one-liner cleanup patches. I don't understand what is so difficult about this.","On 02/02/18 10:55, SF Markus Elfring wrote:  One last time: either post per-driver patches with all the cleanups for a driver in a single patch, or a per-directory patch (drivers/media/pci, usb, etc) doing the same cleanup for all drivers in that directory.  I prefer the first approach, but it's up to you.  We don't have the time to wade through dozens of one-liner cleanup patches.  I don't understand what is so difficult about this.  Regards,  	Hans",not_technical,Hans Verkuil,hverkuil@xs4all.nl,1,0,359,0.3217391304347826,0.9090909090909091,0,0.9492753623188406,0.050724637681159424,0.0,0.0
87,202437,191275,"I preferred to offer source code adjustments according to specific transformation patterns mostly for each software module separately (also in small patch series).I am curious if bigger patch packages would be easier to get accepted. Or would you get frightened still by any other change combination? We have got different preferences for a safe patch granularity. I imagine that there are more development factors involved. It is usual that integration of update suggestions will take some time. How would the situation change if I would dare to regroup possible update steps? There are communication difficulties to consider since your terse information from your conference meeting.If you would insist on patch squashing, would you dare to use a development tool like also on your own once more?"," I preferred to offer source code adjustments according to specific transformation patterns mostly for each software module separately (also in small patch series).    I am curious if bigger patch packages would be easier to get accepted.  Or would you get frightened still by any other change combination?     We have got different preferences for a safe patch granularity.    I imagine that there are more development factors involved.    It is usual that integration of update suggestions will take some time. How would the situation change if I would dare to regroup possible update steps?    There are communication difficulties to consider since your terse information from your conference meeting.  If you would insist on patch squashing, would you dare to use a development tool like “quilt fold” also on your own once more?  Regards, Markus",not_technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,798,0.5956521739130435,0.9545454545454546,0,0.9492753623188406,0.050724637681159424,0.0,0.050724637681159424
88,202437,196318,I find such a change combination unsafe. Would you dare to apply any (of my) scripts for the semantic patch language directly on the whole directory for multi-media software? Can you handle bigger patches really better than similar patch series? Are there any further possibilities to consider around consequences from a general change resistance? Will any development (or management) tools like make the regrouping of possible update steps more convenient and safer?," I find such a change combination unsafe.    Would you dare to apply any (of my) scripts for the semantic patch language directly on the whole directory for multi-media software?    Can you handle bigger patches really better than similar patch series?    Are there any further possibilities to consider around consequences from a general change resistance?  Will any development (or management) tools like “quilt fold” make the regrouping of possible update steps more convenient and safer?   Regards, Markus",not_technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,467,0.3521739130434783,1.0,1,1.0,0.0,0.050724637681159424,0.0
89,202437,505193,Interesting Would you like to share any more information from this meeting? I would appreciate further indications for a corresponding change acceptance. I found a feedback by Mauro Carvalho Chehab more constructive.," Interesting …    Would you like to share any more information from this meeting?    I would appreciate further indications for a corresponding change acceptance.  I found a feedback by Mauro Carvalho Chehab more constructive.  [GIT,PULL,FOR,v4.15] Cleanup fixes https://patchwork.linuxtv.org/patch/43957/  “… This time, I was nice and I took some time doing:  	$ quilt fold < `quilt next` && quilt delete `quilt next` …”   Regards, Markus",not_technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,216,0.15217391304347827,0.5909090909090909,0,0.2608695652173913,0.7391304347826086,0.0,0.0
90,202437,513970,I find it very surprising that you rejected 146 useful update suggestions so easily. What does this software area make it so special in comparison to other Linux subsystems?* Have you taken any other solution approaches into account than  a quick rejection?* Could your reaction have been different if the remarkable number of  change possibilities were sent by different authors (and not only me)?* How should possibly remaining disagreements about affected implementation  details be resolved now?* Are you looking for further improvements around development tools  like patchwork and quilt?* Will you accept increasing risks because of bigger patch sizes?* Can such an information lead to differences in the preferred patch granularity?* How do you think about this detail? How would you ever like to clean up stuff in affected source files which was accumulated (or preserved somehow) over years? I guess that this handling will trigger more communication challenges.Our common sense seems to be occasionally different in significant ways. I distribute my software development capacity over several areas. Does your wording indicate a questionable signal for further contributions?," I find it very surprising that you rejected 146 useful update suggestions so easily.    What does this software area make it so special in comparison to other Linux subsystems?    * Have you taken any other solution approaches into account than   a quick “rejection”?  * Could your reaction have been different if the remarkable number of   change possibilities were sent by different authors (and not only me)?  * How should possibly remaining disagreements about affected implementation   details be resolved now?  * Are you looking for further improvements around development tools   like “patchwork” and “quilt”?  * Will you accept increasing risks because of bigger patch sizes?    * Can such an information lead to differences in the preferred patch granularity?  * How do you think about this detail?    How would you ever like to clean up stuff in affected source files which was accumulated (or preserved somehow) over years?    I guess that this handling will trigger more communication challenges.    Our “common sense” seems to be occasionally different in significant ways.    I distribute my software development capacity over several areas. Does your wording indicate a questionable signal for further contributions?  Regards, Markus",not_technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,1185,0.8782608695652174,0.7272727272727273,0,0.2608695652173913,0.7391304347826086,0.0,0.18840579710144928
91,210458,210459,"Wait, what?  Why would it do that, because it thinks dereferencing NULL is undefined behaviour and it can just do whatever it wants to? That feels crazy, as for these calls we ""know"" it will never be NULL because the previous call to debug fs_file_get() will always ensure it will be correct. So this is a case of the compiler trying to be smarter than it reallyis, and getting things totally wrong :(Has anyone reported this to the clang developers?Papering over compiler foolishness is not something I like to do in kernel code if at all possible...","On Tue, Mar 27, 2018 at 04:55:53PM -0700, Matthias Kaehlcke wrote:  Wait, what?  Why would it do that, because it thinks dereferencing NULL is undefined behaviour and it can just do whatever it wants to?  That feels crazy, as for these calls we know"" it will never be NULL because the previous call to debugfs_file_get() will always ensure it will be correct.  So this is a case of the compiler trying to be smarter than it really is, and getting things totally wrong :(  Has anyone reported this to the clang developers?  Papering over compiler foolishness is not something I like to do in kernel code if at all possible...  thanks,  greg k-h""",not_technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,0,551,1.0,0.2,0,0.0,0.0,0.0,0.0
92,210458,210461,"A: Because it messes up the order in which people normally read text. Q: Why is top-posting such a bad thing? A: Top-posting. Q: What is the most annoying thing in e-mail?A: No. Q: Should I include quotations after my reply? Then fix the tool, the C code is correct :) Then tell clang not to do that, like we tell gcc not to do that as that is a foolish thing for a compiler to do when building the kernel."," A: Because it messes up the order in which people normally read text. Q: Why is top-posting such a bad thing? A: Top-posting. Q: What is the most annoying thing in e-mail?  A: No. Q: Should I include quotations after my reply?  http://daringfireball.net/2007/07/on_top  On Wed, Mar 28, 2018 at 07:47:53AM -0700, Manoj Gupta wrote:  Then fix the tool, the C code is correct :)   Then tell clang not to do that, like we tell gcc not to do that as that is a foolish thing for a compiler to do when building the kernel.  thanks,  greg k-h",not_technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,0,406,0.8672566371681416,0.6,0,0.0,0.0,0.0,0.0
93,210458,210463,"Wait, clang does not have that?  That's crazy, how has this not been hit yet when building the kernel?","On Wed, Mar 28, 2018 at 11:14:56AM -0700, Matthias Kaehlcke wrote:  Wait, clang does not have that?  That's crazy, how has this not been hit yet when building the kernel?  confused,  greg k-h",not_technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,0,102,0.21238938053097345,0.8,0,0.0,0.0,0.0,0.0
94,221804,221853,Choose one of those two. Better to keep in order. Ditto. What's wrong with dev_info() ? Hmm... Can't you use devm_ioremap_resources() to get the virtual address for I/O ? When you use explicit casting in printf() you are doing in 99.9% cases something wrong. Noise.,"On Thu, Mar 1, 2018 at 1:02 PM, Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com> wrote:       Choose one of those two.   Better to keep in order.   Ditto.    return !!(ch & GAYLE_IRQ_IDE),  ?     What's wrong with dev_info() ?      Hmm... Can't you use devm_ioremap_resources() to get the virtual address for I/O ?   When you use explicit casting in printf() you are doing in 99.9% cases something wrong.   Noise.    --  With Best Regards, Andy Shevchenko",not_technical,Andy Shevchenko,andy.shevchenko@gmail.com,1,0,265,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
95,222860,223039,"How many times are we going to allow copy-and-pasting the same driver? Last time we wanted to modify the Rockchip driver, we were told ""consolidate"", because ST had already forked our driver. This nearly halted all progress. I'm going to be real disappointed if we see another fork get merged.(IOW, I would say ""over my dead body,"" but I have no power here.) And why can't you use DRM?","Hi,  On Fri, Mar 02, 2018 at 04:44:06PM +0100, yannick fertre wrote:  How many times are we going to allow copy-and-pasting the same driver? Last time we wanted to modify the Rockchip driver, we were told consolidate"", because ST had already forked our driver. This nearly halted all progress. I'm going to be real disappointed if we see another fork get merged.  (IOW, I would say ""over my dead body,"" but I have no power here.)  And why can't you use DRM?  Brian""",not_technical,Brian Norris,briannorris@chromium.org,1,0,385,1.0,0.6,0,0.0,1.0,0.0,0.0
96,222860,223021,"...wait a second...this looks like it's a u-boot driver. There's a surprising amount of similarity between U-boot and Linux drivers (no coincidence I'm sure), including headers.Since when do U-Boot patches go to LKML and dri-devel? Anyway, I'll try my best to ignore this series.","On Fri, Mar 02, 2018 at 10:57:59AM -0800, Brian Norris wrote:  ...wait a second...this looks like it's a u-boot driver. There's a surprising amount of similarity between U-boot and Linux drivers (no coincidence I'm sure), including <linux/...> headers.  Since when do U-Boot patches go to LKML and dri-devel?  Anyway, I'll try my best to ignore this series.  Brian",not_technical,Brian Norris,briannorris@chromium.org,1,0,279,0.6590909090909091,0.65,0,0.0,1.0,0.0,0.3
97,232313,234089,"meta comment (i.e., not about the merits of the patch itself): You'll need to send the patch to someone if you want it to be merged. Maintainers don't mine mailing lists for patches to apply.","Hi,  On 03/15/2018 12:20 AM, ning.a.zhang@intel.com wrote:  meta comment (i.e., not about the merits of the patch itself):  You'll need to send the patch to someone if you want it to be merged. Maintainers don't mine mailing lists for patches to apply.    --  ~Randy",not_technical,Randy Dunlap,rdunlap@infradead.org,0,0,191,1.0,0.6666666666666666,0,0.6666666666666666,0.3333333333333333,0.6666666666666666,0.3333333333333333
98,239101,240255,"They've been dropped.  BUT please do note that the patches I pushed tolinux-dm.git were rebased on top of the 'check_at_most_once' patch. I never did get an answer about how the sg array is free'd in certain error paths (see ""FIXME:"" in the 2nd patch). Also, I fixed some issues I saw in error paths, and lots of formatting. I'll be pretty frustrated if you submit v2 that is blind to the kinds of changes I made. I'll send you a private copy of the patches just so you have them for your reference.","On Tue, Mar 27 2018 at  4:55am -0400, yael.chemla@foss.arm.com <yael.chemla@foss.arm.com> wrote:   They've been dropped.  BUT please do note that the patches I pushed to linux-dm.git were rebased ontop of the 'check_at_most_once' patch.  I never did get an answer about how the sg array is free'd in certain error paths (see FIXME:"" in the 2nd patch).  Also, I fixed some issues I saw in error paths, and lots of formatting.  I'll be pretty frustrated if you submit v2 that is blind to the kinds of changes I made.  I'll send you a private copy of the patches just so you have them for your reference.  Thanks, Mike  """,not_technical,Mike Snitzer,snitzer@redhat.com,1,0,499,1.0,0.8571428571428571,0,0.03333333333333333,0.9666666666666667,0.0,0.0
99,245912,245922,"Is this include needed ? Please use bool.I am quite completely missing how the two functions above are different. There is a lot of duplication in those functions. Would it be possible to find common code and use functions for it instead of duplicatingeverything several times ? What if nothing is found ? FWIW, it might be better to pass channel as parameter. What if it didn't find a core ? This attribute should not exist. How does this make sense ? Am I missing something, or is the same temperature reported several times ? tjmax is also reported as this, for example. There is again a lot of duplication in those functions. Can this be made less magic with some defines ? Does this mean there will be an error message for each non-supported CPU? Why? this is not an error, and should not result in an error message. Besides, the error can also be propagated from peci core code, and may well be something else. Then what ? Shouldn't this result in probe deferral or something more useful instead of just being ignored? FWIW, this should be two separate patches. Needed ? It might make sense to provide the duplicate functions in a core file. This again looks like duplicate code. Please handle error cases first. More duplicate code.One set of ( ) is unnecessary on each side of the expression. Why is this ""invalid"", and why does it warrant an error message ? ?Or the peci command failed. ?","On Tue, Apr 10, 2018 at 11:32:11AM -0700, Jae Hyun Yoo wrote:  Is this include needed ?   Please use bool.   I am quite completely missing how the two functions above are different.   There is a lot of duplication in those functions. Would it be possible to find common code and use functions for it instead of duplicating everything several times ?   What if nothing is found ?   FWIW, it might be better to pass channel - DEFAULT_CHANNEL_NUMS as parameter.  What if find_core_index() returns priv->gen_info->core_max, ie if it didn't find a core ?   This attribute should not exist.   lcrit is tcontrol - tjmax, and crit_hyst above is tjmax - tcontrol ? How does this make sense ?   Am I missing something, or is the same temperature reported several times ? tjmax is also reported as temp_crit cputemp_read_die(), for example.   There is again a lot of duplication in those functions.   Can this be made less magic with some defines ?   Does this mean there will be an error message for each non-supported CPU ? Why ?   -ENODEV is not an error, and should not result in an error message. Besides, the error can also be propagated from peci core code, and may well be something else.   Then what ? Shouldn't this result in probe deferral or something more useful instead of just being ignored ?   FWIW, this should be two separate patches.   Needed ?   It might make sense to provide the duplicate functions in a core file.   This again looks like duplicate code.   Please handle error cases first.   More duplicate code.   One set of ( ) is unnecessary on each side of the expression.   Why is this invalid"", and why does it warrant an error message ?   Is priv->addr guaranteed to be >= PECI_BASE_ADDR ?  Or the peci command failed.   cancel_delayed_work_sync() ? """,not_technical,Guenter Roeck,linux@roeck-us.net,1,0,1397,0.5381679389312977,0.2222222222222222,0,0.0,1.0,0.0,0.0
100,245912,245916,"As per the in-kernel documentation, I am now allowed to make fun of you. You are trying to ""out smart"" the kernel by getting rid of a warning message that was explicitly put there for you to do something.  To think that by just providing an ""empty"" function you are somehow fulfilling the API requirement is quite bold, don't you think? This has to be fixed.  I didn't put that warning in there for no good reason.  Please go read the documentation again...","On Tue, Apr 10, 2018 at 11:32:05AM -0700, Jae Hyun Yoo wrote:  As per the in-kernel documentation, I am now allowed to make fun of you.  You are trying to out smart"" the kernel by getting rid of a warning message that was explicitly put there for you to do something.  To think that by just providing an ""empty"" function you are somehow fulfilling the API requirement is quite bold, don't you think?  This has to be fixed.  I didn't put that warning in there for no good reason.  Please go read the documentation again...  greg k-h""",not_technical,Greg KH,gregkh@linuxfoundation.org,1,0,457,0.1851145038167939,0.9074074074074074,0,0.9230769230769231,0.07692307692307693,0.23076923076923078,0.0
101,254985,255085,Please do not repost with such a small changes. It is much more important to sort out the big picture first and only then deal with minor implementation details. The more versions you post the more fragmented and messy the discussion will become. You will have to be patient because this is a rather big change and it will take _quite_ some time to get sorted. Thanks!,"On Wed 04-04-18 21:49:54, Buddy Lumpkin wrote:  Please do not repost with such a small changes. It is much more important to sort out the big picture first and only then deal with minor implementation details. The more versions you post the more fragmented and messy the discussion will become.  You will have to be patient because this is a rather big change and it will take _quite_ some time to get sorted.  Thanks! --  Michal Hocko SUSE Labs",not_technical,Michal Hocko,mhocko@kernel.org,1,0,368,1.0,0.4,0,0.0,1.0,0.0,0.2
102,258997,259457,"Pulled, and then immediately unpulled again. The code causes new compiler warnings, and the warnings are valid. If people don't care enough about their code to even check thewarnings, I'm not going to waste one second pulling the resulting garbage. It's that simple.","On Wed, Apr 11, 2018 at 1:41 AM, Zhang Rui <rui.zhang@intel.com> wrote:  Pulled, and then immediately unpulled again.  The code causes new compiler warnings, and the warnings are valid.  If people don't care enough about their code to even check the warnings, I'm not going to waste one second pulling the resulting garbage. It's that simple.                      Linus",not_technical,Linus Torvalds,torvalds@linux-foundation.org,1,0,266,0.8983050847457628,0.08,0,0.0,0.75,0.0,0.0
103,258997,260177,Could you please just merge the obvious fix from Arnd instead?[ it was posted two weeks ago and ACKed by me.,"On Friday, April 13, 2018 01:39:05 PM Zhang Rui wrote:  Could you please just merge the obvious fix from Arnd instead?  [ it was posted two weeks ago and ACKed by me ]  https://patchwork.kernel.org/patch/10313313/  Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",not_technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,108,0.4067796610169492,0.4,0,0.5,0.25,0.0,0.0
104,258997,260189,The init function is making sure cal_type is one or another. Can you fix it correctly by replacing the 'switch' by a 'if' instead of adding dead branches to please gcc?,"On 13/04/2018 11:08, Bartlomiej Zolnierkiewicz wrote:  The init function is making sure cal_type is one or another. Can you fix it correctly by replacing the 'switch' by a 'if' instead of adding dead branches to please gcc?  if (data->cal_type == TYPE_TWO_POINT_TRIMMING) { 	return ..., }  return ...,  --   <http://www.linaro.org/> Linaro.org │ Open source software for ARM SoCs  Follow Linaro:  <http://www.facebook.com/pages/Linaro> Facebook | <http://twitter.com/#!/linaroorg> Twitter | <http://www.linaro.org/linaro-blog/> Blog",not_technical,Daniel Lezcano,daniel.lezcano@linaro.org,1,0,168,0.5932203389830508,0.52,0,0.5,0.25,0.0,0.0
105,259276,259299,"Did you actually test this?  The usual reason for wanting m/u delay isthat the timing must be exact.  The driver is filled with mdelay()s forthis reason.  The one you've picked on is in the init path so it won't affect the runtime in any way.  I also don't think we have the hr timer machinery for usleep_range() to work properly on parisc, so I don't think the replacement works.","On Wed, 2018-04-11 at 23:39 +0800, Jia-Ju Bai wrote:  Did you actually test this?  The usual reason for wanting m/udelay is that the timing must be exact.  The driver is filled with mdelay()s for this reason.  The one you've picked on is in the init path so it won't affect the runtime in any way.  I also don't think we have the hrtimer machinery for usleep_range() to work properly on parisc, so I don't think the replacement works.  James",not_technical,James Bottomley,James.Bottomley@HansenPartnership.com,1,0,380,1.0,0.4,0,0.0,0.0,0.0,0.0
106,261377,261389,"This doesn't have to be on separate lines, as written, it just causes confusion. Good find, but your patch is corrupted to the point where any attenpt to fix it up on my side failed. Please resend without corruption, and please provide a Fixes: line. Thanks, Guenter","On Mon, Apr 16, 2018 at 10:08:19PM +0500, Ahsan Hussain wrote: This doesn't have to be on separate lines, as written, it just causes confusion.   Good find, but your patch is corrupted to the point where any attenpt to fix it up on my side failed. Please resend without corruption, and please provide a Fixes: line.  Thanks, Guenter ",not_technical,Guenter Roeck,linux@roeck-us.net,1,0,266,1.0,0.5,0,0.0,0.0,0.0,0.0
107,261866,263843,"Sorry, but this is a hack to *try* to make multi-slot work and this isn't sufficient. There were good reasons to why the earlier non-working multi slot support was removed from dw_mmc. Let me elaborate a bit for your understanding. The core uses a hostlock (mmc_claim|release_host()) to serialize operations and commands, as to confirm to the SD/SDIO/(e) MMC specs. The above changes gives noguarantees for this. To make that work, we would need a ""mmc bus lock"" to be managed by the core. However, inventing a ""mmc bus lock"" would lead to other problemsrelated to I/O scheduling for upper layers - it simply breaks. Forexample, I/O requests for one card/slot can then starve I/O requests reaching another card/slot.","[...]   Sorry, but this is a hack to *try* to make multi-slot work and this isn't sufficient. There were good reasons to why the earlier non-working multi slot support was removed from dw_mmc.  Let me elaborate a bit for your understanding. The core uses a host lock (mmc_claim|release_host()) to serialize operations and commands, as to confirm to the SD/SDIO/(e)MMC specs. The above changes gives no guarantees for this. To make that work, we would need a mmc bus lock"" to be managed by the core.  However, inventing a ""mmc bus lock"" would lead to other problems related to I/O scheduling for upper layers - it simply breaks. For example, I/O requests for one card/slot can then starve I/O requests reaching another card/slot.  [...]  Kind regards Uffe""",not_technical,Ulf Hansson,ulf.hansson@linaro.org,1,0,716,1.0,0.36363636363636365,0,0.25,0.75,0.25,0.0
108,266017,266025,"What actually took so long?  Could you analyze further instead of blindly putting the flag? thanks,Takashi","On Mon, 23 Apr 2018 14:05:52 +0200, Paul Menzel wrote:  What actually took so long?  Could you analyze further instead of blindly putting the flag?   thanks,  Takashi",not_technical,Takashi Iwai,tiwai@suse.de,1,0,106,0.35714285714285715,0.2,0,0.0,1.0,0.0,0.0
109,266279,266313,"Please enlighten me: how do you think this could be exploited? When an application calls VIDIOC_ENUM_FMT from a /dev/video0 device, it will just enumerate a hardware functionality, with is constantfor a given hard ware piece.The way it works is that userspace do something like: in order to read an entire const table.Usually, it doesn't require any special privilege to call this ioctl, but, even if someone changes its permission to 0x400, a simple lsusboutput is enough to know what hardware model is there. A lsmodor cat /proc/modules) also tells that the tm6000 module was loaded,with is a very good hint that the tm6000 is there or was there in thepast.In the specific case of tm6000, all hardware supports exactly thesame formats, as this is usually defined per-driver. So, a quick lookat the driver is enough to know exactly what the ioctl would answer. Also, the net is full of other resources that would allow anyone to get the supported formats for a piece of hardware. Even assuming that the OS doesn't have lsusb, that /proc is not mounted, that /dev/video0 require special permissions, that the potential attacker doesn't have physical access to the equipment (inorder to see if an USB board is plugged), etc... What possible harmhe could do by identifying a hardware feature?Similar notes for the other patches to drivers/media in thisseries: let's not just start adding bloatware where not needed.Please notice that I'm fine if you want to submit potential Spectre variant 1 fixups, but if you're willing to do so, please provide an explanation about the potential threat scenarios that you're identifying at the code. Dan, It probably makes sense to have somewhere at smatch a place where we could explicitly mark the false-positives, in order to avoiduse to receive patches that would just add an extra delay where it is not needed.","Em Mon, 23 Apr 2018 12:38:03 -0500 Gustavo A. R. Silva"" <gustavo@embeddedor.com> escreveu:   Please enlighten me: how do you think this could be exploited?  When an application calls VIDIOC_ENUM_FMT from a /dev/video0 device, it will just enumerate a hardware functionality, with is constant for a given hardware piece.  The way it works is that userspace do something like:  	int ret = 0,  	for (i = 0, ret == 0, i++) { 		ret = ioctl(VIDIOC_ENUM_FMT, ...), 	}  in order to read an entire const table.  Usually, it doesn't require any special privilege to call this ioctl, but, even if someone changes its permission to 0x400, a simple lsusb output is enough to know what hardware model is there. A lsmod or cat /proc/modules) also tells that the tm6000 module was loaded, with is a very good hint that the tm6000 is there or was there in the past.  In the specific case of tm6000, all hardware supports exactly the same formats, as this is usually defined per-driver. So, a quick look at the driver is enough to know exactly what the ioctl would answer.  Also, the net is full of other resources that would allow anyone to get the supported formats for a piece of hardware.  Even assuming that the OS doesn't have lsusb, that /proc is not mounted, that /dev/video0 require special permissions, that the potential attacker doesn't have physical access to the equipment (in order to see if an USB board is plugged), etc... What possible harm he could do by identifying a hardware feature?  Similar notes for the other patches to drivers/media in this series: let's not just start adding bloatware where not needed.  Please notice that I'm fine if you want to submit potential Spectre variant 1 fixups, but if you're willing to do so, please provide an explanation about the potential threat scenarios that you're identifying at the code.  Dan,  It probably makes sense to have somewhere at smatch a place where we could explicitly mark the false-positives, in order to avoid use to receive patches that would just add an extra delay where it is not needed.  Regards, Mauro""",not_technical,Mauro Carvalho Chehab,mchehab@kernel.org,1,0,1850,1.0,0.325,0,0.0,1.0,0.0,0.0
110,266918,269136,"I suppose these sort of patches are as much a PITA for the sender than for the receivers. I hesitated between a single patch, a series or separated patches. In a sense, the single patch would have been the easier for both sides but I guessed it would not have been very well welcomed. Since for a series, you're supposed to CC the whole series to everyone involved, it would have been, or at least at thought so, maximally noisy for nogood reasons. Finally, as all of these patches are totally independent, I thought it would be the best to send them as separated patches, each drivers maintainers being then free to accept, reject or ignore the patch(es) concerning him/her. It seems it was a bad guess, and yes, I see the point of having a series for this. I'll remember all this for the next time (if next time there is, of course, I was already quite hesitant to spend time to prepareand send patches for these issues with enum/integer mix-up). Sorry for the annoyance,-- Luc","On Tue, Apr 24, 2018 at 10:42:50AM -0400, David Miller wrote:  I suppose these sort of patches are as much a PITA for the sender than for the receivers.  I hesitated between a single patch, a series or separated patches. In a sense, the single patch would have been the easier for both sides but I guessed it would not have been very well welcomed. Since for a series, you're supposed to CC the whole series to everyone involved, it would have been, or at least at thought so, maximaly noisy for no good reasons. Finally, as all of these patches are totally independent, I thought it would be the best to send them as separated patches,  each drivers maintainers being then free to accept, reject or ignore the patch(es) concerning him/her. It seems it was a bad guess, and yes, I see the point of having a series for this.  I'll remember all this for the next time (if next time there is, of course, I was already quite hesitant to spend time to prepare and send patches for these issues with enum/integer mix-up).  Sorry for the annoyance, -- Luc",not_technical,Luc Van Oostenryck,luc.vanoostenryck@gmail.com,1,1,979,1.0,1.0,1,1.0,0.0,1.0,0.0
111,281311,281957,"Either it does exist, or it doesn't. If it exists, it needs to be fixed.  If it doesn't exist, nothing needs to be done. Which is the case?","On Tue, May 8, 2018 at 5:08 AM, Jia-Ju Bai <baijiaju1990@gmail.com> wrote:  Either it does exist, or it doesn't.  If it exists, it needs to be fixed.  If it doesn't exist, nothing needs to be done.  Which is the case?",not_technical,Rafael J. Wysocki,rafael@kernel.org,1,0,139,1.0,0.5,0,0.0,0.0,0.0,0.0
112,281311,283024,It looks like you are not actually sure what you are doing then.,"On Wed, May 9, 2018 at 5:17 AM, Jia-Ju Bai <baijiaju1990@gmail.com> wrote:  It looks like you are not actually sure what you are doing then.",not_technical,Rafael J. Wysocki,rafael@kernel.org,1,0,64,0.3783783783783784,1.0,1,1.0,0.0,0.0,0.0
113,289026,292839,"I do share your view Mike! This all looks so hackish and ad-hoc that I would be tempted to give it an outright nack, but let's here more about why do we need this fiddling at all. I've asked in other email so I guess I will get an answer therebut let me just emphasize again that I absolutely detest a possibilityto put hugetlb pages into the memcg mix. They just do not belong there.Try to look at previous discussions why it has been decided to have a separate hugetlb pages at all. I am also quite confused why you keep distinguishing surplus hugetlbpages from regular preallocated ones. Being a surplus page is an implementation detail that we use for an internal accounting rather than something to exhibit to the userspace even more than we do currently. Just look at what would/ should when you need to adjust accounting - e.g.due to the pool resize. Are you going to uncharge those surplus pages from memcg to reflect their persistence?","On Tue 22-05-18 22:04:23, TSUKADA Koutaro wrote:  I do share your view Mike!   This all looks so hackish and ad-hoc that I would be tempted to give it an outright nack, but let's here more about why do we need this fiddling at all. I've asked in other email so I guess I will get an answer there but let me just emphasize again that I absolutely detest a possibility to put hugetlb pages into the memcg mix. They just do not belong there. Try to look at previous discussions why it has been decided to have a separate hugetlb pages at all.  I am also quite confused why you keep distinguishing surplus hugetlb pages from regular preallocated ones. Being a surplus page is an implementation detail that we use for an internal accounting rather than something to exhibit to the userspace even more than we do currently. Just look at what [sw]hould when you need to adjust accounting - e.g. due to the pool resize. Are you going to uncharge those surplus pages ffrom memcg to reflect their persistence? --  Michal Hocko SUSE Labs",not_technical,Michal Hocko,mhocko@kernel.org,1,0,944,0.7114624505928854,0.6428571428571429,0,0.6666666666666666,0.3333333333333333,0.0,0.0
114,289431,292003,"Where did this come from? XFS doesn't use the underlying block devaddress space, so this does nothing at all and should not be here.So to return errors correctly, xfs_fs_sync_fs() needs to capture errors from the log force (i.e. metadata errors such as filesystem shutdowns, journal IO errors, etc), then check for pending data IOerrors","On Fri, May 18, 2018 at 08:34:12AM -0400, Jeff Layton wrote:  Where did this come from? XFS doesn't use the underlying blockdev address space, so this does nothing at all and should not be here.   So to return errors correctly, xfs_fs_sync_fs() needs to capture errors from the log force (i.e. metadata errors such as filesystem shutdowns, journal IO errors, etc), then check for pending data IO errors. i.e:    STATIC int  xfs_fs_sync_fs(  	struct super_block      *sb,  	int                     wait)  {  	struct xfs_mount        *mp = XFS_M(sb), +	int			err,    	/*  	 * Doing anything during the async pass would be counterproductive.  	 */  	if (!wait)  		return 0,   -	xfs_log_force(mp, XFS_LOG_SYNC), +	err = xfs_log_force(mp, XFS_LOG_SYNC), +	if (err) +		return err, +  	if (laptop_mode) {  		/*  		 * The disk must be active because we're syncing.  		 * We schedule log work now (now that the disk is  		 * active) instead of later (when it might not be).  		 */  		flush_delayed_work(&mp->m_log->l_work),  	}   -	return 0 +	return errseq_check_and_advance(&sb->s_wb_err, since),  }  Cheers,  Dave. --  Dave Chinner david@fromorbit.com",not_technical,Dave Chinner,david@fromorbit.com,0,0,336,0.2986425339366516,0.9444444444444444,0,1.0,0.0,1.0,0.0
115,303621,305468,The SPDX header is explicitly here to remove the license text and create a tag that is in a indirect reference to the license text in LICENSES. It's not going away. I never said we were perfect reviewers. Feel free to help in the process.,"On Mon, Jun 04, 2018 at 06:33:02PM +0100, Bob Ham wrote:  The SPDX header is explicitly here to remove the license text and create a tag that is in a indirect reference to the license text in LICENSES. It's not going away.   I never said we were perfect reviewers. Feel free to help in the process.  Maxime  --  Maxime Ripard, Bootlin (formerly Free Electrons) Embedded Linux and Kernel engineering https://bootlin.com ",not_technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,238,1.0,0.5714285714285714,0,0.2,0.7,0.0,0.0
116,306145,357688,But you did it again....Your email client should not be forcing you to top post. So please don't.,"On Mon, Jul 30, 2018 at 02:00:44PM +0000, Onnasch, Alexander (EXT) wrote:  But you did it again....  Your email client should not be forcing you to top post. So please don't.  	Andrew",not_technical,Andrew Lunn,andrew@lunn.ch,1,0,97,0.4791666666666667,1.0,1,1.0,0.0,0.0,0.0
117,307410,349339,I took a closer look at this and it's not necessary. (Note: I do the majority of my testing in a looped-back setup). What you didn't notice is that split_remote() separates the colon whether there is a host or not. It's not passed to ssh or cat (or whatever) directly. So the change you propose will actually break the how it was designed.,"  On 15/06/18 01:51 PM, Serge Semin wrote: I took a closer look at this and it's not necessary. (Note: I do the majority of my testing in a looped-back setup).  What you didn't notice is that split_remote() separates the colon whether there is a host or not. It's not passed to ssh or cat (or whatever) directly. So the change you propose will actually break the how it was designed.  Logan",not_technical,Logan Gunthorpe,logang@deltatee.com,1,1,339,0.33476394849785407,1.0,1,1.0,0.0,0.8292682926829268,0.0
118,310967,311378,There are no unexpected results. Making a non-fatal error fatal doesn't serve a useful purpose.,"On 06/11/2018 08:32 PM, Zhouyang Jia wrote:  There are no unexpected results.   Making a non-fatal error fatal doesn't serve a useful purpose.  NACK  Guenter ",not_technical,Guenter Roeck,linux@roeck-us.net,1,0,95,1.0,1.0,1,0.0,0.0,0.0,0.0
119,331266,331355,"What is this crazy union for?  Why are you messing around with ""raw"" kobject attributes?  This is a device, you should never have to mess with sysfs calls or kobject calls or structures directly.  If you do, that's a huge hint something is wrong here. You aren't ""adding"" any attributes here, you are only setting them up (in an odd way, see below...) That's an oddly-hard-coded array size for no good reason : (This works?  You normally have to manually initialize a dynamic attribute.  Why are you doing it this way and not using an attribute group? Why are you using a custom device class for a single device? you need to document the heck out of this in the changelog to help explain all of these odd design decisions.","On Tue, Jul 03, 2018 at 05:04:12PM +1000, Andrew Jeffery wrote:  What is this crazy union for?  Why are you messing around with raw"" kobject attributes?  This is a device, you should never have to mess with sysfs calls or kobject calls or structures directly.  If you do, that's a huge hint something is wrong here.    You aren't ""adding"" any attributes here, you are only setting them up (in an odd way, see below...)   That's an oddly-hard-coded array size for no good reason :(    This works?  You normally have to manually initialize a dynamic attribute.  Why are you doing it this way and not using an attribute group?   Why are you using a custom device class for a single device?  you need to document the heck out of this in the changelog to help explain all of these odd design decisions.  thanks,  greg k-h""",not_technical,Greg KH,gregkh@linuxfoundation.org,1,0,722,1.0,0.5,0,0.0,0.0,0.0,0.0
120,331266,331694,"(and replying to your other comments as well)...This is an RFC series, it's not meant for you to take at this point,it's about discussing the overall approach to exposing BMC random ""tunables"" as explained in patch 0 of the series. Yes the individual patches aren't yet at the level of polish for aformal submission, we (naively ?) thought that's what the whole RFC tag is about :-)","On Tue, 2018-07-03 at 09:50 +0200, Greg KH wrote:  Greg (and replying to your other comments as well)...  This is an RFC series, it's not meant for you to take at this point, it's about discussing the overall approach to exposing BMC random tunables"" as explained in patch 0 of the series.  Yes the individual patches aren't yet at the level of polish for a formal submission, we (naively ?) thought that's what the whole RFC tag is about :-)  Cheers, Ben.""",not_technical,Benjamin Herrenschmidt,benh@kernel.crashing.org,1,0,382,0.5584415584415584,0.625,0,0.0,0.0,0.0,0.0
121,331266,331847,"Well, it adds documentation :-) You can just read the patch which is... the documentation :) Yes, you did that's fine. Thanks.","On Tue, 2018-07-03 at 16:31 +0200, Greg KH wrote:  Well, it adds documentation :-) You can just read the patch which is ... the documentation :)  Yes, you did that's fine. Thanks.  Cheers, Ben.",not_technical,Benjamin Herrenschmidt,benh@kernel.crashing.org,1,0,126,0.2012987012987013,0.75,0,0.0,0.0,0.0,0.0
122,331266,331710,"Oh come on, putting a basic ""here is what this patch does"" comment should be part of every patch, otherwise what is there to comment on if we don't know what is going on in the patch itself? Anyway, I provided a bunch of feedback to the ""real"" patch in this series...","On Wed, Jul 04, 2018 at 12:16:49AM +1000, Benjamin Herrenschmidt wrote:  Oh come on, putting a basic here is what this patch does"" comment should be part of every patch, otherwise what is there to comment on if we don't know what is going on in the patch itself?  Anyway, I provided a bunch of feedback to the ""real"" patch in this series...   greg k-h""",not_technical,Greg KH,gregkh@linuxfoundation.org,1,0,267,0.4025974025974026,0.6875,0,0.0,0.0,0.0,0.0
123,336834,337035,"Yeah, not going to happen. You grow that structure from 64 bytes to 96bytes and with that grow the static footprint of the kernel by 512k (in the very best case) possibly again breaking things like sparc (which have a strict limit on the kernel image size). And all that for data that I've never needed and never even considered useful when looking at lock dep output.","On Mon, Jul 09, 2018 at 02:57:25AM +0200, Eugeniu Rosca wrote:  Yeah, not going to happen. You grow that structure from 64 bytes to 96 bytes and with that grow the static footprint of the kernel by 512k (in the very best case) possibly again breaking things like sparc (which have a strict limit on the kernel image size).  And all that for data that I've never needed and never even considered useful when looking at lockdep output.",not_technical,Peter Zijlstra,peterz@infradead.org,1,0,368,0.2857142857142857,0.6666666666666666,0,0.0,0.0,0.0,0.0
124,336834,337316,"I  confirm that in case of x86_64, the bss size is increased by ~1M [1] with standard v4.18-rc4 x86_64_defconfig + CONFIG_LOCKDEP. For sparc there seems to be a dedicated CONFIG_LOCKDEP_SMALL, which seems to downsize the lockdep implementation anyway. It's likely because you infer about certain aspects which are not clearly stated in the deadlock report. As example, the original report doesn't say that the process which holds 'cpu_hotplug_lock.rw_sem' is different to the process which holds the other locks. On thecontrary, it tells the user that all the locks are being held by thesame task, which seems to be wrong.You likely also infer about the order of consuming the locks based onthe contents of the stack dump associated to each lock. Without doing some mental diffs between the backtraces, it's not possible to see thechronological order of consuming the locks. Actually this only works for backtraces with common history, i.e. there is no clue what is the time/point of acquiring 'cpu_hotplug_lock.rw_sem' relative to the otherlocks. The patch mostly shares my personal experience of trying to make sense of lockdep output. It's OK if it doesn't reach mainline. I still hope that I can get some feedback from community regarding the actual cpu freq-related issue pointed out in the splat. I can also reproduce it on v4.14, so it appears to be in the kernel for quite some time.Thank you in advance. BSS size increase after applying the patch","On Mon, Jul 09, 2018 at 10:31:18AM +0200, Peter Zijlstra wrote:  I  confirm that in case of x86_64, the bss size is increased by ~1M [1] with standard v4.18-rc4 x86_64_defconfig + CONFIG_LOCKDEP.   For sparc there seems to be a dedicated CONFIG_LOCKDEP_SMALL, which seems to downsize the lockdep implementation anyway.   It's likely because you infer about certain aspects which are not clearly stated in the deadlock report. As example, the original report doesn't say that the process which holds 'cpu_hotplug_lock.rw_sem' is different to the process which holds the other locks. On the contrary, it tells the user that all the locks are being held by the same task, which seems to be wrong.  You likely also infer about the order of consuming the locks based on the contents of the stack dump associated to each lock. Without doing some mental diffs between the backtraces, it's not possible to see the chronological order of consuming the locks. Actually this only works for backtraces with common history, i.e. there is no clue what is the time/point of acquiring 'cpu_hotplug_lock.rw_sem' relative to the other locks.  The patch mostly shares my personal experience of trying to make sense of lockdep output. It's OK if it doesn't reach mainline.  I still hope that I can get some feedback from community regarding the actual cpufreq-related issue pointed out in the splat. I can also reproduce it on v4.14, so it appears to be in the kernel for quite some time.  Thank you in advance.  Best regards, Eugeniu.  [1] BSS size increase after applying the patch $ bloaty -s vm vmlinux.after -- vmlinux.before         VM SIZE                           FILE SIZE  --------------                     --------------   +8.2% +1024Ki .bss                      0  [ = ]   ----snip----   +2.6% +1024Ki TOTAL               +3.36Ki  +0.0%",not_technical,Eugeniu Rosca,erosca@de.adit-jv.com,0,0,1455,1.0,1.0,1,0.0,0.0,0.0,0.0
125,358863,358900,"These calling conventions are rather suboptimal.  First of all, none of the callbacks will ever get called directly. * there are only 4 callers.  3 of them (all in fs.h) are of the form return ....,  The fourth is which itself is an ->actor() callback.So all these ""return -E..."" in the instances are completely pointless, we should just turn filldir_t into pointer-to-function-returning-booland get rid of that boilerplate, rather than adding more to it. Furthermore, who the hell cares which callback has stepped into it?"" The first time it happened from getdents(2) in a 32bit process and that's all you'll ever get out of me"" seems to be less than helpful...And frankly, I would prefer making that thing return -EUCLEAN or 0.  Quite possibly - inlining it as well...","On Tue, Jul 31, 2018 at 06:10:27PM +0200, Jann Horn wrote:   These calling conventions st^Ware rather suboptimal.  First of all, 	* none of ->actor() callbacks will ever get called directly. 	* there are only 4 callers.  3 of them (all in fs.h) are of the form return ....->actor(...) == 0,  The fourth is         return orig_ctx->actor(orig_ctx, name, namelen, offset, ino, d_type), in ovl_fill_real(), which itself is an ->actor() callback.  So all these return -E..."" in the instances are completely pointless, we should just turn filldir_t into pointer-to-function-returning-bool and get rid of that boilerplate, rather than adding more to it.  Furthermore, who the hell cares which callback has stepped into it? ""The first time it happened from getdents(2) in a 32bit process and that's all you'll ever get out of me"" seems to be less than helpful...  And frankly, I would prefer 	buf->result = check_dirent_name(name, namelen), 	if (unlikely(buf->result)) 		return false, making that thing return -EUCLEAN or 0.  Quite possibly - inlining it as well...""",not_technical,Al Viro,viro@ZenIV.linux.org.uk,1,0,770,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
126,365796,367116,Thanks for the reviewThe problem we have here is there is a potential to control 3  different LED string but only 2 sinks.  So control bank A can control 2 LED strings and control bank b can control 1 LED string.These values represent device level control and configuration of the LED strings to a specific control bank.I racked my brain trying to figure out how to configure the control banks and associated LED strings.These values are for the device configuration itself and the reg below indicates which control bank the LEDnode is assigned to. Don't see how you could compute this.  There is no easy way to give indication to the driver which LEDnode belongs to which control bank.  The control-bank-cfg is a device level property and the reg under the child is a LED string level property denoting the Class node to control bank mapping. Furthermore there are 2 device configurations that can be configured to only use 1 bank for all 3 LED strings. This will be answered in your comments in the code. This I can fix it should be a value between 1 and 6.,"Pavel  Thanks for the review  On 08/08/2018 02:59 PM, Pavel Machek wrote:  The problem we have here is there is a potential to control 3 different LED string but only 2 sinks.  So control bank A can control 2 LED strings and control bank b can control 1 LED string.    These values represent device level control and configuration of the LED strings to a specific control bank.  I racked my brain trying to figure out how to configure the control banks and associated LED strings. These values are for the device configuration itself and the reg below indicates which control bank the LED node is assigned to.   Don't see how you could compute this.  There is no easy way to give indication to the driver which LED node belongs to which control bank.  The control-bank-cfg is a device level property and the reg under the child is a LED string level property denoting the Class node to control bank mapping.  Furthermore there are 2 device configurations that can be configured to only use 1 bank for all 3 LED strings. This will be answered in your comments in the code.   This I can fix it should be a value between 1 and 6    --  ------------------ Dan Murphy",not_technical,Dan Murphy,dmurphy@ti.com,0,1,1059,1.0,0.32,0,0.16666666666666666,0.8333333333333334,0.0,0.0
127,365796,367764,"It is better do add some complexity to the driver than to the user configurable settings like DT. Besides - you will only need tocheck if given led-source is already taken by another node. Some description will be needed for sure, but I don't expect it to be overwhelmingly lengthy.Your control-bank-cfg seemed like having much room for improvement,and it would for sure raise questions on why it was implemented that way. Documenting all available combinations of the configuration is seldom the best solution. It often obscures the issue. In your bindings device configuration is scattered among global control-bank-cfg property and child node's reg property.In my proposal each child node contains all the needed configuration, also in the form of two properties - led-sources and reg. IMHO having all the LED class device related configuration in one place simplifies the analysis.","Dan,  On 08/09/2018 03:30 PM, Dan Murphy wrote:  It is better do add some complexity to the driver than to the user configurable settings like DT. Besides - you will only need to check if given led-source is already taken by another node.   Some description will be needed for sure, but I don't expect it to be overwhelmingly lengthy.   Your control-bank-cfg seemed like having much room for improvement, and it would for sure raise questions on why it was implemented that way. Documenting all available combinations of the configuration is seldom the best solution. It often obscures the issue.   In your bindings device configuration is scattered among global control-bank-cfg property and child node's reg property. In my proposal each child node contains all the needed configuration, also in the form of two properties - led-sources and reg. IMHO having all the LED class device related configuration in one place simplifies the analysis.  --  Best regards, Jacek Anaszewski",not_technical,Jacek Anaszewski,jacek.anaszewski@gmail.com,1,0,885,0.7727272727272727,0.88,0,0.16666666666666666,0.6666666666666666,0.0,0.0
128,372597,372959,"This patch was corrupted by your email client, for example it turned TAB characters into sequences of spaces. Please fix this, email a test patch to yourself, and do not resend the patch to this mailing list until you can successfully extract and cleanly apply the test patch you email to yourself. Thank you.","From: Wang Jian <jianjian.wang1@gmail.com> Date: Thu, 16 Aug 2018 21:01:27 +0800   This patch was corrupted by your email client, for example it turned TAB characters into sequences of spaces.  Please fix this, email a test patch to yourself, and do not resend the patch to this mailing list until you can successfully extract and cleanly apply the test patch you email to yourself.  Thank you.",not_technical,David Miller,davem@davemloft.net,1,0,309,1.0,0.5,0,0.0,0.0,0.0,0.0
129,378505,378562,"Again I'll ask: what is the performance when the log is made large enough that your benchmark is *not hammering the slow path*? i.e. does running instead of using thedefault tiny log on your tiny test file system make the problemgo away? Without that information, we have no idea what the slowpath impact on peformance actually is, and whether it is worth persuing optimising slow path behaviour that very, very fewproduction environments see lock contention in....","On Sun, Aug 26, 2018 at 04:53:12PM -0400, Waiman Long wrote:  Again I'll ask: what is the performance when the log is made large enough that your benchmark is *not hammering the slow path*?   i.e. does running mkfs.xfs -l size=2000m ..."" instead of using the default tiny log on your tiny test filesystem make the problem go away? Without that information, we have no idea what the slow path impact on peformance actually is, and whether it is worth persuing optimising slow path behaviour that very, very few production environments see lock contention in....  Cheers,  Dave. --  Dave Chinner david@fromorbit.com""",not_technical,Dave Chinner,david@fromorbit.com,0,0,465,1.0,0.5,0,0.0,1.0,0.0,0.0
130,383199,392690,"Ick, this is still messy, just try making this: Yeah, it's over 80 columns, but it looks better and is easier to read,right? Also, all your patches have the whitespace turned from tabs into spaces, making them impossible to be applied even if I wanted to :)","On Thu, Aug 30, 2018 at 01:32:17PM -0400, Ray Clinton wrote:  Ick, this is still messy, just try making this:  			err |= comedi_check_trigger_arg_min(&cmd->scan_begin_arg, 							    cmd->convert_arg * cmd->chanlist_len),  Yeah, it's over 80 columns, but it looks better and is easier to read, right?  Also, all your patches have the whitespace turned from tabs into spaces, making them impossible to be applied even if I wanted to :)  thanks,  greg k-h",not_technical,Greg KH,gregkh@linuxfoundation.org,1,0,257,1.0,1.0,1,1.0,0.0,1.0,0.0
131,391895,397297,"I don't call this non-intrusive. I'll beg to differ, this isn't anywhere near something to consider merging. Also 'happened' suggests a certain stage of completeness, this again doesn't qualify.There are known scalability problems with the existing cgroup muck, youjust made things a ton worse. The existing cgroup overhead is significant, you also made that many times worse.The cgroup stuff needs cleanups and optimization, not this. That is the whole and only reason you did this, and it doesn't even begin to cover the requirements for it. Not to mention I detest cgroups, for their inherent complixity and the performance costs associated with them.  _If_ we're going to dosomething for L1TF then I feel it should not depend on cgroups. It is after all, perfectly possible to run a kvm thingy without cgroups. Note that in order to avoid PLE and paravirt spinlocks and paravirttlb-invalidate you have to gang-schedule the _entire_ VM, not just SMTsiblings. Now explain to me how you're going to gang-schedule a VM with a goodnumber of vCPU threads (say spanning a number of nodes) and preserving the rest of CFS without it turning into a massive trainwreck? Such things (gang scheduling VMs) _are_ possible, but not within the confines of something like CFS, they are also fairly inefficient because, as you do note, you will have to explicitly schedule idle time for idle vCPUs.Things like the Tableau scheduler are what come to mind, but I'm not sure how to integrate that with a general purpose scheduling scheme. You pretty much have to dedicate a set of CPUs to just scheduling VMs with such a scheduler.And that would call for cpuset-v2 integration along with a new scheduling class. And then people will complain again that partitioning a system isn't dynamic enough and we need magic :/(and this too would be tricky to virtualize itself) You gloss over a ton of details here, many of which are non trivial and marked broken in your patches. Unless you have solid suggestions on how to deal with all of them, this is a complete non-starter.The per-cpu IRQ/steal time accounting for example. The task timeline isn't the same on every CPU because of those. You now basically require steal time and IRQ load to match between CPUs.That places very strict requirements and effectively breaks virt invariance. That is, the scheduler now behaves significantly different inside a VM than it does outside of it -- without the guest being gang scheduled itself and having physical pinning to reflect the same topology the coschedule=1 thing should not be exposed in a guest. And that is a major failing IMO. Also, I think you're sharing a cfs_rq between CPUs: that is broken, the virtual runtime stuff needs nontrivial modifications for multiple CPUs. And if you do that, I've no idea how you're dealing with SMP affinities.You don't even begin to outline how you preserve smp-nice fairness.IOW it's completely friggin useless for L1TF. Have you actually read your own code? What about that atrocious locking you sprinkle all over the place?'some additional lock contention' doesn't even begin to describe that horror show.Hint: we're not going to increase the lockdep subclasses, and most certainly not for scheduler locking. All in all, I'm not inclined to consider this approach, it complicates an already overly complicated thing (cpu-cgroups) and has a ton of unresolved issues while at the same time it doesn't (and cannot) meet the goal it was made for.","On Fri, Sep 07, 2018 at 11:39:47PM +0200, Jan H. Schnherr wrote:  I don't call this non-intrusive.   I'll beg to differ, this isn't anywhere near something to consider merging. Also 'happened' suggests a certain stage of completeness, this again doesn't qualify.   There are known scalability problems with the existing cgroup muck, you just made things a ton worse. The existing cgroup overhead is significant, you also made that many times worse.  The cgroup stuff needs cleanups and optimization, not this.    That is the whole and only reason you did this, and it doesn't even begin to cover the requirements for it.  Not to mention I detest cgroups, for their inherent complixity and the performance costs associated with them.  _If_ we're going to do something for L1TF then I feel it should not depend on cgroups.  It is after all, perfectly possible to run a kvm thingy without cgroups.   Note that in order to avoid PLE and paravirt spinlocks and paravirt tlb-invalidate you have to gang-schedule the _entire_ VM, not just SMT siblings.  Now explain to me how you're going to gang-schedule a VM with a good number of vCPU threads (say spanning a number of nodes) and preserving the rest of CFS without it turning into a massive trainwreck?  Such things (gang scheduling VMs) _are_ possible, but not within the confines of something like CFS, they are also fairly inefficient because, as you do note, you will have to explicitly schedule idle time for idle vCPUs.  Things like the Tableau scheduler are what come to mind, but I'm not sure how to integrate that with a general purpose scheduling scheme. You pretty much have to dedicate a set of CPUs to just scheduling VMs with such a scheduler.  And that would call for cpuset-v2 integration along with a new scheduling class.  And then people will complain again that partitioning a system isn't dynamic enough and we need magic :/  (and this too would be tricky to virtualize itself)   You gloss over a ton of details here, many of which are non trivial and marked broken in your patches. Unless you have solid suggestions on how to deal with all of them, this is a complete non-starter.  The per-cpu IRQ/steal time accounting for example. The task timeline isn't the same on every CPU because of those.  You now basically require steal time and IRQ load to match between CPUs. That places very strict requirements and effectively breaks virt invariance. That is, the scheduler now behaves significantly different inside a VM than it does outside of it -- without the guest being gang scheduled itself and having physical pinning to reflect the same topology the coschedule=1 thing should not be exposed in a guest. And that is a mayor failing IMO.  Also, I think you're sharing a cfs_rq between CPUs:  +       init_cfs_rq(&sd->shared->rq.cfs),  that is broken, the virtual runtime stuff needs nontrivial modifications for multiple CPUs. And if you do that, I've no idea how you're dealing with SMP affinities.   You don't even begin to outline how you preserve smp-nice fairness.   IOW it's completely friggin useless for L1TF.   Have you actually read your own code?  What about that atrocious locking you sprinkle all over the place? 'some additional lock contention' doesn't even begin to describe that horror show.  Hint: we're not going to increase the lockdep subclasses, and most certainly not for scheduler locking.   All in all, I'm not inclined to consider this approach, it complicates an already overly complicated thing (cpu-cgroups) and has a ton of unresolved issues while at the same time it doesn't (and cannot) meet the goal it was made for.",not_technical,Peter Zijlstra,peterz@infradead.org,1,0,3462,0.38117373619988376,0.6228070175438597,0,0.06896551724137931,0.9310344827586207,0.0,0.0
132,391895,397649,"Mm... there is certainly room for interpretation. :) For example, it is still possible to set affinities, to use nice, and to tune all the other existing CFS knobs. That is, if you have tuned the scheduler to your workload or your workload depends on some CFS feature to work efficiently (whether on purpose or not), then running with this patch set should not change the behavior of said workload.This patch set should ""just"" give the user the additional ability to coordinatesche duling decisions across multiple CPUs. At least, that's my goal. If someone doesn't need it, they don't have to use it. Just like task groups.But maybe, people start experimenting with coordinated scheduling decisions --after all, there is a ton of research on what one *could* do, if there was coscheduling. I did look over much of that research. What I didn't like aboutmany of them, is that evaluation is based on a ""prototype"", that -- whilemaking the point that coscheduling might be beneficial for that use case --totally screws over the scheduler for any other use case. Like coscheduling based on deterministic, timed context switches across all CPUs. Bye bye interactivity. That is, what I call intrusive. As mentioned before, existing scheduler features, like preemption, (should) still work as before with this variant of coscheduling, with the same look andfeel.And who knows, maybe someone will come up with a use case that moves coscheduling out of its niche, like the auto-grouping feature promoted the use of task groups.I agree, that this isn't ready to be merged. Still, the current state is good to start a discussion about the involved mechanics.Are you referring to cgroups in general, or task groups (aka. the cpucontroller) specifically? With respect to scalability: many coscheduling use cases don't requiresynchronization across the whole system. With this patch set, only thoseparts that are actually coscheduled are involved in synchronization.So, conceptually, this scales to larger systems from that point of view.If coscheduling of a larger fraction of the system is required, costs increase. So what? It's a trade-off. It may *still* be beneficial for ause case. If it is, it might get adopted. If not, that particular usecase may be considered impractical unless someone comes up with a better implementation of coscheduling.With respect to the need of cleanups and optimizations: I agree, that task groups are a bit messy. For example, here's my current wish listoff the top of my head:a) lazy scheduler operations, for example: when dequeuing a task, don't bother walking up the task group hierarchy to dequeue all the SEs -- do it lazily   when encountering an empty CFS RQ during picking when we hold the lock anyway.b) ability to move CFS RQs between CPUs: someone changed the affinity of   a cpuset? No problem, just attach the run queue with all the tasks elsewhere.   No need to touch each and every task. c) light-weight task groups: don't allocate a runqueue for every CPU in the   system, when it is known that tasks in the task group will only ever run   on at most two CPUs, or so. (And while there is of course a use case for   VMs in this, another class of use cases are auxiliary tasks, see eg, [1-5].)Is this the level of optimizations, you're thinking about? Or do you want to throw away the whole nested CFS RQ experience in the code?It really isn't. But as your mind seems made up, I'm not going to botherto argue.Yes it is. But, for example, you won't have group-based fairness between multiple kvm thingies.Assuming, there is a cgroup-less solution that can prevent simultaneous execution of tasks on a core, when they're not supposed to. How would youtell the scheduler, which tasks these are?You probably don't -- for the same reason, why it is a bad idea to givean endless loop realtime priority. It's just a bad idea. As I said in thetext you quoted: coscheduling comes with its own set of advantages anddisadvantages. Just because you find one example, where it is a bad idea,doesn't make it a bad thing in general.With gang scheduling as defined by Feitelson and Rudolph [6], you'd have to explicitly schedule idle time. With coscheduling as defined by Ousterhout [7],you don't. In this patch set, the scheduling of idle time is ""merely"" a quirk of the implementation. And even with this implementation, there's nothing stopping you from down-sizing the width of the coscheduled set to take outthe idle vCPUs dynamically, cutting down on fragmentation.Hence my ""counter"" suggestion in the form of this patch set: Integratedinto a general purpose scheduler, no need to partition off a part of the system,not tied to just VM use cases.Yes, I do. :) I wanted a summary, not a design document. Maybe I was a bitto eager in condensing the design to just a few paragraphs...Address them one by one. Probably do some of the optimizations you suggestedto just get rid of some of them. It's work in progress. Though, at thisstage I am also really interested in things that are broken, that I am notaware of yet.I'll have to read up some more code to make a qualified statement here.It is not shared per se. There's only one CPU (the leader) making the scheduling decision for that run queue and if another CPU needs to modify the run queue, it works like it does for CPU run queues as well: the other CPU works with theleader's time. There are also no tasks in a run queue when it is responsible for more than one CPU. Assuming, that a run queue is responsible for a core and there are runnable tasks within the task group on said core, then there will one SE enqueued in that runqueue, a so called SD-SE (scheduling domain SE, or synchronizationdomain SE). This SD-SE represents the per CPU runqueues of this core of thistask group. (As opposed to a ""normal"" task group SE (TG-SE), which representsjust one runqueue in a different task group.) Tasks are still only enqueuedin the per CPU runqueues.Works as before (or will work as before): a coscheduled task group has itsown set of per CPU runqueues that hold the tasks of this group (per CPU).The load balancer will work on this subset of runqueues as it does on the""normal"" per CPU runqueues -- smp-nice fairness and all.Do you believe me now, that L1TF is not ""the whole and only reason"" I did this? :DCurrently, there are more code paths than I like, that climb up the se->parent relation to the top. They need to go, if we want to coschedule larger parts ofthe system in a more efficient manner. Hence, parts of my wish list further up.That said, it is not as bad as you make it sound for the following three reasons:a) The amount of CPUs that compete for a lock is currently governed by the   ""cosched_max_level"" command line argument, making it a conscious decision to   increase the overall overhead. Hence, coscheduling at, e.g., core level   does not have a too serious impact on lock contention.b) The runqueue locks are usually only taken by the leader of said runqueue.   Hence, there is often only one user per lock even at higher levels.   The prominent exception at this stage of the patch set is that enqueue and   dequeue operations walk up the hierarchy up to the ""cosched_max_level"".   And even then, due to lock chaining, multiple enqueue/dequeue operations   on different CPUs can bubble up the shared part of the hierarchy in parallel.c) The scheduling decision does not cause any lock contention by itself. Each   CPU only accesses run queues, where itself is the leader. Hence, once you   have a relatively stable situation, lock contention is not an issue.That's fine. Due to the overhead of nesting cgroups that you mentioned earlier,that many levels in the runqueue hierarchy are likely to be impracticable anyway. For the future, I imagine a more dynamic variant of task groups/scheduling domains, that can provide all the flexibility one would want without that deepof a nesting. At this stage, it is just a way to experiment with larger systemswithout having to disable lockdep.Of course, if you have a suggestion for a different locking scheme, we candiscuss that as well. The current one, is what I considered most suitableamong some alternatives under the premise I was working: integrate coschedulingin a scheduler as an additional feature (instead of, eg, write a scheduler capable of coscheduling). So, I probably haven't considered all alternatives. Even if you're not inclined -- at this stage, if I may be so bold :) --your feedback is valuable. Thank you for that.Regards JanR eferences (for those that are into that kind of thing)","On 09/14/2018 01:12 PM, Peter Zijlstra wrote:  Mm... there is certainly room for interpretation. :) For example, it is still possible to set affinities, to use nice, and to tune all the other existing CFS knobs. That is, if you have tuned the scheduler to your workload or your workload depends on some CFS feature to work efficiently (whether on purpose or not), then running with this patch set should not change the behavior of said workload.  This patch set should just"" give the user the additional ability to coordinate scheduling decisions across multiple CPUs. At least, that's my goal.  If someone doesn't need it, they don't have to use it. Just like task groups.  But maybe, people start experimenting with coordinated scheduling decisions -- after all, there is a ton of research on what one *could* do, if there was coscheduling. I did look over much of that research. What I didn't like about many of them, is that evaluation is based on a ""prototype"", that -- while making the point that coscheduling might be beneficial for that use case -- totally screws over the scheduler for any other use case. Like coscheduling based on deterministic, timed context switches across all CPUs. Bye bye interactivity. That is, what I call intrusive.  As mentioned before, existing scheduler features, like preemption, (should) still work as before with this variant of coscheduling, with the same look and feel.  And who knows, maybe someone will come up with a use case that moves coscheduling out of its niche, like the auto-grouping feature promoted the use of task groups.    I agree, that this isn't ready to be merged. Still, the current state is good to start a discussion about the involved mechanics.    Are you referring to cgroups in general, or task groups (aka. the cpu controller) specifically?   With respect to scalability: many coscheduling use cases don't require synchronization across the whole system. With this patch set, only those parts that are actually coscheduled are involved in synchronization. So, conceptually, this scales to larger systems from that point of view.  If coscheduling of a larger fraction of the system is required, costs increase. So what? It's a trade-off. It may *still* be beneficial for a use case. If it is, it might get adopted. If not, that particular use case may be considered impractical unless someone comes up with a better implementation of coscheduling.   With respect to the need of cleanups and optimizations: I agree, that task groups are a bit messy. For example, here's my current wish list off the top of my head:  a) lazy scheduler operations, for example: when dequeuing a task, don't bother    walking up the task group hierarchy to dequeue all the SEs -- do it lazily    when encountering an empty CFS RQ during picking when we hold the lock anyway.  b) ability to move CFS RQs between CPUs: someone changed the affinity of    a cpuset? No problem, just attach the runqueue with all the tasks elsewhere.    No need to touch each and every task.  c) light-weight task groups: don't allocate a runqueue for every CPU in the    system, when it is known that tasks in the task group will only ever run    on at most two CPUs, or so. (And while there is of course a use case for    VMs in this, another class of use cases are auxiliary tasks, see eg, [1-5].)  Is this the level of optimizations, you're thinking about? Or do you want to throw away the whole nested CFS RQ experience in the code?   It really isn't. But as your mind seems made up, I'm not going to bother to argue.    Yes it is. But, for example, you won't have group-based fairness between multiple kvm thingies.  Assuming, there is a cgroup-less solution that can prevent simultaneous execution of tasks on a core, when they're not supposed to. How would you tell the scheduler, which tasks these are?    You probably don't -- for the same reason, why it is a bad idea to give an endless loop realtime priority. It's just a bad idea. As I said in the text you quoted: coscheduling comes with its own set of advantages and disadvantages. Just because you find one example, where it is a bad idea, doesn't make it a bad thing in general.    With gang scheduling as defined by Feitelson and Rudolph [6], you'd have to explicitly schedule idle time. With coscheduling as defined by Ousterhout [7], you don't. In this patch set, the scheduling of idle time is ""merely"" a quirk of the implementation. And even with this implementation, there's nothing stopping you from down-sizing the width of the coscheduled set to take out the idle vCPUs dynamically, cutting down on fragmentation.    Hence my ""counter"" suggestion in the form of this patch set: Integrated into a general purpose scheduler, no need to partition off a part of the system, not tied to just VM use cases.    Yes, I do. :) I wanted a summary, not a design document. Maybe I was a bit to eager in condensing the design to just a few paragraphs...    Address them one by one. Probably do some of the optimizations you suggested to just get rid of some of them. It's work in progress. Though, at this stage I am also really interested in things that are broken, that I am not aware of yet.    I'll have to read up some more code to make a qualified statement here.    It is not shared per se. There's only one CPU (the leader) making the scheduling decision for that runqueue and if another CPU needs to modify the runqueue, it works like it does for CPU runqueues as well: the other CPU works with the leader's time. There are also no tasks in a runqueue when it is responsible for more than one CPU.  Assuming, that a runqueue is responsible for a core and there are runnable tasks within the task group on said core, then there will one SE enqueued in that runqueue, a so called SD-SE (scheduling domain SE, or synchronization domain SE). This SD-SE represents the per CPU runqueues of this core of this task group. (As opposed to a ""normal"" task group SE (TG-SE), which represents just one runqueue in a different task group.) Tasks are still only enqueued in the per CPU runqueues.    Works as before (or will work as before): a coscheduled task group has its own set of per CPU runqueues that hold the tasks of this group (per CPU). The load balancer will work on this subset of runqueues as it does on the ""normal"" per CPU runqueues -- smp-nice fairness and all.    Do you believe me now, that L1TF is not ""the whole and only reason"" I did this? :D    Currently, there are more code paths than I like, that climb up the se->parent relation to the top. They need to go, if we want to coschedule larger parts of the system in a more efficient manner. Hence, parts of my wish list further up.  That said, it is not as bad as you make it sound for the following three reasons:  a) The amount of CPUs that compete for a lock is currently governed by the    ""cosched_max_level"" command line argument, making it a conscious decision to    increase the overall overhead. Hence, coscheduling at, e.g., core level    does not have a too serious impact on lock contention.  b) The runqueue locks are usually only taken by the leader of said runqueue.    Hence, there is often only one user per lock even at higher levels.    The prominent exception at this stage of the patch set is that enqueue and    dequeue operations walk up the hierarchy up to the ""cosched_max_level"".    And even then, due to lock chaining, multiple enqueue/dequeue operations    on different CPUs can bubble up the shared part of the hierarchy in parallel.  c) The scheduling decision does not cause any lock contention by itself. Each    CPU only accesses runqueues, where itself is the leader. Hence, once you    have a relatively stable situation, lock contention is not an issue.    That's fine. Due to the overhead of nesting cgroups that you mentioned earlier, that many levels in the runqueue hierarchy are likely to be impracticable anyway. For the future, I imagine a more dynamic variant of task groups/scheduling domains, that can provide all the flexibility one would want without that deep of a nesting. At this stage, it is just a way to experiment with larger systems without having to disable lockdep.  Of course, if you have a suggestion for a different locking scheme, we can discuss that as well. The current one, is what I considered most suitable among some alternatives under the premise I was working: integrate coscheduling in a scheduler as an additional feature (instead of, eg, write a scheduler capable of coscheduling). So, I probably haven't considered all alternatives.   Even if you're not inclined -- at this stage, if I may be so bold :) -- your feedback is valuable. Thank you for that.  Regards Jan   References (for those that are into that kind of thing):  [1] D. Kim, S. S.-w. Liao, P. H. Wang, J. del Cuvillo, X. Tian, X. Zou,     H. Wang, D. Yeung, M. Girkar, and J. P. Shen, “Physical experimentation     with prefetching helper threads on Intel’s hyper-threaded processors,”     in Proceedings of the International Symposium on Code Generation and     Optimization (CGO ’04). Los Alamitos, CA, USA: IEEE Computer     Society, Mar. 2004, pp. 27–38.  [2] C. Jung, D. Lim, J. Lee, and D. Solihin, “Helper thread prefetching for     loosely-coupled multiprocessor systems,” in Parallel and Distributed Pro-     cessing Symposium, 2006. IPDPS 2006. 20th International, April 2006.  [3] C. G. Quiñones, C. Madriles, J. Sánchez, P. Marcuello, A. González,     and D. M. Tullsen, “Mitosis compiler: An infrastructure for speculative     threading based on pre-computation slices,” in Proceedings of the 2005     ACM SIGPLAN Conference on Programming Language Design and     Implementation, ser. PLDI ’05. New York, NY, USA: ACM, 2005, pp.     269–279.  [4] J. Mars, L. Tang, and M. L. Soffa, “Directly characterizing cross     core interference through contention synthesis,” in Proceedings of the     6th International Conference on High Performance and Embedded     Architectures and Compilers, ser. HiPEAC ’11. New York, NY, USA:     ACM, 2011, pp. 167–176.  [5] Q. Zeng, D. Wu, and P. Liu, “Cruiser: Concurrent heap buffer overflow     monitoring using lock-free data structures,” in Proceedings of the 32Nd     ACM SIGPLAN Conference on Programming Language Design and     Implementation, ser. PLDI ’11. New York, NY, USA: ACM, 2011, pp.     367–377.  [6] D. G. Feitelson and L. Rudolph, “Distributed hierarchical control for     parallel processing,” Computer, vol. 23, no. 5, pp. 65–77, May 1990.  [7] J. Ousterhout, “Scheduling techniques for concurrent systems,” in     Proceedings of the 3rd International Conference on Distributed Computing     Systems (ICDCS ’82). Los Alamitos, CA, USA: IEEE Computer Society,     Oct. 1982, pp. 22–30.""",not_technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,8557,1.0,0.631578947368421,0,0.06896551724137931,0.9195402298850575,0.0,0.0
133,399771,400133,"How can you set a shared variable with no synchronization ? A bool is particularly dangerous here, at least on some arches.","  On 09/17/2018 12:20 AM, mk.singh@oracle.com wrote:  How can you set a shared variable with no synchronization ?  A bool is particularly dangerous here, at least on some arches. ",not_technical,Eric Dumazet,eric.dumazet@gmail.com,1,0,123,0.3380281690140845,0.3333333333333333,0,0.0,0.9714285714285714,0.0,0.0
134,399771,401526,"If rtnl_trylock() can not grab RTNL,there is no way the current thread can set the  variable without a race, if the word including rtnl_needed is shared by other fields in the structure. Your patch adds a subtle possibility of future bugs, even if it runs fine today. Do not pave the way for future bugs, make your code robust, please.","  On 09/17/2018 10:05 PM, Manish Kumar Singh wrote:  If rtnl_trylock() can not grab RTNL,  there is no way the current thread can set the  variable without a race, if the word including rtnl_needed is shared by other fields in the structure.  Your patch adds a subtle possibility of future bugs, even if it runs fine today.  Do not pave the way for future bugs, make your code robust, please.",not_technical,Eric Dumazet,eric.dumazet@gmail.com,1,0,335,1.0,0.6666666666666666,0,0.02857142857142857,0.9428571428571428,0.0,0.14285714285714285
135,402510,413538,"It would be very helpful if you cc all involved people on the cover letter instead of just cc'ing your own pile of email addresses. CC'ed now. This is really not helpful. The cover letter and the change logs should contain a summary of that discussion and a proper justification of the proposed change. Just saying 'sysadmins might want to allow' is not useful at all, it's yet another 'I want a pony' thing. I read through the previous thread and there was a clear request to involves security people into this. Especially those who are deeply involved with hardware side channels. I don't see anyone Cc'ed on the whole series. For the record, I'm not buying the handwavy 'more noise' argument atall. It wants a proper analysis and we need to come up with criteria which PMUs can be exposed at all. All of this want's a proper documentation clearly explaining the risks andscope of these knobs per PMU. Just throwing magic knobs at sys admins and then saying 'its their problem to figure it out' is not acceptable.","Tvrtko,  On Wed, 19 Sep 2018, Tvrtko Ursulin wrote:  It would be very helpful if you cc all involved people on the cover letter instead of just cc'ing your own pile of email addresses. CC'ed now.   This is really not helpful. The cover letter and the change logs should contain a summary of that discussion and a proper justification of the proposed change. Just saying 'sysadmins might want to allow' is not useful at all, it's yet another 'I want a pony' thing.  I read through the previous thread and there was a clear request to involve security people into this. Especially those who are deeply involved with hardware side channels. I don't see anyone Cc'ed on the whole series.  For the record, I'm not buying the handwavy 'more noise' argument at all. It wants a proper analysis and we need to come up with criteria which PMUs can be exposed at all.  All of this want's a proper documentation clearly explaining the risks and scope of these knobs per PMU. Just throwing magic knobs at sysadmins and then saying 'its their problem to figure it out' is not acceptable.  Thanks,  	tglx",not_technical,Thomas Gleixner,tglx@linutronix.de,1,0,1015,0.5816618911174785,0.23684210526315788,0,0.5333333333333333,0.4,0.0,0.0
136,402510,413650,"Hi,I accept it was by bad to miss adding Cc's on the cover letter, but my own email addresses hopefully should not bother you. It is simply aquestion of what I have in .gitconfig vs what I forgot to do manually. Okay, for the next round I will expand the cover letter with at leastone concrete example on how it is usable and summarize the discussion a bit.Who would you recommend I add? Because I really don't know..Presumably you see adding fine grained control as diminishing the overall security rather than raising it? Could you explain why? Because incompetent sysadmin will turn it off for some PMU, while without havingthe fine-grained control they wouldn't turn it off globally?This feature was requested by the exact opposite concern, that in order to access the i915 PMU, one has to compromise the security of the entiresystem by allowing access to *all* PMU's. Making this ability fine-grained sounds like a logical solution for solving this weakening of security controls. Concrete example was that on video transcoding farms users want to monitor the utilization of GPU engines (like CPU cores) and they can do that via the i915 PMU. But for that to work today they have to dial downthe global perf_event_paranoid setting. Obvious improvement was to allowthem to only dial down the i915.perf_event_paranoid setting. As such, for this specific use case at least, the security is increased. Regards,Tvrtko"," Hi,  On 28/09/2018 11:26, Thomas Gleixner wrote:  I accept it was by bad to miss adding Cc's on the cover letter, but my  own email addresses hopefully should not bother you. It is simply a  question of what I have in .gitconfig vs what I forgot to do manually.   Okay, for the next round I will expand the cover letter with at least  one concrete example on how it is usable and summarize the discussion a bit.   Who would you recommend I add? Because I really don't know..   Presumably you see adding fine grained control as diminishing the  overall security rather than raising it? Could you explain why? Because  incompetent sysadmin will turn it off for some PMU, while without having  the fine-grained control they wouldn't turn it off globally?  This feature was requested by the exact opposite concern, that in order  to access the i915 PMU, one has to compromise the security of the entire  system by allowing access to *all* PMU's.  Making this ability fine-grained sounds like a logical solution for  solving this weakening of security controls.  Concrete example was that on video transcoding farms users want to  monitor the utilization of GPU engines (like CPU cores) and they can do  that via the i915 PMU. But for that to work today they have to dial down  the global perf_event_paranoid setting. Obvious improvement was to allow  them to only dial down the i915.perf_event_paranoid setting. As such,  for this specific use case at least, the security is increased.  Regards,  Tvrtko",not_technical,Tvrtko Ursulin,tvrtko.ursulin@linux.intel.com,1,0,1417,0.7679083094555874,0.2631578947368421,0,0.6,0.4,0.0,0.0
137,402510,413698,"The keyword in the above sentence is 'just'. You can add as many of yours as you want as long as everybody else is cc'ed. Sure, and because you don't know you didn't bother to ask around and ignored the review request. I already added Kees and Jann. Please look for the SECCOMP folks inMAINTAINERS. I did not say at all that this might be diminishing security. And the argumentation with 'incompetent sysadmins' is just the wrong attitude. What I was asking for is proper documentation and this proper documentation is meant for _competent_ sysadmins.That documentation has to clearly describe what kind of information is accessible and what potential side effects security wise this mighthave. You cannot expect that even competent sysadmins know offhand whatwhich PMU might expose. And telling them 'Use Google' is just not the right thing to do. If you can't explain and document it, then providing the knob is justfulfilling somebodys 'I want a pony' request. Sure, and this wants to be documented in the cover letter and the changelogs.But this does also require a proper analysis and documentation why it is not a security risk to expose the i915 PMU or what potential securityissues this can create, so that the competent sysadmin can make a judgement.And the same is required for all other PMUs which can be enabled in the same way for unprivileged access. And we might as well come to the conclusion via analysis that for some PMUs unpriviledged access is just nota good idea and exclude them. I surely know a few which qualify for exclusion, so the right approach is to provide this knob only when the risk is analyzed and documented and the PMU has been flagged as candidate for unpriviledged exposure. I.e. opt in and not opt out.","Tvrtko,  On Fri, 28 Sep 2018, Tvrtko Ursulin wrote:  The keyword in the above sentence is 'just'. You can add as many of yours as you want as long as everybody else is cc'ed.   Sure, and because you don't know you didn't bother to ask around and ignored the review request.  I already added Kees and Jann. Please look for the SECCOMP folks in MAINTAINERS.   I did not say at all that this might be diminishing security. And the argumentation with 'incompetent sysadmins' is just the wrong attitude.  What I was asking for is proper documentation and this proper documentation is meant for _competent_ sysadmins.  That documentation has to clearly describe what kind of information is accessible and what potential side effects security wise this might have. You cannot expect that even competent sysadmins know offhand what which PMU might expose. And telling them 'Use Google' is just not the right thing to do.  If you can't explain and document it, then providing the knob is just fulfilling somebodys 'I want a pony' request.   Sure, and this wants to be documented in the cover letter and the changelogs.  But this does also require a proper analysis and documentation why it is not a security risk to expose the i915 PMU or what potential security issues this can create, so that the competent sysadmin can make a judgement.  And the same is required for all other PMUs which can be enabled in the same way for unprivileged access. And we might as well come to the conclusion via analysis that for some PMUs unpriviledged access is just not a good idea and exclude them. I surely know a few which qualify for exclusion, so the right approach is to provide this knob only when the risk is analyzed and documented and the PMU has been flagged as candidate for unpriviledged exposure. I.e. opt in and not opt out.  Thanks,  	tglx",not_technical,Thomas Gleixner,tglx@linutronix.de,1,0,1742,0.9426934097421203,0.2894736842105263,0,0.6,0.4,0.0,0.0
138,402510,413768,"Sure, but you also used the word ""pile"" and I would argue that made therest of your sentence, after and including ""instead"", sound like it notonly bothers you I forgot to Cc people on the cover letter, but it alsobothers you I included a ""pile"" of my own addresses. If that wasn't yourintention in the slightest then I apologise for misreading it.No, not because of that. You are assuming my actions and motivations and constructing a story.""did not bother"" = negative connotations""ignored"" = negative connotations Note instead the time lapse between this and previous posting of theseries, and if you want to assume something, assume things can getmissed and forgotten without intent or malice.Thanks! Wrong attitude what? I was trying to guess your reasoning (cues in""presumably"" and a lot of question marks) since it wasn't clear to mewhy is your position what it is.I did not mention Google. Well it's not a pony, it is mechanism to avoid having to turn off allsecurity. We can hopefully discuss it without ponies.I am happy to work on the mechanics of achieving this once the securityguys and all PMU owners get involved. Even though I am not convinced thebar to allow fine-grained control should be evaluating all possiblePMUs*, but if the security folks agree that is the case it is fine by me.Regards, Tvrtko*) The part of my reply you did not quote explains how the fine-grained control improves security in existing deployments. The documentation I added refers to the existing perf_event_paranoid documentation for explanation of security concerns involved. Which is not much in itself. But essentially we both have a PMU and a knob already. I don't see why adding the same knob per-PMU needs much more stringent criteria to be accepted. But as said, that's for security people to decide."," On 28/09/2018 15:02, Thomas Gleixner wrote:  Sure, but you also used the word pile"" and I would argue that made the  rest of your sentence, after and including ""instead"", sound like it not  only bothers you I forgot to Cc people on the cover letter, but it also  bothers you I included a ""pile"" of my own addresses. If that wasn't your  intention in the slightest then I apologise for misreading it.   No, not because of that. You are assuming my actions and motivations and  constructing a story.  ""did not bother"" = negative connotations ""ignored"" = negative connotations  Note instead the time lapse between this and previous posting of the  series, and if you want to assume something, assume things can get  missed and forgotten without intent or malice.   Thanks!   Wrong attitude what? I was trying to guess your reasoning (cues in  ""presumably"" and a lot of question marks) since it wasn't clear to me  why is your position what it is.   I did not mention Google.   Well it's not a pony, it is mechanism to avoid having to turn off all  security. We can hopefully discuss it without ponies.   I am happy to work on the mechanics of achieving this once the security  guys and all PMU owners get involved. Even though I am not convinced the  bar to allow fine-grained control should be evaluating all possible  PMUs*, but if the security folks agree that is the case it is fine by me.  Regards,  Tvrtko  *) The part of my reply you did not quote explains how the fine-grained  control improves security in existing deployments. The documentation I  added refers to the existing perf_event_paranoid documentation for  explanation of security concerns involved. Which is not much in itself.  But essentially we both have a PMU and a knob already. I don't see why  adding the same knob per-PMU needs much more stringent criteria to be  accepted. But as said, that's for security people to decide.""",not_technical,Tvrtko Ursulin,tvrtko.ursulin@linux.intel.com,1,0,1799,1.0,0.3157894736842105,0,0.6,0.4,0.0,0.0
139,402510,413778,"Guessing my reasonings has nothing to do with you mentioning incompentent sysadmins.I did not say that you mentioned google. But what is a sysadmin supposed to do when there is no documentation aside of using google? And not having documentation is basically the same thing as telling them to use google.If you want to make a pettifogger contest out of this discussion, then we can stop right here. I explained it technically why just adding a knobwithout further explanation and analysis is not acceptable. Making the knob opt in per PMU does not need all PMU owners to beinvolved. It allows to add the opt in flag on a case by case basis. The fact, that the existing knob is poorly documented does make an excuse for adding more knobs without documentation. Quite the contrary, if we notice that the existing knob lacks proper documentation, then we should fix that first.","Tvrtko,  On Fri, 28 Sep 2018, Tvrtko Ursulin wrote:  Guessing my reasonings has nothing to do with you mentioning incompentent sysadmins.   I did not say that you mentioned google. But what is a sysadmin supposed to do when there is no documentation aside of using google? And not having documentation is basically the same thing as telling them to use google.   If you want to make a pettifogger contest out of this discussion, then we can stop right here. I explained it technically why just adding a knob without further explanation and analysis is not acceptable.   Making the knob opt in per PMU does not need all PMU owners to be involved. It allows to add the opt in flag on a case by case basis.   The fact, that the existing knob is poorly documented does make an excuse for adding more knobs without documentation. Quite the contrary, if we notice that the existing knob lacks proper documentation, then we should fix that first.  Thanks,  	tglx",not_technical,Thomas Gleixner,tglx@linutronix.de,1,0,874,0.4670487106017192,0.3684210526315789,0,0.6,0.4,0.0,0.0
140,402510,414000,"Ah only if google could simply answer all our questions! It's not like there is or isn't a security risk and that you can say that it is or it isn't in a global way.Essentially these are channels of information. The channels always exist in form of timing variances for any shared resource (like shared caches or shared memory/IO/interconnect bandwidth) that can be measured. Perfmon counters make the channels generally less noisy, but they do not cause them.To really close them completely you would need to avoid sharing anything, or not allowing to measure time, neither of which is practical short of an air gap.There are reasonable assesments you can make either way and the answers will be different based on your requirements. There isn't a single answer that works for everyone.There are cases where it isn't a problem at all.If you don't have multiple users on the system your tolerance should be extremely high. For users who have multiple users there can be different tradeoffs. So there isn't a single answer, and that is why it is important that this if configurable."," Ah only if google could simply answer all our questions!   It's not like there is or isn't a security risk and that you can say that it is or it isn't in a global way.  Essentially these are channels of information. The channels always exist in form of timing variances for any shared resource (like shared caches or shared memory/IO/interconnect bandwidth) that can be measured.  Perfmon counters make the channels generally less noisy, but they do not cause them.  To really close them completely you would need to avoid sharing anything, or not allowing to measure time, neither of which is practical short of an air gap.  There are reasonable assesments you can make either way and the answers will be different based on your requirements. There isn't a single answer that works for everyone.   There are cases where it isn't a problem at all.  If you don't have multiple users on the system your tolerance should be extremely high.  For users who have multiple users there can be different tradeoffs.  So there isn't a single answer, and that is why it is important that this if configurable.  -Andi",not_technical,Andi Kleen,ak@linux.intel.com,0,0,1081,0.5816618911174785,0.5263157894736842,0,0.6,0.3333333333333333,0.0,0.0
141,402510,414266,"I said clearly that I'm not opposed against making it configurable. But because there is no single answer, it's even more important to have proper documentation. And that's all I'm asking for aside of making it opt-in instead of a wholesale expose everything approach.","On Fri, 28 Sep 2018, Andi Kleen wrote:  I said clearly that I'm not opposed against making it configurable. But because there is no single answer, it's even more important to have proper documentation. And that's all I'm asking for aside of making it opt-in instead of a wholesale expose everything approach.  Thanks,  	tglx",not_technical,Thomas Gleixner,tglx@linutronix.de,1,0,268,0.1489971346704871,0.7105263157894737,0,0.6,0.3333333333333333,0.0,0.0
142,410867,414489,"Even though the return type of ndo_start_xmit is netdev_tx_t, negative error codes arestill allowed I believe. Look, reviewing these are pretty stressful for me, because you aren't documenting yourchanges and in many cases the transformations look incorrect. I'm tossing the rest of your changes in this area for now, sorry. Please double check your work and resubmit this at some time in the not-too-near future. Thank you.","From: YueHaibing <yuehaibing@huawei.com> Date: Wed, 26 Sep 2018 17:27:05 +0800   Even though the return type of ndo_start_xmit is netdev_tx_t, negative error codes are still allowed I believe.  Look, reviewing these are pretty stressful for me, because you aren't documenting your changes and in many cases the transformations look incorrect.  I'm tossing the rest of your changes in this area for now, sorry.  Please double check your work and resubmit this at some time in the not-too-near future.  Thank you.",not_technical,David Miller,davem@davemloft.net,1,0,424,1.0,0.6666666666666666,0,1.0,0.0,1.0,0.0
143,422694,423291,"The way I see it, it is pretty well marked up as is. So, this paragraph is not describing the change.What is not ""proper"" about the existing comment? Yes yes, I *know* that GCC is not very intelligent about it and requires hand-holding, bu tblaming the existing comment for not *properly* marking an intentional fall through is ... rich.Adding some more context here. Considering the above added context, I have to say that this mindlesschange is not an improvement, as you have just destroyed the continued sentence from the previous comment. You must have noticed that this was the end of a continued sentence, as you even quoted it in the commit message. The big question is why you did not stop to think and consider the context? Yes, I'm annoyed by mindless changes. Especially mindless changes aimed at improving readability while in fact making things less readable.TL,DR, if you are desperate to fix ""the problem"" with this fall through comment, please do so in a way that preserves overall readability. And it would be nice to not blame the existing code for brain damage in GCC and various other static analyzers.","On 2018-10-08 19:35, Gustavo A. R. Silva wrote:    The way I see it, it is pretty well marked up as is. So, this paragraph  is not describing the change.      What is not proper"" about the existing comment? Yes yes, I *know* that  GCC is not very intelligent about it and requires hand-holding, but  blaming the existing comment for not *properly* marking an intentional  fall through is ... rich.      Adding some more context here.    		case IIO_VAL_INT:  			/*  			 * Convert integer scale to fractional scale by  			 * setting the denominator (val2) to one...    Considering the above added context, I have to say that this mindless  change is not an improvement, as you have just destroyed the continued  sentence from the previous comment. You must have noticed that this  was the end of a continued sentence, as you even quoted it in the commit  message. The big question is why you did not stop to think and consider  the context?    Yes, I'm annoyed by mindless changes. Especially mindless changes aimed  at improving readability while in fact making things less readable.    TL,DR, if you are desperate to fix ""the problem"" with this fall through  comment, please do so in a way that preserves overall readability. And  it would be nice to not blame the existing code for brain damage in GCC  and various other static analyzers.    Cheers,  Peter  """,not_technical,Peter Rosin,peda@axentia.se,1,0,1123,1.0,0.18181818181818182,0,0.0,1.0,0.0,0.0
144,422694,428645,"I still object. It would have been so damn easy and it does not take a wholelot of imagination to quiet down GCC while keeping the comments readable. Just move the ""and"" to the previous comment, like this.IO_VAL_FRACTIONAL","On 2018-10-13 14:38, Jonathan Cameron wrote:    I still object. It would have been so damn easy and it does not take a whole  lot of imagination to quiet down GCC while keeping the comments readable. Just  move the and"" to the previous comment, like this.    		case IIO_VAL_INT:  			/*  			 * Convert integer scale to fractional scale by  			 * setting the denominator (val2) to one, and...  			 */  			*val2 = 1,  			ret = IIO_VAL_FRACTIONAL,  			/* fall through */  		case IIO_VAL_FRACTIONAL:    Or add a sentence, like this (which is a bit more fun IMO)    		case IIO_VAL_INT:  			/*  			 * Convert integer scale to fractional scale by  			 * setting the denominator (val2) to one...  			 */  			*val2 = 1,  			ret = IIO_VAL_FRACTIONAL,  			/* ...and fall through. Say it again for GCC. */  			/* fall through */  		case IIO_VAL_FRACTIONAL:    Cheers,  Peter  """,not_technical,Peter Rosin,peda@axentia.se,1,0,222,0.19369369369369369,0.5454545454545454,0,0.5,0.375,0.0,0.0
145,424089,424518,"Which CPU architecture?  Most important architectures appear to define__HAVE_ARCH_MEMCMP.What the heck does __visible do?This is going to do bad things if the incoming addresses aren't suitably aligned.Certainly, byte-at-a-time is a pretty lame implementation when the addresses are suitably aligned.  A fallback to the lame version whenthere is misalignment will be simple to do.  And presumably there will be decent benefits to whoever is actually using this code.  But I'm wondering who is actually using this code!","On Tue,  9 Oct 2018 16:28:11 +0200 Jack Wang <xjtuwjp@gmail.com> wrote:   Which CPU architecture?  Most important architectures appear to define __HAVE_ARCH_MEMCMP.   What the heck does __visible do?   This is going to do bad things if the incoming addresses aren't suitably aligned.  Certainly, byte-at-a-time is a pretty lame implementation when the addresses are suitably aligned.  A fallback to the lame version when there is misalignment will be simple to do.  And presumably there will be decent benefits to whoever is actually using this code.  But I'm wondering who is actually using this code!",not_technical,Andrew Morton,akpm@linux-foundation.org,1,0,518,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
146,467091,471421,"Hi Boris The major feature close to USB is this one and it can be found in others protocols (standardization process).Just to close this topic I3C vs USB, IMO it's wrong to pass the message that the I3C is closer to USB than I2C even more because I3C support theI2C on the fly. Sorry, with the proliferation of sensors I cannot see a multi mastersensor network based on USB. Yes, we already talked about secondary master support. I would bet to do something like in i2c, we don't need the same level of complexity found in USB.I agree with the controller folder but not with prefix. Please check what is already in the kernel. In this case and taking what is already in the kernel it will bedrivers/i3c/{master, slave, dwc, other with the same architecture as dwc}.I miss to mention PCI but since the beginning refer the slave and the common part. Splitting the driver is something that soon or later I will have to do.If you prefer later I'm ok with that.I think this discussion is starting to be counterproductive with arguing of both parts. Unfortunately I don't see anyone given their inputs too.To be clear, the subsystem is nice and I working with daily. As I said this is something that I dealing now and I'm telling what I think that is not correct.","Hi Boris  On 26/11/18 21:33, Boris Brezillon wrote:   The major feature close to USB is this one and it can be found in others  protocols (standardization process).  Just to close this topic I3C vs USB, IMO it's wrong to pass the message  that the I3C is closer to USB than I2C even more because I3C support the  I2C on the fly.     Sorry, with the proliferation of sensors I cannot see a multi master  sensor network based on USB.     Yes, we already talked about secondary master support.     I would bet to do something like in i2c, we don't need the same level of  complexity found in USB.     I agree with the controller folder but not with prefix. Please check  what is already in the kernel.     In this case and taking what is already in the kernel it will be  drivers/i3c/{master, slave, dwc, other with the same architecture as dwc}.     I miss to mention PCI but since the beginning refer the slave and the  common part.  Splitting the driver is something that soon or later I will have to do.  If you prefer later I'm ok with that.   I think this discussion is starting to be counterproductive with arguing  of both parts. Unfortunately I don't see anyone given their inputs too.  To be clear, the subsystem is nice and I working with daily. As I said  this is something that I dealing now and I'm telling what I think that  is not correct.",not_technical,vitor,vitor.soares@synopsys.com,1,0,1257,0.5019607843137255,0.8235294117647058,0,0.36363636363636365,0.5454545454545454,0.0,0.0
147,467091,471458,"I think you didn't read my reply carefully. I'm not saying I3C == USB, I'm just saying that the way you interact with an I3C from a SW PoV isnot at all the same as you would do for an I2C device. Do you deny that? Looks like there's a misunderstanding here. The question is not whether I3C will replace I2C or USB, of course it's meant to overcome the limitations of I2C. I'm just pointing out that, if we have to expose I3C devices, we should look at what other discoverable buses do (PCI,USB, ...), not what I2C does.There's a difference between a secondary master that waits for its time to become the currrent master, and a secondary master that provides I3C device features when it's acting as a slave (sensor, GPIOcontroller, ...). So far we focused on supporting the former. If there's a need for the latter, then we should start thinking about the slave framework...Can you detail a bit more what you have in mind? I don't think we can do like I2C, simply because we need to expose a valid DCR +manuf-ID/PID so that other masters can bind the device to the appropriate driver on their side. Plus, if we're about to expose generic profiles, we likely don't want each I3C slave controller driver to do that on its own.If we mix everything in the same subdir, I'd like to have an easy way to quickly identify those that are slave controllers and those that are master controllers. For the dual-role thing, maybe we can consider them as master (ones with advances slave features). Would you be okay with this..., so that you can have all designware drivers (for both slave and masterblocks) in the same dir? For those that are placed directly under this...(because they only have one .c file), I'd like to keep a standard prefix.And again, I'm questioning the necessity of per-IP directories at theroot level. I'm not against per-IP directories, as long as they are classified like other HW blocks...No it's not vain, it's how we do discuss things in the community. I'm not saying I'm always right, but I need to understand the problems you're trying to solve to take a decision, and I don't think you initially gave all the details I needed to understand your PoV. That's a bit clearer now, even if I still disagree on a few aspects.They will come. Come on! All I've seen so far are complaints on tiny details, it definitely doesn't prevent you from adding new features.","Hi Vitor,  On Tue, 27 Nov 2018 11:50:53 +0000 vitor <vitor.soares@synopsys.com> wrote:   I think you didn't read my reply carefully. I'm not saying I3C == USB, I'm just saying that the way you interact with an I3C from a SW PoV is not at all the same as you would do for an I2C device. Do you deny that?   Looks like there's a misunderstanding here. The question is not whether I3C will replace I2C or USB, of course it's meant to overcome the limitations of I2C. I'm just pointing out that, if we have to expose I3C devices, we should look at what other discoverable buses do (PCI, USB, ...), not what I2C does.   There's a difference between a secondary master that waits for its time to become the currrent master, and a secondary master that provides I3C device features when it's acting as a slave (sensor, GPIO controller, ...). So far we focused on supporting the former. If there's a need for the latter, then we should start thinking about the slave framework...   Can you detail a bit more what you have in mind? I don't think we can do like I2C, simply because we need to expose a valid DCR + manuf-ID/PID so that other masters can bind the device to the appropriate driver on their side. Plus, if we're about to expose generic profiles, we likely don't want each I3C slave controller driver to do that on its own.   If we mix everything in the same subdir, I'd like to have an easy way to quickly identify those that are slave controllers and those that are master controllers. For the dual-role thing, maybe we can consider them as master (ones with advances slave features).  Would you be okay with drivers/i3c/controllers/{designware,dw}/..., so that you can have all designware drivers (for both slave and master blocks) in the same dir?  For those that are placed directly under drivers/i3c/controllers/... (because they only have one .c file), I'd like to keep a standard prefix.   And again, I'm questioning the necessity of per-IP directories at the root level. I'm not against per-IP directories, as long as they are classified like other HW blocks: drivers/i3c/{master,slave}/<ip>/...   No it's not vain, it's how we do discuss things in the community. I'm not saying I'm always right, but I need to understand the problems you're trying to solve to take a decision, and I don't think you initially gave all the details I needed to understand your PoV. That's a bit clearer now, even if I still disagree on a few aspects.   They will come.   Come on! All I've seen so far are complaints on tiny details, it definitely doesn't prevent you from adding new features.  Regards,  Boris",not_technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,2375,1.0,0.8823529411764706,0,0.36363636363636365,0.5454545454545454,0.0,0.5454545454545454
148,467091,479249,"If you want. Actually that's the most interesting part for me:  discussing how we want to support I3C slave controllers or mixedmaster/slave controllers. All the driver split we're talking abouthere is just bikeshedding. Ok.I don't see why. If the driver is simple enough to fit in one file,there's no reason to create a new subdir. You think your DW IP is so complex and configurable that it requires several source files, fine,but please don't force others to do the same.Yes. You mean, inside a sub-folder? It depends what you do with those source files. If they are to be exposed directly as modules, then they should be prefixed. On the other hand, if you create a single module out of several source files, source files don't need to be prefixed, as long as the resulting module as a proper prefix.I'm not saying the discussion is useless, just that it's happening waytoo early compared to the other things we should work on. If you were adding support for slaves, and were doing this split as part of this patch series explaining that part of the code between slave and master can be shared, then we wouldn't have this debate. But right now, you'retelling me that we need to split the DW driver to prepare for features that have not even been discussed/proposed. That's what I'm complaining about.","On Tue, 4 Dec 2018 00:34:20 +0000 vitor <vitor.soares@synopsys.com> wrote:     If you want. Actually that's the most interesting part for me: discussing how we want to support I3C slave controllers or mixed master/slave controllers. All the driver split we're talking about here is just bikeshedding.   Ok.   I don't see why. If the driver is simple enough to fit in one file, there's no reason to create a new subdir. You think your DW IP is so complex and configurable that it requires several source files, fine, but please don't force others to do the same.   Yes.   You mean, inside a sub-folder (drivers/i3c/controllers/{vendor}/)? It depends what you do with those source files. If they are to be exposed directly as modules, then they should be prefixed (i3c-<role>-<vendor>.c). On the other hand, if you create a single module out of several source files, source files don't need to be prefixed, as long as the resulting module as a proper prefix.   I'm not saying the discussion is useless, just that it's happening way too early compared to the other things we should work on. If you were adding support for slaves, and were doing this split as part of this patch series explaining that part of the code between slave and master can be shared, then we wouldn't have this debate. But right now, you're telling me that we need to split the DW driver to prepare for features that have not even been discussed/proposed. That's what I'm complaining about.",not_technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,1304,0.5196078431372549,1.0,1,1.0,0.0,0.0,0.0
149,476403,477115,So I strongly disagree with this. Anybody that has trouble with 0/1 vsfalse/true needs to stay the heck away from C.I would suggest we delete that stupid coccinelle scripts that generates these pointless warns.,"On Sat, Dec 01, 2018 at 12:37:01PM -0800, Paul E. McKenney wrote:  So I strongly disagree with this. Anybody that has trouble with 0/1 vs false/true needs to stay the heck away from C.  I would suggest we delete that stupid coccinelle scripts that generates these pointless warns.",not_technical,Peter Zijlstra,peterz@infradead.org,1,0,210,0.3829787234042553,0.375,0,1.0,0.0,0.5,0.0
150,476403,477136,Not to mention that WARN is gramatically incorrect. We're not assigning 'bool' to 0/1 but the other way around. What crap..,"On Mon, Dec 03, 2018 at 09:35:00AM +0100, Peter Zijlstra wrote:  Not to mention that WARN is gramatically incorrect. We're not assigning 'bool' to 0/1 but the other way around.  What crap.. ",not_technical,Peter Zijlstra,peterz@infradead.org,1,0,123,0.2765957446808511,0.5,0,1.0,0.0,0.0,0.0
151,476403,478120,"Then those tools are broken per the C spec.The C language spec, specifies _Bool as an integer type wide enough to at least store 0 and 1.IOW, 0 and 1 are perfectly valid value to assign to a _Bool.And fundamentally that has to be so. That's how computers work. 0 is false, 1 is true.The kernel is not the place to try and abstract such stuff, C is our portable assembler. We muck with hardware, we'd better know how the heck it works.","On Mon, Dec 03, 2018 at 10:20:42AM +0100, Julia Lawall wrote:  Then those tools are broken per the C spec.   The C language spec, specifies _Bool as an integer type wide enough to at least store 0 and 1.  IOW, 0 and 1 are perfectly valid valus to assign to a _Bool.  And fundamentally that has to be so. That's how computers work. 0 is false, 1 is true.  The kernel is not the place to try and abstract such stuff, C is our portable assembler. We muck with hardware, we'd better know how the heck it works.",not_technical,Peter Zijlstra,peterz@infradead.org,1,0,434,1.0,0.875,0,1.0,0.0,0.0,0.0
152,482005,482038,"Note that this patch does *not* remove the nasty trap caused by the garbage in question - struct file can be freed before we even return from->unlocked_ioctl().  Could you describe in details the desired behaviourof this interface? How about grabbing the references to all victims (*before* screwing with this), sticking them into a structure with embedded callback on it, the callback doing those fput()? The callback would trigger before the return to userland, so observable timing of the final close wouldn't be changed.  And it would avoid the kludges like this.Of course, the proper fix would require TARDIS and set of instruments for treating severe case of retrocranial inversion, so that this ""ABI"" would've never existed, but...","On Wed, Dec 05, 2018 at 01:16:01PM -0800, Todd Kjos wrote:  Note that this patch does *not* remove the nasty trap caused by the garbage in question - struct file can be freed before we even return from ->unlocked_ioctl().  Could you describe in details the desired behaviour of this interface?  How about grabbing the references to all victims (*before* screwing with ksys_close()), sticking them into a structure with embedded callback_head and using task_work_add() on it, the callback doing those fput()?  The callback would trigger before the return to userland, so observable timing of the final close wouldn't be changed.  And it would avoid the kludges like this.  Of course, the proper fix would require TARDIS and set of instruments for treating severe case of retrocranial inversion, so that this ABI"" would've never existed, but...""",not_technical,Al Viro,viro@zeniv.linux.org.uk,1,0,738,1.0,0.4,0,0.0,0.0,0.0,0.0
153,498594,573986,What's advertisement there? Huch? Care to tell what's a lie instead of making bold statements?,"On Mon, 11 Mar 2019, Pavel Machek wrote:  What's advertisement there?   Huch? Care to tell what's a lie instead of making bold statements?  Thanks,  	tglx",not_technical,Thomas Gleixner,tglx@linutronix.de,1,1,94,0.05167958656330749,0.45454545454545453,0,0.32270916334661354,0.6772908366533864,0.0,0.0
154,498594,573990,"No problem here, no performance issues, nothing to be seen unless youare running VM. Take a care to look at the patch I submitted? Lie: A system with an up to date kernel is protected against attacks from # malicious user space applications.3GB system running 32bit kernel is not protected. Same is true for for really big 64 bit systems.If I do what he suggests, this becomes untrue. The Linux kernel contains a mitigation for this attack vector, PTE# inversion, which is permanently enabled and has no performance# impact.Limiting memory to 2GB _is_ going to have severe perfomance impact. Ok, I guess L1TF was a lot of fun, and there was not time for a good documentation.There's admin guide that is written as an advertisment, and unfortunately is slightly ""inaccurate"" at places (to the point of lying).Plus, I believe it should go to x86/ directory, as this is really Intel issue, and not anything ARM (or RISC-V) people need to know.","On Mon 2019-03-11 14:05:07, Thomas Gleixner wrote:  No problem here, no performance issues, nothing to be seen unless you are running VM.""   Take a care to look at the patch I submitted?  Lie:  # A system with an up to date kernel is protected against attacks from # malicious user space applications.  3GB system running 32bit kernel is not protected. Same is true for for really big 64bit systems.  If I do what dmesg suggests, this becomes untrue:  # The Linux kernel contains a mitigation for this attack vector, PTE # inversion, which is permanently enabled and has no performance # impact.  Limiting memory to 2GB _is_ going to have severe perfomance impact.  								Pavel  commit 9664b4dabdb132433a6843aefe05814953f1342f Author: Pavel <pavel@ucw.cz> Date:   Thu Jan 3 00:48:40 2019 +0100      Ok, I guess L1TF was a lot of fun, and there was not time for a good     documentation.          There's admin guide that is written as an advertisment, and     unfortunately is slightly ""inaccurate"" at places (to the point of     lying).          Plus, I believe it should go to x86/ directory, as this is really     Intel issue, and not anything ARM (or RISC-V) people need to know.          Signed-off-by: Pavel Machek <pavel@ucw.cz>  diff --git a/Documentation/admin-guide/l1tf.rst b/Documentation/admin-guide/l1tf.rst index 9af9773..05c5422 100644 --- a/Documentation/admin-guide/l1tf.rst +++ b/Documentation/admin-guide/l1tf.rst @@ -1,10 +1,11 @@  L1TF - L1 Terminal Fault  ========================   -L1 Terminal Fault is a hardware vulnerability which allows unprivileged -speculative access to data which is available in the Level 1 Data Cache -when the page table entry controlling the virtual address, which is used -for the access, has the Present bit cleared or other reserved bits set. +L1 Terminal Fault is a hardware vulnerability on most recent Intel x86 +CPUs which allows unprivileged speculative access to data which is +available in the Level 1 Data Cache when the page table entry +controlling the virtual address, which is used for the access, has the +Present bit cleared or other reserved bits set.    Affected processors  ------------------- @@ -76,12 +77,14 @@ Attack scenarios     deterministic and more practical.       The Linux kernel contains a mitigation for this attack vector, PTE -   inversion, which is permanently enabled and has no performance -   impact. The kernel ensures that the address bits of PTEs, which are not -   marked present, never point to cacheable physical memory space. - -   A system with an up to date kernel is protected against attacks from -   malicious user space applications. +   inversion, which is permanently enabled and has no measurable +   performance impact in most configurations. The kernel ensures that +   the address bits of PTEs, which are not marked present, never point +   to cacheable physical memory space. On x86-32, this physical memory +   needs to be limited to 2GiB to make mitigation effective. + +   Mitigation is present in kernels v4.19 and newer, and in +   recent -stable kernels.    2. Malicious guest in a virtual machine  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html """,not_technical,Pavel Machek,pavel@ucw.cz,1,0,940,0.4883720930232558,0.5454545454545454,0,0.32270916334661354,0.6772908366533864,0.0,0.0
155,498594,574882,"I agree that this statement is incorrect. Calling this a lie is a completly unjustified personal attack on those who spent quite a lot of time on writing up documentation in the first place. It's suggesting that this document was written with malicious intent  and the purpose of deceiving someone. Care to explain why you are assuming this to be the case? Sure. That still does not justify the ""changelog"" you provided.It's interesting that quite some people were actually happy about that document. Sorry, that we weren't able to live up to your high standards.What is the advertisement part again? It's a document targeted at system administrators and it definitely should not be buried somewhere in Documentation/x86. As there are more documents being worked on for the other issues, I have a patch ready which moves that stuff into a separate hardware vulnerabilites folder in the admin-guide.FWIW, to the best of my knowledge the documentation about writing changelogs is neither incorrect nor is it optional to adhere to it.The 'Affected processors' section right below this is very clear about this being an Intel only issue (for now). So what exactly is the point of this change? On x86-32? That's incorrect, because there are a lot of x86-32 systems which are not affected. Also it has nothing to do with the bit-width of the hardware. A 32bit kernel booted on a 64bit capable CPU has the same issue. For further correctness, this needs to mention that !PAE enabled kernels cannot do PTE inversion at all.The 2G limitation is not a general limitation. The limitation depends on the number of physical address bits supported by the cache (not the number of physical addresss bits exposed as pins) and is definitely not hardcoded to 2G. Just because your machine emits the 2G number does not make it universally correct. On a system with 36bit physical address space the limit is 32G and on some CPUs that's actually wrong as well, see this. Quoting yourself: Where is the explanation for the 'really big 64bit systems' issue for correctness sake?","Pavel,  On Mon, 11 Mar 2019, Pavel Machek wrote:  I agree that this statement is incorrect.  Calling this a lie is a completly unjustified personal attack on those who spent quite a lot of time on writing up documentation in the first place. It's suggesting that this document was written with malicious intent and the purpose of deceiving someone. Care to explain why you are assuming this to be the case?   Sure. That still does not justify the changelog"" you provided.   It's interesting that quite some people were actually happy about that document. Sorry, that we weren't able to live up to your high standards.   What is the advertisement part again?   It's a document targeted at system administrators and it definitely should not be burried somewhere in Documentation/x86. As there are more documents being worked on for the other issues, I have a patch ready which moves that stuff into a separate hardware vulnerabilites folder in the admin-guide.  FWIW, to the best of my knowledge the documentation about writing changelogs is neither incorrect nor is it optional to adhere to it.   The 'Affected processors' section right below this is very clear about this being an Intel only issue (for now). So what exactly is the point of this change?   On x86-32? That's incorrect, because there are a lot of x86-32 systems which are not affected. Also it has nothing to do with the bit-width of the hardware. A 32bit kernel booted on a 64bit capable CPU has the same issue. For further correctness, this needs to mention that !PAE enabled kernels cannot do PTE inversion at all.   The 2G limitation is not a general limitation. The limitation depends on the number of physical address bits supported by the cache (not the number of physical addresss bits exposed as pins) and is definitely not hardcoded to 2G. Just because your machine emits the 2G number does not make it universally correct. On a system with 36bit physical address space the limit is 32G and on some CPUs that's actually wrong as well, see: override_cache_bits().  Quoting yourself:   Where is the explanation for the 'really big 64bit systems' issue for correctness sake?  Thanks,  	tglx""",not_technical,Thomas Gleixner,tglx@linutronix.de,1,1,2055,1.0,0.7272727272727273,0,0.32270916334661354,0.6733067729083665,0.0,0.0
156,498594,576302,"So how should it be called? I initally used less strong words, only to get ""Care to tell what's a lie instead of making bold statements?"" back. Also look at the timing of the thread.Ok, now can we have that document updated to meet the standards?Making it very clear from the begining this is x86-only issue. Yes, you can kind-of figure it out from the next section... except for Intel StrongArm.Next sentence speaks about ""present bit"" of ""page table entry"". That may be confusing for people familiar with other architectures, which may not have such bit. We should mention this is x86 before usingx86-specific terminology.Ok.I don't know the detailed limits for each system, what about this?","On Mon 2019-03-11 23:31:08, Thomas Gleixner wrote:  So how should it be called? I initally used less strong words, only to get Care to tell what's a lie instead of making bold statements?"" back. Also look at the timing of the thread.   Ok, now can we have that document updated to meet the standards?   Making it very clear from the begining this is x86-only issue. Yes, you can kind-of figure it out from the next section... except for Intel StrongArm.  Next sentence speaks about ""present bit"" of ""page table entry"". That may be confusing for people familiar with other architectures, which may not have such bit. We should mention this is x86 before using x86-specific terminology.   Ok.   I don't know the detailed limits for each system, what about this?  Signed-off-by: Pavel Machek <pavel@ucw.cz>  									Pavel  diff --git a/Documentation/admin-guide/l1tf.rst b/Documentation/admin-guide/l1tf.rst index 9af9773..cbf02a4 100644 --- a/Documentation/admin-guide/l1tf.rst +++ b/Documentation/admin-guide/l1tf.rst @@ -1,10 +1,11 @@  L1TF - L1 Terminal Fault  ========================   -L1 Terminal Fault is a hardware vulnerability which allows unprivileged -speculative access to data which is available in the Level 1 Data Cache -when the page table entry controlling the virtual address, which is used -for the access, has the Present bit cleared or other reserved bits set. +L1 Terminal Fault is a hardware vulnerability on most recent Intel x86 +CPUs which allows unprivileged speculative access to data which is +available in the Level 1 Data Cache when the page table entry +controlling the virtual address, which is used for the access, has the +Present bit cleared or other reserved bits set.    Affected processors  ------------------- @@ -76,12 +77,15 @@ Attack scenarios     deterministic and more practical.       The Linux kernel contains a mitigation for this attack vector, PTE -   inversion, which is permanently enabled and has no performance -   impact. The kernel ensures that the address bits of PTEs, which are not -   marked present, never point to cacheable physical memory space. - -   A system with an up to date kernel is protected against attacks from -   malicious user space applications. +   inversion, which has no measurable performance impact in most +   configurations. The kernel ensures that the address bits of PTEs, +   which are not marked present, never point to cacheable physical +   memory space. For mitigation to be effective, physical memory needs +   to be limited in some configurations. + +   Mitigation is present in kernels v4.19 and newer, and in +   recent -stable kernels. PAE needs to be enabled for mitigation to +   work.    2. Malicious guest in a virtual machine  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html """,not_technical,Pavel Machek,pavel@ucw.cz,1,0,693,0.35917312661498707,0.8181818181818182,0,0.32669322709163345,0.6733067729083665,0.0,0.04780876494023904
157,498594,590551,"You called it a lie from the very beginning or what do you think made me tell you that? Here is what you said: Nice try.What is 'the standards'? Your's or is there a general agreement?It's pretty clear, but yes admittedly we forgot to mention that Intel StrongARM is not affected. That's truly important because its widely deployed in the cloud space and elsewhere. X86 terminology? Care to check how pte_present() is implemented across the architectures? Most of them use the PRESENT bit naming convention, just afew use VALID. That's truly confusing and truly x86 specific.It's not about detailed limits for particular systems. It's about the way the limit is determined on certain class of systems. And that can be deduced from the code.If you want to provide more accurate documentation then you better come upwith something which is helpful instead of completely useless blurb likethe below: How is the admin going to figure that out? What kind of systems might be affected by this? No. The mitigation is available when the kernel provides it. Numbers are irrelevant because that documentation has to be applicable for stable kernels as well. And what is a recent - stable kernel?Also the PAE part needs to go to a completely different section.","Pavel,  On Tue, 12 Mar 2019, Pavel Machek wrote:  You called it a lie from the very beginning or what do you think made me tell you that? Here is what you said:   Nice try.   What is 'the standards'? Your's or is there a general agreement?   It's pretty clear, but yes admittedly we forgot to mention that Intel StrongARM is not affected. That's truly important because its widely deployed in the cloud space and elsewhere.   X86 terminology? Care to check how pte_present() is implemented across the architectures? Most of them use the PRESENT bit naming convention, just a few use VALID. That's truly confusing and truly x86 specific.   It's not about detailed limits for particular systems. It's about the way the limit is determined on certain class of systems. And that can be deduced from the code.  If you want to provide more accurate documentation then you better come up with something which is helpful instead of completely useless blurb like the below:   How is the admin going to figure that out? What kind of systems might be affected by this?   No. The mitigation is available when the kernel provides it. Numbers are irrelevant because that documentation has to be applicable for stable kernels as well. And what is a recent -stable kernel?  Also the PAE part needs to go to a completely different section.  Thanks,  	tglx",not_technical,Thomas Gleixner,tglx@linutronix.de,1,1,1249,0.6227390180878553,0.9090909090909091,0,0.3745019920318725,0.6254980079681275,0.04780876494023904,0.6254980079681275
158,498594,770007,"Hi! Actually, I still call it a lie. Document clearly says that bug is fixed in non-virtualized cases, when in fact it depends on PAE and limited memory.At this point I want you to fix it yourself. Lying about security bugs being fixed when they are not is not cool. I tried to be helpful and submit a patch, but I don't feel like you are cooperating on getting the patch applied.","Hi!   Actually, I still call it a lie. Document clearly says that bug is fixed in non-virtualized cases, when in fact it depends on PAE and limited memory.   At this point I want you to fix it yourself. Lying about security bugs being fixed when they are not is not cool. I tried to be helpful and submit a patch, but I don't feel like you are cooperating on getting the patch applied.   Best regards, 								Pavel   --  DENX Software Engineering GmbH,      Managing Director: Wolfgang Denk HRB 165235 Munich, Office: Kirchenstr.5, D-82194 Groebenzell, Germany ",not_technical,Pavel Machek,pavel@denx.de,1,0,380,0.20930232558139536,1.0,1,1.0,0.0,0.6254980079681275,0.0
159,511533,512783,Thanks for testing this and letting me know.,"On Thu, Jan 10, 2019 at 12:19:36PM +0530, Sabyasachi Gupta wrote:  Again, no.",not_technical,Greg KH,gregkh@linuxfoundation.org,1,0,44,1.0,1.0,1,1.0,0.0,1.0,0.0
160,522705,522783,"I would drop this patch for being too ugly and if nothing else, for lack of users (epoll will no longer need dlock).","I would drop this patch for being too ugly and if nothing else, for lack of users (epoll will no longer need dlock).  Thanks, Davidlohr",not_technical,Davidlohr Bueso,dave@stgolabs.net,1,0,116,1.0,0.25,0,0.0,1.0,0.0,0.0
161,539613,546377,"Since when is the cover letter mandatory? I understand that is helps for a complicated patch set to explain the problem and solution in the cover letter, but for this simple test case addition what's the point? And there is nothing forcing a cover letter in this. Also double tags seams to be quite common for selftest. See this.","On 2/11/19 8:48 AM, Jarkko Sakkinen wrote:  Since when is the cover letter mandatory? I understand that is helps for a complicated patch set to explain the problem and solution in the cover letter, but for this simple test case addition what's the point? And there is nothing forcing a cover letter in https://www.kernel.org/doc/html/v4.20/process/submitting-patches.html  Also double tags seams to be quite common for selftest. See git log tools/testing/selftests/ Thanks, --  Tadeusz",not_technical,Tadeusz Struk,tadeusz.struk@intel.com,0,1,329,1.0,0.8,0,0.7142857142857143,0.14285714285714285,0.0,0.14285714285714285
162,541450,541515,"I'm not sure that forcing a library on users is a good reason to break UAPI.The patch is going into the latest, but can also be backported on future stables.I don't think ""not fixing it because it's not fixed yet"" is a good reason to keep things the way they are. But maybe that's just me. Given that the structure has already been extended several times, there is pretty much nothing to keep this from happening again and again.","  On 2/6/19 1:23 PM, Neil Horman wrote:  I'm not sure that forcing a library on users is a good reason to break UAPI.   The patch is going into the latest, but can also be backported on future stables. I don't think not fixing it because it's not fixed yet"" is a good reason to keep things the way they are. But maybe that's just me. Given that the structure has already been extended several times, there is pretty much nothing to keep this from happening again and again.   --  Julien Gomes""",not_technical,Julien Gomes,julien@arista.com,0,1,429,0.13215859030837004,0.4230769230769231,0,0.0,1.0,0.0,0.0
163,541450,542126,"Thats a misleading statement.  We've never supported running newer applications on older kernels, and no one is forcing anyone to use the lksctp-tools library, I was only suggesting that, if we were to support this compatibility, that might be a place to offer it.Its also worth noting that we have precident for this.  If you look at the gitlog, this particular structure has been extended about 6 times in the life of sctp. Also misleading, as it assumes that we're not intentionally doing this.  I get wanting to support running applications built for newer kernels on older kernels, but thats just not something that we do, and to say thats broken is misleading.  Older applications are required to run on newer kernels, but not vice versa, which is what you are asking for.And yes, this patch can be backported to older stable kernels, but by that same token, so can the patches that extend the struct, which would also fix the problem, while supporting the newer features, which seems to me to be the better solution for applications which are looking for that support.","On Wed, Feb 06, 2019 at 01:48:42PM -0800, Julien Gomes wrote: Thats a misleading statement.  We've never supported running newer applications on older kernels, and no one is forcing anyone to use the lksctp-tools library, I was only suggesting that, if we were to support this compatibility, that might be a place to offer it.  Its also worth noting that we have precident for this.  If you look at the git log, this particular structure has been extended about 6 times in the life of sctp.  Also misleading, as it assumes that we're not intentionally doing this.  I get wanting to support running applications built for newer kernels on older kernels, but thats just not something that we do, and to say thats broken is misleading.  Older applications are required to run on newer kernels, but not vice versa, which is what you are asking for.    And yes, this patch can be backported to older stable kernels, but by that same token, so can the patches that extend the struct, which would also fix the problem, while supporting the newer features, which seems to me to be the better solution for applications which are looking for that support.  ",not_technical,Neil Horman,nhorman@tuxdriver.com,1,0,1075,0.3054331864904552,0.5384615384615384,0,0.0,1.0,0.0,0.0
164,541450,544269,"What a complete mess we have here. Use new socket option numbers next time, do not change the size and/or layout of existing socket options. This whole thread, if you read it, is basically ""if we compatability this way, that breaks, and if we do compatability this other way oh shit this other thing doesn't work."" I think we really need to specifically check for the difference sizes that existed one by one, clear out the part not given by the user, and backport this as far back as possible in a way that in the older kernels we see if the user is actually trying to use the new features and if so error out. Which, btw, is terrible behavior.  Newly compiled apps should work on older kernels if they don't try to use the new features, and if they can the ones that want to try to use the new features should be able to fall back when that feature isn't available in a non-ambiguous and precisely defined way.The fact that the use of the new feature is hidden in the new structure elements is really rotten. This patch, at best, needs some work and definitely a longer and more detailed commit message.","From: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com> Date: Wed, 6 Feb 2019 18:37:54 -0200   What a complete mess we have here.  Use new socket option numbers next time, do not change the size and/or layout of existing socket options.  This whole thread, if you read it, is basically if we compatability this way, that breaks, and if we do compatability this other way oh shit this other thing doesn't work.""  I think we really need to specifically check for the difference sizes that existed one by one, clear out the part not given by the user, and backport this as far back as possible in a way that in the older kernels we see if the user is actually trying to use the new features and if so error out.  Which, btw, is terrible behavior.  Newly compiled apps should work on older kernels if they don't try to use the new features, and if they can the ones that want to try to use the new features should be able to fall back when that feature isn't available in a non-ambiguous and precisely defined way.  The fact that the use of the new feature is hidden in the new structure elements is really rotten.  This patch, at best, needs some work and definitely a longer and more detailed commit message.""",not_technical,David Miller,davem@davemloft.net,1,0,1105,0.33627019089574156,0.7692307692307693,0,0.5,0.5,0.16666666666666666,0.0
165,541450,541516,"There probably is a decent compromise to find between ""not accepting a single additional byte"" and accepting several GB.For example how likely is it that the growth of this structure make it go over a page? I would hope not at all. By choosing a large but decent high limit, I think we can find a future-compatible compromise that doesn't rely on this just for structure trucation decision...","  On 2/6/19 1:39 PM, Neil Horman wrote:  There probably is a decent compromise to find between not accepting a single additional byte"" and accepting several GB. For example how likely is it that the growth of this structure make it go over a page? I would hope not at all.  By choosing a large but decent high limit, I think we can find a future-compatible compromise that doesn't rely on a preliminary getsockopt() just for structure trucation decision...   --  Julien Gomes""",not_technical,Julien Gomes,julien@arista.com,0,1,392,0.11013215859030837,0.46153846153846156,0,0.0,1.0,0.0,0.0
166,541450,543076,"I disagree with this, at least as a unilateral statement.  I would assert that an old program, within the constraints of the issue being discussed here, will run perfectly well, when built and run against a new kernel.At issue is the size of the structure sctp_event_subscribe, and the fact that in several instances over the last few years, its been extended to be larger and encompass more events to subscribe to. Nominally an application will use this structure (roughly) as follows. Assume this code will be built and run against kernel versions A and B, in which: A) has a struct sctp_event_subscribe with a size of 9 bytes B) has a struct sctp_event_subscribe with a size of 10 bytes (due to the addedfield sctp_sender_dry_event)  That gives us 4 cases to handle 1) Application build against kernel A and run on kernel A.  This works fine, the sizes of the struct in question will always match 2) Application is built against kernel A and run on kernel B.  In this case,everything will work because the application passes a buffer of size 9, and the kernel accepts it, because it allows for buffers to be shorter than the current struct sctp_event_subscribe size. The kernel simply operates on the options available in the buffer.  The application is none the wiser, because it has no knoweldge of the new option, nor should it because it was built against kernelA, that never offered that option 3) Application is built against kernel B and run on kernel B.  This works fine for the same reason as (1). 4) Application is built against kernel B and run on kernel A.  This will break because the application is passing a buffer that is larger than what the kernel expects, and rightly so.   The application is passing in a buffer that is incompatible with what the running kernel expects.We could look into ways in which to detect the cases in which this might be'ok', but I don't see why we should bother, because at some point its still an error to pass in an incompatible buffer.  In my mind this is no different than trying to run a program that allocates hugepages on a kernel that doesn't support hugepages (just to make up an example).  Applications built against newer kernel can't expect all the features/semantics/etc to be identical toolder kernels.It shouldn't.  Assuming you have a program built against headers from kernel B(above), if you set a field in the structure that only exists in kernel B, and try to run it on kernel A, you will get an EINVAL return, which is correct behavior because you are attempting to deliver information to the kernel that kernel A (the running kernel) doesn't know about.  Thats correct behavior.I won't disagree about the niceness of versioning, but that ship has sailed.To be clear,  this is situation (1) above, and yeah, running on the kernel you built your application against should always work from a compatibility standpoint.Yes, but this is alawys the case for structures that change.  If you have an application built against kernel (B), and uses structure fields that only exist in that version of the kernel (and not earlier) will fail to compile when built against kernel (A) headers, and thats expected.  This happens with any kernel api that exists in a newer kernel but not an older kernel .Any time you make a system call to the kernel, you have to be prepared to handle the resulting error condition, thats not unexpected.  To assume that a system call will always work is bad programming practice.","On Fri, Feb 08, 2019 at 09:53:03AM +0000, David Laight wrote: I disagree with this, at least as a unilateral statement.  I would assert that an old program, within the constraints of the issue being discussed here, will run perfectly well, when built and run against a new kernel.  At issue is the size of the structure sctp_event_subscribe, and the fact that in several instances over the last few years, its been extended to be larger and encompass more events to subscribe to.  Nominally an application will use this structure (roughly) as follows:  ... struct sctp_event_subscribe events, size_t evsize = sizeof(events),  memset(&events, 0, sizeof(events)),  events.sctp_send_failure_event = 1, /*example event subscription*/  if (sctp_setsocktpt(sctp_fd, SOL_SCTP, SCTP_EVENTS, &events, &evsize) < 0) { 	/* do error recovery */ }  ....   Assume this code will be built and run against kernel versions A and B, in which: A) has a struct sctp_event_subscribe with a size of 9 bytes B) has a struct sctp_event_subscribe with a size of 10 bytes (due to the added field sctp_sender_dry_event)  That gives us 4 cases to handle  1) Application build against kernel A and run on kernel A.  This works fine, the sizes of the struct in question will always match  2) Application is built against kernel A and run on kernel B.  In this case, everything will work because the application passes a buffer of size 9, and the kernel accepts it, because it allows for buffers to be shorter than the current struct sctp_event_subscribe size. The kernel simply operates on the options available in the buffer.  The application is none the wiser, because it has no knoweldge of the new option, nor should it because it was built against kernel A, that never offered that option  3) Application is built against kernel B and run on kernel B.  This works fine for the same reason as (1).  4) Application is built against kernel B and run on kernel A.  This will break because the application is passing a buffer that is larger than what the kernel expects, and rightly so.   The application is passing in a buffer that is incompatible with what the running kernel expects.  We could look into ways in which to detect the cases in which this might be 'ok', but I don't see why we should bother, because at some point its still an error to pass in an incompatible buffer.  In my mind this is no different than trying to run a program that allocates hugepages on a kernel that doesn't support hugepages (just to make up an example).  Applications built against newer kernel can't expect all the features/semantics/etc to be identical to older kernels.  It shouldn't.  Assuming you have a program built against headers from kernel B (above), if you set a field in the structure that only exists in kernel B, and try to run it on kernel A, you will get an EINVAL return, which is correct behavior because you are attempting to deliver information to the kernel that kernel A (the running kernel) doesn't know about.  Thats correct behavior.  I won't disagree about the niceness of versioning, but that ship has sailed.  To be clear,  this is situation (1) above, and yeah, running on the kernel you built your application against should always work from a compatibility standpoint.   Yes, but this is alawys the case for structures that change.  If you have an application built against kernel (B), and uses structure fields that only exist in that version of the kernel (and not earlier) will fail to compile when built against kernel (A) headers, and thats expected.  This happens with any kernel api that exists in a newer kernel but not an older kernel.  Any time you make a system call to the kernel, you have to be prepared to handle the resulting error condition, thats not unexpected.  To assume that a system call will always work is bad programming practice.  Neil ",not_technical,Neil Horman,nhorman@tuxdriver.com,1,0,3469,1.0,0.7307692307692307,0,0.16666666666666666,0.8333333333333334,0.0,0.16666666666666666
167,546823,547238,Looking more flexible does not make it more correct.,"On Tue, 12 Feb 2019, Li, Aubrey wrote:  Looking more flexible does not make it more correct.  Thanks,  	tglx",not_technical,Thomas Gleixner,tglx@linutronix.de,1,0,52,1.0,0.7142857142857143,0,0.0,0.0,0.0,0.0
