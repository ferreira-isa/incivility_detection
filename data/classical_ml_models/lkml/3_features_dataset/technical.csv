,thread_id,email_id,ahlaam_preprocessed_text,original_text,email_classification,author_name,author_email,author_role,is_first_author_thread,nr_characters,ratio_words_email_thread,position_comment_thread,is_last_comment,time_start_to_email,time_email_to_end,time_previous_to_email,time_email_to_next
0,72369,470438,"What do you mean by 1st and 2nd level? Pretty much. (3) is true in the sense that memory is first allocated from hostmem_resource (which is non-dom0 RAM). Not anymore, as far as that particular commit is concerned, but that's because of this commit (Move and shrink AMD 64-bit window to avoid conflict) which was introduced after balloon patch. IIRC there were some issues with it unrelated to balloon. The concern is that in principle nothing prevents someone else to do exact same thing commit did, which is grab something from right above end of RAM as the kernel sees it. And that can be done at any point.","On 11/25/18 8:00 PM, Igor Druzhinin wrote:  What do you mean by 1st and 2nd level?     Pretty much. (3) is true in the sense that memory is first allocated from hostmem_resource (which is non-dom0 RAM).    Not anymore, as far as that particular commit is concerned, but that's because of 03a551734 (x86/PCI: Move and shrink AMD 64-bit window to avoid conflict"") which was introduced after balloon patch. IIRC there were some issues with fa564ad96366unrelated to balloon.    The concern is that in principle nothing prevents someone else to do exact same thing fa564ad96366 did, which is grab something from right above end of RAM as the kernel sees it. And that can be done at any point.   -boris """,technical,Boris Ostrovsky,boris.ostrovsky@oracle.com,1,1,610,0.5806451612903226,0.5454545454545454,0,0.9970760233918129,0.0,0.0,0.0
1,72369,470664,"Ah, OK. Doesn't additional_memory_resource()->insert_resource(iomem_resource) place the RAM at 1st level? And if not, can we make it so? Since this seems to have broken existing feature this would be an option. But before going that route I'd like to see if we can fix the patch. I have been unable to reproduce your problem. Can you describe what you did? I am not sure I agree that this is plainly wrong. If not for BIOS issues that 03a551734cf mentions I think what the original implementation offa564ad963 did was perfectly reasonable. Which is why I would prefer to keep keep the hostmem resource *if possible*.","On 11/26/18 12:10 PM, Igor Druzhinin wrote:  Ah, OK. Doesn't additional_memory_resource()->insert_resource(iomem_resource) place the RAM at 1st level? And if not, can we make it so?   Since this seems to have broken existing feature this would be an option. But before going that route I'd like to see if we can fix the patch.  I have been unable to reproduce your problem. Can you describe what you did?    I am not sure I agree that this is plainly wrong. If not for BIOS issues that 03a551734cf mentions I think what the original implementation of fa564ad963 did was perfectly reasonable. Which is why I would prefer to keep keep the hostmem resource *if possible*.   -boris",technical,Boris Ostrovsky,boris.ostrovsky@oracle.com,1,1,616,0.5898617511520737,0.7272727272727273,0,0.9970760233918129,0.0,0.0,0.0
2,72369,470674,"That'd mean splitting ""Unusable memory"" resource. Since it's allocated from bootmem it has proven to be quite difficult but there are seem to be special functions available particularly for memory resource management operations that I've not yet experimented with. So the answer is probably - maybe yes but not straightforward. It doesn't happen on all configurations as sometimes the memory is successfully hot plugged to a hole depending on the size of Dom0 memory. But we reproduced it quite reliably with small Dom0 sizes like 752MB. XenServer is using this feature to hot plug additional memory for grantable operations so we started a VM and observed a stable hang. Exactly, those *are* BIOS issues and are not supposed to be workarounded by the OS. And as the next commit showed even the workaround didn't quite helped with it. I agree that having hotmem as a precaution is fine but only if there is a non-cringy way to keep things working with it which I'm not sure does exist.","On 26/11/2018 19:42, Boris Ostrovsky wrote:  That'd mean splitting Unusable memory"" resource. Since it's allocated from bootmem it has proven to be quite difficult but there are seem to be special functions available particularly for memory resource management operations that I've not yet experimented with. So the answer is probably - maybe yes but not straightforward.   It doesn't happen on all configurations as sometimes the memory is successfully hotplugged to a hole depending on the size of Dom0 memory. But we reproduced it quite reliably with small Dom0 sizes like 752MB.  XenServer is using this feature to hotplug additional memory for grant table operations so we started a VM and observed a stable hang.   Exactly, those *are* BIOS issues and are not supposed to be workarounded by the OS. And as the next commit showed even the workaround didn't quite helped with it.  I agree that having hotmem as a precaution is fine but only if there is a non-cringy way to keep things working with it which I'm not sure does exist.  Igor """,technical,Igor Druzhinin,igor.druzhinin@citrix.com,0,0,985,0.8663594470046083,0.8181818181818182,0,0.9970760233918129,0.0,0.0,0.0
3,72369,471026,"We have most of the interfaces in the resource framework to do what we want. I put together a semi-working prototype but the tricky part is resource locking --- we need to remove a chunk from hostmem (which will cause hostmem to be resized and possibly split), and insert this chunk to iomem's top level as System RAM, all while holding resource_lock.I haven't been able to come up with an acceptable interface for that. Given that we are actually broken I guess I am OK with reverting the patch, but please make sure this works on AMD boxes (I think family 15his what needs to be tested).","On 11/26/18 2:57 PM, Igor Druzhinin wrote:  We have most of the interfaces in the resource framework to do what we want. I put together a semi-working prototype but the tricky part is resource locking --- we need to remove a chunk from hostmem (which will cause hostmem to be resized and possibly split), and insert this chunk to iomem's top level as System RAM, all while holding resource_lock.  I haven't been able to come up with an acceptable interface for that.  Given that we are actually broken I guess I am OK with reverting the patch, but please make sure this works on AMD boxes (I think family 15h is what needs to be tested).  -boris",technical,Boris Ostrovsky,boris.ostrovsky@oracle.com,1,1,589,0.5529953917050692,0.9090909090909091,0,0.9970760233918129,0.0,0.0,0.0
4,72369,471576,"After their last commit I don't see how this can be broken:1) They only claim addresses starting from this*unconditionally* which means if there is some memory behind this range on the host (regardless if it's Dom0 or native Linux) they'll break their own systems. 2) So, theoretically, to trigger the original issue we'd need to have a system with RAM higher than it and that shouldn't be assigned to Dom0 but that contradicts (1).Igor","On 27/11/2018 03:28, Boris Ostrovsky wrote:  After their last commit I don't see how this can be broken: 1) They only claim addresses starting from 0xbd00000000 *unconditionally* which means if there is some memory behind this range on the host (regardless if it's Dom0 or native Linux) they'll break their own systems.  2) So, theoretically, to trigger the original issue we'd need to have a system with RAM higher than 0xbd00000000 and that shouldn't be assigned to Dom0 but that contradicts (1).  Igor",technical,Igor Druzhinin,igor.druzhinin@citrix.com,0,0,436,0.423963133640553,1.0,1,1.0,0.0,0.0,0.0
5,72369,469497,"This commit breaks Xen balloon memory hot plug for us in Dom0 with""hoplug_unpopulated"" set to 1. The issue is that the common kernel memory onlining procedures require ""System RAM"" resource to be 1-stlevel. That means by inserting it under ""Unusable memory"" as the commit above does (intentionally or not) we make it 2-nd level and break mem or yonlining.There are multiple ways to fix it depending on what was the intention of original commit and what exactly it tried to workaround. It seems it does several things at once:1) Marks non-Dom0 host memory ""Unusable memory"" in resource tree.2) Keeps track of all the areas safe for hot plug in Dom03) Changes allocation algorithms itself in balloon driver to use those areas Are all the things above necessary to cover the issue in fa564ad96366(""x86/PCI: Enable a 64bit BAR on AMD Family 15h (Models 00-1f, 30-3f,60-7f)"")? Can we remove ""Unusable memory"" resources as soon as we finished booting? Is removing on-demand is preferable over ""shoot them all"" in that case? Does it even make sense to remove the 1-st level only restriction in kernel/ resource.c ?","On 20/12/2017 14:05, Boris Ostrovsky wrote:  This commit breaks Xen balloon memory hotplug for us in Dom0 with hoplug_unpopulated"" set to 1. The issue is that the common kernel memory onlining procedures require ""System RAM"" resource to be 1-st level. That means by inserting it under ""Unusable memory"" as the commit above does (intentionally or not) we make it 2-nd level and break memory onlining.  There are multiple ways to fix it depending on what was the intention of original commit and what exactly it tried to workaround. It seems it does several things at once: 1) Marks non-Dom0 host memory ""Unusable memory"" in resource tree. 2) Keeps track of all the areas safe for hotplug in Dom0 3) Changes allocation algorithms itself in balloon driver to use those areas  Are all the things above necessary to cover the issue in fa564ad96366 (""x86/PCI: Enable a 64bit BAR on AMD Family 15h (Models 00-1f, 30-3f, 60-7f)"")?  Can we remove ""Unusable memory"" resources as soon as we finished booting? Is removing on-demand is preferable over ""shoot them all"" in that case?  Does it even make sense to remove the 1-st level only restriction in kernel/resource.c ?  Igor""",technical,Igor Druzhinin,igor.druzhinin@citrix.com,0,0,1107,1.0,0.45454545454545453,0,0.9941520467836257,0.0029239766081871343,0.9824561403508771,0.0
6,135037,152994,"Forget it 0-day bot, and assorted crickets :) With stock knob settings, that's too late to switch from llc -> l2 affinity for sync wakeups, and completely demolished tbench top end on huge socket NUMA box with lots of bandwidth. Lovely for desktop, somewhere below gawd-awful for big box performance.","On Mon, 2018-01-01 at 11:37 +0100, Mike Galbraith wrote:  Forget it 0-day bot, and assorted crickets :)   With stock knob settings, that's too late to switch from llc -> l2 affinity for sync wakeups, and completely demolished tbench top end on huge socket NUMA box with lots of bandwidth. Lovely for desktop, somewhere below gawd-awful for big box performance.  	-Mike",technical,Mike Galbraith,efault@gmx.de,0,1,300,1.0,1.0,1,1.0,0.0,1.0,0.0
7,135434,135461,crap. google helpfully disabled this account while I was posting this set. I'll repost can call it v3,crap.  google helpfully disabled this account while I was posting this set.  I'll repost can call it v3,technical,Bryan O'Donoghue,pure.logic@nexus-software.ie,1,1,101,1.0,1.0,1,0.0,0.0,0.0,0.0
8,143665,144213,Please change it to read the hardware directly and not use this.,"On 01/02, Amit Nischal wrote:  Please change it to read the hardware directly and not use __clk_is_enabled() or clk_hw_is_prepared().  --  Qualcomm Innovation Center, Inc. is a member of Code Aurora Forum, a Linux Foundation Collaborative Project",technical,Stephen Boyd,sboyd@codeaurora.org,1,0,64,0.03735632183908046,0.8,0,0.0,1.0,0.0,1.0
9,143665,156632,"Thanks for the review the change. Here intention is to know the software status of the RCG instead of HW status and we have intentionally not defined the 'is_enabled'ops for clk_rcg2_shared_ops. This clk_rcg2_shared_ops are only applicable for the RCGs with shared branches across different subsystems. Reason for using the same is mentioned below. When RCG gets enabled by other subsystem (outside the Application processor subsystem):   In this case when RCG gets enabled by branch clock managed by   other subsystem (outside the Application processor subsystem)   and if we check HW status of RCG in clk_rcg2_shared_set_rate()   instead of checking its software status then it will give the status as ENABLED without overlying software knowing its status   and during source switch, update configuration will get fail as   new parent will be in disabled state.   In above scenario, clock framework will not enable the new   parent before configuration update as enable and prepare counts   are zero for RCG clock and clk_set_rate() will follow below path.   clk_rcg2_shared_set_rate()   __clk_set_parent_before()-->New parent will be disabled as prepare count = 0   clk_change_rate()   clk_set_rate()So solution of this problem is as follows and same is explained in the commit text of. If software status of the RCG is disabled(enable/prepare counts are0)    then just cache or store the rate in current_freq variable and if    software status is enabled then follow the normal update procedure.2. Set the rate and switch to new source only inclk_rcg2_shared_enable()    i.e. during RCG enable sequence. This will make sure that required    parents are already in enable state before configuration update and    RCG switch will happen successfully every time. In past, We have encountered similar RCG update configuration failure issues for some display RCGs, where there are two branch clocks, one is controlled by application processor subsystem and another one controlled by other subsystem. So to handle such cases, we need clk_rcg2_shared_ops.","On 2018-01-02 23:43, Stephen Boyd wrote:  Hi Stephen,  Thanks for the review the change.  Here intention is to know the software status of the RCG instead of HW status and we have intentionally not defined the 'is_enabled' ops for clk_rcg2_shared_ops. This clk_rcg2_shared_ops are only applicable for the RCGs with shared branches across different subsystems. Reason for using the same is mentioned below.  When RCG gets enabled by other subsystem (outside the Application processor subsystem):    In this case when RCG gets enabled by branch clock managed by    other subsystem (outside the Application processor subsystem)    and if we check HW status of RCG in clk_rcg2_shared_set_rate()    instead of checking its software status then it will give the    status as ENABLED without overlying software knowing its status    and during source switch, update configuration will get fail as    new parent will be in disabled state.     In above scenario, clock framework will not enable the new    parent before configuration update as enable and prepare counts    are zero for RCG clock and clk_set_rate() will follow below path.     clk_rcg2_shared_set_rate()    __clk_set_parent_before()-->New parent will be disabled as prepare  count = 0    clk_change_rate()    clk_set_rate()  So solution of this problem is as follows and same is explained in the commit text of https://patchwork.kernel.org/patch/10139985/ 1. If software status of the RCG is disabled(enable/prepare counts are  0)     then just cache or store the rate in current_freq variable and if     software status is enabled then follow the normal update procedure.  2. Set the rate and switch to new source only in  clk_rcg2_shared_enable()     i.e. during RCG enable sequence. This will make sure that required     parents are already in enable state before configuration update and     RCG switch will happen successfully every time.  In past, We have encountered similar RCG update configuration failure  issues for some display RCGs, where there are two branch clocks, one is  controlled by application processor subsystem and another one controlled by other  subsystem. So to handle such cases, we need clk_rcg2_shared_ops.",technical,Amit Nischal,anischal@codeaurora.org,0,1,2052,1.0,1.0,1,1.0,0.0,1.0,0.0
10,143688,152862,"Hello, Ideally you would mention the commit description since the id is not yet usptream.  I found it here (its 1 in this series):  .Ideally we could move <asm-generic/io.h> include down to the bottom of the file and not have to do the defines like like this, it seems clumsy to me.  In'cris', 'nios2' and other architectures I can see they have the generic include at the bottom of the file and not need for #define's.I tried that but I get a lot of errors.  Does your patch to asm-generic/io.h cause build issues for those architectures as well?","Hello,  On Tue, Jan 02, 2018 at 04:24:34PM +0800, Greentime Hu wrote:  Ideally you would mention the commit description since the id is not yet usptream.  I found it here (its 1 in this series):    https://github.com/andestech/linux/commit/d25ea659   asm-generic/io.h: move ioremap_nocache/ioremap_uc/ioremap_wc/ioremap_...   Ideally we could move <asm-generic/io.h> include down to the bottom of the file and not have to do the defines like like this, it seems clumsy to me.  In 'cris', 'nios2' and other architectures I can see they have the generic include at the bottom of the file and not need for #define's.  I tried that but I get a lot of errors.  Does your patch to asm-generic/io.h cause build issues for those architectures as well?  -Stafford",technical,Stafford Horne,shorne@gmail.com,1,0,547,1.0,0.9111111111111111,0,1.0,0.0,1.0,0.0
11,143688,152892,I got this email from kbuild test robot. I personally tried arm64/x86before I sent the generic asm io.h patch.I tried openrisc/sparc before I sent these v5 patches.,"Hi, Stafford:  2018-01-03 22:38 GMT+08:00 Stafford Horne <shorne@gmail.com>:  I got this email from kbuild test robot. I personally tried arm64/x86 before I sent the generic asm io.h patch. I tried openrisc/sparc before I sent these v5 patches.  BUILD REGRESSION  tree/branch: https://github.com/0day-ci/linux Greentime-Hu/Andes-nds32-Linux-Kernel/20171220-155937 branch HEAD: 9353e22157b9b69be3a3beea3553b5a105a45516  dt-bindings: timer: Add andestech atcpit100 timer binding doc  Regressions in current branch:  arch/cris/mm/ioremap.c:79:15: note: in expansion of macro 'ioremap_nocache' arch/openrisc/include/asm/io.h:38:29: error: conflicting types for 'ioremap' arch/openrisc/include/asm/io.h:44:29: note: in expansion of macro 'ioremap_nocache' arch/sparc/include/asm/io_32.h:129:15: error: conflicting types for 'ioremap' arch/sparc/include/asm/io_32.h:130:0: warning: ioremap_nocache"" redefined arch/sparc/include/asm/io_32.h:131:0: warning: ""ioremap_wc"" redefined arch/sparc/include/asm/io_32.h:132:0: warning: ""ioremap_wt"" redefined arch/sparc/kernel/ioport.c:124:15: error: conflicting types for 'ioremap' arch/sparc/kernel/ioport.c:131:1: note: in expansion of macro 'EXPORT_SYMBOL' drivers/net/ethernet/faraday/ftmac100.c:205:32: sparse: restricted __le32 degrades to integer drivers/net/ethernet/faraday/ftmac100.c:221:23: sparse: incorrect type in assignment (different base types) drivers/net/ethernet/faraday/ftmac100.c:251:16: sparse: cast to restricted __le32 drivers/net/ethernet/faraday/ftmac100.c:262:23: sparse: invalid assignment: &= drivers/net/ethernet/faraday/ftmac100.c:274:23: sparse: incorrect type in assignment (different base types) drivers/net/ethernet/faraday/ftmac100.c:288:18: warning: cast from pointer to integer of different size [-Wpointer-to-int-cast] drivers/net/ethernet/faraday/ftmac100.c:293:9: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] drivers/net/ethernet/faraday/ftmac100.c:534:23: sparse: incorrect type in assignment (different base types) include/asm-generic/io.h:864:15: error: conflicting types for 'ioremap' include/asm-generic/io.h:865:25: error: conflicting types for 'ioremap_nocache' include/asm-generic/io.h:866:29: note: in expansion of macro 'ioremap_nocache'  Error ids grouped by kconfigs:  recent_errors ├── cris-etrax-100lx_v2_defconfig │   └── arch-cris-mm-ioremap.c:note:in-expansion-of-macro-ioremap_nocache ├── openrisc-or1ksim_defconfig │   ├── arch-openrisc-include-asm-io.h:error:conflicting-types-for-ioremap │   └── arch-openrisc-include-asm-io.h:note:in-expansion-of-macro-ioremap_nocache ├── sparc64-allyesconfig │   ├── drivers-net-ethernet-faraday-ftmac100.c:warning:cast-from-pointer-to-integer-of-different-size │   └── drivers-net-ethernet-faraday-ftmac100.c:warning:cast-to-pointer-from-integer-of-different-size ├── sparc-defconfig │   ├── arch-sparc-include-asm-io_32.h:error:conflicting-types-for-ioremap │   ├── arch-sparc-include-asm-io_32.h:warning:ioremap_nocache-redefined │   ├── arch-sparc-include-asm-io_32.h:warning:ioremap_wc-redefined │   ├── arch-sparc-include-asm-io_32.h:warning:ioremap_wt-redefined │   ├── arch-sparc-kernel-ioport.c:error:conflicting-types-for-ioremap │   └── arch-sparc-kernel-ioport.c:note:in-expansion-of-macro-EXPORT_SYMBOL ├── x86_64-allmodconfig │   ├── drivers-net-ethernet-faraday-ftmac100.c:sparse:cast-to-restricted-__le32 │   ├── drivers-net-ethernet-faraday-ftmac100.c:sparse:incorrect-type-in-assignment-(different-base-types)-expected-unsigned-int-unsigned-rxdes0-got-restrunsigned-int-unsigned-rxdes0 │   ├── drivers-net-ethernet-faraday-ftmac100.c:sparse:incorrect-type-in-assignment-(different-base-types)-expected-unsigned-int-unsigned-rxdes2-got-restrunsigned-int-unsigned-rxdes2 │   ├── drivers-net-ethernet-faraday-ftmac100.c:sparse:incorrect-type-in-assignment-(different-base-types)-expected-unsigned-int-unsigned-txdes2-got-restrunsigned-int-unsigned-txdes2 │   ├── drivers-net-ethernet-faraday-ftmac100.c:sparse:invalid-assignment: │   ├── drivers-net-ethernet-faraday-ftmac100.c:sparse:restricted-__le32-degrades-to-integer │   ├── drivers-net-ethernet-faraday-ftmac100.c:warning:cast-from-pointer-to-integer-of-different-size │   └── drivers-net-ethernet-faraday-ftmac100.c:warning:cast-to-pointer-from-integer-of-different-size └── xtensa-allmodconfig     ├── include-asm-generic-io.h:error:conflicting-types-for-ioremap     ├── include-asm-generic-io.h:error:conflicting-types-for-ioremap_nocache     └── include-asm-generic-io.h:note:in-expansion-of-macro-ioremap_nocache  elapsed time: 359m  configs tested: 128  i386                               tinyconfig i386                   randconfig-x016-201751 i386                   randconfig-x011-201751 i386                   randconfig-x014-201751 i386                   randconfig-x017-201751 i386                   randconfig-x019-201751 i386                   randconfig-x018-201751 i386                   randconfig-x010-201751 i386                   randconfig-x013-201751 i386                   randconfig-x015-201751 i386                   randconfig-x012-201751 i386                     randconfig-n0-201751 x86_64                 randconfig-x003-201751 x86_64                 randconfig-x002-201751 x86_64                 randconfig-x006-201751 x86_64                 randconfig-x007-201751 x86_64                 randconfig-x000-201751 x86_64                 randconfig-x005-201751 x86_64                 randconfig-x004-201751 x86_64                 randconfig-x009-201751 x86_64                 randconfig-x008-201751 x86_64                 randconfig-x001-201751 ia64                              allnoconfig ia64                                defconfig ia64                             alldefconfig i386                   randconfig-i0-12180843 i386                   randconfig-i1-12180843 x86_64                 randconfig-x012-201751 x86_64                 randconfig-x010-201751 x86_64                 randconfig-x011-201751 x86_64                 randconfig-x015-201751 x86_64                 randconfig-x019-201751 x86_64                 randconfig-x014-201751 x86_64                 randconfig-x013-201751 x86_64                 randconfig-x016-201751 x86_64                 randconfig-x017-201751 x86_64                 randconfig-x018-201751 i386                     randconfig-a0-201751 i386                     randconfig-a1-201751 c6x                        evmc6678_defconfig xtensa                       common_defconfig m32r                       m32104ut_defconfig score                      spct6600_defconfig xtensa                          iss_defconfig m32r                         opsput_defconfig m32r                           usrv_defconfig m32r                     mappi3.smp_defconfig nios2                         10m50_defconfig h8300                    h8300h-sim_defconfig cris                 etrax-100lx_v2_defconfig blackfin                  TCM-BF537_defconfig blackfin            BF561-EZKIT-SMP_defconfig blackfin                BF533-EZKIT_defconfig blackfin                BF526-EZBRD_defconfig i386                              allnoconfig i386                                defconfig i386                             alldefconfig i386                     randconfig-s1-201751 i386                     randconfig-s0-201751 mn10300                     asb2364_defconfig openrisc                    or1ksim_defconfig um                           x86_64_defconfig um                             i386_defconfig frv                                 defconfig tile                         tilegx_defconfig i386                             allmodconfig microblaze                      mmu_defconfig microblaze                    nommu_defconfig sh                            titan_defconfig sh                          rsk7269_defconfig sh                  sh7785lcr_32bit_defconfig sh                                allnoconfig i386                   randconfig-x007-201751 i386                   randconfig-x008-201751 i386                   randconfig-x009-201751 i386                   randconfig-x004-201751 i386                   randconfig-x002-201751 i386                   randconfig-x005-201751 i386                   randconfig-x001-201751 i386                   randconfig-x006-201751 i386                   randconfig-x003-201751 i386                   randconfig-x000-201751 m68k                           sun3_defconfig m68k                          multi_defconfig m68k                       m5475evb_defconfig mips                                   jz4740 mips                      malta_kvm_defconfig mips                         64r6el_defconfig mips                           32r2_defconfig mips                              allnoconfig mips                      fuloong2e_defconfig mips                                     txx9 sparc                               defconfig sparc64                           allnoconfig sparc64                             defconfig x86_64                           allmodconfig parisc                        c3000_defconfig parisc                         b180_defconfig parisc                              defconfig alpha                               defconfig parisc                            allnoconfig s390                        default_defconfig arm                         at91_dt_defconfig arm                               allnoconfig arm                           efm32_defconfig arm64                               defconfig arm                        multi_v5_defconfig arm                           sunxi_defconfig arm64                             allnoconfig arm                          exynos_defconfig arm                        shmobile_defconfig arm                        multi_v7_defconfig i386                   randconfig-x072-201751 i386                   randconfig-x078-201751 i386                   randconfig-x071-201751 i386                   randconfig-x077-201751 i386                   randconfig-x070-201751 i386                   randconfig-x074-201751 i386                   randconfig-x073-201751 i386                   randconfig-x079-201751 i386                   randconfig-x076-201751 i386                   randconfig-x075-201751 x86_64                             acpi-redef x86_64                           allyesdebian x86_64                                nfsroot x86_64                                  kexec x86_64                                   rhel x86_64                               rhel-7.2""",technical,Greentime Hu,green.hu@gmail.com,1,1,164,0.25663716814159293,0.9333333333333333,0,1.0,0.0,0.0,0.0
12,143688,153057,"unit address without reg is not valid. Drop the ""@0"".All the memory mapped peripherals should be under at least one simple-bus node. ..","On Tue, Jan 2, 2018 at 2:24 AM, Greentime Hu <green.hu@gmail.com> wrote:  unit address without reg is not valid. Drop the @0"".   All the memory mapped peripherals should be under at least one simple-bus node.   ethernet@... """,technical,Rob Herring,robh+dt@kernel.org,1,0,135,0.25663716814159293,0.9555555555555556,0,1.0,0.0,0.0,0.0
13,143688,156727,I'd like to modify it like this in the next version patch.,"2018-01-04 3:14 GMT+08:00 Rob Herring <robh+dt@kernel.org>:  Hi, Rob:  I'd like to modify it like this in the next version patch.           clock: clk {                  #clock-cells = <0>,                  compatible = fixed-clock"",                  clock-frequency = <30000000>,          },           apb {                  compatible = ""simple-bus"",                  #address-cells = <1>,                  #size-cells = <1>,                  ranges,                   serial0: serial@f0300000 {                          compatible = ""andestech,uart16550"", ""ns16550a"",                          reg = <0xf0300000 0x1000>,                          interrupts = <8>,                          clock-frequency = <14745600>,                          reg-shift = <2>,                          reg-offset = <32>,                          no-loopback-test = <1>,                  },                   timer0: timer@f0400000 {                          compatible = ""andestech,atcpit100"",                          reg = <0xf0400000 0x1000>,                          interrupts = <2>,                          clocks = <&clock>,                          clock-names = ""PCLK"",                  },          },           ahb {                  compatible = ""simple-bus"",                  #address-cells = <1>,                  #size-cells = <1>,                  ranges,                   L2: cache-controller@e0500000 {                          compatible = ""andestech,atl2c"",                          reg = <0xe0500000 0x1000>,                          cache-unified,                          cache-level = <2>,                  },                   mac0: ethernet@e0100000 {                          compatible = ""andestech,atmac100"",                          reg = <0xe0100000 0x1000>,                          interrupts = <18>,                 },         },""",technical,Greentime Hu,green.hu@gmail.com,1,1,58,0.12389380530973451,1.0,1,1.0,0.0,0.0,0.0
14,152625,156614,"HI, If necessary to handle these, symlink might help here i believe. Upon trying to understand memory-barriers.txt, i felt that it might be better to have it in PDF/HTML format, thus attempted to convert it to rst. And i see it not being welcomed, hence shelving the conversion.","Hi,  On Thu, Jan 04, 2018 at 09:48:50AM +0800, Boqun Feng wrote:    If necessary to handle these, symlink might help here i believe.  Upon trying to understand memory-barriers.txt, i felt that it might be better to have it in PDF/HTML format, thus attempted to convert it to rst. And i see it not being welcomed, hence shelving the conversion.  afzal",technical,afzal mohammed,afzal.mohd.ma@gmail.com,0,1,278,0.11594202898550725,0.625,0,0.0,0.5,0.0,0.0
15,152625,157783,"Yes, I understand that some of us have a (reasonable) doubt about the reST markup.  It is not perfect in any matter, e.g. I don't like the ``monospace`` markup.  But this is my home opinion.    My hope is, that those of us who have a doubt give reST    a chance ... it is a compromise, not as bad as you might    think first ... your cooperation and your criticism is    needed and welcome. Please let me invite you / read on: There are other plain-text markups e.g. AsciiDoc or Markdown. The reST markup and the Sphinx-builder is a compromise from a evaluation in 2016 (see linux-doc ML subject ""muddying the waters"" [1]). Jani wrote an article about the evaluation and it results [2]. And there are other articles documenting all the various aspects.- A report from the documentation maintainer [3]- Kernel documentation with Sphinx, part 2: how it works [4]- Kernel documentation update [5]To summarize it with my words: The old DocBook-based toolchain was hard to maintain and of course who want's to write XML? A consistent plain-text markup for articles in /Documentation/* and source-code comments (kernel-doc)was needed. The markup: IMO reST wins that race, because it has a extendable markup specification while others plain-text markups like Markdown have various (HTML) builders with various markup dialects[7] (which is more a mess than a definition).The builder: IMO Sphinx-Doc wins that race, since it is (well) maintained, widely used and has a interface for extensions.I.e. the one extension we wrote: 'kernel-doc' to parse kernel-doc comments from source code and include them in the articles. Perspective: Sphinx-Doc also offers solutions we might use in the future (e.g. building man-pages). Not to end in a mess, extensions should be implemented cautiously and deliberately (be patient). But that should not fool you, yes we have known problems with our toolchain and it is not yet ,) perfect in any matter (e.g. the highlighting in kernel-doc comments or the PDF generation or the sphinx-doc versions shipped with various distributions or ..) Anyway, today we have more than before: The reST learning curve is(compared to DocBook) not hard for newbees and our toolchain is flexible for all the requirements which might come up in the future. IMO the actual challenge is the content and the organization of the doc-tree and for this That's exactly what I mean: give reST a chance :)","[...]   Yes, I understand that some of us have a (reasonable) doubt about the reST markup.  It is not perfect in any matter, e.g. I don't like the ``monospace`` markup.  But this is my home opinion.      My hope is, that those of us who have a doubt give reST     a chance ... it is a compromise, not as bad as you might     think first ... your cooperation and your criticism is     needed and welcome. Please let me invite you / read on:  There are other plain-text markups e.g. AsciiDoc or Markdown. The reST markup and the Sphinx-builder is a compromise from a evaluation in 2016 (see linux-doc ML subject muddying the waters"" [1]).  Jani wrote an article about the evaluation and it results [2]. And there are other articles documenting all the various aspects.  - A report from the documentation maintainer [3] - Kernel documentation with Sphinx, part 2: how it works [4] - Kernel documentation update [5]  To summarize it with my words:  The old DocBook-based toolchain was hard to maintain and of course who want's to write XML? A consistent plain-text markup for articles in /Documentation/* and source-code comments (kernel-doc) was needed.  The markup: IMO reST wins that race, because it has a extendable markup specification while others plain-text markups like Markdown have various (HTML) builders with various markup dialects[7] (which is more a mess than a definition).  The builder: IMO Sphinx-Doc wins that race, since it is (well) maintained, widely used and has a interface for extensions. I.e. the one extension we wrote: 'kernel-doc' to parse kernel-doc comments from source code and include them in the articles. Perspective: Sphinx-Doc also offers solutions we might use in the future (e.g. building man-pages). Not to end in a mess, extensions should be implemented cautiously and deliberately (be patient).  But that should not fool you, yes we have known problems with our toolchain and it is not yet ,) perfect in any matter (e.g. the highlighting in kernel-doc comments or the PDF generation or the sphinx-doc versions shipped with various distributions or ..)   Anyway, today we have more than before: The reST learning curve is (compared to DocBook) not hard for newbees and our toolchain is flexible for all the requirements wich might come up in the future.  IMO the actual challenge is the content and the organization of the doc-tree and for this   [1] https://www.mail-archive.com/search?q=muddying+the+waters&l=linux-doc%40vger.kernel.org [2] https://lwn.net/Articles/692704/ [3] https://lwn.net/Articles/704613/ [4] https://lwn.net/Articles/692705/ [5] https://lwn.net/Articles/705224/ [6] http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html [7] http://pandoc.org/MANUAL.html#markdown-variants   Thats exactly what I mean: give reST a chance :)  -- Markus --""",technical,Markus Heiser,markus.heiser@darmarit.de,0,0,2402,1.0,1.0,1,1.0,0.0,0.0,0.0
16,152625,156607,"Hi, Okay, the outcome is exactly as was feared. Abandoning the patch, let this be >","Hi,  On Thu, Jan 04, 2018 at 12:48:28AM +0100, Peter Zijlstra wrote:    Okay, the outcome is exactly as was feared.  Abondoning the patch, let this be > /dev/null  afzal",technical,afzal mohammed,afzal.mohd.ma@gmail.com,0,1,83,0.041407867494824016,0.5,0,0.0,0.5,0.0,0.0
17,157450,160256,"Hello, I don't think we want to fully guarantee the current behavior.  On the scheduler side, I don't think it's likely to change but blkio side*might* change.  Can you please collect the root behavior in a separate section and clearly note that the behaviors are subject to change?","Hello,  On Thu, Jan 04, 2018 at 10:57:00PM +0100, Maciej S. Szmigiero wrote:  I don't think we want to fully guarantee the current behavior.  On the scheduler side, I don't think it's likely to change but blkio side *might* change.  Can you please collect the root behavior in a separte section and clearly note that the behaviors are subject to change?  Thanks.  --  tejun",technical,Tejun Heo,tj@kernel.org,1,0,282,1.0,0.6666666666666666,0,1.0,0.0,1.0,0.0
18,157450,160628,"Hello, Will do.","Hello,  On 08.01.2018 13:15, Tejun Heo wrote:  Will do.   Maciej",technical,Maciej S. Szmigiero,mail@maciej.szmigiero.name,0,1,15,0.0847457627118644,1.0,1,1.0,0.0,0.0,0.0
19,159136,159192,Seems like kallsyms would be one to absolutely scan... it shouldn't cause hangs either.,"On Fri, Jan 5, 2018 at 2:59 PM, Tobin C. Harding <me@tobin.cc> wrote:  Seems like kallsyms would be one to absolutely scan... it shouldn't cause hangs either.  -Kees     --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,0,87,0.2,0.6666666666666666,0,0.0,0.0,0.0,0.0
20,159136,159235,Haven't we fixed kallsyms now? Do you mean that we should be checking to see if the scanned kernel has been patched to include the kallsysms fixes in 4.14? If so perhaps we should add functionality to just check the first line for an address and warn if one is found. No real reason to include ever address in kallsyms in the output. Script doesn't hang but it times out with the default timer (10 seconds).,"On Fri, Jan 05, 2018 at 04:11:07PM -0800, Kees Cook wrote:  Haven't we fixed kallsyms now? Do you mean that we should be checking to see if the scanned kernel has been patched to include the kallsysms fixes in 4.14? If so perhaps we should add functionality to just check the first line for an address and warn if one is found. No real reason to include ever address in kallsyms in the output.  Script doesn't hang but it times out with the default timer (10 seconds).   thanks, Tobin.",technical,Tobin C. Harding,me@tobin.cc,1,1,407,1.0,1.0,1,0.0,0.0,0.0,0.0
21,159308,177062,"(versions of patches 1,2 and 4 have been queued by Catalin)(Nit 'ACPI / APEI:' is the normal subject prefix for ghes.c, this helps the maintainers know which patches they need to pay attention to when you are touching multiple trees) This reads as if this patch is handling SError RAS notifications generated by a CPU with the RAS extensions. These are about CPU->Software notifications. APEIand GHES are a firmware first mechanism which is Software->Software. Reading the v8.2 documents won't help anyone with the APEI/GHES code. Please describe this from the ACPI view, ""ACPI 6.x adds support for NOTIFY_SEIas a GHES notification mechanism... "",  its up to the arch code to spot a v8.2RAS Error based on the cpu caps. There are problems with doing this: How do SEA and SEI interact? As far as I can see they can both interrupt each other, which isn't something the single in_nmi() path in APEI can handle. I thinks we should fix this first.[..] SEA gets away with a lot of things because its synchronous. SEI isn't. Xie XiuQi pointed to the memory_failure_queue() code. We can use this directly from SEA, but not SEI. (what happens if an SError arrives while we are queueing memory_failure work from an IRQ). The one that scares me is the trace-point reporting stuff. What happens if an SError arrives while we are enabling a trace point? (these are static-keys right?)  I don't think we can just plumb SEI in like this and be done with it.  (I'm looking at teasing out the estatus cache code from being x86:NMI only.  This way we solve the same 'cant do this from NMI context' with the same  code'.)I will post what I've got for this estatus-cache thing as an RFC, its not ready to be considered yet.external modules? You mean called by the arch code when it gets this NOTIFY_SEI?","Hi Dongjiu Geng,  (versions of patches 1,2 and 4 have been queued by Catalin)  (Nit 'ACPI / APEI:' is the normal subject prefix for ghes.c, this helps the maintainers know which patches they need to pay attention to when you are touching multiple trees)  On 06/01/18 16:02, Dongjiu Geng wrote:   This reads as if this patch is handling SError RAS notifications generated by a CPU with the RAS extensions. These are about CPU->Software notifications. APEI and GHES are a firmware first mechanism which is Software->Software. Reading the v8.2 documents won't help anyone with the APEI/GHES code.  Please describe this from the ACPI view, ACPI 6.x adds support for NOTIFY_SEI as a GHES notification mechanism... "",  its up to the arch code to spot a v8.2 RAS Error based on the cpu caps.    There are problems with doing this:  Oct. 18, 2017, 10:26 a.m. James Morse wrote: | How do SEA and SEI interact? | | As far as I can see they can both interrupt each other, which isn't something | the single in_nmi() path in APEI can handle. I thinks we should fix this | first.  [..]  | SEA gets away with a lot of things because its synchronous. SEI isn't. Xie | XiuQi pointed to the memory_failure_queue() code. We can use this directly | from SEA, but not SEI. (what happens if an SError arrives while we are | queueing memory_failure work from an IRQ). | | The one that scares me is the trace-point reporting stuff. What happens if an | SError arrives while we are enabling a trace point? (these are static-keys | right?) | |  I don't think we can just plumb SEI in like this and be done with it. |  (I'm looking at teasing out the estatus cache code from being x86:NMI only. |  This way we solve the same 'cant do this from NMI context' with the same |  code'.)   I will post what I've got for this estatus-cache thing as an RFC, its not ready to be considered yet.    external modules? You mean called by the arch code when it gets this NOTIFY_SEI?   Thanks,  James""",technical,James Morse,james.morse@arm.com,1,0,1783,0.8637413394919169,0.32142857142857145,0,0.16666666666666666,0.8333333333333334,0.16666666666666666,0.0
22,159308,177269,"If your patch can be consider that, this patch can based on your patchset. thanks. yes, called by kernel ARCH code, such as below, I remember I have discussed with you.","Hi James,  On 2018/1/23 3:39, James Morse wrote:  If your patch can be consider that, this patch can based on your patchset. thanks.  yes, called by kernel ARCH code, such as below, I remember I have discussed with you.   asmlinkage void do_serror(struct pt_regs *regs, unsigned int esr)  {  	nmi_enter(),   	if (!ghes_notify_sei()) 		return,     	/* non-RAS errors are not containable */  	if (!arm64_is_ras_serror(esr) || arm64_is_fatal_ras_serror(regs, esr))  		arm64_serror_panic(regs, esr),  	nmi_exit(), }",technical,gengdongjiu,gengdongjiu@huawei.com,0,0,168,0.08775981524249422,0.35714285714285715,0,0.16666666666666666,0.8333333333333334,0.0,0.0
23,159308,177292,"sorry fix a typo. Yes, I know you are dong that. Your serial's patch will consider all above things, right? If your patch can be consider that, this patch can based on your patchset. thanks.","sorry fix a typo.  On 2018/1/23 17:23, gengdongjiu wrote:  Yes, I know you are dong that. Your serial's patch will consider all above things, right? If your patch can be consider that, this patch can based on your patchset. thanks.",technical,gengdongjiu,gengdongjiu@huawei.com,0,0,190,0.10161662817551963,0.39285714285714285,0,0.16666666666666666,0.8333333333333334,0.0,0.0
24,159308,177719,"After this patch user-space can trigger an SError in the guest. If it wants to migrate the guest, how does the pending SError get migrated? I think we need to fix migration first. Andrew Jones suggested using KVM_GET/SET_VCPU_EVENTS:Given KVM uses kvm_inject_vabt() on v8.0 hardware too, we should cover systems without the v8.2 RAS Extensions with the same API. I think this means a bit to read/write whether SError is pending, and another to indicate the ESR should be set/read.CPUs without the v8.2 RAS Extensions can reject pending-SError that had an ESR.user-space can then use the 'for migration' calls to make a 'new' SError pending. Now that the CPU feature bits are queued, I think this can be split up into two separate series for v4.16-rc1, one to tackle NOTIFY_SEI and the associated plumbing. The second for the KVM 'make SError pending' API. Does nothing in the patch that adds the support? This is a bit odd.(oh, its hiding in patch 6...)","Hi Dongjiu Geng,  On 06/01/18 16:02, Dongjiu Geng wrote:  After this patch user-space can trigger an SError in the guest. If it wants to migrate the guest, how does the pending SError get migrated?  I think we need to fix migration first. Andrew Jones suggested using KVM_GET/SET_VCPU_EVENTS: https://www.spinics.net/lists/arm-kernel/msg616846.html  Given KVM uses kvm_inject_vabt() on v8.0 hardware too, we should cover systems without the v8.2 RAS Extensions with the same API. I think this means a bit to read/write whether SError is pending, and another to indicate the ESR should be set/read. CPUs without the v8.2 RAS Extensions can reject pending-SError that had an ESR.  user-space can then use the 'for migration' calls to make a 'new' SError pending.  Now that the cpufeature bits are queued, I think this can be split up into two separate series for v4.16-rc1, one to tackle NOTIFY_SEI and the associated plumbing. The second for the KVM 'make SError pending' API.    Does nothing in the patch that adds the support? This is a bit odd. (oh, its hiding in patch 6...)   Thanks,  James",technical,James Morse,james.morse@arm.com,1,0,953,0.43187066974595845,0.42857142857142855,0,0.17708333333333334,0.8229166666666666,0.0,0.0
25,159308,178350,"Thanks a lot for your review and comments. For the CPUs without the v8.2 RAS Extensions, its ESR is always 0, we only can inject a SError with ESR 0 to guest, cannot set its ESR. About how about to use the KVM_GET/SET_VCPU_EVENTS, I will check the code, and consider your suggestion at the same time. The IOCTL KVM_GET/SET_VCPU_EVENTS has been used by X86. Ok, thanks for your suggestion, will split it. To make this patch simple and small, I add it in patch 6.","Hi James,    Thanks a lot for your review and comments.   For the CPUs without the v8.2 RAS Extensions, its ESR is always 0, we only can inject a SError with ESR 0 to guest, cannot set its ESR. About how about to use the KVM_GET/SET_VCPU_EVENTS, I will check the code, and consider your suggestion at the same time.  The IOCTL KVM_GET/SET_VCPU_EVENTS has been used by X86.   Ok, thanks for your suggestion, will split it.   To make this patch simple and small, I add it in patch 6.",technical,gengdongjiu,gengdongjiu@huawei.com,0,0,461,0.23094688221709006,0.5,0,0.1875,0.8125,0.010416666666666666,0.0
26,159308,178564,"thanks for the review. yeah, I have seen that. I pick your modification of setting an impdef ESR for Virtual-SError, so add your name, I change it to 'CC'will follow that. Ok, I will adjust that. thanks, I will directly call pend_guest_serror() in this function. Thanks, I will call it","Hi James,   thanks for the review.  On 2018/1/24 3:07, James Morse wrote: yeah, I have seen that.   I pick your modification of setting an impdef ESR for Virtual-SError, so add your name, I change it to 'CC'  will follow that.   Ok, I will adjust that.  thanks, I will directly call pend_guest_serror() in this function.  thanks, I will call pend_guest_serror() in the kvm_arm_set_sei_esr().",technical,gengdongjiu,gengdongjiu@huawei.com,0,0,285,0.14549653579676675,0.5357142857142857,0,0.1875,0.8125,0.0,0.052083333333333336
27,159308,180962,"It's always implementation-defined. On Juno it seems to be always-0, but other systems may behave differently. (Juno may generate another ESR value when I'm not watching it...) Just because we can't control the ESR doesn't mean injecting an SError isn't something user-space may want to do. If we tackle migration of pending-SError first, I think that will give us the API to create a new pending SError with/without an ESR as appropriate.(Not my suggestion, It was Andrew Jones idea.)We would be re-using the struct to have values with slightly different meanings. But for migration the upshot is the same, call KVM_GET_VCPU_EVENTS on one system, and pass the struct to KVM_SET_VCPU_EVENTS on the new system. If we're lucky Qemu may be able to do this in shared x86/arm64 code. But that made the functionality of this patch: A new API to return -EINVAL from the kernel. Swapping the patches round would have avoided this. Regardless, I think this will fold out in a rebase.","Hi gengdongjiu,  On 24/01/18 20:06, gengdongjiu wrote:  0? It's always implementation-defined. On Juno it seems to be always-0, but other systems may behave differently. (Juno may generate another ESR value when I'm not watching it...)  Just because we can't control the ESR doesn't mean injecting an SError isn't something user-space may want to do. If we tackle migration of pending-SError first, I think that will give us the API to create a new pending SError with/without an ESR as appropriate.    (Not my suggestion, It was Andrew Jones idea.)   We would be re-using the struct to have values with slightly different meanings. But for migration the upshot is the same, call KVM_GET_VCPU_EVENTS on one system, and pass the struct to KVM_SET_VCPU_EVENTS on the new system. If we're lucky Qemu may be able to do this in shared x86/arm64 code.    But that made the functionality of this patch: A new API to return -EINVAL from the kernel.  Swapping the patches round would have avoided this. Regardless, I think this will fold out in a rebase.   Thanks,  James",technical,James Morse,james.morse@arm.com,1,0,974,0.44803695150115475,0.5714285714285714,0,0.25,0.75,0.052083333333333336,0.0
28,159308,180980,"Assuming I got it right, yes. It currently makes the race Xie XiuQi spotted worse, which I want to fix too. (details on the cover letter) I'd like to pick these patches onto the end of that series, but first I want to know what NOTIFY_SEI means for any OS. The ACPI spec doesn't say, and because its asynchronous, route-able and mask-able, there are many more corners than NOTFIY_SEA.This thing is a notification using an emulated SError exception. (emulated because physical-SError must be routed to EL3 for firmware-first, and virtual-SError belongs to EL2). Does your firmware emulate SError exactly as the TakeException() pseudo code in the Arm-Arm? Is the emulated SError routed following the routing rules for HCR_EL2.{AMO, TGE}?What does your firmware do when it wants to emulate SError but its masked?(e.g.1: The physical-SError interrupted EL2 and the SPSR shows EL2 had PSTATE. A set. e.g.2: The physical-SError interrupted EL2 but HCR_EL2 indicates the emulated SError should go to EL1. This effectively masks SError.) Answers to these let us determine whether a bug is in the firmware or the kernel. If firmware is expecting the OS to do something special, I'd like to know about it from the beginning! Sure. The phrase 'external modules' usually means the '.ko' files that live in/lib/modules, nothing outside the kernel tree should be doing this stuff.","Hi gengdongjiu,  On 23/01/18 09:23, gengdongjiu wrote:   Assuming I got it right, yes. It currently makes the race Xie XiuQi spotted worse, which I want to fix too. (details on the cover letter)    I'd like to pick these patches onto the end of that series, but first I want to know what NOTIFY_SEI means for any OS. The ACPI spec doesn't say, and because its asynchronous, route-able and mask-able, there are many more corners than NOTFIY_SEA.  This thing is a notification using an emulated SError exception. (emulated because physical-SError must be routed to EL3 for firmware-first, and virtual-SError belongs to EL2).  Does your firmware emulate SError exactly as the TakeException() pseudo code in the Arm-Arm? Is the emulated SError routed following the routing rules for HCR_EL2.{AMO, TGE}? What does your firmware do when it wants to emulate SError but its masked? (e.g.1: The physical-SError interrupted EL2 and the SPSR shows EL2 had PSTATE.A  set.  e.g.2: The physical-SError interrupted EL2 but HCR_EL2 indicates the emulated  SError should go to EL1. This effectively masks SError.)   Answers to these let us determine whether a bug is in the firmware or the kernel. If firmware is expecting the OS to do something special, I'd like to know about it from the beginning!     Sure. The phrase 'external modules' usually means the '.ko' files that live in /lib/modules, nothing outside the kernel tree should be doing this stuff.   Thanks,  James",technical,James Morse,james.morse@arm.com,1,0,1366,0.6212471131639723,0.6071428571428571,0,0.25,0.75,0.0,0.052083333333333336
29,159308,192200,"Thank you for your time to reply me. For the armv8.0 cpu without RAS Extensions, it does not have vsesr_el2, so when guest take a virtual SError, its ESR is 0, can not control the virtual SError's syndrome value, it does not have such registers to control that. Does Juno not have RAS Extension? if yes, I think we can only inject an SError, but can not change its ESR value, because it does not have vsesr_el2. yes, we may need to support user-space injects an SError even through CPU does not have RAS Extension. sure, we should. Last week, I checked the KVM_GET/SET_VCPU_EVENTS IOCTL, it should meet our migration requirements Got it. Thanks for the reminder, I know your meaning. In the x86, the kvm_vcpu_events includes exception/interrupt/nmi/smi. For the RAS, we only involves the exception, so Qemu handling logic is different. Anyway, I will try to share code for the two platform in Qemu. yes, I will, thanks for your kind suggestion.","James,    Thank you for your time to reply me.  On 2018/1/31 3:21, James Morse wrote: For the armv8.0 cpu without RAS Extensions, it does not have vsesr_el2, so when guest take a virtual SError, its ESR is 0, can not control the virtual SError's syndrom value, it does not have such registers to control that.  Does Juno not have RAS Extension? if yes, I think we can only inject an SError, but can not change its ESR value, because it does not have vsesr_el2.  yes, we may need to support user-space injects an SError even through CPU does not have RAS Extension.   sure, we should. Last week, I checked the KVM_GET/SET_VCPU_EVENTS IOCTL, it should meet our migration requirements  Got it.  Thanks for the reminder, I know your meaning. In the x86, the kvm_vcpu_events includes exception/interrupt/nmi/smi. For the RAS, we only involves the exception, so Qemu handling logic is different. Anyway, I will try to share code for the two platform in Qemu.   /* for KVM_GET/SET_VCPU_EVENTS */ struct kvm_vcpu_events { 	struct { 		__u8 injected, 		__u8 nr, 		__u8 has_error_code, 		__u8 pad, 		__u32 error_code, 	} exception, 	struct { 		__u8 injected, 		__u8 nr, 		__u8 soft, 		__u8 shadow, 	} interrupt, 	struct { 		__u8 injected, 		__u8 pending, 		__u8 masked, 		__u8 pad, 	} nmi, 	__u32 sipi_vector, 	__u32 flags, 	struct { 		__u8 smm, 		__u8 pending, 		__u8 smm_inside_nmi, 		__u8 latched_init, 	} smi, 	__u32 reserved[9], },  yes, I will, thanks for your kind suggestion.",technical,gengdongjiu,gengdongjiu@huawei.com,0,0,944,0.4503464203233256,0.6428571428571429,0,0.3020833333333333,0.6979166666666666,0.052083333333333336,0.0
30,159308,192321,", it is.Yes, it is. Currently we does not consider much about the mask status(SPSR).I remember that you ever suggested firmware should reboot if the mask status is set(SPSR), right? I ever suggest our firmware team to evaluate that, but there is no response. I CC ""liu jun""who is our UEFI firmware Architect, if you have firmware requirements, you can raise again. I know your meaning, thanks for raising it again. I will rename 'external modules' to other name. Thanks.","[...]  Ok.   Yes, it is.   Yes, it is.   Currently we does not consider much about the mask status(SPSR). I remember that you ever suggested firmware should reboot if the mask status is set(SPSR), right? I ever suggest our firmware team to evaluate that, but there is no response.  I CC liu jun"" <liujun88@hisilicon.com> who is our UEFI firmware Architect, if you have firmware requirements, you can raise again.   I know your meaning, thanks for raising it again.   I will rename 'external modules' to other name. Thanks. """,technical,gengdongjiu,gengdongjiu@huawei.com,0,0,470,0.24018475750577367,0.6785714285714286,0,0.3020833333333333,0.6979166666666666,0.0,0.041666666666666664
31,159308,196054,"My point was its more nuanced than this: the ARM- ARM'sTake Virtual SErrorException() pseudo-code lets virtual-SError have an implementation defined syndrome. I've never seen Juno generate anything other than '0', but it might do something different on a Thursday. The point? We can't know what a CPU without the RAS extensions puts in there. Why Does this matter? When migrating a pending SError we have to know the difference between 'use this 64bit value', and 'the CPU will generate it'. If I make an SError pending with ESR=0 on a CPU with VSESR, I can't migrated to a system that generates an impdef SError-ESR, because I can't know it will be 0.It's two types of v8.0 CPU, no RAS extensions. I agree, this means we need to be able to tell the difference between 'pending 'and 'pending with this ESR'. Great!","Hi gengdongjiu,  On 05/02/18 06:19, gengdongjiu wrote:    My point was its more nuanced than this: the ARM-ARM's TakeVirtualSErrorException() pseudo-code lets virtual-SError have an implementation defined syndrome. I've never seen Juno generate anything other than '0', but it might do something different on a thursday.  The point? We can't know what a CPU without the RAS extensions puts in there.  Why Does this matter? When migrating a pending SError we have to know the difference between 'use this 64bit value', and 'the CPU will generate it'. If I make an SError pending with ESR=0 on a CPU with VSESR, I can't migrated to a system that generates an impdef SError-ESR, because I can't know it will be 0.    It's two types of v8.0 CPU, no RAS extensions.    I agree, this means we need to be able to tell the difference between 'pending' and 'pending with this ESR'.     Great!   Thanks,  James",technical,James Morse,james.morse@arm.com,1,0,814,0.3903002309468822,0.7142857142857143,0,0.3541666666666667,0.6458333333333334,0.041666666666666664,0.020833333333333332
32,159308,196843,"Thanks for the mail. I checked it, you are right, the virtual SError's syndrome value can be 0 or implementation defined value, not always 0,which is decided by the ""exception.syndrome<24>"".thanks for the clarification. Yes, But it will have a issue, For the target system, before taking the SError, no one can know whether its syndrome value is IMPLEMENTATION DEFINED or architecturally defined. when the virtual SError is taken, the ESR_ELx.IDS will be updated, then we can know whether the ESR value is impdef or architecturally defined. It seems migration is only allowed only when target system and source system all support RAS extension, because we do not know whether its syndrome is IMPLEMENTATION DEFINED or architecturally defined.","Hi James,  Thanks for the mail.  On 2018/2/10 1:44, James Morse wrote: [...]  I checked the aarch64/exceptions/asynch/AArch64.TakeVirtualSErrorException"", you are right, the virtual SError's syndrome value can be 0 or implementation defined value, not always 0, which is decided by the ""exception.syndrome<24>"". thanks for the clarification.  Yes, But it will have a issue, For the target system, before taking the SError, no one can know whether its syndrome value is IMPLEMENTATION DEFINED or architecturally defined.  when the virtual SError is taken, the ESR_ELx.IDS will be updated, then we can know whether the ESR value is impdef or architecturally defined.  It seems migration is only allowed only when target system and source system all support RAS extension, because we do not know whether its syndrome is IMPLEMENTATION DEFINED or architecturally defined. """,technical,gengdongjiu,gengdongjiu@huawei.com,0,0,742,0.325635103926097,0.75,0,0.375,0.625,0.020833333333333332,0.03125
33,159308,199663,"For a virtual-SError, the hypervisor knows what it generated. (do I haveVSESR_EL2? What did I put in there?).True, the guest can't know anything about a pending virtual SError until it takes it. Why is this a problem? I don't think Qemu allows migration between hosts with differing guest-ID registers. But we shouldn't depend on this, and we may want to hide the v8.2 RAS features from the guest's ID register, but still use them from the host. The way I imagined it working was we would pack the following information into that events.The problem I was trying to describe is because there is no value of serror_esrwe can use to mean 'Ignore this, I'm a v8.0 CPU'. VSESR_EL2 is a 64bit register, any bits we abuse may get a meaning we want to use in the future. When it comes to migration, v8.{0,1} systems can only GET/SET events where serror_has_esr == false, they can't use the serror_esr. On v8.2 systems we should require serror_has_esr to be true.If we need to support migration from v8.{0,1} to v8.2, we can make up an impdefserror_esr.We will need to decide what KVM does when SET is called but an SError was already pending. 2.5.3 ""Multiple SError interrupts"" of [0] has something to say.","Hi gengdongjiu,  On 12/02/18 10:19, gengdongjiu wrote:   For a virtual-SError, the hypervisor knows what it generated. (do I have VSESR_EL2? What did I put in there?).    True, the guest can't know anything about a pending virtual SError until it takes it. Why is this a problem?    I don't think Qemu allows migration between hosts with differing guest-ID registers. But we shouldn't depend on this, and we may want to hide the v8.2 RAS features from the guest's ID register, but still use them from the host.  The way I imagined it working was we would pack the following information into that events struct: { 	bool serror_pending, 	bool serror_has_esr, 	u64  serror_esr, }  The problem I was trying to describe is because there is no value of serror_esr we can use to mean 'Ignore this, I'm a v8.0 CPU'. VSESR_EL2 is a 64bit register, any bits we abuse may get a meaning we want to use in the future.  When it comes to migration, v8.{0,1} systems can only GET/SET events where serror_has_esr == false, they can't use the serror_esr. On v8.2 systems we should require serror_has_esr to be true.  If we need to support migration from v8.{0,1} to v8.2, we can make up an impdef serror_esr.  We will need to decide what KVM does when SET is called but an SError was already pending. 2.5.3 Multiple SError interrupts"" of [0] has something to say.   Happy new year,  James   [0] https://static.docs.arm.com/ddi0587/a/RAS%20Extension-release%20candidate_march_29.pdf""",technical,James Morse,james.morse@arm.com,1,0,1198,0.581986143187067,0.7857142857142857,0,0.4166666666666667,0.5833333333333334,0.03125,0.0
34,159308,225742,"sorry for my late response due to chines new year. so when migration from v8.{0,1} to v8.2, QEMU should make up an impdefserror_esr for the v8.2 target. Can you give me some suggestion how to set that register in the QEMU?I do not familiar with the QEMU migration. Thanks very much. how about KVM set again to the same VCPU? thanks!","Hi James,    sorry for my late response due to chines new year.  2018-02-16 1:55 GMT+08:00 James Morse <james.morse@arm.com>:  I have used your suggestion struct  yes, I agreed.   For the Qemu migration, I need to check more the QEMU code.   Hi Andrew,       I use KVM_GET/SET_VCPU_EVENTS IOCTL to migrate the Serror exception status of VM, The even struct is shown below:  {       bool serror_pending,       bool serror_has_esr,      u64  serror_esr, }  Only when the target machine is armv8.2, it needs to set the serror_esr(SError Exception Syndrome Register). for the armv8.0,  software can not set the serror_esr(SError Exception Syndrome Register). so when migration from v8.{0,1} to v8.2, QEMU should make up an impdef serror_esr for the v8.2 target. can you give me some suggestion how to set that register in the QEMU? I do not familar with the QEMU migration. Thanks very much.   how about KVM set again to the same VCPU?  thanks!",technical,gengdongjiu,gengdj.1984@gmail.com,0,0,332,0.17090069284064666,0.8571428571428571,0,0.625,0.375,0.20833333333333334,0.07291666666666667
35,159308,199659,"and yet ..... this is a problem. If you ignore SPSR_EL3 you may deliver an SError to EL1 when the exception interrupted EL2. Even if you setup the EL1 register correctly, EL1 can't eret toEL2. This should never happen, SError is effectively masked if you are running at an EL higher than the one its routed to. More obviously: if the exception came from the EL that SError should be routed to, but PSTATE. A was set, you can't deliver SError. Masking SError is the only way the OS has to indicate it can't take an exception right now. VBAR_EL1 may be 'wrong' if we're doing some power-management, the registers may contain live values that the OS would lose if you deliver another exception over the top. If you deliver an emulated-SError as the OS eret's, your new ELR will point at the eret instruction and the CPU will spin on this instruction forever. You have to honour the masking and routing rules for SError, otherwise no OS can run safely with this firmware. Yes, this is my suggestion of what to do if you can't deliver an SError: store the RAS error in the BERT and 'reboot'.(UEFI? I didn't think there was any of that at EL3, but I'm not familiar with all the 'PI' bits).The requirement is your emulated-SError from EL3 looks exactly like a physical-SError as if EL3 wasn't implemented. Your CPU has to handle cases where it can't deliver an SError, your emulation has to do the same. This is not something any OS can work around.","Hi gengdongjiu, liu jun  On 05/02/18 11:24, gengdongjiu wrote:  ... and yet ...    .. this is a problem.  If you ignore SPSR_EL3 you may deliver an SError to EL1 when the exception interrupted EL2. Even if you setup the EL1 register correctly, EL1 can't eret to EL2. This should never happen, SError is effectively masked if you are running at an EL higher than the one its routed to.  More obviously: if the exception came from the EL that SError should be routed to, but PSTATE.A was set, you can't deliver SError. Masking SError is the only way the OS has to indicate it can't take an exception right now. VBAR_EL1 may be 'wrong' if we're doing some power-management, the FAR/ELR/ESR/SPSR registers may contain live values that the OS would lose if you deliver another exception over the top.  If you deliver an emulated-SError as the OS eret's, your new ELR will point at the eret instruction and the CPU will spin on this instruction forever.  You have to honour the masking and routing rules for SError, otherwise no OS can run safely with this firmware.    Yes, this is my suggestion of what to do if you can't deliver an SError: store the RAS error in the BERT and 'reboot'.    (UEFI? I didn't think there was any of that at EL3, but I'm not familiar with all the 'PI' bits).  The requirement is your emulated-SError from EL3 looks exactly like a physical-SError as if EL3 wasn't implemented. Your CPU has to handle cases where it can't deliver an SError, your emulation has to do the same.  This is not something any OS can work around.     Happy new year,  James",technical,James Morse,james.morse@arm.com,1,0,1442,0.6997690531177829,0.8214285714285714,0,0.4166666666666667,0.5833333333333334,0.0,0.20833333333333334
36,159308,233051,"Happy new year, Ah! This is where it came from. Sorry, this was just to illustrate the information/sizes we wanted to transfer.... I didn't mean it literally. I should have said ""64 bits of ESR, so that we can transfer anything that is added to VSESR_EL2 in the future, a flag somewhere to indicate an serror is pending, and another flag to indicate the ESR has a value we should use"".","Hi gengdongjiu,  On 08/03/18 06:18, gengdongjiu wrote:  Happy new year,    Ah! This is where it came from. Sorry, this was just to illustrate the information/sizes we wanted to transfer.... I didn't mean it literally.  I should have said 64 bits of ESR, so that we can transfer anything that is added to VSESR_EL2 in the future, a flag somewhere to indicate an serror is pending, and another flag to indicate the ESR has a value we should use"".   Thanks/Sorry!  James    """,technical,James Morse,james.morse@arm.com,1,0,385,0.19399538106235567,0.8928571428571429,0,0.7083333333333334,0.2916666666666667,0.07291666666666667,0.28125
37,159308,259512,"Thanks for this mail and sorry for my late response. James, I  summarized the masking and routing rules for SError to confirm with you for the firmware first solution,1. If this is set, which means the SError should route to EL2, When system happens SError and trap to EL3,   If EL3 find them.A are both set, and find this SError come from EL2, it will not deliver an SError: store the RAS error in the BERT and 'reboot', but if it find that this SError come from EL1 or EL0, it also need to deliver an SError, right?2. If the HCR_EL2.{AMO,TGE} is not set, which means the SError should route to EL1, When system happens SError and trap to EL3, If EL3 findHCR_EL2.{AMO,TGE} and SPSR_EL3.A are both not set, and find this SError come from EL1, it will not deliver an SError: store the RAS error in the BERT and 'reboot', but if it find that this SError come from EL0, it also need to deliver an SError, right?","Dear James,         Thanks for this mail and sorry for my late response.   2018-02-16 1:55 GMT+08:00 James Morse <james.morse@arm.com>: [....]  James, I  summarized the masking and routing rules for SError to confirm with you for the firmware first solution,  1. If the HCR_EL2.{AMO,TGE} is set, which means the SError should route to EL2, When system happens SError and trap to EL3,   If EL3 find HCR_EL2.{AMO,TGE} and SPSR_EL3.A are both set, and find this SError come from EL2, it will not deliver an SError: store the RAS error in the BERT and 'reboot', but if it find that this SError come from EL1 or EL0, it also need to deliver an SError, right?   2. If the HCR_EL2.{AMO,TGE} is not set, which means the SError should route to EL1, When system happens SError and trap to EL3, If EL3 find HCR_EL2.{AMO,TGE} and SPSR_EL3.A are both not set, and find this SError come from EL1, it will not deliver an SError: store the RAS error in the BERT and 'reboot', but if it find that this SError come from EL0, it also need to deliver an SError, right?",technical,gengdongjiu,gengdj.1984@gmail.com,0,0,908,0.48036951501154734,0.9285714285714286,0,0.9895833333333334,0.010416666666666666,0.28125,0.0
38,159308,259854,"You also said ""Currently we does not consider much about the mask status(SPSR)."" If one or the other of these bits is set. Yes. If neither of these bits is set:(I'm reading this as all three of these bits are clear) No, this means SError is routed to EL1, this exception interrupted EL1 and the A bit was clear, so EL1 can take an SError. The two cases here are: AMO==0,TGE==0 means SError should be routed to EL1. If SPSR_EL3 says the exception interrupted EL1 and the A bit was set, you need to do the BERT trick. If SPSR_EL3 says the exception interrupted EL2, you need to do the BERT trick regardless of the A bit, as SError is implicitly masked by running at a higher exception level than it was routed to.(this is re-iterating the two-cases above:)'not be routed to' is one of two things: EL1 is fine, regardless of SPSR_EL3.A the emulatedSError can be delivered to EL2, as EL2 can't mask SError when executing at a lower EL.Route-to-EL1+interrupted-EL2 is the problem. SError is implicitly masked by running at a higher EL. Regardless of SPSR_EL3.A, the emulated SError can not be delivered. KVM does this on the way out of a guest, if an SError occurs during this time the CPU will wait until execution returns to EL1 before delivering the SError.Your firmware has to do the same.Table D1-15 in ""D1.14.2 Asynchronous exception masking"" has a table with all the combinations. The ARM-ARM is what we need to match with this behaviour. I thought interrupted-EL0 could always be delivered: but re-reading the ARM-ARM's ""D1.14.2 Asynchronous exception masking"", if asynchronous exceptions are routed to EL1 then EL0&EL1 are treated the same. So if SError is routed to EL1, the exception interrupted EL0, and SPSR_EL3.A was set, you still can't deliver the emulated-SError you have to do the BERT-trick. Linux doesn't do this today, but another OS might (e.g. UEFI), and we might do this in the future. This is really tricky for firmware to get right. Another alternative would be to put the CPER records in a Polled buffer, unless something needs doing right now, in which case a BERT-reboot is probably best.","Hi gengdongjiu,  On 12/04/18 06:00, gengdongjiu wrote:   You also said Currently we does not consider much about the mask status(SPSR).""    If one or the other of these bits is set: (AMO==1 || TGE==1)   Yes.    If neither of these bits is set: (AMO==0 && TGE == 0)   (I'm reading this as all three of these bits are clear)   No, (AMO==0 && TGE == 0) means SError is routed to EL1, this exception interrupted EL1 and the A bit was clear, so EL1 can take an SError.  The two cases here are: AMO==0,TGE==0 means SError should be routed to EL1. If SPSR_EL3 says the exception interrupted EL1 and the A bit was set, you need to do the BERT trick.  If SPSR_EL3 says the exception interrupted EL2, you need to do the BERT trick regardless of the A bit, as SError is implicitly masked by running at a higher exception level than it was routed to.    (this is re-iterating the two-cases above:) 'not be routed to' is one of two things: Route-to-EL2+interruted-EL1, or Route-to-EL1+interrupted-EL2.  Route-to-EL2+interrupted-EL1 is fine, regardless of SPSR_EL3.A the emulated SError can be delivered to EL2, as EL2 can't mask SError when executing at a lower EL.  Route-to-EL1+interrupted-EL2 is the problem. SError is implicitly masked by running at a higher EL. Regardless of SPSR_EL3.A, the emulated SError can not be delivered. KVM does this on the way out of a guest, if an SError occurs during this time the CPU will wait until execution returns to EL1 before delivering the SError. Your firmware has to do the same.  Table D1-15 in ""D1.14.2 Asynchronous exception masking"" has a table with all the combinations. The ARM-ARM is what we need to match with this behaviour.    I thought interrupted-EL0 could always be delivered: but re-reading the ARM-ARM's ""D1.14.2 Asynchronous exception masking"", if asynchronous exceptions are routed to EL1 then EL0&EL1 are treated the same. So if SError is routed to EL1, the exception interrupted EL0, and SPSR_EL3.A was set, you still can't deliver the emulated-SError you have to do the BERT-trick. Linux doesn't do this today, but another OS might (e.g. UEFI), and we might do this in the future.  This is really tricky for firmware to get right. Another alternative would be to put the CPER records in a Polled buffer, unless something needs doing right now, in which case a BERT-reboot is probably best.   Thanks,  James""",technical,James Morse,james.morse@arm.com,1,0,2112,1.0,0.9642857142857143,0,1.0,0.0,0.0,0.0
39,159308,260361,"Thanks for this mail. Yes, we currently do not consider much it. After clarification with you, we want to modify the EL3 firmware to follow this rule. sorry, it is a typo issue.it should be HCR_EL2.AMO and HCR_EL2.TGE are both clear, but SPSR_EL3.A is set. Agree. ""BERT trick"" is storing the RAS error in the BERT and 'reboot, right? Agree. ""can not be delivered"" means storing the RAS error in the BERT and 'reboot, right? In the Table D1-15 in ""D1.14.2 Asynchronous exception masking"", for the case, it is ""C""C""means SError is not taken regardless of the value of the Process state interrupt mask. for this case, whether it will be unsafe if  BIOS directly reboot? For this case, whether it will be unsafe if  BIOS directly reboot? For example, for some test purpose, EL0 set PSTATE. A, just right happen SError, then BIOS will reboot system. I am afraid that system will become unsafe because BIOS will reboot system. In summary:[1]:Route-to-EL1 + interrupted-EL1, if SPSR_EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'reboot.Route-to-EL2 + interrupted-EL2, if SPSR_EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'reboot. I agree above two cases, but maybe we need to ensure that only in EL2 SError handler and EL1 SError exception handler the PSTATE.A is set, for other places, the PSTATE.A is not set. Then BIOS can know this is nested-SError when find the SPSR_EL3.A is set, can we ensure that in the Linux kernel code and KVM code?[2]:Route-to-EL2 + interrupted-EL1, regardless of SPSR_EL3.A the emulated SError can be delivered to EL2.Route-to-EL2 + interrupted-EL0, regardless of SPSR_EL3.A the emulated SError can be delivered to EL2.I agree above two cases.[3]:Route-to-EL1+interrupted-EL0, if SPSR_EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'rebootRoute-to-EL1+interrupted-EL2, EL3 firmware store the RAS error in the BERT and 'reboot regardless of SPSR_EL3.A.For above two cases, I am worried system will become unsafe because BIOS will reboot system.","James,    Thanks for this mail.  On 2018/4/13 0:14, James Morse wrote: Yes, we currently do not consider much it. After clarification with you, we want to modify the EL3 firmware to follow this rule.  sorry, it is a typo issue. it should be HCR_EL2.AMO and HCR_EL2.TGE are both clear, but SPSR_EL3.A is set.   Agree.   BERT trick"" is storing the RAS error in the BERT and 'reboot, right?    Agree.  ""can not be delivered"" means storing the RAS error in the BERT and 'reboot, right? In the Table D1-15 in ""D1.14.2 Asynchronous exception masking"", for the case, it is ""C"" ""C""means SError is not taken regardless of the value of the Process state interrupt mask. for this case, whether it will be unsafe if  BIOS directly reboot?   For this case, whether it will be unsafe if  BIOS directly reboot? For example, for some test purpose, EL0 set PSTATE.A, just right happen SError, then BIOS will reboot system. I am afraid that system will become unsafe because BIOS will reboot system.   In summary:  [1]: Route-to-EL1 + interrupted-EL1, if SPSR_EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'reboot. Route-to-EL2 + interrupted-EL2, if SPSR_EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'reboot.  I agree above two cases, but maybe we need to ensure that only in EL2 SError handler and EL1 SError exception handler the PSTATE.A is set, for other places, the PSTATE.A is not set. then BIOS can know this is nested-SError when find the SPSR_EL3.A is set, can we ensure that in the Linux kernel code and KVM code?  [2]: Route-to-EL2 + interrupted-EL1, regardless of SPSR_EL3.A the emulated SError can be delivered to EL2. Route-to-EL2 + interrupted-EL0, regardless of SPSR_EL3.A the emulated SError can be delivered to EL2.  I agree above two cases.  [3]: Route-to-EL1+interrupted-EL0, if SPSR_EL3.A is set, EL3 firmware can't deliver the emulated-SError, store the RAS error in the BERT and 'reboot Route-to-EL1+interrupted-EL2, EL3 firmware store the RAS error in the BERT and 'reboot regardless of SPSR_EL3.A.  For above two cases, I am worried system will become unsafe because BIOS will reboot system.  """,technical,gengdongjiu,gengdongjiu@huawei.com,0,0,2123,0.9722863741339491,1.0,1,1.0,0.0,0.0,0.0
40,161016,161204,The patch should be split into two: one is for dt-bindings part and the other is for driver part. Where dt-binding part should require additionally to send to Rob should include 2018 ? how about use devm_request_irq to simplify error path?,"On Mon, 2018-01-08 at 14:10 -0800, Jolly Shah wrote:   The patch should be split into two: one is for dt-bindings part and the other is for driver part. Where dt-binding part should require additionally to send to Rob and Cc. devicetree@vger.kernel.org.   should include 2018 ?    how about use devm_request_irq to simplify error path?",technical,Sean Wang,sean.wang@mediatek.com,1,0,239,1.0,0.6,0,0.0,1.0,0.0,1.0
41,161016,163525,"Thanks for review, Sure. Will do it in next version.Will fix in next version Will change in next version","Thanks Sean for review,      Sure. Will do it in next version.        Will fix in next version      Will change in next version    Thanks,  Jolly Shah    ",technical,Jolly Shah,JOLLYS@xilinx.com,0,0,104,0.4772727272727273,0.8,1,1.0,0.0,1.0,0.0
42,161240,161996,"Ok, makes sense. Still one minor thing left","On 01/09/2018 05:52 AM, Alexei Starovoitov wrote:  Ok, makes sense.  [...]  Still one minor thing left:   When CONFIG_BPF_JIT_ALWAYS_ON is disabled, this will throw the following warning:  [...]   CC      kernel/bpf/core.o kernel/bpf/core.c:1360:21: warning: ‘__bpf_prog_ret0’ defined but not used [-Wunused-function]  static unsigned int __bpf_prog_ret0(const void *ctx,                      ^~~~~~~~~~~~~~~  Probably just best to wrap it under ifdef CONFIG_BPF_JIT_ALWAYS_ON.",technical,Daniel Borkmann,daniel@iogearbox.net,1,0,43,1.0,1.0,1,0.0,0.0,0.0,0.0
43,161354,161722,Thanks for adding the comment.,"On Tue, Jan 09, 2018 at 09:59:45AM +0100, Antoine Tenart wrote:  Thanks for adding the comment.  Reviewed-by: Andrew Lunn <andrew@lunn.ch>      Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,30,0.10909090909090909,0.5454545454545454,0,0.0,0.0,0.0,0.0
44,161354,161795,"Sorry, also...I think you'll find you don't need to set MII_SPEED here, since MII_SPEED selects between 10 and 100, GMII_SPEED always takes precedence selecting 1000, and 2500 is done by the comphy increasing the clocks by 2.5x","On Tue, Jan 09, 2018 at 09:59:45AM +0100, Antoine Tenart wrote:  Sorry, also...   I think you'll find you don't need to set MII_SPEED here, since MII_SPEED selects between 10 and 100, GMII_SPEED always takes precidence selecting 1000, and 2500 is done by the comphy increasing the clocks by 2.5x.  --  RMK's Patch system: http://www.armlinux.org.uk/developer/patches/ FTTC broadband for 0.8mile line in suburbia: sync at 8.8Mbps down 630kbps up According to speedtest.net: 8.21Mbps down 510kbps up",technical,Russell King - ARM Linux,linux@armlinux.org.uk,1,0,227,0.8181818181818182,0.7272727272727273,0,0.0,0.0,0.0,0.0
45,161354,161824,The mvpp2 driver uses phy_ethtool_get_link_ksettings() to report the link speed to Ethtool. So it's reporting the speed set by the PHYdriver.So it'll be something to ensure when adding PHYs supporting the mode. We'll have the opportunity to see this when adding the last mcbin interface. Thanks!,"Hi Russell,  On Tue, Jan 09, 2018 at 02:42:38PM +0000, Russell King - ARM Linux wrote:  The mvpp2 driver uses phy_ethtool_get_link_ksettings() to report the link speed to Ethtool. So it's reporting the speed set by the PHY driver.  So it'll be something to ensure when adding PHYs supporting the mode. We'll have the opportunity to see this when adding the last mcbin interface.  Thanks! Antoine  --  Antoine Tnart, Free Electrons Embedded Linux and Kernel engineering http://free-electrons.com",technical,Antoine Tenart,antoine.tenart@free-electrons.com,1,1,295,1.0,0.8181818181818182,0,0.0,0.0,0.0,0.0
46,161354,161829,"Comments always welcomed :)I just had a look at the datasheet, and as you say it seems GMII_SPEED takes over MII_SPEED. I'll see if there is a corner case here or if selecting MII_SPEED doesn't make sense, and update accordingly. Thanks!","Hi Russell,  On Tue, Jan 09, 2018 at 02:44:48PM +0000, Russell King - ARM Linux wrote:  Comments always welcomed :)   I just had a look at the datasheet, and as you say it seems GMII_SPEED takes over MII_SPEED. I'll see if there is a corner case here or if selecting MII_SPEED doesn't make sense, and update accordingly.  Thanks! Antoine  --  Antoine Tnart, Free Electrons Embedded Linux and Kernel engineering http://free-electrons.com",technical,Antoine Tenart,antoine.tenart@free-electrons.com,1,1,237,0.9090909090909091,0.9090909090909091,0,0.0,0.0,0.0,0.0
47,161354,161913,"I just checked, this can be removed for this mode. I'll update the patch. Thanks!","Hi Russell,  On Tue, Jan 09, 2018 at 04:17:35PM +0100, Antoine Tenart wrote:  I just checked, this can be removed for this mode. I'll update the patch.  Thanks! Antoine  --  Antoine Tnart, Free Electrons Embedded Linux and Kernel engineering http://free-electrons.com",technical,Antoine Tenart,antoine.tenart@free-electrons.com,1,1,81,0.36363636363636365,1.0,1,0.0,0.0,0.0,0.0
48,163871,174195,Could you please have a look of this thanks,Hi Peter  Could you please have a look of this  thanks  2018-01-11 11:09 GMT+08:00  <linxiulei@gmail.com>:,technical,Lin Xiulei,linxiulei@gmail.com,0,0,43,1.0,1.0,1,1.0,0.0,1.0,0.0
49,165231,165276,"That was an extensive changelog, thanks for the details and for working on this!","On 12 January 2018 at 14:10, Rafael J. Wysocki <rjw@rjwysocki.net> wrote:  That was an extensive changlog, thanks for the details and for working on this!  Acked-by: Ulf Hansson <ulf.hansson@linaro.org>  Kind regards Uffe",technical,Ulf Hansson,ulf.hansson@linaro.org,1,0,80,0.128,0.3125,0,0.0,1.0,0.0,0.0
50,165231,165312,"I've tested this on two very similar systems. On the M3-based system, everything seems to work fine. On the H3-based system, the serial console (the /dev/ttySC0 device, not kernel serial output) is dead after resume from s2ram, with and without no_console_suspend.With no_console_suspend, I see this. 1 input overrun(s) after typing on the serial console, so it looks like an interrupt problem. This issue seems to be caused by patch [1/2]. But I have no idea what's really happening, and why the two systems behave differently. Oh well, have a nice weekend!","Hi Rafael,  On Fri, Jan 12, 2018 at 2:00 PM, Rafael J. Wysocki <rjw@rjwysocki.net> wrote:  I've tested this on two very similar systems: Salvator-XS with R-Car H3 ES2.0, and Salvator-X with R-Car M3-W ES1.0.  On the M3-based system, everything seems to work fine. On the H3-based system, the serial console (the /dev/ttySC0 device, not kernel serial output) is dead after resume from s2ram, with and without no_console_suspend.  With no_console_suspend, I see:      ttySC ttySC0: 1 input overrun(s)  after typing on the serial console, so it looks like an interrupt problem.  This issue seems to be caused by patch [1/2]. But I have no idea what's really happening, and why the two systems behave differently.  Oh well, have a nice weekend!  Gretje,eetings,                          Geert  -- Geert Uytterhoeven -- There's lots of Linux beyond ia32 -- geert@linux-m68k.org  In personal conversations with technical people, I call myself a hacker. But when I'm talking to journalists I just say programmer"" or something like that.                                 -- Linus Torvalds""",technical,Geert Uytterhoeven,geert@linux-m68k.org,1,0,558,0.92,0.375,0,0.0,1.0,0.0,0.0
51,165231,165703,"Good. Well, that's not dramatic. Let's make a deal that we'll fix this on top of [1/2].Which driver is this BTW?  sh-sci?  That one doesn't even support runtime PM, confusingly enough. Thanks, you too!","On Friday, January 12, 2018 3:31:09 PM CET Geert Uytterhoeven wrote:  Good.   Well, that's not dramatic.  Let's make a deal that we'll fix this on top of [1/2].  Which driver is this BTW?  sh-sci?  That one doesn't even support runtime PM, confusingly enough.   Thanks, you too!",technical,Rafael J. Wysocki,rjw@rjwysocki.net,1,1,201,0.4,0.4375,0,0.0,1.0,0.0,0.0
52,165231,165705,"What do you mean by ""managed by runtime PM""? I will do that, no worries.OK, I'll send a patch on top of this series.","On Friday, January 12, 2018 2:59:38 PM CET Ulf Hansson wrote:  What do you mean by managed by runtime PM""?   I will do that, no worries.   OK, I'll send a patch on top of this series.  Thanks, Rafael""",technical,Rafael J. Wysocki,rjw@rjwysocki.net,1,1,116,0.248,0.5,0,0.0,1.0,0.0,0.25
53,165231,165967,"Could be a firmware issue, too.While the kernel images are identical, the ARM trusted firmware configs aren't (same version, though).I'll do some more investigation...,-) Yes, sh-sci. It does make pm_runtime_*() calls. And of course there's uart_ops.pm, which is driven from serial_core...","Hi Rafael,  On Sat, Jan 13, 2018 at 1:38 AM, Rafael J. Wysocki <rjw@rjwysocki.net> wrote:  Could be a firmware issue, too. While the kernel images are identical, the ARM trusted firmware configs aren't (same version, though).  I'll do some more investigation...   ,-)   Yes, sh-sci. It does make pm_runtime_*() calls. And of course there's uart_ops.pm, which is driven from serial_core...  Gr{oetje,eeting}s,                          Geert  -- Geert Uytterhoeven -- There's lots of Linux beyond ia32 -- geert@linux-m68k.org  In personal conversations with technical people, I call myself a hacker. But when I'm talking to journalists I just say programmer"" or something like that.                                 -- Linus Torvalds""",technical,Geert Uytterhoeven,geert@linux-m68k.org,1,0,289,0.496,0.5625,0,0.25,0.75,0.25,0.0
54,165231,166155,"OK, thanks! It also would be good to know the topology of the device hierarchy and how that maps to the domains on the failing system (and which UARTclocks are operated by genpd). Hmm.  I overlooked that part. This is sort of unusual, because the driver doesn't provide any runtime PM callbacks, but still it does provided system suspend ones.It looks like the idea is to never put it into runtime suspend if any ports are enabled and always put it into runtime suspend otherwise. Which one is the case in your testing?  Is the port disabled or enabled during system-wide suspend? What does this point to for that particular device?","On Sun, Jan 14, 2018 at 10:48 AM, Geert Uytterhoeven <geert@linux-m68k.org> wrote:  OK, thanks!  It also would be good to know the topology of the device hierarchy and how that maps to the domains on the failing system (and which UART clocks are operated by genpd).   Hmm.  I overlooked that part.  This is sort of unusual, because the driver doesn't provide any runtime PM callbacks, but still it does provided system suspend ones. It looks like the idea is to never put it into runtime suspend if any ports are enabled and always put it into runtime suspend otherwise.  Which one is the case in your testing?  Is the port disabled or enabled during system-wide suspend?   What does this point to for that particular device?",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,632,1.0,0.625,0,0.5,0.5,0.0,0.0
55,165231,166306,"The topology is the same on both systems. The UART's module clock is operated by genpd, on both systems. It's enabled on both systems, as a getty is running.sci_pm(), on both systems. See, there's no difference in topology on both systems, so I'll have to look a bit deeper first...","Hi Rafael,  On Mon, Jan 15, 2018 at 1:04 AM, Rafael J. Wysocki <rafael@kernel.org> wrote:  The topology is the same on both systems. The UART's module clock is operated by genpd, on both systems.   It's enabled on both systems, as a getty is running.   sci_pm(), on both systems.  See, there's no difference in topology on both systems, so I'll have to look a bit deeper first...  Gretje,eetings,                          Geert  -- Geert Uytterhoeven -- There's lots of Linux beyond ia32 -- geert@linux-m68k.org  In personal conversations with technical people, I call myself a hacker. But when I'm talking to journalists I just say programmer"" or something like that.                                 -- Linus Torvalds""",technical,Geert Uytterhoeven,geert@linux-m68k.org,1,0,282,0.52,0.6875,0,0.5,0.5,0.0,0.0
56,165231,166764,"I did miss a small difference in topology: in pm/linux-next, H3 has DMA enabled for SCIF2, while M3 hasn't (yet).With DMA enabled on M3, it fails in the same way. As it no longer calls this is no longer called, and the DMAC's registers are no longer reinitialized after system resume, breaking the serial port.","Hi Rafael,  On Mon, Jan 15, 2018 at 9:16 AM, Geert Uytterhoeven <geert@linux-m68k.org> wrote:  I did miss a small difference in topology: in pm/linux-next, H3 has DMA enabled for SCIF2, while M3 hasn't (yet). With DMA enabled on M3, it fails in the same way.  As genpd_resume_noirq() no longer calls pm_runtime_force_resume(), rcar_dmac_runtime_resume() is no longer called, and the DMAC's registers are no longer reinitialized after system resume, breaking the serial port.  Gretje,eetings,                          Geert  -- Geert Uytterhoeven -- There's lots of Linux beyond ia32 -- geert@linux-m68k.org  In personal conversations with technical people, I call myself a hacker. But when I'm talking to journalists I just say programmer"" or something like that.                                 -- Linus Torvalds""",technical,Geert Uytterhoeven,geert@linux-m68k.org,1,0,310,0.544,0.75,0,0.75,0.25,0.0,0.0
57,165231,167740,In drivers I would try to replace the below line in case that may be too early to suspend the dma device (which is rather common for dma devices) then try this,"On 15 January 2018 at 14:22, Geert Uytterhoeven <geert@linux-m68k.org> wrote:  In drivers/dma/sh/rcar-dmac.c, I would try to replace the below line: SET_SYSTEM_SLEEP_PM_OPS(rcar_dmac_sleep_suspend, rcar_dmac_sleep_resume)  with: SET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend, pm_runtime_force_resume)  in case that may be too early to suspend the dma device (which is rather common for dma devices) then try,  SET_LATE_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend, pm_runtime_force_resume)  Kind regards Uffe",technical,Ulf Hansson,ulf.hansson@linaro.org,1,0,159,0.272,0.8125,0,0.75,0.25,0.0,0.0
58,165231,167844,"[cut]Yes, that probably is the least intrusive thing that can be done to address the issue. Good suggestion, and I would go straight for it anyway. can you try if this works, please?","On Mon, Jan 15, 2018 at 3:26 PM, Ulf Hansson <ulf.hansson@linaro.org> wrote:  [cut]   Yes, that probably is the least intrusive thing that can be done to address the issue.   Good suggestion, and I would go straight for it anyway.  Geert, can you try if this works, please?  Thanks, Rafael",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,182,0.336,0.875,0,0.75,0.25,0.0,0.25
59,165231,169709,"Works. Both using them, But given this is a DMA engine driver, I'd settle for the latter. And I did verify doing so doesn't break the system without the patch in subject.Thanks! Will send a patch…","Hi Rafael,  On Mon, Jan 15, 2018 at 5:17 PM, Rafael J. Wysocki <rafael@kernel.org> wrote:  Works. Both using SET_SYSTEM_SLEEP_PM_OPS() and SET_LATE_SYSTEM_SLEEP_PM_OPS(). But given this is a DMA engine driver, I'd settle for the latter.  And I did verify doing so doesn't break the system without the patch in $subject.  Thanks!  Will send a patch...  Gr{oetje,eeting}s,                          Geert  -- Geert Uytterhoeven -- There's lots of Linux beyond ia32 -- geert@linux-m68k.org  In personal conversations with technical people, I call myself a hacker. But when I'm talking to journalists I just say programmer"" or something like that.                                 -- Linus Torvalds""",technical,Geert Uytterhoeven,geert@linux-m68k.org,1,0,196,0.344,0.9375,0,1.0,0.0,0.25,0.0
60,165657,169351,"Thanks.I just sent a v3 that changes the VERMAGIC only, based on Greg's earlier feedback.It has the drawbacks that it:- refuses loading instead of warns- doesn't stop refusing when the feature is runtime disabled But it's much simpler, just a few lines of ifdef. We can either go with the v3, or rework this one into a v4?","Thanks.  I just sent a v3 that changes the VERMAGIC only, based on Greg's  earlier feedback.  It has the drawbacks that it: - refuses loading instead of warns - doesn't stop refusing when the feature is runtime disabled  But it's much simpler, just a few lines of ifdef.  We can either go with the v3, or rework this one into a v4?  -Andi",technical,Andi Kleen,andi@firstfloor.org,0,1,322,1.0,0.6,0,1.0,0.0,0.0,0.0
61,165657,169361,I think simple is good at this point,  I think simple is good at this point    ,technical,"Van De Ven, Arjan",arjan.van.de.ven@intel.com,0,0,36,0.11764705882352941,0.8,0,1.0,0.0,0.0,0.0
62,165657,169368,V3 is fine. Not loading is the right thing to do :),"On Tue, 16 Jan 2018, Andi Kleen wrote:   V3 is fine. Not loading is the right thing to do :)  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,51,0.20588235294117646,1.0,1,1.0,0.0,0.0,0.0
63,166193,168162,"You are setting ssi->synchronous in the AC'97 mode here, the old code didn't do that (see the next patch hunk below). Since in the previous patch you have replaced cpu_dai_drv.symmetric_rateswith ssi->synchronous this will likely break asymmetric rate support in the AC'97 mode, since the driver will use STCCR for programming of both playback and capture. The next patch in this series (17) also looks affected by this change. You can see it here that the old code didn't set ssi->synchronous in theAC'97 mode.","On 15.01.2018 05:21, Nicolin Chen wrote:  You are setting ssi->synchronous in the AC'97 mode here, the old code didn't do that (see the next patch hunk below).  Since in the previous patch you have replaced cpu_dai_drv.symmetric_rates with ssi->synchronous this will likely break asymmetric rate support in the AC'97 mode, since the driver will use STCCR for programming of both playback and capture.  The next patch in this series (17) also looks affected by this change.   You can see it here that the old code didn't set ssi->synchronous in the AC'97 mode.  Maciej",technical,Maciej S. Szmigiero,mail@maciej.szmigiero.name,0,0,511,1.0,0.9130434782608695,0,0.0,0.0,0.0,0.0
64,166193,168164,Will modify this part. Thanks,"On Mon, Jan 15, 2018 at 10:16:39PM +0100, Maciej S. Szmigiero wrote:   Will modify this part. Thanks",technical,Nicolin Chen,nicoleotsuka@gmail.com,1,1,29,0.0594059405940594,0.9565217391304348,0,0.0,0.0,0.0,0.0
65,166193,168165,I neglected the comments in the middle. Sorry. Will add it back then.,"On Mon, Jan 15, 2018 at 10:16:28PM +0100, Maciej S. Szmigiero wrote:   I neglected the comments in the middle. Sorry. Will add it back then.",technical,Nicolin Chen,nicoleotsuka@gmail.com,1,1,69,0.15841584158415842,1.0,1,0.0,0.0,0.0,0.0
66,166459,168206,"The above totally does not parse (no pun intended).Are you trying to say: ""User space can pass in a C null character '\0' along with its input. The function trace_get_user() will try to process it as a normal character, and that will fail to parse.The above should be something like: ""Have the parser stop on '\0' and cease any further parsing. Only process the characters up to the null '\0' character and do not process it.""","On Mon, 15 Jan 2018 19:41:12 +0800 changbin.du@intel.com wrote:   The above totally does not parse (no pun intended).  Are you trying to say:  User space can pass in a C nul character '\0' along with its input. The function trace_get_user() will try to process it as a normal character, and that will fail to parse.   The above should be something like:  ""Have the parser stop on '\0' and cease any further parsing. Only process the characters up to the nul '\0' character and do not process it.""  -- Steve  """,technical,Steven Rostedt,rostedt@goodmis.org,1,0,426,1.0,0.8333333333333334,0,0.0,0.0,0.0,0.0
67,166459,168314,"Thanks for your polish, let me update commit msg with your words.--Thanks","Hi Rostedt, Thanks for your polish, let me update commit msg with your words.  On Mon, Jan 15, 2018 at 06:20:00PM -0500, Steven Rostedt wrote:  --  Thanks, Changbin Du",technical,"Du, Changbin",changbin.du@intel.com,0,0,73,0.16129032258064516,1.0,1,0.0,0.0,0.0,0.0
68,167856,167856,"RESEND: fix typo in email address. HI, A few weeks ago, I have sent an RFC about adding bias support for GPIOs [1].It was motivated by the fact that I wanted to enable the pinmuxing strict mode for my pin controller which can muxed a pin as a peripheral or as a GPIO. Enabling the strict mode prevents several devices to be probed because requesting a GPIO fails. The pin request function complains about the ownership of the GPIO which is different from the mux ownership. I have to remove my pinctrl node to avoid this conflict but I need it to configure mypins and to set a pull-up bias for my GPIOs. My first idea was to add new flags in addition to GPIO_ACTIVE_HIGH and others. Obviously, it was not the way to go since many new flags may be added: strength, debounce, etc. Then I proposed a very ""quick and dirty"" patch to give the picture of what I have in mind but I had no feedback. It was probably too dirty. The idea was to add a cell to the gpios property with a phandle on a pinctrl node which contains only the pinconf, no pinmux. The configuration is applied later when requesting the GPIO. The main issue is that enabling the strict mode will break old DTBs. I was going to submit patches for this but, after using the sysfs which still show me a bad ownership, I decided that it should be fixed. So I did these patches. Unfortunately, there are several ways to lead to gpiod_request(). It does the trick only for the gpiod_get family. The issue is still present with legacy gpio_request and fwnode_get_named_gpiod. It seems that more and more drivers are converted to use GPIO descriptors so there is some hope. The advantage of this solution is to not break old DTBs. As I am not aware of all usage of the gpiolib, I tried to implement it in the safest way.","RESEND: fix typo in email address.  Hi,  A few weeks ago, I have sent an RFC about adding bias support for GPIOs [1].  It was motivated by the fact that I wanted to enable the pinmuxing strict mode for my pin controller which can muxed a pin as a peripheral or as a GPIO.  Enabling the strict mode prevents several devices to be probed because requesting a GPIO fails. The pin request function complains about the ownership of the GPIO which is different from the mux ownership. I have to remove my pinctrl node to avoid this conflict but I need it to configure my pins and to set a pull-up bias for my GPIOs.  My first idea was to add new flags in addition to GPIO_ACTIVE_HIGH and others. Obviously, it was not the way to go since many new flags may be added: strength, debounce, etc.  Then I proposed a very quick and dirty"" patch to give the picture of what I have in mind but I had no feedback. It was probably too dirty. The idea was to add a cell to the gpios property with a phandle on a pinctrl node which contains only the pinconf, no pinmux. The configuration is applied later when requesting the GPIO. The main issue is that enabling the strict mode will break old DTBs. I was going to submit patches for this but, after using the sysfs which still show me a bad ownership, I decided that it should be fixed.  So I did these patches. Unfortunately, there are several ways to lead to gpiod_request(). It does the trick only for the gpiod_get family. The issue is still present with legacy gpio_request and fwnode_get_named_gpiod. It seems that more and more drivers are converted to use GPIO descriptors so there is some hope. The advantage of this solution is to not break old DTBs. As I am not aware of all usage of the gpiolib, I tried to implement it in the safest way.  Regards  Ludovic  [1] https://www.spinics.net/lists/arm-kernel/msg623149.html   Ludovic Desroches (2):   pinctrl: add consumer variant for gpio request   gpio: provide a consumer when requesting a gpio   drivers/gpio/gpiolib.c           | 40 +++++++++++++++++++++++++++++++++-------  drivers/pinctrl/core.c           | 13 ++++++++++---  drivers/pinctrl/pinmux.c         | 16 ++++++++++++++--  drivers/pinctrl/pinmux.h         | 10 ++++++++++  include/linux/gpio/driver.h      |  5 +++++  include/linux/pinctrl/consumer.h |  6 ++++++  6 files changed, 78 insertions(+), 12 deletions(-)  --  2.12.2""",technical,Ludovic Desroches,ludovic.desroches@microchip.com,1,1,1775,1.0,0.07692307692307693,0,0.0,1.0,-1.0769230769230769,0.0
69,167856,174352,"thanks for your patches! I was confused I think, because the issue of ownership and adding bias support were conflated. I think I discussed properly the ideas I have for pin control properties vs the GPIOlib API/ABI in my response to patch 1.So that is a different thing from bias support. Okay I think the right solution is to fix the ownership issue, and setup bias using pin control/config but use the line through gpiolib for now. Yeah we need to work around that. Yep :) fwnode_get_named_gpiod() must really be fixed too. You probably want to have things like LEDs and GPIO keys working even if your pin controller is strict. I don't care so much about the old functions, I guess you just have to make sure that the drivers for *your* pin controller all use descriptors so that you can enable strict mode on *your* pin controller, right? Restrict your task to this, I'd say. Yeah I'm doing this when I have time. There is plenty of work...Help appreciated.","Hi Ludovic, thanks for your patches!  On Mon, Jan 15, 2018 at 5:24 PM, Ludovic Desroches <ludovic.desroches@microchip.com> wrote:   I was confused I think, because the issue of ownership and adding bias support were conflated.  I think I discussed properly the ideas I have for pin control properties vs the GPIOlib API/ABI in my response to patch 1.   So that is a different thing from bias support.   Okay I think the right solution is to fix the ownership issue, and set up bias using pin control/config but use the line through gpiolib for now.   Yeah we need to work around that.   Yep :)   fwnode_get_named_gpiod() must really be fixed too. You probably want to have things like LEDs and GPIO keys working even if your pin controller is strict.  I don't care so much about the old functions, I guess you just have to make sure that the drivers for *your* pin controller all use descriptors so that you can enable strict mode on *your* pin controller, right?  Restrict your task to this, I'd say.   Yeah I'm doing this when I have time. There is plenty of work... Help appreciated.  Yours, Linus Walleij",technical,Linus Walleij,linus.walleij@linaro.org,1,0,961,0.5392953929539296,0.3076923076923077,0,0.15384615384615385,0.8461538461538461,0.15384615384615385,0.0
70,167856,174357,"I think we need to think over what is a good way to share ownership of a pin. Russell pointed me to a similar problem incidentally and I briefly looked into it: there are cases when several devices may need to hold the same pin. Can't we just look up the associated gpio_chip from the GPIO range, and in case the pin is connected between the pin controller and the GPIO chip, then we allow the gpiochip to also take a reference?I.e. in that case you just allow gpio_owner to proceed and take the pin just like with a non-strict controller.","On Mon, Jan 15, 2018 at 5:24 PM, Ludovic Desroches <ludovic.desroches@microchip.com> wrote:   I think we need to think over what is a good way to share ownership of a pin.  Russell pointed me to a similar problem incidentally and I briefly looked into it: there are cases when several devices may need to hold the same pin.  Can't we just look up the associated gpio_chip from the GPIO range, and in case the pin is connected between the pin controller and the GPIO chip, then we allow the gpiochip to also take a reference?  I.e. in that case you just allow gpio_owner to proceed and take the pin just like with a non-strict controller.  Yours, Linus Walleij",technical,Linus Walleij,linus.walleij@linaro.org,1,0,539,0.3008130081300813,0.38461538461538464,0,0.15384615384615385,0.8461538461538461,0.0,0.0
71,167856,174601,"It's the probably the way to go, it was Maxime's proposal and Andy seems to agree this solution.","On Thu, Jan 18, 2018 at 11:30:00AM +0100, Linus Walleij wrote:  It's the probably the way to go, it was Maxime's proposal and Andy seems to agree this solution.   Regards  Ludovic",technical,Ludovic Desroches,ludovic.desroches@microchip.com,1,1,96,0.05962059620596206,0.5384615384615384,0,0.15384615384615385,0.7692307692307693,0.0,0.07692307692307693
72,167856,175436,"I'm looking into SPI and regulators for the next kernel cycle, so those will hopefully get fixed.","On Thu, Jan 18, 2018 at 4:12 PM, Ludovic Desroches <ludovic.desroches@microchip.com> wrote:   I'm looking into SPI and regulators for the next kernel cycle, so those will hopefully get fixed.  Yours, Linus Walleij",technical,Linus Walleij,linus.walleij@linaro.org,1,0,97,0.05420054200542006,0.6153846153846154,0,0.3076923076923077,0.6923076923076923,0.07692307692307693,0.3076923076923077
73,167856,178116,"If pin_request() is called with gpio_range not NULL, it means that the requests comes from a GPIO chip and the pin controller handles this pin. In this case, I would say the pin is connected between the pincontroller and the GPIO chip. Is my assumption right? I am not sure it will fit all the cases:- case 1: device A requests the pin (pinctrl-default state) and mux it  as a GPIO. Later, it requests the pin as a GPIO (gpiolib). This 'weird'  situation happens because some strict pin controllers were not declared  as strict and/or pinconf is needed.- case 2: device A requests the pin (pinctrl-default state). Device B  requests the pin as a GPIO (gpiolib).In case 1, pin_request must not return an error. In case 2, pin_request must return an error even if the pin is connected between the pincontroller and the GPIO chip.","On Thu, Jan 18, 2018 at 04:22:28PM +0100, Ludovic Desroches wrote:  If pin_request() is called with gpio_range not NULL, it means that the requests comes from a GPIO chip and the pin controller handles this pin. In this case, I would say the pin is connected between the pin controller and the GPIO chip. Is my assumption right?  I am not sure it will fit all the cases:  - case 1: device A requests the pin (pinctrl-default state) and mux it   as a GPIO. Later,it requests the pin as a GPIO (gpiolib). This 'weird'   situation happens because some strict pin controllers were not declared   as strict and/or pinconf is needed.  - case 2: device A requests the pin (pinctrl-default state). Device B   requests the pin as a GPIO (gpiolib).  In case 1, pin_request must not return an error. In case 2, pin_request must return an error even if the pin is connected between the pin controller and the GPIO chip.   Regards  Ludovic",technical,Ludovic Desroches,ludovic.desroches@microchip.com,1,1,827,0.4715447154471545,0.6923076923076923,0,0.6153846153846154,0.38461538461538464,0.3076923076923077,0.0
74,167856,178214,"How do you find my proposal about introducing ownership level (not requested yet, exclusive, shared)? Confirm with caveat that this is a fix for subset of cases. I think it doesn't cover cases when you have UART + UART + GPIO (I posted early a use case example).But at least it doesn't move things in a wrong direction. For these cases looks OK to me.","On Wed, Jan 24, 2018 at 3:07 PM, Ludovic Desroches <ludovic.desroches@microchip.com> wrote:   How do you find my proposal about introducing ownership level (not requested yet, exclusive, shared)?   Confirm with caveat that this is a fix for subset of cases.   I think it doesn't cover cases when you have UART + UART + GPIO (I posted early a use case example).  But at least it doesn't move things in a wrong direction.   For these cases looks OK to me.   --  With Best Regards, Andy Shevchenko",technical,Andy Shevchenko,andy.shevchenko@gmail.com,1,0,351,0.21138211382113822,0.7692307692307693,0,0.6153846153846154,0.3076923076923077,0.0,0.07692307692307693
75,167856,179048,"Yes but I don't see how I can fix my issue with these levels. In my case, I need an exclusive ownership at device level not at pin level. In reality, it is at pin level but I am in this situation because my pincontroller was introduced as non strict and also because I need to set the configuration of the pin which is going to be used as a GPIO.If the ownership is exclusive, pinmuxing coming from pinctrl-default will be accepted but the GPIO request will fail even if it comes from the same device.If the ownership is shared then, pinmuxing coming from pinctrl-default will be accepted but a GPIO request from another device will be accepted too. Both situations are incorrect in my case. Let me know if I have not well understood your proposal. My concern is to get out of this situation without breaking current DTs.","On Wed, Jan 24, 2018 at 05:42:15PM +0200, Andy Shevchenko wrote:  Yes but I don't see how I can fix my issue with these levels. In my case, I need an exclusive ownership at device level not at pin level. In reality, it is at pin level but I am in this situation because my pin controler was introduced as non strict and also because I need to set the configuration of the pin which is going to be used as a GPIO.  If the ownership is exclusive, pinmuxing coming from pinctrl-default will be accepted but the GPIO request will fail even if it comes from the same device.  If the ownership is shared then, pinmuxing coming from pinctrl-default will be accepted but a GPIO request from another device will be accepted too.  Both situations are incorrect in my case.  Let me know if I have not well understood your proposal. My concern is to get out of this situation without breaking current DTs.  Regards  Ludovic",technical,Ludovic Desroches,ludovic.desroches@microchip.com,1,1,821,0.4363143631436314,0.8461538461538461,0,0.7692307692307693,0.23076923076923078,0.07692307692307693,0.0
76,167856,179444,"The problem here is to declare a right consumer of the resource. My understanding that consumer at the end is device or device(s):none: resource is free to acquire exclusive: certain device has access to the resource (pin) shared: several devices may access to the resource In both cases couple of caveats:- power management has a special access level to the resource on behalf of the owner(s)- it can have some flags, like 'locked', which means no more owners can be changed / added, but still possible to free resource by all owners to go to state 'none' Yes, since the ownership design is based on subsystem rather consumer device. See above, hope it clarifies a bit.","On Fri, Jan 26, 2018 at 9:32 AM, Ludovic Desroches <ludovic.desroches@microchip.com> wrote:    The problem here is to declare a right consumer of the resource.  My understanding that consumer at the end is device or device(s):  none: resource is free to acquire exclusive: certain device has access to the resource (pin) shared: several devices may access to the resource  In both cases couple of caveats: - power management has a special access level to the resource on behalf of the owner(s) - it can have some flags, like 'locked', which means no more owners can be changed / added, but still possible to free resource by all owners to go to state 'none'   Yes, since the ownership design is based on subsystem rather consumer device.   See above, hope it clarifies a bit.  --  With Best Regards, Andy Shevchenko",technical,Andy Shevchenko,andy.shevchenko@gmail.com,1,0,670,0.38482384823848237,0.9230769230769231,0,0.8461538461538461,0.15384615384615385,0.0,0.15384615384615385
77,167856,180187,"Yes I get it but I still don't see how I can use your approach to solve my issue. We have a situation for several pin controllers. If I can't know who is requesting the GPIO, I have no idea about how to solve this issue. Bypassing the strict mode, as suggested, if the pin controller is also a gpio controller may lead, IMO, to wrong behaviors. Do I have to try to find a way to fix this situation? Maybe, it will be easier to progress on the muxing and configuration topic and to introduce a DT property to enable the strict mode or whatever modes you want once everything is ready and DTs fixed. I'd prefer to fix the current situation then to improve muxing and configuration stuff because it will take time.","On Fri, Jan 26, 2018 at 07:13:32PM +0200, Andy Shevchenko wrote:  Yes I get it but I still don't see how I can use your approach to solve my issue. We have a situation for several pin controllers. If I can't know who is requesting the GPIO, I have no idea about how to solve this issue. Bypassing the strict mode, as suggested, if the pin controller is also a gpio controller may lead, IMO, to wrong behaviors.   Do I have to try to find a way to fix this situation? Maybe, it will be easier to progress on the muxing and configuration topic and to introduce a DT property to enable the strict mode or wathever modes you want once everything is ready and DTs fixed.  I'd prefer to fix the current situation then to improve muxing and configuration stuff because it will take time.  Regards  Ludovic",technical,Ludovic Desroches,ludovic.desroches@microchip.com,1,1,711,0.4092140921409214,1.0,1,1.0,0.0,0.15384615384615385,0.0
78,167953,168278,Maybe you can apply a similar idea to kvm nested on kvm.,"2018-01-16 1:30 GMT+08:00 Vitaly Kuznetsov <vkuznets@redhat.com>:  Maybe you can apply a similar idea to kvm nested on kvm.  Regards, Wanpeng Li",technical,Wanpeng Li,kernellwp@gmail.com,1,0,56,0.12745098039215685,0.47058823529411764,0,0.0,0.0,0.0,0.0
79,167953,168754,"Yes we can. Basically, that would mean directly accessing 'structvmcs12' from L1 hypervisor.","Wanpeng Li <kernellwp@gmail.com> writes:   Yes we can. Basically, that would mean directly accessing 'struct vmcs12' from L1 hypervisor.  --    Vitaly",technical,Vitaly Kuznetsov,vkuznets@redhat.com,1,1,92,0.16666666666666666,0.5294117647058824,0,0.0,0.0,0.0,0.0
80,167953,168847,"Haven't looked into the details, but we have to watch out for other VCPUs trying to modify that vmcs12.Basically because other VCPUs could try to modify values in vmcs12 while we are currently building vmcs02. Nasty races could result in us copying stuff (probably unchecked) into vmcs02 and therefore running something that was not intended. If this is not possible with the current design, perfect :)","On 16.01.2018 13:05, Vitaly Kuznetsov wrote:  Haven't looked into the details, but we have to watch out for other VCPUs trying to modify that vmcs12.  Basically because other VCPUs could try to modify values in vmcs12 while we are currently building vmcs02. Nasty races could result in us copying stuff (probably unchecked) into vmcs02 and therefore running something that was not intended.  If this is not possible with the current design, perfect :)  --   Thanks,  David / dhildenb",technical,David Hildenbrand,david@redhat.com,1,0,402,0.7254901960784313,0.5882352941176471,0,0.0,0.0,0.0,0.0
81,167953,168850,"Yes, the vmcs12 would have to be copied from memory to internal hypervisor data before prepare_vmcs02. I'm curious how well the ""clean"" flags overlap with the choice of fields for which we allow shadow VMCS vmread/vmwrite.","On 16/01/2018 14:39, David Hildenbrand wrote:  Yes, the vmcs12 would have to be copied from memory to internal hypervisor data before prepare_vmcs02.  I'm curious how well the clean"" flags overlap with the choice of fields for which we allow shadow VMCS vmread/vmwrite.  Paolo""",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,222,0.4117647058823529,0.6470588235294118,0,0.0,0.0,0.0,0.0
82,167953,168856,"I don't think we share VMCS among vCPUs, do we?","David Hildenbrand <david@redhat.com> writes:   I don't think we share VMCS among vCPUs, do we?  --    Vitaly",technical,Vitaly Kuznetsov,vkuznets@redhat.com,1,1,47,0.12745098039215685,0.7058823529411765,0,0.0,0.0,0.0,0.0
83,167953,168892,"VMCS is just memory, so who knows what a malicious L1 guest will do. But for vmread/vmwrite we can go through hypervisor memory, for enlightened VMCS we cannot.","On 16/01/2018 14:58, Vitaly Kuznetsov wrote:  VMCS is just memory, so who knows what a malicious L1 guest will do. But for vmread/vmwrite we can go through hypervisor memory, for enlightened VMCS we cannot.  Paolo",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,160,0.3235294117647059,0.7647058823529411,0,0.0,0.0,0.0,0.0
84,167953,168925,"True, not sure if Hyper-V actually copies the data to some internal storage, probably it does. TLFS explicitly forbids making the same enlightened VMCS active on several vCPUs simultaneously but again, this is just memory…","Paolo Bonzini <pbonzini@redhat.com> writes:   True, not sure if Hyper-V actually copies the data to some internal storage, probably it does. TLFS explicitly forbids making the same enlightened VMCS active on several vCPUs simultaneously but again, this is just memory...  --    Vitaly",technical,Vitaly Kuznetsov,vkuznets@redhat.com,1,1,222,0.38235294117647056,0.8235294117647058,0,0.0,0.0,0.0,0.0
85,167953,168937,"FWIF, on nested s390x we pin the guest provided SIE control block (""SCB""- s390x VMCS12). As this is just guest memory, another VCPU can write to that memory. When building our shadow SCB (""VMCS02""), we directly access the pinned block, but we basically only copy values and mask them for the critical parts (execution controls).However, as I realize, the compiler might fetch values several times, so we better add READ_ONCE() to these places. Will look into that.","On 16.01.2018 15:43, Vitaly Kuznetsov wrote:  FWIF, on nested s390x we pin the guest provided SIE control block (SCB"" - s390x VMCS12). As this is just guest memory, another VCPU can write to that memory.  When building our shadow SCB (""VMCS02""), we directly access the pinned block, but we basically only copy values and mask them for the critical parts (execution controls).  However, as I realize, the compiler might fetch values several times, so we better add READ_ONCE() to these places. Will look into that.  --   Thanks,  David / dhildenb""",technical,David Hildenbrand,david@redhat.com,1,0,464,1.0,0.8823529411764706,0,0.0,0.0,0.0,0.0
86,167953,168960,"You don't even need to make them active, you can just scribble on it simultaneously with a VMRESUME.","On 16/01/2018 15:43, Vitaly Kuznetsov wrote:  You don't even need to make them active, you can just scribble on it simultaneously with a VMRESUME.  Paolo",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,100,0.20588235294117646,0.9411764705882353,0,0.0,0.0,0.0,0.0
87,167953,169158,"Nice!IIUC, eVMCS replaces VMCS when enabled, hence doing it for all VMs would be simplest -- we wouldn't need to setup VMCS nor reconfigure Hyper-V on the fly.  (I'm thinking we could have a union in loaded_vmcs for actually used type of VMCS.)Static keys seem like a good choice. I'd go for a separate mapping from Intel VMCS into its MS eVMCS and dirty bit, something like vmcs_field_to_offset_table.Thanks.","2018-01-15 18:30+0100, Vitaly Kuznetsov:  Nice!   IIUC, eVMCS replaces VMCS when enabled, hence doing it for all VMs would be simplest -- we wouldn't need to setup VMCS nor reconfigure Hyper-V on the fly.  (I'm thinking we could have a union in loaded_vmcs for actually used type of VMCS.)   Static keys seem like a good choice.   I'd go for a separate mapping from Intel VMCS into its MS eVMCS and dirty bit, something like vmcs_field_to_offset_table.  Thanks.",technical,Radim Krčmář,rkrcmar@redhat.com,0,0,409,0.8137254901960784,1.0,1,0.0,0.0,0.0,0.0
88,168060,169232,This wants to be split into x86 and core changes. Ideally you make the core changes before the previous patch and add the empty inline into Linux/processor.h....,"On Mon, 15 Jan 2018, Mathieu Desnoyers wrote:   This wants to be split into x86 and core changes. Ideally you make the core changes before the previous patch and add the empty inline into linux/processor.h....  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,161,0.2815533980582524,0.7777777777777778,0,0.0,0.0,0.0,0.0
89,168060,169255,"Good point, will fix. I agree with you that this behavior fits better a ""global"" definition than a ""shared"" one, especially given that it does not target a specific shared memory mapping. The main issue I have is due to the pre-existing MEMBARRIER_CMD_SHARED introduced in Linux 4.3. That one should also have been called ""MEMBARRIER_CMD_GLOBAL"" based on the current line of thoughts. Do you envision a way to transition forward to a new ""MEMBARRIER_CMD_GLOBAL"" for the currently existing MEMBARRIER_CMD_SHARED ?Perhaps with a duplicated enum entry ?enum membarrier_cmd","----- On Jan 16, 2018, at 1:20 PM, Thomas Gleixner tglx@linutronix.de wrote:   Good point, will fix.   I agree with you that this behavior fits better a global"" definition than a ""shared"" one, especially given that it does not target a specific shared memory mapping. The main issue I have is due to the pre-existing MEMBARRIER_CMD_SHARED introduced in Linux 4.3. That one should also have been called ""MEMBARRIER_CMD_GLOBAL"" based on the current line of thoughts.  Do you envision a way to transition forward to a new ""MEMBARRIER_CMD_GLOBAL"" for the currently existing MEMBARRIER_CMD_SHARED ?  Perhaps with a duplicated enum entry ?  enum membarrier_cmd {         MEMBARRIER_CMD_QUERY                                    = 0,         MEMBARRIER_CMD_SHARED                                   = (1 << 0), /* use MEMBARRIER_CMD_GLOBAL instead */         MEMBARRIER_CMD_GLOBAL                                   = (1 << 0), [...] },  Thanks,  Mathieu   --  Mathieu Desnoyers EfficiOS Inc. http://www.efficios.com""",technical,Mathieu Desnoyers,mathieu.desnoyers@efficios.com,1,1,569,1.0,0.8333333333333334,0,0.0,0.0,0.0,0.0
90,168060,169259,"That should work. Though I doubt that you ever can get rid of CMD_SHARED, but at least the code is clearer that way.","On Tue, 16 Jan 2018, Mathieu Desnoyers wrote:  That should work. Though I doubt that you ever can get rid of CMD_SHARED, but at least the code is clearer that way.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,116,0.2524271844660194,0.8888888888888888,0,0.0,0.0,0.0,0.0
91,168060,169273,"Good point, done. The first commit introducing the new command now also introduces the generic stuff moved from the x86 patches.","----- On Jan 16, 2018, at 1:29 PM, Thomas Gleixner tglx@linutronix.de wrote:   Good point, done. The first commit introducing the new command now also introduces the generic stuff moved from the x86 patches.  Thanks,  Mathieu   --  Mathieu Desnoyers EfficiOS Inc. http://www.efficios.com",technical,Mathieu Desnoyers,mathieu.desnoyers@efficios.com,1,1,128,0.23300970873786409,0.9444444444444444,0,1.0,0.0,0.0,0.0
92,168060,169342,Scratch this: it's cleaner if I add a separate generic patch to introduce just the empty inline into linux/processor.h and the ARCH_HAS_SYNC_CORE_BEFORE_USERMODE in init/Kconfig.,"----- On Jan 16, 2018, at 2:22 PM, Mathieu Desnoyers mathieu.desnoyers@efficios.com wrote:   Scratch this: it's cleaner if I add a separate generic patch to introduce just the empty inline into linux/processor.h and the ARCH_HAS_SYNC_CORE_BEFORE_USERMODE in init/Kconfig.  Thanks,  Mathieu    --  Mathieu Desnoyers EfficiOS Inc. http://www.efficios.com",technical,Mathieu Desnoyers,mathieu.desnoyers@efficios.com,1,1,178,0.2621359223300971,1.0,1,1.0,0.0,0.0,0.0
93,168169,173260,"FWIW, SLCG stands for ""second level clock gating"".","On 01/16/2018 12:06 AM, Lyude Paul wrote:  FWIW, SLCG stands for second level clock gating"".  Cheers, Mikko""",technical,Mikko Perttunen,cyndis@kapsi.fi,0,0,50,1.0,0.8571428571428571,0,0.2,0.6,0.2,0.6
94,168668,168726,"boutside protection'? In general I'd rather have the tracepoints when actually submitting the request, with this tracepoint we might be getting a trace which doesn't really indicate if the command was submitted at all.","On 01/16/2018 11:34 AM, Johannes Thumshirn wrote: 'boutside proctetion'?  In general I'd rather have the tracepoints when actually submitting the request, with this tracepoint we might be getting a trace which doesn't really indicate if the command was submitted at all.  Cheers,  Hannes --  Dr. Hannes Reinecke		   Teamlead Storage & Networking hare@suse.de			               +49 911 74053 688 SUSE LINUX GmbH, Maxfeldstr. 5, 90409 Nürnberg GF: F. Imendörffer, J. Smithard, J. Guild, D. Upmanyu, G. Norton HRB 21284 (AG Nürnberg)",technical,Hannes Reinecke,hare@suse.de,1,0,218,0.975609756097561,0.5714285714285714,0,0.0,0.0,0.0,0.0
95,168668,168728,Pleas keep the trace header under drivers/nvme/host/,"On Tue, Jan 16, 2018 at 11:34:00AM +0100, Johannes Thumshirn wrote:  Pleas keep the trace header under drivers/nvme/host/",technical,Christoph Hellwig,hch@lst.de,1,0,52,0.17073170731707318,0.8571428571428571,0,0.0,0.0,0.0,0.0
96,168668,168880,yes but if we really want to be 100% certain we need to take the tracepoints in either in blk_mq_dispatch_rq_list() and trace the return value of the respective->queue_rq() or in the PCI/FC/RDMA drivers.,"On Tue, Jan 16, 2018 at 12:27:33PM +0100, Hannes Reinecke wrote:  yes but if we really want to be 100% certain we need to take the tracepoints in either in blk_mq_dispatch_rq_list() and trace the return value of the respective ->queue_rq() or in the PCI/FC/RDMA drivers.  --  Johannes Thumshirn                                          Storage jthumshirn@suse.de                                +49 911 74053 689 SUSE LINUX GmbH, Maxfeldstr. 5, 90409 Nrnberg GF: Felix Imendrffer, Jane Smithard, Graham Norton HRB 21284 (AG Nrnberg) Key fingerprint = EC38 9CAB C2C4 F25D 8600 D0D0 0393 969D 2D76 0850",technical,Johannes Thumshirn,jthumshirn@suse.de,1,1,203,1.0,1.0,1,0.0,0.0,0.0,0.0
97,169133,177133,"The timer is supposed to be triggered by carrier detect interrupt. After remove the line noise, the carrier detect interrupt is never triggered again, because the carrier is always ok and it only trigger the timer once, Since the protocol was terminated and no new interrupts happen, the link will never be back. So the case here is that the line noise is good and just good to make the carrier detect still good  but the protocol fail, the timer will be never triggered again. Of course, if you increase the noise and make even the carrier detect fail, then remove the noise, the link will be up, Because the carrier down and up again and then trigger the timer to restart. The timer is supposed to restart the protocol again, that's how this whole thing is designed to work. I think you are making changes to the symptom rather than the true cause of the problems you are seeing. Sorry, I will not apply this until the exact issue is better understood. Thank you.","The timer is supposed to be triggered by carrier detect interrupt. After remove the line noise, the carrier detect interrupt is never triggered again, because the carrier is always ok and it only trigger the timer once, Since the protocol was terminated and no new interrupts happen, the link will never be back. So the case here is that the line noise is good and just good to make the carrier detect still good  but the protocol fail, the timer will be never triggered again.  Of course, if you increase the noise and make even the carrier detect fail, then remove the noise, the link will be up, Because the carrier down and up again and then trigger the timer to restart.  Denis Du     On Monday, January 22, 2018, 3:25:16 PM EST, David Miller <davem@davemloft.net> wrote:       From: Denis Du <dudenis2000@yahoo.ca> Date: Tue, 16 Jan 2018 16:58:25 +0000 (UTC)   The timer is supposed to restart the protocol again, that's how this whole thing is designed to work.  I think you are making changes to the symptom rather than the true cause of the problems you are seeing.  Sorry, I will not apply this until the exact issue is better understood.  Thank you.",technical,Denis Du,dudenis2000@yahoo.ca,0,1,965,1.0,0.3,0,0.16216216216216217,0.8108108108108109,0.0,0.0
98,169133,193237,"Ok, I submit it  again. In drivers/net/wan/hdlc_ppp.c, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation This patch is against the kernel version Linux 4.15-rc8From: Please resubmit it and I'll think about it again, thank you.","Ok, I submit it  again.   In drivers/net/wan/hdlc_ppp.c, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation  This patch is against the kernel version Linux 4.15-rc8      On Tuesday, February 6, 2018, 10:29:53 AM EST, David Miller <davem@davemloft.net> wrote:       From: Denis Du <dudenis2000@yahoo.ca>  Date: Tue, 6 Feb 2018 15:15:28 +0000 (UTC)    Please resubmit it and I'll think about it again, thank you.",technical,Denis Du,dudenis2000@yahoo.ca,0,1,332,0.3333333333333333,0.8,0,0.5405405405405406,0.43243243243243246,0.0,0.3783783783783784
99,169133,203117,"How   is your thinking about this patch? [PATCH] netdev: carrier detect ok, don't turn off negotiation Sometimes when physical lines have a just good noise to make the protocol handshaking fail, but the carrier detect still good. Then after remove of the noise, nobody will trigger this protocol to be start again to cause the link to never come back. The fix is when the carrier is still on, not terminate the protocol handshaking.  Ok, I submit it   again. In drivers/net/wan/hdlc_ppp.c, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation This patch is against the kernel version Linux 4.15-rc8  Please resubmit it and I'll think about it again, thank you.","Hi, David:  How  is your thinking about this patch?    From: Denis Du <dudenis2000@yahoo.ca>  Date: Mon, 15 Jan 2018 17:26:06 -0500  Subject: [PATCH] netdev: carrier detect ok, don't turn off negotiation   Sometimes when physical lines have a just good noise to make the protocol  handshaking fail, but the carrier detect still good. Then after remove of  the noise, nobody will trigger this protocol to be start again to cause  the link to never come back. The fix is when the carrier is still on, not  terminate the protocol handshaking.   Signed-off-by: Denis Du <dudenis2000@yahoo.ca>  ---  drivers/net/wan/hdlc_ppp.c | 5 ++++-  1 file changed, 4 insertions(+), 1 deletion(-)   diff --git a/drivers/net/wan/hdlc_ppp.c b/drivers/net/wan/hdlc_ppp.c  index afeca6b..ab8b3cb 100644  --- a/drivers/net/wan/hdlc_ppp.c  +++ b/drivers/net/wan/hdlc_ppp.c  @@ -574,7 +574,10 @@ static void ppp_timer(struct timer_list *t)              ppp_cp_event(proto->dev, proto->pid, TO_GOOD, 0, 0,                       0, NULL),              proto->restart_counter--,  -        } else  +        } else if (netif_carrier_ok(proto->dev))  +            ppp_cp_event(proto->dev, proto->pid, TO_GOOD, 0, 0,  +                     0, NULL),  +        else              ppp_cp_event(proto->dev, proto->pid, TO_BAD, 0, 0,                       0, NULL),          break,  --  2.1.4       On Tuesday, February 06, 2018 12:06:43 PM EST, Denis Du <dudenis2000@yahoo.ca> wrote:         Ok, I submit it  again.   In drivers/net/wan/hdlc_ppp.c, some noise on physical line can cause the carrier detect still ok, but the protocol will fail. So if carrier detect ok, don't turn off protocol negotiation  This patch is against the kernel version Linux 4.15-rc8      On Tuesday, February 6, 2018, 10:29:53 AM EST, David Miller <davem@davemloft.net> wrote:       From: Denis Du <dudenis2000@yahoo.ca>  Date: Tue, 6 Feb 2018 15:15:28 +0000 (UTC)    Please resubmit it and I'll think about it again, thank you.",technical,Denis Du,dudenis2000@yahoo.ca,0,1,763,0.7692307692307693,0.9,0,0.9459459459459459,0.02702702702702703,0.3783783783783784,0.02702702702702703
100,169133,193120,"Please resubmit it and I'll think about it again, thank you.","From: Denis Du <dudenis2000@yahoo.ca> Date: Tue, 6 Feb 2018 15:15:28 +0000 (UTC)   Please resubmit it and I'll think about it again, thank you.",technical,David Miller,davem@davemloft.net,1,0,60,0.07179487179487179,0.7,0,0.5405405405405406,0.43243243243243246,0.0,0.0
101,170193,170195,Well this introduces significant overhead for large sized allocation. Does this not matter because the areas are small? Would it not be better to use compound page allocations here? page_head (whatever) gets you the head page where you can store all sorts of information about the chunk of memory.,"On Tue, 30 Jan 2018, Igor Stoppa wrote:   Well this introduces significant overhead for large sized allocation. Does this not matter because the areas are small?  Would it not be better to use compound page allocations here? page_head(whatever) gets you the head page where you can store all sorts of information about the chunk of memory.",technical,Christopher Lameter,cl@linux.com,1,0,297,0.22357723577235772,0.36,0,0.07692307692307693,0.8461538461538461,0.07692307692307693,0.0
102,170193,188756,"IIUC, he means PageHead(), which is also hard to grep for, since it is a constructed name, via Page","On Thu, Feb 1, 2018 at 11:42 PM, Igor Stoppa <igor.stoppa@huawei.com> wrote:  IIUC, he means PageHead(), which is also hard to grep for, since it is a constructed name, via Page##uname in include/linux/page-flags.h:  __PAGEFLAG(Head, head, PF_ANY) CLEARPAGEFLAG(Head, head, PF_ANY)  -Kees  --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,0,99,0.1016260162601626,0.44,0,0.15384615384615385,0.7692307692307693,0.0,0.0
103,170193,190494,"Thank you for the patch! Yet something to improve:[auto build test ERROR on linus/master][also build test ERROR on v4.17-rc7][cannot apply to next-20180530][if your patch is applied to the wrong git tree, please drop us a note to help improve the system]","Hi Igor,  Thank you for the patch! Perhaps something to improve:  [auto build test WARNING on linus/master] [also build test WARNING on v4.15] [cannot apply to next-20180201] [if your patch is applied to the wrong git tree, please drop us a note to help improve the system]  url:    https://github.com/0day-ci/linux/commits/Igor-Stoppa/mm-security-ro-protection-for-dynamic-data/20180202-123437 config: i386-randconfig-x071-201804 (attached as .config) compiler: gcc-7 (Debian 7.2.0-12) 7.2.1 20171025 reproduce:         # save the attached .config to linux build tree         make ARCH=i386   All warnings (new ones prefixed by >>):     mm/pmalloc.c: In function 'pmalloc_pool_show_avail':      return sprintf(buf, %lu\n"", gen_pool_avail(data->pool)),                           ~~^     ~~~~~~~~~~~~~~~~~~~~~~~~~~                           %u    mm/pmalloc.c: In function 'pmalloc_pool_show_size':    mm/pmalloc.c:81:25: warning: format '%lu' expects argument of type 'long unsigned int', but argument 3 has type 'size_t {aka unsigned int}' [-Wformat=]      return sprintf(buf, ""%lu\n"", gen_pool_size(data->pool)),                           ~~^     ~~~~~~~~~~~~~~~~~~~~~~~~~                           %u  vim +71 mm/pmalloc.c      63	     64	static ssize_t pmalloc_pool_show_avail(struct kobject *dev,     65					       struct kobj_attribute *attr,     66					       char *buf)     67	{     68		struct pmalloc_data *data,     69	     70		data = container_of(attr, struct pmalloc_data, attr_avail),   > 71		return sprintf(buf, ""%lu\n"", gen_pool_avail(data->pool)),     72	}     73	  --- 0-DAY kernel test infrastructure                Open Source Technology Center https://lists.01.org/pipermail/kbuild-all                   Intel Corporation """,technical,kbuild test robot,lkp@intel.com,0,0,254,0.23170731707317074,0.48,0,0.15384615384615385,0.7692307692307693,0.0,0.0
104,170193,190492,"Thank you for the patch! Yet something to improve:[auto build test ERROR on linus/master][also build test ERROR on v4.17-rc7][cannot apply to next-20180530][if your patch is applied to the wrong git tree, please drop us a note to help improve the system]","Hi Igor,  Thank you for the patch! Yet something to improve:  [auto build test ERROR on linus/master] [also build test ERROR on v4.15] [cannot apply to next-20180201] [if your patch is applied to the wrong git tree, please drop us a note to help improve the system]  url:    https://github.com/0day-ci/linux/commits/Igor-Stoppa/mm-security-ro-protection-for-dynamic-data/20180202-123437 config: xtensa-allyesconfig (attached as .config) compiler: xtensa-linux-gcc (GCC) 7.2.0 reproduce:         wget https://raw.githubusercontent.com/intel/lkp-tests/master/sbin/make.cross -O ~/bin/make.cross         chmod +x ~/bin/make.cross         # save the attached .config to linux build tree         make.cross ARCH=xtensa   All error/warnings (new ones prefixed by >>):     mm/pmalloc-selftest.c: In function 'pmalloc_selftest':      var_vmall = vmalloc(SIZE_2),                  ^~~~~~~                  kvmalloc      var_vmall = vmalloc(SIZE_2),                ^      vfree(var_vmall),      ^~~~~      kvfree    cc1: some warnings being treated as errors  vim +43 mm/pmalloc-selftest.c      19	     20	#define validate_alloc(expected, variable, size)	\     21		pr_notice(must be "" expected "": %s"",		\     22			  is_pmalloc_object(variable, size) > 0 ? ""ok"" : ""no"")     23	     24	#define is_alloc_ok(variable, size)	\     25		validate_alloc(""ok"", variable, size)     26	     27	#define is_alloc_no(variable, size)	\     28		validate_alloc(""no"", variable, size)     29	     30	void pmalloc_selftest(void)     31	{     32		struct gen_pool *pool_unprot,     33		struct gen_pool *pool_prot,     34		void *var_prot, *var_unprot, *var_vmall,     35	     36		pr_notice(""pmalloc self-test""),     37		pool_unprot = pmalloc_create_pool(""unprotected"", 0),     38		pool_prot = pmalloc_create_pool(""protected"", 0),     39		BUG_ON(!(pool_unprot && pool_prot)),     40	     41		var_unprot = pmalloc(pool_unprot,  SIZE_1 - 1, GFP_KERNEL),     42		var_prot = pmalloc(pool_prot,  SIZE_1, GFP_KERNEL),   > 43		var_vmall = vmalloc(SIZE_2),     44		is_alloc_ok(var_unprot, 10),     45		is_alloc_ok(var_unprot, SIZE_1),     46		is_alloc_ok(var_unprot, PAGE_SIZE),     47		is_alloc_no(var_unprot, SIZE_1 + 1),     48		is_alloc_no(var_vmall, 10),     49	     50	     51		pfree(pool_unprot, var_unprot),   > 52		vfree(var_vmall),  --- 0-DAY kernel test infrastructure                Open Source Technology Center https://lists.01.org/pipermail/kbuild-all                   Intel Corporation """,technical,kbuild test robot,lkp@intel.com,0,0,254,0.23170731707317074,0.56,0,0.15384615384615385,0.7692307692307693,0.0,0.0
105,170193,188757,"Thank you, I'll try to provide a meaningful reply soon, but I'll be AFK during most of next 2 weeks, so it might be delayed :-(","On 01/02/18 23:11, Kees Cook wrote:   Thank you, I'll try to provide a meaningful reply soon, but I'll be AFK during most of next 2 weeks, so it might be delayed :-(  -- igor",technical,Igor Stoppa,igor.stoppa@huawei.com,0,1,127,0.13414634146341464,0.64,0,0.23076923076923078,0.7692307692307693,0.0,0.0
106,170193,188752,Ok its compound_head(). See also the use in the SLAB and SLUB allocator. If you save the size in the head page struct then you could do that pretty fast.compund pages are higher order pages that are handled as a single page by the VM. See,"On Thu, 1 Feb 2018, Igor Stoppa wrote:   Ok its compound_head(). See also the use in the SLAB and SLUB allocator.   If you save the size in the head page struct then you could do that pretty fast.   compund pages are higher order pages that are handled as a single page by the VM. See https://lwn.net/Articles/619514/",technical,Christopher Lameter,cl@linux.com,1,0,238,0.2073170731707317,0.68,0,0.23076923076923078,0.6923076923076923,0.0,0.0
107,170193,188753,"Ok, now I get what you mean. But it doesn't seem to fit the intended use case, for other reasons(maybe the same, from 2 different POV):- compound pages are aggregates of regular pages, in numbers that are powers of 2, while the amount of pages to allocate is not known upfront. One *could* give a hint to pmalloc about how many pages to allocate every time there is a need to grow the pool.  Iow it would be the size of a chunk. But I'm afraid the granularity would still be pretty low, so maybe it would be 2-4 times less.- the property of the compound page will affect the property of all the pages in the compound, so when one is write protected, it can generate a lot of wasted memory, if there is too much slack (because of the order) With vmalloc, I can allocate any number of pages, minimizing the waste. Finally, there was a discussion about optimization:  The patch I sent does indeed take advantage of the new information, not just for pmalloc use. I have not measured if/where/what there is gain, but it does look like the extra info can be exploited also elsewhere.","On 02/02/18 20:43, Christopher Lameter wrote:  [...]   Ok, now I get what you mean. But it doesn't seem to fit the intended use case, for other reasons (maybe the same, from 2 different POV):  - compound pages are aggregates of regular pages, in numbers that are powers of 2, while the amount of pages to allocate is not known upfront. One *could* give a hint to pmalloc about how many pages to allocate every time there is a need to grow the pool. Iow it would be the size of a chunk. But I'm afraid the granularity would still be pretty low, so maybe it would be 2-4 times less.  - the property of the compound page will affect the property of all the pages in the compound, so when one is write protected, it can generate a lot of wasted memory, if there is too much slack (because of the order) With vmalloc, I can allocate any number of pages, minimizing the waste.   Finally, there was a discussion about optimization: http://www.openwall.com/lists/kernel-hardening/2017/08/07/2  The patch I sent does indeed take advantage of the new information, not just for pmalloc use.  I have not measured if/where/what there is gain, but it does look like the extra info can be exploited also elsewhere.  -- igor",technical,Igor Stoppa,igor.stoppa@huawei.com,0,1,1077,0.943089430894309,0.72,0,0.3076923076923077,0.6923076923076923,0.0,0.07692307692307693
108,170193,188754,I thought the intend here is to create a pool where the whole pool becomes RO?,"On Sat, 3 Feb 2018, Igor Stoppa wrote:   I thought the intend here is to create a pool where the whole pool becomes RO?",technical,Christopher Lameter,cl@linux.com,1,0,78,0.06910569105691057,0.76,0,0.46153846153846156,0.5384615384615384,0.07692307692307693,0.0
109,170193,188749,"LOCAL variable names should be short, and to the point.  If you have some random integer loop counter, it should probably be called ``i` `Calling it ``loop_counter`` is non-productive, if there is no chance of it being mis-understood.  Similarly, ``tmp`` can be just about any type of variable that is used to hold a temporary value.","On Tue, Jan 30, 2018 at 05:14:43PM +0200, Igor Stoppa wrote:  LOCAL variable names should be short, and to the point.  If you have some random integer loop counter, it should probably be called ``i``. Calling it ``loop_counter`` is non-productive, if there is no chance of it being mis-understood.  Similarly, ``tmp`` can be just about any type of variable that is used to hold a temporary value.  (Documentation/process/coding-style.rst)",technical,Matthew Wilcox,willy@infradead.org,1,0,333,0.2845528455284553,0.8,0,0.46153846153846156,0.46153846153846156,0.0,0.15384615384615385
110,170193,188755,"Yes, but why would I force the number of pages in the pool to be a power of 2, when it can be any number? If a need, say, 17 pages, I would have to allocate 32.But it can be worse than that. Since the size of the overall allocated memory is not known upfront, Iwold have a problem to decide how many pages to allocate, every time there is need to grow the pool. Or push the problem to the user of the API, who might be equally unaware. Notice that there is already a function (prealloc) available to the user of the API, if the size is known upfront. So I do not really see how using compound pages would make memory utilization better or even not worse.","On 05/02/18 17:33, Christopher Lameter wrote:  Yes, but why would I force the number of pages in the pool to be a power of 2, when it can be any number?  If a need, say, 17 pages, I would have to allocate 32. But it can be worse than that. Since the size of the overall allocated memory is not known upfront, I wold have a problem to decide how many pages to allocate, every time there is need to grow the pool.  Or push the problem to the user of the API, who might be equally unaware.  Notice that there is already a function (prealloc) available to the user of the API, if the size is known upfront.  So I do not really see how using compound pages would make memory utilization better or even not worse.  -- igor",technical,Igor Stoppa,igor.stoppa@huawei.com,0,1,654,0.5975609756097561,0.84,0,0.6923076923076923,0.23076923076923078,0.15384615384615385,0.0
111,170193,188750,"ok, will do, thanks for the pointer!","On 06/02/18 14:37, Matthew Wilcox wrote:  [...]   [...]   ok, will do, thanks for the pointer!  -- igor",technical,Igor Stoppa,igor.stoppa@huawei.com,0,1,36,0.04065040650406504,0.88,0,0.6923076923076923,0.23076923076923078,0.0,0.0
112,170193,188746,I've done this as the first line of my new documentation files:.. SPDX-License-Identifier: CC-BY-SA-4.0I think this is the CC license that's closest in spirit to the GPL without the unintended consequences of the GPL when used on documentation.  The GFDL seems to be out of favour these days.,"On Fri, Feb 02, 2018 at 05:56:29PM +0200, Igor Stoppa wrote:  I've done this as the first line of my new documentation files:  .. SPDX-License-Identifier: CC-BY-SA-4.0  I think this is the CC license that's closest in spirit to the GPL without the unintended consequences of the GPL when used on documentation.  The GFDL seems to be out of favour these days.",technical,Matthew Wilcox,willy@infradead.org,1,0,292,0.22357723577235772,0.92,0,0.7692307692307693,0.15384615384615385,0.0,0.15384615384615385
113,170193,188747,"I think that's a great license.  I still fear that it is not suitable for kernel documentation, though, especially when we produce documents that include significant text from the (GPL-licensed) kernel source.  The result is almost certainly not distributable, and I don't think that's a good thing.  The GPL is not perfect for documentation, but I don't think that we have a better alternative for in-kernel docs.jon","On Fri, 9 Feb 2018 19:37:14 -0800 Matthew Wilcox <willy@infradead.org> wrote:   I think that's a great license.  I still fear that it is not suitable for kernel documentation, though, especially when we produce documents that include significant text from the (GPL-licensed) kernel source.  The result is almost certainly not distributable, and I don't think that's a good thing.  The GPL is not perfect for documentation, but I don't think that we have a better alternative for in-kernel docs.  jon",technical,Jonathan Corbet,corbet@lwn.net,1,0,417,0.3252032520325203,0.96,0,1.0,0.0,0.15384615384615385,0.0
114,170193,188748,"That's a reasonable concern.  I've read other reasonable concerns about the unintended effects of using the GPL to produce a printed book (e.g. can you print it in a proprietary font, do you have to provide an electronic version of the text, and so on).  I fear these wise words still ring true:  But the real problem is that we as a community lack a copyleft license  that works well for both code and text. About the only thing that even  comes close to working is putting the documentation under the GPL as  well, but the GPL is a poor fit for text. Nonetheless, it may be the  best we have in cases where GPL-licensed code is to be incorporated  into documentation. I dare suggest another possibility: that we create a further exception to the license that the kernel is distributed under.  Something along these lines: Documentation [1] extracted from files marked as GPL [2] may be distributed under the terms of the CC-BY-SA-4.0 license.[1] This includes text explicitly marked for extraction using the kernel-doctool.  It may include short example code sequences.  It does not include code that would normally be expected to be compiled.[We'd want to run it by a lawyer, of course, to have them check for unintended consequences.","On Mon, Feb 12, 2018 at 08:28:49AM -0700, Jonathan Corbet wrote:  That's a reasonable concern.  I've read other reasonable concerns about the unintended effects of using the GPL to produce a printed book (eg can you print it in a proprietary font, do you have to provide an electronic version of the text, and so on).  I fear these wise words still ring true:    But the real problem is that we as a community lack a copyleft license   that works well for both code and text. About the only thing that even   comes close to working is putting the documentation under the GPL as   well, but the GPL is a poor fit for text. Nonetheless, it may be the   best we have in cases where GPL-licensed code is to be incorporated   into documentation.  I dare suggest another possibility: that we create a further exception to the license that the kernel is distributed under.  Something along these lines:  Documentation [1] extracted from files marked as GPL [2] may be distributed under the terms of the CC-BY-SA-4.0 license.  [1] This includes text explicitly marked for extraction using the kernel-doc tool.  It may include short example code sequences.  It does not include  code that would normally be expected to be compiled.  [2] GPL-2.0, GPL-2.0+, GPL-1.0+, LGPL-2.0, LGPL-2.0+, LGPL-2.1, LGPL-2.1+  We'd want to run it by a lawyer, of course, to have them check for unintended consequences.",technical,Matthew Wilcox,willy@infradead.org,1,0,1237,1.0,1.0,1,1.0,0.0,0.0,0.0
115,173153,173154,May be you can merge above with the previous entry which already has it. Otherwise looks good,"On 22/01/18 16:09, Aishwarya Pant wrote:  May be you can merge above with the previous entry which already has /sys/devices/system/cpu/cpuidle/current_driver /sys/devices/system/cpu/cpuidle/current_governer_ro  Otherwise looks good.  --  Regards, Sudeep",technical,Sudeep Holla,sudeep.holla@arm.com,1,0,93,1.0,1.0,1,0.0,0.0,0.0,0.0
116,174463,174487,I already sent a fix for this,"Thu, Jan 18, 2018 at 02:17:28PM CET, arnd@arndb.de wrote:  I already sent a fix for this: http://patchwork.ozlabs.org/patch/862787/",technical,Jiri Pirko,jiri@resnulli.us,1,0,29,1.0,0.5,0,0.0,0.0,0.0,0.0
117,174463,174517,Okay. Will add comment.,"Thu, Jan 18, 2018 at 03:19:14PM CET, arnd@arndb.de wrote:  Okay. Will add comment.",technical,Jiri Pirko,jiri@resnulli.us,1,0,23,0.8571428571428571,1.0,1,0.0,0.0,0.0,0.0
118,174735,175104,"I am removing checks from core. Export and import were optional in beginning of crypto framework, but as time goes on they become mandatory.","On 18.01.2018 22:31, Marek Vasut wrote:  I am removing checks from core. Export and import were optional in beginnig of crypto framework, but as time goes on they become mandatory.   --  Best regards, Kamil Konieczny Samsung R&D Institute Poland",technical,Kamil Konieczny,k.konieczny@partner.samsung.com,0,1,140,0.3103448275862069,0.4444444444444444,0,0.0,0.9642857142857143,0.0,0.0
119,174735,175112,"Seems like if the driver doesn't implement those, the core can easily detect that and perform the necessary action. Moving the checks out of core seems like the wrong thing to do, rather you should enhance the checks in core if they're insufficient in my opinion.","On 01/19/2018 10:53 AM, Kamil Konieczny wrote:  Seems like if the driver doesn't implement those, the core can easily detect that and perform the necessary action. Moving the checks out of core seems like the wrong thing to do, rather you should enhance the checks in core if they're insufficient in my opinion.    --  Best regards, Marek Vasut",technical,Marek Vasut,marex@denx.de,1,0,263,0.5977011494252874,0.5,0,0.0,0.9642857142857143,0.0,0.0
120,174735,175137,"I removed all checks. No checks in driver and no checks in crypto framework. If you would like any check, I think the place to add them is in ahash alg registration, in function ahash_prepare_alg add something like","On 19.01.2018 11:08, Marek Vasut wrote:  I removed all checks. No checks in driver and no checks in crypto framework.  If you would like any check, I think the place to add them is in ahash alg registraction, in function ahash_prepare_alg add something like:  	if ((alg->init == NULL) || 	    (alg->digest == NULL) || 	    (alg->final == NULL) || 	    (alg->update == NULL) || 	    (alg->export == NULL) || 	    (alg->import == NULL)) 		return -EINVAL,  The only downsize is this will be usefull in backport (to prevent NULL dereference), as any new driver will have all those pointers sets.    --  Best regards, Kamil Konieczny Samsung R&D Institute Poland",technical,Kamil Konieczny,k.konieczny@partner.samsung.com,0,1,214,0.4827586206896552,0.5555555555555556,0,0.0,0.9642857142857143,0.0,0.9642857142857143
121,174735,199556,All applied.  Thanks.-,"On Thu, Jan 18, 2018 at 07:33:59PM +0100, Kamil Konieczny wrote:  All applied.  Thanks. --  Email: Herbert Xu <herbert@gondor.apana.org.au> Home Page: http://gondor.apana.org.au/~herbert/ PGP Key: http://gondor.apana.org.au/~herbert/pubkey.txt",technical,Herbert Xu,herbert@gondor.apana.org.au,1,0,22,0.04597701149425287,0.6111111111111112,0,0.9642857142857143,0.0,0.9642857142857143,0.0
122,174735,199597,"The bug can only be in driver which will not implement those two functions ,but we already had all drivers with those due to patches 1..4 All other drivers do have them. Additionally, with crypto we want minimize code and run as fast as possible. Moving checks out of core will impose on driver author need for implement those functions, or declare them empty, but in case of empty ones crypto will not work properly with such driver.","On 15.02.2018 17:27, Marek Vasut wrote:  The bug can only be in driver which will not implement those two functions, but we already had all drivers with those due to patches 1..4 All other drivers do have them.  Additionally, with crypto we want minimize code and run as fast as possible.  Moving checks out of core will impose on driver author need for implement those functions, or declare them empty, but in case of empty ones  crypto will not work properly with such driver.  --  Best regards, Kamil Konieczny Samsung R&D Institute Poland",technical,Kamil Konieczny,k.konieczny@partner.samsung.com,0,1,434,1.0,0.7222222222222222,0,0.9642857142857143,0.0,0.0,0.0
123,180488,180504,"Page tables are protected by their locks.  VMAs may change while migration is active on them, but does that need locking against? I have not been keeping up with Michal's recent migration changes, but migrate_pages() never used to need mmap_sem held (despite being called with an mmap_sem held from some of its call sites), and it would be a backward step to require that now. There is not even an mm argument to migrate_pages(), so which mm->mmap_sem do you think would be required for it?  There may be particular cases in which it is required (when the new_page function involves the old_page's vma - is that so below?), but in general not.","On Mon, 29 Jan 2018, Zi Yan wrote:  Page tables are protected by their locks.  VMAs may change while migration is active on them, but does that need locking against?   I have not been keeping up with Michal's recent migration changes, but migrate_pages() never used to need mmap_sem held (despite being called with an mmap_sem held from some of its callsites), and it would be a backward step to require that now.  There is not even an mm argument to migrate_pages(), so which mm->mmap_sem do you think would be required for it?  There may be particular cases in which it is required (when the new_page function involves the old_page's vma - is that so below?), but in general not.  Hugh",technical,Hugh Dickins,hughd@google.com,1,0,643,0.9060402684563759,0.18181818181818182,0,0.0,1.0,0.0,0.0
124,180488,180546,"This doesn't make much sense to me, to be honest. We are holding mmap_sem for _read_ so we allow parallel updates like page faults or unmaps. Therefore we are isolating pages prior to the migration. The sole purpose of the mmap_sem in add_page_for_migration is to protect from vma going away _while_ need it to get the proper page. Moving the lock up is just wrong because it allows caller to hold the lock for way too long if a lot of pages is migrated. Not only that, it is even incorrect because we are doing get_user() (aka page fault) and while read lock recursion is OK, we might block and deadlock when there is a writer pending. I haven't checked the current implementation of semaphores but I believe we do not allow recursive locking","On Mon 29-01-18 22:00:11, Zi Yan wrote:  This doesn't make much sense to me, to be honest. We are holding mmap_sem for _read_ so we allow parallel updates like page faults or unmaps. Therefore we are isolating pages prior to the migration.  The sole purpose of the mmap_sem in add_page_for_migration is to protect from vma going away _while_ need it to get the proper page.  Moving the lock up is just wrong because it allows caller to hold the lock for way too long if a lot of pages is migrated. Not only that, it is even incorrect because we are doing get_user() (aka page fault) and while read lock recursion is OK, we might block and deadlock when there is a writer pending. I haven't checked the current implementation of semaphores but I believe we do not allow recursive locking.   --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,743,1.0,0.2727272727272727,0,0.0,0.0,0.0,0.0
125,180488,180839,mmap_sem is held during migrate_pages() in current implementation.,"Hugh Dickins wrote:  mmap_sem is held during migrate_pages() in current implementation. http://elixir.free-electrons.com/linux/latest/source/mm/migrate.c#L1576   --  Best Regards, Yan Zi",technical,Zi Yan,zi.yan@cs.rutgers.edu,0,0,66,0.0738255033557047,0.5454545454545454,0,0.0,0.0,0.0,0.0
126,180488,180846,You mean in the original code? I strongly suspect this was to not take it for each page.,"On Tue 30-01-18 10:52:58, Zi Yan wrote:  You mean in the original code? I strongly suspect this was to not take it for each page. --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,88,0.1342281879194631,0.7272727272727273,0,0.0,0.0,0.0,0.0
127,180488,180956,"Right. The original code gathers 169 pages, whose information (struct page_to_node, 24bytes) fits into a 4KB page, then migrates them at a time. So mmap_sem is not held for long in the original code, because of this design. I think the question is whether we need to hold mmap_sem for migrate_pages(). Hugh also agrees it is not necessary on a separate email. But it is held in the original code.","On 30 Jan 2018, at 11:10, Michal Hocko wrote:   Right. The original code gathers 169 pages, whose information (struct page_to_node, 24bytes) fits into a 4KB page, then migrates them at a time. So mmap_sem is not held for long in the original code, because of this design.  I think the question is whether we need to hold mmap_sem for migrate_pages(). Hugh also agrees it is not necessary on a separate email. But it is held in the original code.  -- Best Regards Yan Zi",technical,Zi Yan,zi.yan@cs.rutgers.edu,0,0,396,0.5637583892617449,0.8181818181818182,0,0.0,0.0,0.0,0.0
128,180488,181353,"[...]I would be really surprised if we really needed the lock. If we do, however, then we really need a very good explanation why. The code used to do so is not a valid reason.","On Tue 30-01-18 14:12:28, Zi Yan wrote: [...]  I would be really surprised if we really needed the lock. If we do, however, then we really need a very good explanation why. The code used to do so is not a valid reason.  --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,176,0.28859060402684567,1.0,1,1.0,0.0,0.0,0.0
129,183468,183474,Compiled and booted on my test system. No dmesg regressions. Thanks,"On 02/26/2018 01:15 PM, Greg Kroah-Hartman wrote:  Compiled and booted on my test system. No dmesg regressions.  thanks, -- Shuah",technical,Shuah Khan,shuahkh@osg.samsung.com,1,0,67,0.10655737704918032,0.7142857142857143,0,0.0,0.0,0.0,0.0
130,183468,183471,"There are 13 patches in this series, all will be posted as a response  to this one.   If anyone has any issues with these being applied, please let me know. Responses should be made by Wed. Anything received after that time might be too late.    The whole patch series can be found in one patch at  or in the git tree and branch at:              git: linux-3.18.y  and the diffstat can be found below.  No regressions noticed on the tree requires reverting commit  to avoid conflicting with the patch titled &quot,usb: gadget: f_fs: Process all descriptors during, has no merge problems. Thanks for the update.","<br><br><div class=gmail_quote""><div dir=""ltr"">On Tue 27 Feb, 2018, 1:47 AM Greg Kroah-Hartman, &lt,<a href=""mailto:gregkh@linuxfoundation.org"">gregkh@linuxfoundation.org</a>&gt, wrote:<br></div><blockquote class=""gmail_quote"" style=""margin:0 0 0 .8ex,border-left:1px #ccc solid,padding-left:1ex"">This is the start of the stable review cycle for the 3.18.97 release.<br> There are 13 patches in this series, all will be posted as a response<br> to this one.  If anyone has any issues with these being applied, please<br> let me know.<br> <br> Responses should be made by Wed Feb 28 20:15:12 UTC 2018.<br> Anything received after that time might be too late.<br> <br> The whole patch series can be found in one patch at:<br>         <a href=""https://www.kernel.org/pub/linux/kernel/v3.x/stable-review/patch-3.18.97-rc1.gz"" rel=""noreferrer"" target=""_blank"">https://www.kernel.org/pub/linux/kernel/v3.x/stable-review/patch-3.18.97-rc1.gz</a><br> or in the git tree and branch at:<br>         git://<a href=""http://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable-rc.git"" rel=""noreferrer"" target=""_blank"">git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable-rc.git</a> linux-3.18.y<br> and the diffstat can be found below.<br></blockquote></div><div><br></div><div>No regressions noticed on the OnePlus 3T. CAF&#39,s msm-3.18 tree requires reverting commit <a href=""https://source.codeaurora.org/quic/la/kernel/msm-3.18/commit?id=1278f001ef9bf1329bc2aa123f6038ad9f8a65ee"">https://source.codeaurora.org/quic/la/kernel/msm-3.18/commit?id=1278f001ef9bf1329bc2aa123f6038ad9f8a65ee</a> to avoid conflicting with the patch titled &quot,usb: gadget: f_fs: Process all descriptors during bind&quot,, kernel-common has no merge problems. Thanks for the update.</div><div><br></div><div>Harsh Shandilya</div><div><br></div><div class=""gmail_quote""><blockquote class=""gmail_quote"" style=""margin:0 0 0 .8ex,border-left:1px #ccc solid,padding-left:1ex""> </blockquote></div> """,technical,Harsh Shandilya,msfjarvis@gmail.com,0,0,610,1.0,0.7619047619047619,0,0.0,0.0,0.0,0.0
131,183468,183470,xtensa patch is now dropped,"On Tue, Feb 27, 2018 at 06:53:58AM -0800, Guenter Roeck wrote:  xtensa patch is now dropped, thanks.  greg k-h",technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,1,27,0.040983606557377046,1.0,1,0.0,0.0,0.0,0.0
132,188789,188794,"I wanted, I got. This is upstream already (66f793099a63) so this commit message possibly wants tweaking slightly","On Mon, 2018-02-12 at 13:48 +0100, Peter Zijlstra wrote:  I wanted, I got. This is upstream already (66f793099a63) so this commit message possibly wants tweaking slightly.   Acked-by: David Woodhouse <dwmw@amazon.co.uk>",technical,David Woodhouse,dwmw2@infradead.org,1,0,112,0.5833333333333334,0.8461538461538461,0,0.0,0.0,0.0,0.0
133,188789,188795,"Right you are, I should have re-read these Changelogs before posting ,-)","On Mon, Feb 12, 2018 at 01:01:20PM +0000, David Woodhouse wrote:  Right you are, I should have re-read these Changelogs before posting ,-)",technical,Peter Zijlstra,peterz@infradead.org,1,1,72,0.4166666666666667,0.9230769230769231,0,0.0,0.0,0.0,0.0
134,188789,188792,"I couldn't make it work there, but it could be my makefile foo isn't strong enough. The ordering of arch/*/Makefile vs scripts/Makefile.build is forever confusing me.","On Mon, Feb 12, 2018 at 12:57:40PM +0000, David Woodhouse wrote:  I couldn't make it work there, but it could be my makefile foo isn't strong enough. The ordering of arch/*/Makefile vs scripts/Makefile.build is forever confusing me.",technical,Peter Zijlstra,peterz@infradead.org,1,1,166,0.9166666666666666,1.0,1,0.0,0.0,0.0,0.0
135,188789,188791,Can't you do that with the existing check in arch/x86/Makefile? cf.   which is going nowhere until gets fixed but still I'd like to have the check in *one* place.,"On Mon, 2018-02-12 at 13:48 +0100, Peter Zijlstra wrote:  Can't you do that with the existing check in arch/x86/Makefile?   cf. http://git.infradead.org/users/dwmw2/linux-retpoline.git/commitdiff/eb12299ed which is going nowhere until https://bugs.llvm.org/show_bug.cgi?id=36329 gets fixed but still I'd like to have the check in *one* place.",technical,David Woodhouse,dwmw2@infradead.org,1,0,162,1.0,0.7692307692307693,0,0.0,0.0,0.0,0.0
136,189397,189398,any objection to this patch?,"On Tue, Feb 06, 2018 at 11:09:37AM -0800, Jin Qian wrote:  Ted, any objection to this patch?  thanks,  greg k-h",technical,Greg KH,gregkh@linuxfoundation.org,1,0,28,0.023166023166023165,0.2857142857142857,0,0.0,0.0,0.0,0.0
137,189397,189399,"No objections with my ext4 hat on. It should be noted though that this is a partial backport because it only fixes ext4, while Al's original upstream fix addressed a much larger set of file systems.  In the Android kernel the f2fs fix had been backported separately.  But for the upstream kernel, it *might*be the case that we should try backporting the original commit so that in case there is some other general purpose distribution decides (a)to base their system on 4.4, and (b) support a 32-bit kernel, they get the more general bug fixes which applies for btrfs, isofs, ocfs2, nfs,etc.I haven't been paying attention to what LTS kernels general purpose distro's are using, so I don't know how important this would be.  And if there are companies like Cloudflare which are using upstream LTS kernel, it seems unlikely they would want to use a 32-bit kernel, so.... shrug.  Greg, I'll let you decide if you want to backport the full commit or not.(We had a similar discussion on the AOSP kernel, and came to the conclusion that we only needed to make the patch support ext4.  No one was going to test the other file systems besides ext4 and f2fs, anyway.  But the calculus might be different might be different for the general upstream LTS kernel.)","On Tue, Feb 06, 2018 at 12:39:53PM -0800, Greg KH wrote:  No objections with my ext4 hat on.  It should be noted though that this is a partial backport because it only fixes ext4, while Al's original upstream fix addressed a much larger set of file systems.  In the Android kernel the f2fs fix had been backported separately.  But for the upstream kernel, it *might* be the case that we should try backporting the original commit so that in case there is some other general purpose distribution decides (a) to base their system on 4.4, and (b) support a 32-bit kernel, they get the more general bug fixes which applies for btrfs, isofs, ocfs2, nfs, etc.  I haevn't been paying attention to what LTS kernels general purpose distro's are using, so I don't know how important this would be.  And if there are companies like Cloudflare which are using upstream LTS kernel, it seems unlikely they would want to use a 32-bit kernel, so.... shrug.  Greg, I'll let you decide if you want to backport the full commit or not.  (We had a similar discussion on the AOSP kernel, and came to the conclusion that we only needed to make the patch support ext4.  No one was going to test the other file systems besides ext4 and f2fs, anyway.  But the calculus might be different might be different for the general upstream LTS kernel.)  				- Ted",technical,Theodore Ts'o,tytso@mit.edu,1,0,1252,1.0,0.42857142857142855,0,0.0,0.0,0.0,0.0
138,189397,189400,"Well, the main point of backporting this change is to fix symlink decryption on32-bit systems.  So, it would be needed on both ext4 and f2fs.  Jin, it might be a good idea to fix f2fs in this patch at well, since unlike the AOSP kernels, the LTS kernels do not have the latest f2fs backported to them. I don't think backporting this change for other filesystems is particularly important, since if I understand correctly, the reasons that Al made the change originally were:- to allow following symlinks in RCU mode, but that's not implemented in old  kernels- to prevent a process from using up all kmaps and deadlocking the system, which  I'm not sure is a real problem (someone would need to try to put together a  reproducer), but if so it would probably just be a local device of service. Also if we actually backported the full commit there are follow-on fixes such ase8ecde25f5e that would be needed as well but might be missed.","On Tue, Feb 06, 2018 at 06:11:49PM -0500, Theodore Ts'o wrote:  Well, the main point of backporting this change is to fix symlink decryption on 32-bit systems.  So, it would be needed on both ext4 and f2fs.  Jin, it might be a good idea to fix f2fs in this patch at well, since unlike the AOSP kernels, the LTS kernels do not have the latest f2fs backported to them.  I don't think backporting this change for other filesystems is particularly important, since if I understand correctly, the reasons that Al made the change originally were:  - to allow following symlinks in RCU mode, but that's not implemented in old   kernels  - to prevent a process from using up all kmaps and deadlocking the system, which   I'm not sure is a real problem (someone would need to try to put together a   reproducer), but if so it would probably just be a local device of service.  Also if we actually backported the full commit there are follow-on fixes such as e8ecde25f5e that would be needed as well but might be missed.  Thanks,  - Eric",technical,Eric Biggers,ebiggers3@gmail.com,1,0,935,0.722007722007722,0.5714285714285714,0,0.0,0.0,0.0,0.0
139,189397,189402,"Sure, uploaded. PTAL.jin","Sure, uploaded https://lkml.org/lkml/2018/2/6/856. PTAL.  jin   On Tue, Feb 6, 2018 at 3:38 PM Eric Biggers <ebiggers3@gmail.com> wrote:",technical,Jin Qian,jinqian@google.com,0,0,24,0.019305019305019305,0.7142857142857143,0,0.0,0.0,0.0,0.0
140,189397,189401,"Yup... and *that's* only a problem on 32-bit systems.  And aside from Android, it's unclear to me how much we need to support 32-bit systems on upstream LTS kernels.  I suppose there might be Raspberry PI's which are 32-bits and which might want to use btrfs.  Personally I'm not sure we should care all that much, but others who care more about LTS kernels and 32-bit systems might have a different opinion.","On Tue, Feb 06, 2018 at 03:38:09PM -0800, Eric Biggers wrote:  Yup.   .. and *that's* only a problem on 32-bit systems.  And aside from Android, it's unclear to me how much we need to support 32-bit systems on upstream LTS kernels.  I suppose there might be Rasperry PI's which are 32-bits and which might want to use btrfs.  Personally I'm not sure we should care all that much, but others who care more about LTS kernels and 32-bit systems might have a different opinion.   Good point.  					- Ted",technical,Theodore Ts'o,tytso@mit.edu,1,0,408,0.3281853281853282,1.0,1,0.0,0.0,0.0,0.0
141,191357,191568,"HI, You have on extra line here The leading 0 will generate a DT warning Since you have that extra line above, you can just drop this new line here. Thanks!","Hi,  On Fri, Feb 02, 2018 at 10:01:51PM +0800, Icenowy Zheng wrote:  You have on extra line here   The leading 0 will generate a DT warning   Since you have that extra line above, you can just drop this new line here.  Thanks! Maxime  --  Maxime Ripard, Bootlin (formerly Free Electrons) Embedded Linux and Kernel engineering http://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,156,1.0,0.7142857142857143,0,0.0,1.0,0.0,0.0
142,191357,191567,This should be the whole size of the memory region.,"On Fri, Feb 02, 2018 at 10:01:52PM +0800, Icenowy Zheng wrote:  This should be the whole size of the memory region.  Maxime  --  Maxime Ripard, Bootlin (formerly Free Electrons) Embedded Linux and Kernel engineering http://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,51,0.3142857142857143,0.8571428571428571,0,0.0,1.0,0.0,1.0
143,191357,192759,OK. (Although on the datasheet digital part and analog part is listed as one region.),"于 2018年2月3日 GMT+08:00 上午3:49:35, Maxime Ripard <maxime.ripard@bootlin.com> 写到:  OK. (Although on the datasheet digital part and analog part is listed as one region.)",technical,Icenowy Zheng,icenowy@aosc.io,1,1,85,0.5142857142857142,1.0,1,1.0,0.0,1.0,0.0
144,195592,197910,"Queued, thanks.","On 08/02/2018 22:35, David Rientjes wrote:  Queued, thanks.  Paolo",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,15,0.0392156862745098,0.14285714285714285,0,0.8,0.0,0.8,0.0
145,195592,197915,I am not arguing about the kvm change but do we actually want to warn for 0 sized allocations? This just doesn't make much sense to me. In other words don't we want this?,"On Thu 08-02-18 13:35:08, David Rientjes wrote:  I am not arguing about the kvm change but do we actaully want to warn for 0 sized allocations? This just doesn't make much sense to me. In other words don't we want this?  diff --git a/mm/vmalloc.c b/mm/vmalloc.c index 673942094328..c5d832510c54 100644 --- a/mm/vmalloc.c +++ b/mm/vmalloc.c @@ -1748,7 +1748,9 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,  	unsigned long real_size = size,    	size = PAGE_ALIGN(size), -	if (!size || (size >> PAGE_SHIFT) > totalram_pages) +	if (!size) +		return NULL, +	if ((size >> PAGE_SHIFT) > totalram_pages)  		goto fail,    	area = __get_vm_area_node(size, align, VM_ALLOC | VM_UNINITIALIZED | --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,170,0.38235294117647056,0.21428571428571427,0,0.8,0.0,0.0,0.0
146,195592,197935,"There have been quite a few reports of this from syzkaller and generally we've fixed them.  It does seem like a recipe for NULL-pointer dereferences when the size is user-controlled (as in this case).But here I'm actually not sure that the ""allocation failure: 0 bytes ""can happen, since we have a check above for ""if (routing.nr)"", and there is a check also so that the maximum allocation here is a meagre 128 KiB. So I'm wondering if this patch is obsolete actually after commit.","On 13/02/2018 15:48, Michal Hocko wrote:  There have been quite a few reports of this from syzkaller and generally we've fixed them.  It does seem like a recipe for NULL-pointer dereferences when the size is user-controlled (as in this case).  But here I'm actually not sure that the allocation failure: 0 bytes"" can happen, since we have a check above for ""if (routing.nr)"", and there is a check also so that the maximum allocation here is a meager 128 KiB.  So I'm wondering if this patch is obsolete actually after commit f8c1b85b2523.  David?  Thanks,  Paolo  Paolo""",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,481,1.0,0.2857142857142857,0,0.8,0.0,0.0,0.0
147,195592,197940,Are you sure that you got the right vmalloc?,"On 02/08/2018 10:35 PM, David Rientjes wrote: [...]                                                                         ^^^^^         ^^^^^   Are you sure that you got the right vmalloc?",technical,Christian Borntraeger,borntraeger@de.ibm.com,1,0,44,0.09803921568627451,0.35714285714285715,0,0.8,0.0,0.0,0.0
148,195592,197946,"Nice catch!  But well, it's the only one in the whole file. :) That seems very much like an old patch then.  I'm unqueuing it.","On 13/02/2018 16:14, Christian Borntraeger wrote:  Nice catch!  But well, it's the only one in the whole file. :)  That seems very much like an old patch then.  I'm unqueuing it.  Paolo",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,126,0.3235294117647059,0.42857142857142855,0,0.8,0.0,0.0,0.0
149,195592,197985,We do return NULL for that case regardless the above. The patch just doesn't warn. Or do you think it is helpful to warn?,"On Tue 13-02-18 16:03:09, Paolo Bonzini wrote:  We do return NULL for that case regardless the above. The patch just doesn't warn. Or do you think it is helpful to warn? --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,121,0.27450980392156865,0.5,0,0.8,0.0,0.0,0.0
150,195592,197990,"It certainly helps bringing potential issues in the spotlight (through fuzzing, mostly).","On 13/02/2018 16:44, Michal Hocko wrote:  It certainly helps bringing potential issues in the spotlight (through fuzzing, mostly).  Paolo",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,88,0.1568627450980392,0.5714285714285714,0,0.8,0.0,0.0,0.0
151,195592,198359,"It's not a catch at all, the fact that I saw this warning with an older kernel for KVM_SET_GSI_ROUTING doesn't mean that I can't patch it with an upstream kernel.  Would you prefer I remove the stack trace completely?","On Tue, 13 Feb 2018, Paolo Bonzini wrote:   It's not a catch at all, the fact that I saw this warning with an older  kernel for KVM_SET_GSI_ROUTING doesn't mean that I can't patch it with an  upstream kernel.  Would you prefer I remove the stack trace completely?",technical,David Rientjes,rientjes@google.com,1,1,217,0.4411764705882353,0.7142857142857143,0,1.0,0.0,0.0,0.0
152,195592,198460,"FWIW, your stack trace did not complain about a too big allocation, it complained about 0 allocation. this case should be prevented. The only question is does your patch makes sense nevertheless as we gracefully handle the ENOMEM case? So a reproducer on a newer kernel would be good. Maybe use the ""vmalloc"" kernel parameter to force this.","On 02/14/2018 02:03 AM, David Rientjes wrote:  FWIW, your stack trace did not complain about a too big allocation, it complained about 0 allocation:  ----- snip ------ vmalloc: allocation failure: 0 bytes, mode:0x24000c2(GFP_KERNEL|__GFP_HIGHMEM) ----- snip ------  After commit f8c1b85b2523 (KVM: x86: avoid vmalloc(0) in the KVM_SET_CPUID)"" this case should be prevented. The only question is does your patch makes sense nevertheless as we gracefully handle the ENOMEM case? So a reproducer on a newer kernel would be good. Maybe use the ""vmalloc"" kernel parameter to force this.""",technical,Christian Borntraeger,borntraeger@de.ibm.com,1,0,340,0.6568627450980392,0.7857142857142857,0,1.0,0.0,0.0,0.0
153,195592,198520,"The upstream kernel doesn't warn.  It checks ""if (routing.nr)"" before calling vmalloc. Paolo","On 14/02/2018 02:03, David Rientjes wrote:  The upstream kernel doesn't warn.  It checks if (routing.nr)"" before calling vmalloc.  Paolo""",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,92,0.19607843137254902,0.8571428571428571,0,1.0,0.0,0.0,0.0
154,195592,198558,It will warn of the vmalloc space is really exhausted. But then I really ask myself if we really want to suppress this warning. This should be a big ALERT to the host admin.,"On 02/14/2018 11:10 AM, Paolo Bonzini wrote:  It will warn of the vmalloc space is really exhausted. But then I really ask myself if we really want to suppress this warning. This should be a big ALERT to the host admin.",technical,Christian Borntraeger,borntraeger@de.ibm.com,1,0,173,0.3627450980392157,0.9285714285714286,0,1.0,0.0,0.0,0.0
155,195592,198582,Especially since the biggest allocation it can do is 128 KiB...Paolo,"On 14/02/2018 12:14, Christian Borntraeger wrote:  Especially since the biggest allocation KVM_SET_GSI_ROUTING can do is 128 KiB...  Paolo",technical,Paolo Bonzini,pbonzini@redhat.com,1,0,68,0.12745098039215685,1.0,1,1.0,0.0,0.0,0.0
156,197454,197455,"Shouldn't this be the other way around?  Everyone is used to 7 now, so you're changing the default back to 6.  I would think that it should be 7 by default, and platforms like Brahma-B53 should force it to 6.","On 02/12/2018 05:45 PM, Florian Fainelli wrote:  Shouldn't this be the other way around?  Everyone is used to 7 now, so  you're changing the default back to 6.  I would think that it should be  7 by default, and platforms like Brahma-B53 should force it to 6.  --  Qualcomm Datacenter Technologies, Inc. as an affiliate of Qualcomm Technologies, Inc.  Qualcomm Technologies, Inc. is a member of the Code Aurora Forum, a Linux Foundation Collaborative Project.",technical,Timur Tabi,timur@codeaurora.org,1,0,208,0.31125827814569534,0.2857142857142857,0,0.0,0.8571428571428571,0.0,0.0
157,197454,197457,"That is debatable, is there a good publicly available table of what the typical L1 cache line size is on ARMv8 platforms?","On 02/12/2018 03:52 PM, Timur Tabi wrote:  That is debatable, is there a good publicly available table of what the typical L1 cache line size is on ARMv8 platforms? --  Florian",technical,Florian Fainelli,f.fainelli@gmail.com,1,1,121,0.15894039735099338,0.42857142857142855,0,0.0,0.8571428571428571,0.0,0.0
158,197454,197460,"I don't have that, but I was under the impression that we moved from 6to 7 because more and more ARMv8 platforms have 128-byte caches, so that is the ""new normal""","On 02/12/2018 05:57 PM, Florian Fainelli wrote:  I don't have that, but I was under the impression that we moved from 6  to 7 because more and more ARMv8 platforms have 128-byte caches, so that  is the new normal"".  --  Qualcomm Datacenter Technologies, Inc. as an affiliate of Qualcomm Technologies, Inc.  Qualcomm Technologies, Inc. is a member of the Code Aurora Forum, a Linux Foundation Collaborative Project.""",technical,Timur Tabi,timur@codeaurora.org,1,0,162,0.23841059602649006,0.5714285714285714,0,0.0,0.8571428571428571,0.0,0.0
159,197454,197461,"That does not seem to be the data that I am collecting from ARM's website and some quick googling: The following cores appear to have a 64bytes L1D cache line size: A55,A73 (fixed), A35, A32, A53, A57 (fixed), A72 (fixed) even the Falkor seems to be that way according to [1].APM Mustang also seems to be 64b L1D according to [2]And then we seem to covering what the ARM64 mainline kernel knows about non-ARM implementations: ThunderX and ThunderX2 (formerly BroadcomVulcan). There is possibly the Qualcomm Kryo is different, but wikipedia seems to suggest it is a derivative of existing Cortex-A CPUs which have a 64b cache line size. Let's see what Catalin and Will think about what the default should be.","On 02/12/2018 04:10 PM, Timur Tabi wrote:  That does not seem to be the data that I am collecting from ARM's website and some quick googling:  The following cores appear to have a 64bytes L1D cache line size: A55, A73 (fixed), A35, A32, A53, A57 (fixed), A72 (fixed) even the Falkor seems to be that way according to [1].  APM Mustang also seems to be 64b L1D according to [2].  [1]: https://en.wikichip.org/wiki/qualcomm/microarchitectures/falkor [2]: http://www.7-cpu.com/cpu/X-Gene.html  And then we seem to covering what the ARM64 mainline kernel knows about non-ARM implementations: ThunderX and ThunderX2 (formerly Broadcom Vulcan). There is possibly the Qualcomm Kryo is different, but wikipedia seems to suggest it is a derivative of existing Cortex-A CPUs which have a 64b cache line size.  Let's see what Catalin and Will think about what the default should be.  Thanks! --  Florian",technical,Florian Fainelli,f.fainelli@gmail.com,1,1,707,1.0,0.7142857142857143,0,0.0,0.8571428571428571,0.0,0.0
160,197454,197737,This approach has been raised before ([1] as an example but you can probably find other threads) and NAK'ed. I really don't want this macro to be configurable as we aim for a single kernel Image. My proposal was to move it back to 6 and this to 128 as this is the largest known CWG. The networking code is wrong in assuming SKB_DATA_ALIGN only needs SMP_CACHE_BYTES for DMA alignment but we can add some safety checks (i.e. WARN_ON) in the arch dma ops code if the device is non-coherent. I'll send a patch to the list (hopefully later today).,"On Mon, Feb 12, 2018 at 03:45:23PM -0800, Florian Fainelli wrote:  This approach has been raised before ([1] as an example but you can probably find other threads) and NAK'ed. I really don't want this macro to be configurable as we aim for a single kernel Image.  My proposal was to move L1_CACHE_SHIFT back to 6 and ARCH_DMA_MIN_ALIGN to 128 as this is the largest known CWG. The networking code is wrong in assuming SKB_DATA_ALIGN only needs SMP_CACHE_BYTES for DMA alignment but we can add some safety checks (i.e. WARN_ON) in the arch dma ops code if the device is non-coherent.  I'll send a patch to the list (hopefully later today).  Catalin  [1] https://patchwork.kernel.org/patch/8634481/",technical,Catalin Marinas,catalin.marinas@arm.com,1,0,543,0.7682119205298014,0.8571428571428571,0,0.0,0.8571428571428571,0.0,0.8571428571428571
161,197454,201666,"With a server hat on...There are many ARMv8 server platforms that do 64b today, but future designs are likely to head toward 128b (for a variety of reasons). Many of the earlier designs were 64b because that's what certain other arches were using in their server cores. I doubt Vulcan will remain a unique and special case for very long. On the CCIX side of things, I've been trying to push people to go with 128b lines in future designs too.","On 02/12/2018 07:17 PM, Florian Fainelli wrote:  With a server hat on...  There are many ARMv8 server platforms that do 64b today, but future designs are likely to head toward 128b (for a variety of reasons). Many of the earlier designs were 64b because that's what certain other arches were using in their server cores. I doubt Vulcan will remain a unique and special case for very long. On the CCIX side of things, I've been trying to push people to go with 128b lines in future designs too.  Jon.",technical,Jon Masters,jcm@jonmasters.org,0,0,442,0.6158940397350994,1.0,1,1.0,0.0,0.8571428571428571,0.0
162,198233,198541,"Mmm, I'm afraid we can't do this. __sched_setscheduler might be called from interrupt contex by normalize_rt_tasks().","Hi Mathieu,  On 13/02/18 13:32, Mathieu Poirier wrote:  [...]   Mmm, I'm afraid we can't do this. __sched_setscheduler might be called from interrupt contex by normalize_rt_tasks().  Best,  - Juri",technical,Juri Lelli,juri.lelli@redhat.com,1,0,117,0.6388888888888888,0.6666666666666666,0,0.0,1.0,0.0,0.0
163,198233,198548,Maybe conditionally grabbing it if pi is true could do? I guess we don't care much about domains when sysrq.,"On 14/02/18 11:36, Juri Lelli wrote:  Maybe conditionally grabbing it if pi is true could do? I guess we don't care much about domains when sysrq.",technical,Juri Lelli,juri.lelli@redhat.com,1,0,108,0.6388888888888888,0.7222222222222222,0,0.0,1.0,0.0,0.0
164,198233,198817,Arrghhh... Back to the drawing board.,"On 14 February 2018 at 04:27, Juri Lelli <juri.lelli@redhat.com> wrote:   Arrghhh... Back to the drawing board.",technical,Mathieu Poirier,mathieu.poirier@linaro.org,1,1,37,0.2222222222222222,0.8333333333333334,0,0.0,0.0,0.0,0.0
165,198233,198890,Eh.. even though the warning simply happens because unlocking of CPU set lock is missing,"On 14/02/18 08:33, Mathieu Poirier wrote:  Eh.. even though the warning simply happens because unlocking of cpuset lock is missing  --->8--- @@ -4312,6 +4312,7 @@ static int __sched_setscheduler(struct task_struct *p,                                                                     /* Avoid rq from going away on us: */                                preempt_disable(),                 task_rq_unlock(rq, p, &rf),                                  +       cpuset_unlock(),                                                      if (pi)                                    rt_mutex_adjust_pi(p), --->8---  Still grabbing it is a no-go, as do_sched_setscheduler calls sched_setscheduler from inside an RCU read-side critical section.  So, back to the drawing board indeed. :/  Thanks,  - Juri",technical,Juri Lelli,juri.lelli@redhat.com,1,0,88,0.4444444444444444,0.8888888888888888,0,0.0,0.0,0.0,0.0
166,198233,199337,I was then actually thinking that try locking might do.. not sure however if failing with -EBUSY in the contended case is feasible (and about the general ugliness of the solution :/),"On 14/02/18 17:31, Juri Lelli wrote:  [...]   I was then actually thinking that trylocking might do.. not sure however if failing with -EBUSY in the contended case is feasible (and about the general uglyness of the solution :/).  --->8---  include/linux/cpuset.h | 4 ++--  kernel/cgroup/cpuset.c | 6 +++---  kernel/sched/core.c    | 4 +++-  3 files changed, 8 insertions(+), 6 deletions(-)  diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h index 4bbb3f5a3020..53c3f4e13cb0 100644 --- a/include/linux/cpuset.h +++ b/include/linux/cpuset.h @@ -55,7 +55,7 @@ extern void cpuset_init_smp(void),  extern void cpuset_force_rebuild(void),  extern void cpuset_update_active_cpus(void),  extern void cpuset_wait_for_hotplug(void), -extern void cpuset_lock(void), +extern int cpuset_trylock(void),  extern void cpuset_unlock(void),  extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask),  extern void cpuset_cpus_allowed_fallback(struct task_struct *p), @@ -178,7 +178,7 @@ static inline void cpuset_update_active_cpus(void)    static inline void cpuset_wait_for_hotplug(void) { }   -static inline void cpuset_lock(void) { } +static inline int cpuset_trylock(void) { return 1, }    static inline void cpuset_unlock(void) { }   diff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c index 41f8391640e6..995aac5b032d 100644 --- a/kernel/cgroup/cpuset.c +++ b/kernel/cgroup/cpuset.c @@ -2410,11 +2410,11 @@ void __init cpuset_init_smp(void)  }    /** - * cpuset_lock - Grab the cpuset_mutex from another subsysytem + * cpuset_trylock - Try to grab the cpuset_mutex atomically from another subsytem   */ -void cpuset_lock(void) +int cpuset_trylock(void)  { -	mutex_lock(&cpuset_mutex), +	return mutex_trylock(&cpuset_mutex),  }    /** diff --git a/kernel/sched/core.c b/kernel/sched/core.c index 0d8badcf1f0f..b491b406a835 100644 --- a/kernel/sched/core.c +++ b/kernel/sched/core.c @@ -4180,7 +4180,8 @@ static int __sched_setscheduler(struct task_struct *p,  	 * domains can be rebuilt or modified while operations like DL  	 * admission checks are carried out.  	 */ -	cpuset_lock(), +	if (!cpuset_trylock()) +		return -EBUSY,    	/*  	 * Make sure no PI-waiters arrive (or leave) while we are @@ -4312,6 +4313,7 @@ static int __sched_setscheduler(struct task_struct *p,  	/* Avoid rq from going away on us: */  	preempt_disable(),  	task_rq_unlock(rq, p, &rf), +	cpuset_unlock(),    	if (pi)  		rt_mutex_adjust_pi(p),",technical,Juri Lelli,juri.lelli@redhat.com,1,0,182,1.0,0.9444444444444444,0,1.0,0.0,0.0,0.0
167,198233,199345,"Or, as suggested by Peter in IRC, the following (which still would require conditional locking for the sysrq case) Avoid rq from going away on us:","On 15/02/18 11:33, Juri Lelli wrote:  Or, as suggested by Peter in IRC, the following (which still would require conditional locking for the sysrq case).  --->8---  kernel/sched/core.c | 21 +++++++++++++++++----  1 file changed, 17 insertions(+), 4 deletions(-)  diff --git a/kernel/sched/core.c b/kernel/sched/core.c index 0d8badcf1f0f..4e9405d50cbd 100644 --- a/kernel/sched/core.c +++ b/kernel/sched/core.c @@ -4312,6 +4312,7 @@ static int __sched_setscheduler(struct task_struct *p,  	/* Avoid rq from going away on us: */  	preempt_disable(),  	task_rq_unlock(rq, p, &rf), +	cpuset_unlock(),    	if (pi)  		rt_mutex_adjust_pi(p), @@ -4409,10 +4410,16 @@ do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)  	rcu_read_lock(),  	retval = -ESRCH,  	p = find_process_by_pid(pid), -	if (p != NULL) -		retval = sched_setscheduler(p, policy, &lparam), +	if (!p) { +		rcu_read_unlock(), +		goto exit, +	} +	get_task_struct(p),  	rcu_read_unlock(), +	retval = sched_setscheduler(p, policy, &lparam), +	put_task_struct(p),   +exit:  	return retval,  }   @@ -4540,10 +4547,16 @@ SYSCALL_DEFINE3(sched_setattr, pid_t, pid, struct sched_attr __user *, uattr,  	rcu_read_lock(),  	retval = -ESRCH,  	p = find_process_by_pid(pid), -	if (p != NULL) -		retval = sched_setattr(p, &attr), +	if (!p) { +		rcu_read_unlock(), +		goto exit, +	} +	get_task_struct(p),  	rcu_read_unlock(), +	retval = sched_setattr(p, &attr), +	put_task_struct(p),   +exit:  	return retval,  }",technical,Juri Lelli,juri.lelli@redhat.com,1,0,146,0.8611111111111112,1.0,1,1.0,0.0,0.0,0.0
168,202437,505149,"Just for the record, while this may work for media, it won't work for all subsystems.  One will quickly get a complaint that the big patch needs to go into multiple trees.","On Mon, 30 Oct 2017, SF Markus Elfring wrote:   Just for the record, while this may work for media, it won't work for all subsystems.  One will quickly get a complaint that the big patch needs to go into multiple trees.  julia",technical,Julia Lawall,julia.lawall@lip6.fr,1,0,171,0.29133858267716534,0.5,0,0.2536231884057971,0.7391304347826086,0.0,0.0
169,202437,505158,For the record: this only applies to drivers/media. We discussed what do to with series like this during our media summit last Friday and this was the conclusion of that. Obviously I can't talk about other subsystems.,"On 10/30/2017 10:47 AM, Julia Lawall wrote:  For the record: this only applies to drivers/media. We discussed what do to with series like this during our media summit last Friday and this was the conclusion of that. Obviously I can't talk about other subsystems.  Regards,  	Hans",technical,Hans Verkuil,hverkuil@xs4all.nl,1,0,217,0.33070866141732286,0.5454545454545454,0,0.2536231884057971,0.7391304347826086,0.0,0.0
170,202437,505225,I have got an other impression. I continued with the presentation of suggestions from my selection of change possibilities. It seems that there are very different expectations for the preferred patch granularity. Can it happen again that you are going to use a development tool like  (as a maintainer) for the desired recombination of possible update steps?," I have got an other impression.    I continued with the presentation of suggestions from my selection of change possibilities. It seems that there are very different expectations for the preferred patch granularity.  Can it happen again that you are going to use a development tool like “quilt” (as a maintainer) for the desired recombination of possible update steps?  Regards, Markus",technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,357,0.49606299212598426,0.6818181818181818,0,0.2608695652173913,0.7391304347826086,0.0,0.0
171,202437,505124,I find it safer in this way  while I was browsing through the landscape of Linux software components. It is just the case that there are so many remaining open issues. Thanks for a bit of change acceptance. Will any chances evolve to integrate 146 patches in any other combination? Can we achieve an agreement on the shown change patterns? Is a consensus possible for involved update candidates? I find that I did this already. I got an other software development opinion. Do we need to try any additional communication tools out? I hope that various change possibilities (from my selection) will become useful for more Linux users. How will the clarification evolve further?,"I find it safer in this way  while I was browsing through the landscape of Linux software components.    It is just the case that there are so many remaining open issues.    Thanks for a bit of change acceptance.    Will any chances evolve to integrate 146 patches in any other combination?    Can we achieve an agreement on the shown change patterns?  Is a consensus possible for involved update candidates?    I find that I did this already.    I got an other software development opinion.    Do we need to try any additional communication tools out?    I hope that various change possibilities (from my selection) will become useful for more Linux users. How will the clarification evolve further?   Regards, Markus",technical,SF Markus Elfring,elfring@users.sourceforge.net,0,1,675,1.0,0.45454545454545453,0,0.2536231884057971,0.7391304347826086,0.0,0.0
172,207400,207416,"Do both I2C and OF have stubs so that a driver will build when they are both disabled?  I.e., only COMPILE_TEST is enabled?","On 02/27/2018 05:21 PM, Tim Harvey wrote:  Do both I2C and OF have stubs so that a driver will build when they are both disabled?  I.e., only COMPILE_TEST is enabled?   thanks, --  ~Randy",technical,Randy Dunlap,rdunlap@infradead.org,0,0,123,0.07449856733524356,0.35294117647058826,0,0.0,0.0,0.0,0.0
173,207400,207417,"Please no.16 temperature channels ...Excessive [and inconsistent] ( ) Please make this if (ret)  return ret, ... return 0, return followed by break doesn't make sense. &buf -> buf Unnecessary ( )... but this array only has 8+1 elements. This seems inconsistent. How about using some defines for array sizes ?Also, why initialize those arrays ? You are overwriting them below. You could just use a static size array instead. I assume it is guaranteed that there is only exactly one instance of this device in the system. Have you tried what happens if you declare two instances anyway ? The result must be interesting,  with all those static variables. The matching gsc_fan_ch has only 5 entries. You declare local 'dev' variables all over the place, except here, where it would actually be used multiple times. Please either declare one here as well, or drop all the others. It must be interesting to see what happens if no 'label' property is provided. Have you tried ? Also, no validation of 'reg' and 'type' ?Are you sure ?This leaves the string unterminated if it is too long. Have you tested what happens in this situation ? Consider using strlcpy instead. Also please use sizeof() instead of '32'. I would suggest to abort with EINVAL if this happens. Otherwise the last entry is overwritten, which doesn't make much sense. Also, this accepts up to ARRAY_SIZE() entries, leaving no termination. So a reg value of 0xXXyy is auto-converted to 0xYY ? It is going to be interesting to see what happens if there are more than5 such entries. All other types are silently ignored ? I would suggest to move above code into a separate function. The error would be ENOMEM. Is it necessary to report that again ?","On 02/27/2018 05:21 PM, Tim Harvey wrote:  Please no.   16 temperature channels ...   Excessive [and inconsistent] ( )  Please make this 	if (ret) 		return ret, 	... 	return 0,   return followed by break doesn't make sense.   	&buf -> buf   Unnecessary ( )   ... but this array only has 8+1 elements. This seems inconsistent. How about using some defines for array sizes ?  Also, why initialize those arrays ? You are overwriting them below. You could just use a static size array instead.  I assume it is guaranteed that there is only exactly one instance of this device in the system. Have you tried what happens if you declare two instances anyway ? The result must be interesting, with all those static variables.   The matching gsc_fan_ch has only 5 entries.   You declare local 'dev' variables all over the place, except here, where it would actually be used multiple times.  Please either declare one here as well, or drop all the others.   It must be interesting to see what happens if no 'label' property is provided. Have you tried ? Also, no validation of 'reg' and 'type' ? Are you sure ?   This leaves the string unterminated if it is too long. Have you tested what happens in this situation ? Consider using strlcpy instead.  Also please use sizeof() instead of '32'.   I would suggest to abort with EINVAL if this happens. Otherwise the last entry is overwritten, which doesn't make much sense. Also, this accepts up to ARRAY_SIZE() entries, leaving no termination.   So a reg value of 0xXXyy is auto-converted to 0xYY ?   It is going to be interesting to see what happens if there are more than 5 such entries.   All other types are silently ignored ?  I would suggest to move above code into a separate function.   The error would be ENOMEM. Is it necessary to report that again ?",technical,Guenter Roeck,linux@roeck-us.net,1,0,1707,1.0,0.4117647058823529,0,0.0,0.0,0.0,0.0
174,207400,207460,"To compile this driver as a module... Let's keep the same // comment block fir the copyright notice as well. An one-line describing what this driver is would be appreciated too. Please no. Why is this needed? To clear irq? What happens if several events happen at the same time? Do we lose one of them? Could we provide the mapping in DTS instead of hard-coding them?  input_sync(), Why not use threaded interrupt? Not needed - it is set by devm_input_allocate_device().I'd say mapping should be done by MFD piece. You can add interrupts as resources and fetch them here.","Hi Tim,  On Tue, Feb 27, 2018 at 05:21:14PM -0800, Tim Harvey wrote:  To compile this driver as a module...""   Let's keep the same // comment block fir the copyright notice as well. An one-line describing what this driver is would be appreciated too.   Please no.   Why is this needed? To clear irq? What happens if several events happen at the same time? Do we lose one of them?   Could we provide the mapping in DTS instead of hard-coding them?   		input_sync(),   Why not use threaded interrupt?   Not needed - it is set by devm_input_allocate_device().   I'd say mapping should be done by MFD piece. You can add interrupts as resources and fetch them here.   Thanks.  --  Dmitry""",technical,Dmitry Torokhov,dmitry.torokhov@gmail.com,1,0,571,0.34097421203438394,0.47058823529411764,0,0.0,0.0,0.0,0.0
175,207400,207813,"Maybe it is in these patches, and i missed it....How do these emulated devices work? Does the controller respond to different addresses for these different emulated devices? Or is it anI2c bus mux?","On Tue, Feb 27, 2018 at 05:21:10PM -0800, Tim Harvey wrote:  Hi Tim  Maybe it is in these patches, and i missed it....  How do these emulated devices work? Does the controller respond to different addresses for these different emulated devices? Or is it an I2c bus mux?      Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,197,0.11174785100286533,0.5294117647058824,0,0.0,0.0,0.0,0.0
176,207400,207917,"You didn't miss it - I probably need to explain it better. The 'emulated devices' do respond on different slave addresses (which match one of or the only the slave addresses those parts support). For example the device-tree for the GW54xx has the following which are all from the GSC which is the only thing on i2c1, One issue I'm trying to figure out the best way to deal with is the fact that the GSC can 'NAK' transactions occasionally which is why I override the regmap read/write functions and provide retries. This resolves the issue for the mfd core driver and sub-module drivers but doesn't resolve the issue with these 'emulated devices' which have their own stand-alone drivers. I'm not sure how to best deal with that yet. I tried to add retires to the i2c adapter but that wasn't accepted upstream because it was too generic and I was told I need to work around it in device-drivers. I'm guessing I need to write my own sub-module drivers that will largely duplicate what's in the stand-alone drivers (ds1672, at24, pca9553x) .","On Wed, Feb 28, 2018 at 6:44 AM, Andrew Lunn <andrew@lunn.ch> wrote:  Andrew,  You didn't miss it - I probably need to explain it better.  The 'emulated devices' do respond on different slave addresses (which match one of or the only the slave addresses those parts support). For example the device-tree for the GW54xx has the following which are all from the GSC which is the only thing on i2c1:  &i2c1 {         clock-frequency = <100000>,         pinctrl-names = default"",         pinctrl-0 = <&pinctrl_i2c1>,         status = ""okay"",          gsc: gsc@20 {                 compatible = ""gw,gsc_v2"",                 reg = <0x20>,                 interrupt-parent = <&gpio1>,                 interrupts = <4 GPIO_ACTIVE_LOW>,                 interrupt-controller,                 #interrupt-cells = <1>,                  gsc_input {                         compatible = ""gw,gsc-input"",                 },                  gsc_hwmon {                         compatible = ""gw,gsc-hwmon"",                         #address-cells = <1>,                         #size-cells = <0>,                          hwmon@0 { /* A0: Board Temperature */                                 type = <0>,                                 reg = <0x00>,                                 label = ""temp"",                                 /* lookup table */                         },                          hwmon@1 { /* A1: Input Voltage */                                 type = <1>,                                 reg = <0x02>,                                 label = ""Vin"",                                 gw,voltage-divider = <22100 1000>,                                 gw,offset = <800>,                         },                          hwmon@2 { /* A2: 5P0 */                                 type = <1>,                                 reg = <0x0b>,                                 label = ""5P0"",                                 gw,voltage-divider = <22100 1000>,                         },                          hwmon@4 { /* A4: 0-5V input */                                 type = <1>,                                 reg = <0x14>,                                 label = ""ANL0"",                                 gw,voltage-divider = <10000 10000>,                         },                          hwmon@5 { /* A5: 2P5 PCIe/GigE */                                 type = <1>,                                 reg = <0x23>,                                 label = ""2P5"",                                 gw,voltage-divider = <10000 10000>,                         },                          hwmon@6 { /* A6: 1P8 Aud/Vid */                                 type = <1>,                                 reg = <0x1d>,                                 label = ""1P8"",                         },                          hwmon@7 { /* A7: GPS */                                 type = <1>,                                 reg = <0x26>,                                 label = ""GPS"",                                 gw,voltage-divider = <4990 10000>,                         },                          hwmon@12 { /* A12: VDD_CORE */                                 type = <1>,                                 reg = <0x3>,                                 label = ""VDD_CORE"",                         },                          hwmon@13 { /* A13: VDD_SOC */                                 type = <1>,                                 reg = <0x11>,                                 label = ""VDD_SOC"",                         },                          hwmon@14 { /* A14: 1P0 PCIe SW */                                 type = <1>,                                 reg = <0x20>,                                 label = ""1P0"",                         },                          hwmon@15 { /* fan0 */                                 type = <2>,                                 reg = <0x2c>,                                 label = ""fan_50p"",                         },                          hwmon@16 { /* fan1 */                                 type = <2>,                                 reg = <0x2e>,                                 label = ""fan_60p"",                         },                          hwmon@17 { /* fan2 */                                 type = <2>,                                 reg = <0x30>,                                 label = ""fan_70p"",                         },                          hwmon@18 { /* fan3 */                                 type = <2>,                                 reg = <0x32>,                                 label = ""fan_80p"",                         },                          hwmon@19 { /* fan4 */                                 type = <2>,                                 reg = <0x34>,                                 label = ""fan_90p"",                         },                          hwmon@20 { /* fan5 */                                 type = <2>,                                 reg = <0x36>,                                 label = ""fan_100p"",                         },                 },         },          gsc_gpio: pca9555@23 {                 compatible = ""nxp,pca9555"",                 reg = <0x23>,                 gpio-controller,                 #gpio-cells = <2>,                 interrupt-parent = <&gsc>,                 interrupts = <4>,         },          eeprom1: eeprom@50 {                 compatible = ""atmel,24c02"",                 reg = <0x50>,                 pagesize = <16>,         },          eeprom2: eeprom@51 {                 compatible = ""atmel,24c02"",                 reg = <0x51>,                 pagesize = <16>,         },          eeprom3: eeprom@52 {                 compatible = ""atmel,24c02"",                 reg = <0x52>,                 pagesize = <16>,         },          eeprom4: eeprom@53 {                 compatible = ""atmel,24c02"",                 reg = <0x53>,                 pagesize = <16>,         },          rtc: ds1672@68 {                 compatible = ""dallas,ds1672"",                 reg = <0x68>,         }, },   One issue I'm trying to figure out the best way to deal with is the fact that the GSC can 'NAK' transactions occasionally which is why I override the regmap read/write functions and provide retries. This resolves the issue for the mfd core driver and sub-module drivers but doesn't resolve the issue with these 'emulated devices' which have their own stand-alone drivers. I'm not sure how to best deal with that yet. I tried to add retires to the i2c adapter but that wasn't accepted upstream because it was too generic and I was told I need to work around it in device-drivers. I'm guessing I need to write my own sub-module drivers that will largely duplicate whats in the stand-alone drivers (ds1672, at24, pca9553x).  Regards,  Tim""",technical,Tim Harvey,tharvey@gateworks.com,1,1,1039,0.5902578796561605,0.5882352941176471,0,0.0,0.0,0.0,0.0
177,207400,208059,There appears to be a few spaces vs tabs issues in this file., Hi Tim  There appears to be a few spaces vs tabs issues in this file.        Andrew,technical,Andrew Lunn,andrew@lunn.ch,1,0,61,0.04011461318051576,0.7058823529411765,0,0.0,0.0,0.0,0.0
178,207400,208144,Thanks. I'll run through checkpatch prior to v2.,"On Wed, Feb 28, 2018 at 10:53 AM, Andrew Lunn <andrew@lunn.ch> wrote:  Thanks. I'll run through checkpatch prior to v2.  Tim",technical,Tim Harvey,tharvey@gateworks.com,1,1,48,0.03151862464183381,0.8823529411764706,0,0.0,0.0,0.0,0.0
179,207400,208186,"That was just to point out that you use smaller arrays later on.That is what you do, isn't it ? So they will both map to 0xff. Or, in other word, the code happily accepts invalid values and converts them into something else. Much preferred.","On Wed, Feb 28, 2018 at 01:44:36PM -0800, Tim Harvey wrote: That was just to point out that you use smaller arrays later on.   That is what you do, isn't it ? So reg=0xffff and reg=0xfeff will both map to 0xff. Or, in other word, the code happily accepts invalid values and converts them into something else.  Much preferred.  Thanks, Guenter",technical,Guenter Roeck,linux@roeck-us.net,1,0,240,0.1489971346704871,1.0,1,0.0,0.0,0.0,0.0
180,210458,210460,"Please note that there is nothing wrong in the generated code, just that it confuses objtool. Clang has simply omitted the statement where NULL is returned since the pointer was always dereferenced post in lining. Note that GCC will also remove the NULL pointers if it knows that the pointer is dereferenced. Here is an example.","Please note that there is nothing wrong in the generated code, just that it confuses objtool. Clang has simply omitted the statement where NULL is returned since the pointer was always dereferenced post inlining.  Note that GCC will also remove the NULL pointers if it knows that the pointer is dereferenced. Here is an example.  void null_check(int *P) {   int deref = *P,   if (P == 0) // GCC won't check the condition.     return,   *P = 4, }  Compiling with gcc -O2 gives:         movl    $4, (%rdi)         ret  Thanks, Manoj  On Tue, Mar 27, 2018 at 11:16 PM, Greg Kroah-Hartman <gregkh@linuxfoundation.org> wrote:",technical,Manoj Gupta,manojgupta@chromium.org,0,0,328,0.5041322314049587,0.3,0,0.0,0.0,0.0,0.0
181,210458,210466,... but returning NULL would be far more sane than falling through to the next function. This is why we use -fno-delete-null-pointer-checks.,"On Wed, Mar 28, 2018 at 07:47:53AM -0700, Manoj Gupta wrote:  ... but returning NULL would be far more sane than falling through to the next function.   This is why we use -fno-delete-null-pointer-checks.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,140,0.19834710743801653,0.4,0,0.0,0.0,0.0,0.0
182,210458,210467,"Or, as the case may be, oopsing at the point of failure.","On Wed, Mar 28, 2018 at 10:30:51AM -0500, Josh Poimboeuf wrote:  Or, as the case may be, oopsing at the point of failure.    --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,56,0.12396694214876033,0.5,0,0.0,0.0,0.0,0.0
183,210458,210462,"Thanks all for your input, we'll try to get-fno-delete-null-pointer-checks or a similar flag to be added to clang.","El Wed, Mar 28, 2018 at 08:05:56PM +0200 Greg Kroah-Hartman ha dit:   Thanks all for your input, we'll try to get -fno-delete-null-pointer-checks or a similar flag to be added to clang.",technical,Matthias Kaehlcke,mka@chromium.org,0,1,114,0.17355371900826447,0.7,0,0.0,0.0,0.0,0.0
184,210458,210464,"Nope, clang doesn't currently have such a flag. IIRC this patch was needed to work around the lack of the flag.  But when building the kernel with Clang,    the compiler assumes &pos->member cannot be NULL if the member's offset    is greater than 0 (which would be equivalent to the object being    non-contiguous in memory).  Therefore the loop condition is always true,    and the loops become infinite.    To work around this, introduce the member_address_is_nonnull() macro,    which casts object pointer to uintptr_t, thus letting the member pointer    to be NULL.    Other than that I am not aware of any known issues.","El Wed, Mar 28, 2018 at 08:19:36PM +0200 Greg Kroah-Hartman ha dit:   Nope, clang doesn't currently have such a flag.   IIRC this patch was needed to work around the lack of the flag:  commit beaec533fc2701a28a4d667f67c9f59c6e4e0d13 Author: Alexander Potapenko <glider@google.com> Date:   Wed Jul 19 20:27:30 2017 +0200      llist: clang: introduce member_address_is_nonnull()      Currently llist_for_each_entry() and llist_for_each_entry_safe() iterate     until &pos->member != NULL.  But when building the kernel with Clang,     the compiler assumes &pos->member cannot be NULL if the member's offset     is greater than 0 (which would be equivalent to the object being     non-contiguous in memory).  Therefore the loop condition is always true,     and the loops become infinite.      To work around this, introduce the member_address_is_nonnull() macro,     which casts object pointer to uintptr_t, thus letting the member pointer     to be NULL.      Signed-off-by: Alexander Potapenko <glider@google.com>     Tested-by: Sodagudi Prasad <psodagud@codeaurora.org>     Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>   Other than that I am not aware of any known issues.",technical,Matthias Kaehlcke,mka@chromium.org,0,1,625,1.0,0.9,0,0.0,0.0,0.0,0.0
185,210458,210465,I think you have gotten lucky :),"On Wed, Mar 28, 2018 at 11:50:09AM -0700, Matthias Kaehlcke wrote:  I think you have gotten lucky :)  greg k-h",technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,0,32,0.06611570247933884,1.0,1,0.0,0.0,0.0,0.0
186,212056,212061,Looks good.  Thanks!,"Looks good.  Thanks!  regards, dan carpenter  _______________________________________________ devel mailing list devel@linuxdriverproject.org http://driverdev.linuxdriverproject.org/mailman/listinfo/driverdev-devel",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,20,0.3125,0.5,0,0.0,0.8333333333333334,0.0,0.8333333333333334
187,212056,212058,He just hasn't gotten to it yet.,"On Tue, Mar 27, 2018 at 02:00:45PM +0900, Ji-Hun Kim wrote:   Looks good.  Thanks!  Greg just hasn't gotten to it yet.  regards, dan carpenter   _______________________________________________ devel mailing list devel@linuxdriverproject.org http://driverdev.linuxdriverproject.org/mailman/listinfo/driverdev-devel",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,32,0.5625,0.8333333333333334,0,1.0,0.0,0.0,0.0
188,212056,212057,Are there any opinions? I'd like to know how this patch is going.,"On Wed, Mar 21, 2018 at 01:39:09PM +0900, Ji-Hun Kim wrote:  Are there any opinions? I'd like to know how this patch is going.  Best regards, Ji-Hun",technical,Ji-Hun Kim,ji_hun.kim@samsung.com,0,1,65,1.0,0.6666666666666666,0,1.0,0.0,0.8333333333333334,0.0
189,212056,212059,He does not take drivers/staging/media/* patches :),"On Tue, Mar 27, 2018 at 08:20:59AM +0300, Dan Carpenter wrote:  Greg does not take drivers/staging/media/* patches :) _______________________________________________ devel mailing list devel@linuxdriverproject.org http://driverdev.linuxdriverproject.org/mailman/listinfo/driverdev-devel",technical,Greg KH,gregkh@linuxfoundation.org,1,0,51,0.5625,1.0,1,1.0,0.0,0.0,0.0
190,214922,214927,"I don't know the state in 3.16, but in 3.12, I had to fix the 32bit entry on 64bit in arch/x86/ia32/ia32entry.S (ia32_sysenter_target &others) too. Thanks","On 03/12/2018, 04:06 AM, Ben Hutchings wrote:  I don't know the state in 3.16, but in 3.12, I had to fix the 32bit entry on 64bit in arch/x86/ia32/ia32entry.S (ia32_sysenter_target & others) too.  thanks, --  js suse labs",technical,Jiri Slaby,jslaby@suse.cz,1,0,154,0.5714285714285714,0.9506172839506173,0,0.0,1.0,0.0,0.0
191,214922,214923,"Build result. The failures are due to newly added tests, the init process crashes.v3.16 passes those tests, so the problem was introduced later. I'll run a bisect later to see if I can find the culprit. If not, I'll drop the new tests from this kernel version.","On Mon, Mar 12, 2018 at 03:06:11AM +0000, Ben Hutchings wrote:  Build results: 	total: 136 pass: 136 fail:0 Qemu test results: 	total: 115 pass: 112 fail:3 Failed tests: 	mipsel:24Kf:malta_defconfig:smp:rootfs 	mipsel64:malta_defconfig:nosmp:rootfs 	mipsel64:malta_defconfig:smp:rootfs  The failures are due to newly added tests, the init process crashes. v3.16 passes those tests, so the problem was introduced later. I'll run a bisect later to see if I can find the culprit. If not, I'll drop the new tests from this kernel version.  Guenter",technical,Guenter Roeck,linux@roeck-us.net,1,0,260,1.0,0.9629629629629629,0,0.0,1.0,0.0,0.0
192,214922,214924,Turns out I did the bisect k already. Attached. It does suggest that there may be a real problem. bad signals sent from emulation too,"On Mon, Mar 12, 2018 at 08:00:53AM -0700, Guenter Roeck wrote: Turns out I did the bisect kalready. Attached. It does suggest that there may be a real problem.  Guenter  --- # bad: [3e50cd97ed730bb0abfcdbc8c1a18871c2750c33] Linux 3.16.55 # good: [19583ca584d6f574384e17fe7613dfaeadcdc4a6] Linux 3.16 git bisect start 'HEAD' 'v3.16' # good: [d1afef76e102be87955151c93bd51fd04c1c0c01] arm64: mm: ensure that the zero page is visible to the page table walker git bisect good d1afef76e102be87955151c93bd51fd04c1c0c01 # good: [b6927bd60d353de044584ab9400aaccd8694fe1e] can: Fix kernel panic at security_sock_rcv_skb git bisect good b6927bd60d353de044584ab9400aaccd8694fe1e # good: [aa9a2ec0e82b64db1851d96ab1e9c83f8ea17a39] ARM: kexec: Make .text R/W in machine_kexec git bisect good aa9a2ec0e82b64db1851d96ab1e9c83f8ea17a39 # good: [bf5ac638a0ffa923ed03ba8cdb8241b812f5fe4f] can: gs_usb: fix busy loop if no more TX context is available git bisect good bf5ac638a0ffa923ed03ba8cdb8241b812f5fe4f # good: [957a3d249cb16292a199f73b7138d23ee44ca433] Revert x86: kvmclock: Disable use from vDSO if KPTI is enabled"" git bisect good 957a3d249cb16292a199f73b7138d23ee44ca433 # bad: [66fe40226beb16fb7809d275aec362f479388935] USB: serial: option: adding support for YUGA CLM920-NC5 git bisect bad 66fe40226beb16fb7809d275aec362f479388935 # good: [0b6433856a149885470f2ab3a138e99347c323a4] arm64: fpsimd: Prevent registers leaking from dead tasks git bisect good 0b6433856a149885470f2ab3a138e99347c323a4 # good: [d6e7dd39a7f036eb3e48032d68d9e70f2e9781cf] MIPS: Clear [MSA]FPE CSR.Cause after notify_die() git bisect good d6e7dd39a7f036eb3e48032d68d9e70f2e9781cf # bad: [d97c5dd698a37a6f4fcce8132853620f7390f797] MIPS: Fix an FCSR access API regression with NT_PRFPREG and MSA git bisect bad d97c5dd698a37a6f4fcce8132853620f7390f797 # bad: [b18b5d55c0e8b2bccda919f5f227ec3ba1056f2a] MIPS: Fix a preemption issue with thread's FPU defaults git bisect bad b18b5d55c0e8b2bccda919f5f227ec3ba1056f2a # good: [0efd2f915bbc608f66065c36b291d37efe0a0b0f] MIPS: Always clear FCSR cause bits after emulation git bisect good 0efd2f915bbc608f66065c36b291d37efe0a0b0f # bad: [3127c502272aca5f46b04c0b11afb464ad4fcbaf] MIPS: math-emu: Define IEEE 754-2008 feature control bits git bisect bad 3127c502272aca5f46b04c0b11afb464ad4fcbaf # bad: [8605aa2fea28c0485aeb60c114a9d52df1455915] MIPS: Set `si_code' for SIGFPE signals sent from emulation too git bisect bad 8605aa2fea28c0485aeb60c114a9d52df1455915 # first bad commit: [8605aa2fea28c0485aeb60c114a9d52df1455915] MIPS: Set `si_code' for SIGFPE signals sent from emulation too""",technical,Guenter Roeck,linux@roeck-us.net,1,0,133,0.5,0.9753086419753086,0,0.0,1.0,0.0,0.75
193,214922,214928,"Thank you, yes I need to fix them in 3.16 too.  I also failed to use retpolines there. The first rule of tautology club is the first rule of tautology club.","On Mon, 2018-03-12 at 08:32 +0100, Jiri Slaby wrote:  Thank you, yes I need to fix them in 3.16 too.  I also failed to use retpolines there.  Ben.  --  Ben Hutchings The first rule of tautology club is the first rule of tautology club.",technical,Ben Hutchings,ben@decadent.org.uk,0,1,156,0.625,0.9876543209876543,0,0.75,0.125,0.75,0.125
194,214922,214925,"worked out that it depends on commit (""MIPS: Normalise code flow in the CpU exception handler"") which I mistakenly omitted.  I'll include that in the next update (3.16.57).","On Mon, 2018-03-12 at 09:45 -0700, Guenter Roeck wrote:  Maciej W. Rozycki worked out that it depends on commit 27e28e8ec47a (MIPS: Normalise code flow in the CpU exception handler"") which I mistakenly omitted.  I'll include that in the next update (3.16.57).  Ben.  --  Ben Hutchings Time is nature's way of making sure that everything doesn't happen at once. """,technical,Ben Hutchings,ben@decadent.org.uk,0,1,172,0.6785714285714286,1.0,1,1.0,0.0,0.125,0.0
195,216128,216134,"As I watched this email send, I noticed the ""3/3"" in the Subject. ,) I see the support now. :P This question still stands, though.","On Wed, Mar 14, 2018 at 3:58 PM, Kees Cook <keescook@chromium.org> wrote:  As I watched this email send, I noticed the 3/3"" in the Subject. ,) I see the amdcz support now. :P https://patchwork.kernel.org/project/LKML/list/?submitter=18441   This question still stands, though.  -Kees  --  Kees Cook Pixel Security""",technical,Kees Cook,keescook@chromium.org,1,0,130,0.5483870967741935,0.5454545454545454,0,0.0,1.0,0.0,0.0
196,216128,216135,"wrote: From the same patch: if this happens, in other words, if serial ports are disabled, but we still want to parse the APCI_SPCR_TABLE, which ""defaults y if X86"". Perhaps that logic should be changed (no need to parse ACPI SPCR table if we are going to disable serial anyway)?","On Wed, Mar 14, 2018 at 5:00 PM Kees Cook <keescook@chromium.org> wrote:  wrote:      From the same patch: https://patchwork.kernel.org/patch/10283641/  if (CONFIG_ACPI_SPCR_TABLE && !CONFIG_SERIAL_8250)  in other words, if serial ports are disabled, but we still want to parse the APCI_SPCR_TABLE, which defaults y if X86"". Perhaps that logic should be changed (no need to parse ACPI SPCR table if we are going to disable serial anyway)?   """,technical,Daniel Kurtz,djkurtz@chromium.org,0,1,279,1.0,0.6363636363636364,0,0.0,1.0,0.0,0.0
197,216128,216136,"But won't this break? ""static const bool ..."" but the code tries to set a value but I'd expect the compiler to still yet about it? I think you could drop the .h #ifdef and use it.","On Wed, Mar 14, 2018 at 5:23 PM, Daniel Kurtz <djkurtz@chromium.org> wrote:  But won't this break? static const bool ..."" but the code tries to set a value but I'd expect the compiler to still yet about it?  I think you could drop the .h #ifdef and use:  if (IS_ENABLED(CONFIG_SERIAL_825) && amdcz_present(...)) {  -Kees  --  Kees Cook Pixel Security""",technical,Kees Cook,keescook@chromium.org,1,0,179,0.7258064516129032,0.7272727272727273,0,0.0,1.0,0.0,0.0
198,216128,216137,"if er... yeah.oooohhh... nice.  I like it, will change.","On Wed, Mar 14, 2018 at 6:55 PM Kees Cook <keescook@chromium.org> wrote:  wrote: wrote: mention b/include/linux/serial_8250.h CONFIG_SERIAL_8250 if   er... yeah.     oooohhh... nice.  I like it, will change.",technical,Daniel Kurtz,djkurtz@chromium.org,0,1,55,0.22580645161290322,0.8181818181818182,0,0.0,1.0,0.0,0.5
199,216128,216132,"Thank you for the patch! Yet something to improve:[auto build test ERROR on linus/master][also build test ERROR on v4.17-rc7][cannot apply to next-20180530][if your patch is applied to the wrong git tree, please drop us a note to help improve the system]","Hi Daniel,  Thank you for the patch! Yet something to improve:  [auto build test ERROR on v4.16-rc4] [also build test ERROR on next-20180316] [if your patch is applied to the wrong git tree, please drop us a note to help improve the system]  url:    https://github.com/0day-ci/linux/commits/Daniel-Kurtz/Add-earlycon-support-for-AMD-Carrizo-Stoneyridge/20180318-025754 config: x86_64-allmodconfig (attached as .config) compiler: gcc-7 (Debian 7.3.0-1) 7.3.0 reproduce:         # save the attached .config to linux build tree         make ARCH=x86_64   All errors (new ones prefixed by >>):     drivers//acpi/spcr.c: In function 'acpi_parse_spcr':       serial8250_skip_old_ports = true,                                 ^ --     bool serial8250_skip_old_ports,          ^~~~~~~~~~~~~~~~~~~~~~~~~    In file included from drivers//tty/serial/8250/8250_core.c:29:0:    include/linux/serial_8250.h:142:19: note: previous declaration of 'serial8250_skip_old_ports' was here     static const bool serial8250_skip_old_ports,                       ^~~~~~~~~~~~~~~~~~~~~~~~~  sparse warnings: (new ones prefixed by >>)     drivers/acpi/spcr.c: In function 'acpi_parse_spcr':    drivers/acpi/spcr.c:212:29: error: assignment of read-only variable 'serial8250_skip_old_ports'       serial8250_skip_old_ports = true,                                 ^  vim +/serial8250_skip_old_ports +212 drivers//acpi/spcr.c      94	     95	/**     96	 * acpi_parse_spcr() - parse ACPI SPCR table and add preferred console     97	 *     98	 * @enable_earlycon: set up earlycon for the console specified by the table     99	 * @enable_console: setup the console specified by the table.    100	 *    101	 * For the architectures with support for ACPI, CONFIG_ACPI_SPCR_TABLE may be    102	 * defined to parse ACPI SPCR table.  As a result of the parsing preferred    103	 * console is registered and if @enable_earlycon is true, earlycon is set up.    104	 * If @enable_console is true the system console is also configured.    105	 *    106	 * When CONFIG_ACPI_SPCR_TABLE is defined, this function should be called    107	 * from arch initialization code as soon as the DT/ACPI decision is made.    108	 *    109	 */    110	int __init acpi_parse_spcr(bool enable_earlycon, bool enable_console)    111	{    112		static char opts[64],    113		struct acpi_table_spcr *table,    114		acpi_status status,    115		char *uart,    116		char *iotype,    117		int baud_rate,    118		int err,    119	    120		if (acpi_disabled)    121			return -ENODEV,    122	    123		status = acpi_get_table(ACPI_SIG_SPCR, 0,    124					(struct acpi_table_header **)&table),    125	    126		if (ACPI_FAILURE(status))    127			return -ENOENT,    128	    129		if (table->header.revision < 2)    130			pr_info(SPCR table version %d\n"", table->header.revision),    131	    132		if (table->serial_port.space_id == ACPI_ADR_SPACE_SYSTEM_MEMORY) {    133			switch (ACPI_ACCESS_BIT_WIDTH((    134				table->serial_port.access_width))) {    135			default:    136				pr_err(""Unexpected SPCR Access Width.  Defaulting to byte size\n""),    137				/* fall through */    138			case 8:    139				iotype = ""mmio"",    140				break,    141			case 16:    142				iotype = ""mmio16"",    143				break,    144			case 32:    145				iotype = ""mmio32"",    146				break,    147			}    148		} else    149			iotype = ""io"",    150	    151		switch (table->interface_type) {    152		case ACPI_DBG2_ARM_SBSA_32BIT:    153			iotype = ""mmio32"",    154			/* fall through */    155		case ACPI_DBG2_ARM_PL011:    156		case ACPI_DBG2_ARM_SBSA_GENERIC:    157		case ACPI_DBG2_BCM2835:    158			uart = ""pl011"",    159			break,    160		case ACPI_DBG2_16550_COMPATIBLE:    161		case ACPI_DBG2_16550_SUBSET:    162			uart = ""uart"",    163			break,    164		default:    165			err = -ENOENT,    166			goto done,    167		}    168	    169		switch (table->baud_rate) {    170		case 3:    171			baud_rate = 9600,    172			break,    173		case 4:    174			baud_rate = 19200,    175			break,    176		case 6:    177			baud_rate = 57600,    178			break,    179		case 7:    180			baud_rate = 115200,    181			break,    182		default:    183			err = -ENOENT,    184			goto done,    185		}    186	    187		/*    188		 * If the E44 erratum is required, then we need to tell the pl011    189		 * driver to implement the work-around.    190		 *    191		 * The global variable is used by the probe function when it    192		 * creates the UARTs, whether or not they're used as a console.    193		 *    194		 * If the user specifies ""traditional"" earlycon, the qdf2400_e44    195		 * console name matches the EARLYCON_DECLARE() statement, and    196		 * SPCR is not used.  Parameter ""earlycon"" is false.    197		 *    198		 * If the user specifies ""SPCR"" earlycon, then we need to update    199		 * the console name so that it also says ""qdf2400_e44"".  Parameter    200		 * ""earlycon"" is true.    201		 *    202		 * For consistency, if we change the console name, then we do it    203		 * for everyone, not just earlycon.    204		 */    205		if (qdf2400_erratum_44_present(&table->header)) {    206			qdf2400_e44_present = true,    207			if (enable_earlycon)    208				uart = ""qdf2400_e44"",    209		}    210	    211		if (amdcz_present(table)) {  > 212			serial8250_skip_old_ports = true,  --- 0-DAY kernel test infrastructure                Open Source Technology Center https://lists.01.org/pipermail/kbuild-all                   Intel Corporation """,technical,kbuild test robot,lkp@intel.com,0,0,254,0.9193548387096774,0.9090909090909091,0,0.75,0.25,0.5,0.25
200,220831,220832,Please take a look at drivers/staging/ipx/TODO,"On Mon, 2018-03-05 at 20:19 +0100, Horatiu Vultur wrote:  Please take a look at drivers/staging/ipx/TODO   _______________________________________________ devel mailing list devel@linuxdriverproject.org http://driverdev.linuxdriverproject.org/mailman/listinfo/driverdev-devel",technical,Eric Dumazet,eric.dumazet@gmail.com,1,0,46,1.0,1.0,1,0.0,0.0,0.0,0.0
201,221804,221856,"II address space is always mapped, and address conversion done through this.Indeed. This should print the physical addresses, using pdata->base instead of base","Hi Andy,  On Thu, Mar 1, 2018 at 1:32 PM, Andy Shevchenko <andy.shevchenko@gmail.com> wrote:  Zorro II address space is always mapped, and address conversion done through ZTWO_VADDR()/ZTWO_PADDR().   Indeed. This should print the physical addresses, using pdata->base instead of base.  Gretje,eetings,                          Geert  -- Geert Uytterhoeven -- There's lots of Linux beyond ia32 -- geert@linux-m68k.org  In personal conversations with technical people, I call myself a hacker. But when I'm talking to journalists I just say programmer"" or something like that.                                 -- Linus Torvalds""",technical,Geert Uytterhoeven,geert@linux-m68k.org,1,0,159,1.0,1.0,1,0.0,0.0,0.0,0.0
202,222860,224687,Does this use driver model? I cannot see it.,"On 2 March 2018 at 08:44, yannick fertre <yannick.fertre@st.com> wrote:  Does this use driver model? I cannot see it.  Regards, Simon",technical,Simon Glass,sjg@chromium.org,0,0,44,0.4444444444444444,0.7,0,0.4,0.6,0.3,0.6
203,222860,230473,I would replace debug by dev_err() here.,"Hi Yannick  On 03/02/2018 04:44 PM, yannick fertre wrote:  I would replace debug by dev_err() here.   Ditto   Ditto   Ditto   Ditto   Ditto   Ditto",technical,Patrice CHOTARD,patrice.chotard@st.com,0,0,40,0.37037037037037035,0.8,0,1.0,0.0,0.0,0.0
204,222860,230482,"As you get access to the struct udevice, prefer dev_err() here. Ditto","Hi Yannick  On 03/02/2018 04:44 PM, yannick fertre wrote:  As you get access to the struct udevice, prefer dev_err() here.   ditto  ditto ditto  dev_info()   dev_err()   ditto   ditto   ditto    dev_info()   dev_err()   ditto   ditto   dev_info()   Thanks  Patrice",technical,Patrice CHOTARD,patrice.chotard@st.com,0,0,69,0.5925925925925926,0.85,0,1.0,0.0,0.0,0.0
205,222860,230495,dev_err() Is it useful to print this each time the backlight is enabled ? Ditto dev_err() ditto debug() ?,"Hi Yannick  On 03/02/2018 04:44 PM, yannick fertre wrote:   dev_err()   Is it useful to print this each time the backligth is enabled ?   ditto   dev_err()   ditto   debug() ?",technical,Patrice CHOTARD,patrice.chotard@st.com,0,0,105,0.9259259259259259,0.9,0,1.0,0.0,0.0,0.0
206,222860,230503,is it useful to print this each time ? or is it for debug purpose ? In this case use debug() dev_err() ditto,"Hi yannick  On 03/02/2018 04:44 PM, yannick fertre wrote:  dev_err()   dev_err()   dev_warn() or dev_err() ?   is it useful to print this each time ? or is it for debug purpose ? in  this case use debug()   dev_err()   ditto   ditto   ditto   ditto   ditto   ditto   ditto",technical,Patrice CHOTARD,patrice.chotard@st.com,0,0,108,1.0,0.95,0,1.0,0.0,0.0,0.0
207,222860,230505,I would replace debug by dev_err() here.,"Hi yannick  On 03/02/2018 04:44 PM, yannick fertre wrote:  pr_err()   ditto   debug() ?   debug() ?   dev_info()   ditto   debug() ?   dev_err()",technical,Patrice CHOTARD,patrice.chotard@st.com,0,0,40,0.37037037037037035,1.0,1,1.0,0.0,0.0,0.0
208,227363,227369,I'll send a patch for xtables too to reject bogus names coming from userspace (syzbot reports WARN() ).,Alexey Dobriyan <adobriyan@gmail.com> wrote:  Acked-by: Florian Westphal <fw@strlen.de>  I'll send a patch for xtables too to reject bogus names coming from userspace (syzbot reports WARN() ).,technical,Florian Westphal,fw@strlen.de,1,0,103,0.32857142857142857,0.2857142857142857,0,0.0,1.0,0.0,0.3333333333333333
209,227363,227838,"Hmm, patch is probably good idea, but now it means that userspace can trigger WARN()s, and can hide objects from root by naming them '.' and'..'... which is not good. If you know where this happens, it would be nice to fix them in addition to this patch.","On Sat 2018-03-10 03:12:23, Alexey Dobriyan wrote:  Hmm, patch is probably good idea, but now it means that userspace can trigger WARN()s, and can hide objects from root by naming them '.' and '..'... which is not good.  If you know where this happens, it would be nice to fix them in addition to this patch. --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,254,0.9,0.42857142857142855,0,0.3333333333333333,0.3333333333333333,0.3333333333333333,0.0
210,227363,227843,"Patch rejects creation of such entries. And they should be harmless as VFS lookup won't find them, only readdir would. It not clear how they could be useful.","On Sun, Mar 11, 2018 at 10:30:58PM +0100, Pavel Machek wrote:  Patch rejects creation of such entries.  And they should be harmless as VFS lookup won't find them, only readdir would. It not clear how they could be useful.",technical,Alexey Dobriyan,adobriyan@gmail.com,0,1,157,0.4714285714285714,0.5714285714285714,0,0.3333333333333333,0.3333333333333333,0.0,0.0
211,227363,227844,"Yeah, as I said, that's half of problem. If I can name my object ""."" and it will be hidden from root, that sounds like a security hole to be prevented. So if you know _which_ subsystem allow creating files and directories in /proc with names directly controlled by userspace, please let us know, we want to fix that.","On Mon 2018-03-12 00:35:34, Alexey Dobriyan wrote:  Yeah, as I said, that's half of problem.  If I can name my object ."" and it will be hidden from root, that sounds like a security hole to be prevented.  So if you know _which_ subsystem allow creating files and directories in /proc with names directly controlled by userspace, please let us know, we want to fix that.  								Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html """,technical,Pavel Machek,pavel@ucw.cz,1,0,316,1.0,0.7142857142857143,0,0.3333333333333333,0.3333333333333333,0.0,0.3333333333333333
212,227363,230277,"is neater, but the whole function should be thus converted and I'll let you decide on that","On Sat, 10 Mar 2018 03:12:23 +0300 Alexey Dobriyan <adobriyan@gmail.com> wrote:   --- a/fs/proc/generic.c~proc-reject-and-as-filenames-fix +++ a/fs/proc/generic.c @@ -387,10 +387,8 @@ static struct proc_dir_entry *__proc_cre  		WARN(1, name len %u\n"", qstr.len),  		return NULL,  	} -	if (qstr.len == 1 && fn[0] == '.') { -		WARN(1, ""name '.'\n""), +	if (WARN(qstr.len == 1 && fn[0] == '.', ""name '.'\n""))  		return NULL, -	}  	if (qstr.len == 2 && fn[0] == '.' && fn[1] == '.') {  		WARN(1, ""name '..'\n""),  		return NULL,  is neater, but the whole function should be thus converted and I'll let you decide on that.""",technical,Andrew Morton,akpm@linux-foundation.org,1,0,90,0.2714285714285714,0.8571428571428571,0,0.6666666666666666,0.0,0.3333333333333333,0.0
213,227363,230427,"Oh, I hate this style of WARN. For one thing it overlaps with comma operator.","On Mon, Mar 12, 2018 at 04:00:18PM -0700, Andrew Morton wrote:  Oh, I hate this style of WARN. For one thing it overlaps with comma operator.",technical,Alexey Dobriyan,adobriyan@gmail.com,0,1,77,0.2571428571428571,1.0,1,1.0,0.0,0.0,0.0
214,230843,230952,"This is slightly dodgy. You are assuming this, which may change depending on configuration or future changes. Could you add a BUILD_BUG_ON() here to ensure that this remains the case? Where is plt_ent ever used? Please move the if () check into prealloc_fixed(), and only keep the loop below Please use a normal for loop here and iterate upward starting at 0","On 13 March 2018 at 13:53, Alexander Sverdlin <alexander.sverdlin@nokia.com> wrote: ...  This is slightly dodgy. You are assuming that sizeof(plt->lit) >= sizeof(fixed_plts), which may change depending on configuration or future changes.  Could you add a BUILD_BUG_ON() here to ensure that this remains the case?   Where is plt_ent ever used?   Please move the if () check into prealloc_fixed(), and only keep the loop below    Please use a normal for loop here and iterate upward starting at 0",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,358,0.6697247706422018,0.30434782608695654,0,0.0,1.0,0.0,0.0
215,230843,231032,"Above is exactly the place it's used. I need to cache it because after the module load is finished the ELF header is freed, pointer is not valid any more. With the above modification it's possible to call the function during the whole lifetime of the module .I'll prepare v5 based on your other comments.","Hello Ard,  On 13/03/18 17:12, Ard Biesheuvel wrote:                                                            ^^^^^^^^^^^ (*)   Above is exactly the place it's used. I need to cache it because after the module load is finished the ELF header is freed, pltsec->plt pointer (*) is not valid any more. With the above modification it's possible to call the function during the whole life time of the module.   I'll prepare v5 based on your other comments.  --  Best regards, Alexander Sverdlin.",technical,Alexander Sverdlin,alexander.sverdlin@nokia.com,1,1,304,0.5688073394495413,0.34782608695652173,0,0.0,1.0,0.0,0.0
216,230843,231041,"Right, ok. That's a problem. This means that you are relying on get_module_plt() being called at least once at module load time, which is not guaranteed.Instead, you should set this member (and perhaps the entire prealloc routine) in a module_finalize() implementation.","On 13 March 2018 at 17:13, Alexander Sverdlin <alexander.sverdlin@nokia.com> wrote:  Right, ok. That's a problem.  This means that you are relying on get_module_plt() being called at least once at module load time, which is not guaranteed.  Instead, you should set this member (and perhaps the entire prealloc routine) in a module_finalize() implementation.",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,269,0.48623853211009177,0.43478260869565216,0,0.0,1.0,0.0,0.0
217,230843,231063,"I think it would be much better to use the module_finalize() hook for this, given that it is only called once already, and all the data you need is still available.Note that ARM already has such a function, so you'll need to add a call there,or something like that. The CONFIG_FTRACE dependency can be kept local to module-plts.c","On 13 March 2018 at 17:32, Alexander Sverdlin <alexander.sverdlin@nokia.com> wrote:  I think it would be much better to use the module_finalize() hook for this, given that it is only called once already, and all the data you need is still available.  Note that ARM already has such a function, so you'll need to add a call there, i.e.,  if (IS_ENABLED(CONFIG_ARM_MODULE_PLTS))     module_plt_alloc_fixed(),  or something like that. The CONFIG_FTRACE dependency can be kept local to module-plts.c",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,329,0.6146788990825688,0.5652173913043478,0,0.0,1.0,0.0,0.0
218,230843,231081,Do you consider this a legal C code if without module-plts.o the function would not exist at all? That's too much relying on optimizer I think…,"On 13/03/18 18:39, Ard Biesheuvel wrote:  Do you consider this a legal C code if without module-plts.o the function would not exist at all? That's too much relying on optimizer I think...  --  Best regards, Alexander Sverdlin.",technical,Alexander Sverdlin,alexander.sverdlin@nokia.com,1,1,143,0.25688073394495414,0.6086956521739131,0,0.0,1.0,0.0,0.0
219,230843,231083,"Yes, we rely on that in many different places in the kernel.","On 13 March 2018 at 17:49, Alexander Sverdlin <alexander.sverdlin@nokia.com> wrote:  Yes, we rely on that in many different places in the kernel.",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,60,0.12844036697247707,0.6956521739130435,0,0.0,1.0,0.0,0.0
220,230843,231142,"However, this approach still allows the C compiler to see the code inside the block, and check it for correctness (syntax, types, symbol references, etc).  Thus, you still have to use an #ifdef if the code inside the block references symbols that will not exist if the condition is not met. ""But we can of course ignore it","Hi!  On 13/03/18 18:51, Ard Biesheuvel wrote:  https://www.kernel.org/doc/Documentation/process/coding-style.rst: However, this approach still allows the C compiler to see the code inside the block, and check it for correctness (syntax, types, symbol references, etc).  Thus, you still have to use an #ifdef if the code inside the block references symbols that will not exist if the condition is not met.""   But we can of course ignore it.  --  Best regards, Alexander Sverdlin.""",technical,Alexander Sverdlin,alexander.sverdlin@nokia.com,1,1,322,0.6422018348623854,0.7391304347826086,0,0.0,1.0,0.0,0.0
221,230843,231144,"No problem, but some kind of (*) block would still be required, because get_module_plt() has to work after module_finalize() as well as *before* it.So before module_finalize() we would have to dereference it conditionally.","Hi!  On 13/03/18 18:39, Ard Biesheuvel wrote: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ (*)   No problem, but some kind of (*) block would still be required, because get_module_plt() has to work after module_finalize() as well as *before* it. So before module_finalize() we would have to dereference pltsec->plt->sh_addr conditionally.    --  Best regards, Alexander Sverdlin.",technical,Alexander Sverdlin,alexander.sverdlin@nokia.com,1,1,222,0.42201834862385323,0.8260869565217391,0,0.0,1.0,0.0,0.0
222,230843,231145,"will not exist is ambiguous here. It is rather common to declare symbols, but only define them conditionally, and use IS_ENABLED() to refer to them. As the documentation says, this gets rid of #ifdefs, making the code always visible to the compiler which is a good thing.","On 13 March 2018 at 18:24, Alexander Sverdlin <alexander.sverdlin@nokia.com> wrote:  will not exist"" is ambiguous here. It is rather common to declare symbols, but only define them conditionally, and use IS_ENABLED() to refer to them. As the documentation says, this gets rid of #ifdefs, making the code always visible to the compiler which is a good thing.""",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,271,0.5229357798165137,0.9130434782608695,0,0.0,1.0,0.0,1.0
223,230843,235463,"now when I have a new implementation via module_finalize(), I must admit it's not possible to do it sanely this way.module_finalize() can only add entries at the end of PLT, which means, they will be different from the entries module loader/relocator has created before, which means, FTRACE will not be able to replace these entries with NOPs.As I don't want to do O(N) search on every dynamic ftrace operation, seems this is not an option. Either v4 has to be accepted, or I cannot propose a solution for upstream  combination.","Hello Ard,  On 13/03/18 18:32, Alexander Sverdlin wrote:  now when I have a new implementation via module_finalize(), I must admit it's not possible to do it sanely this way.  module_finalize() can only add entries at the end of PLT, which means, they will be different from the entries module loader/relocator has created before, which means, FTRACE will not be able to replace these entries with NOPs. As I don't want to do O(N) search on every dynamic ftrace operation, seems this is not an option. Either v4 has to be accepted, or I cannot propose a solution for upstream FTRACE+MODULES_PLT combination.  --  Best regards, Alexander Sverdlin.",technical,Alexander Sverdlin,alexander.sverdlin@nokia.com,1,1,528,1.0,0.9565217391304348,1,1.0,0.0,1.0,0.0
224,231231,231979,I'm trying to parse this but I'm not really sure. All I know is and that is really a 4-byte unsigned quantity so anything less is an arbitrary limitation.,"On Tue, Mar 13, 2018 at 10:06:34PM +0100, Maciej S. Szmigiero wrote:  I'm trying to parse this but I'm not really sure.  All I know is:  	unsigned int size = ibuf[2],  and that is really a 4-byte unsigned quantity so anything less is an arbitrary limitation.  --  Regards/Gruss,     Boris.  Good mailing practices for 400: avoid top-posting and trim the reply.",technical,Borislav Petkov,bp@alien8.de,1,0,154,0.3793103448275862,0.2857142857142857,0,0.0,0.0,0.0,0.0
225,231231,232201,"There is no limit on CPU equivalence table length in this patch series like it was in the previous version. The maximum possible value returned by comes from the maximum value of this 'size'variable (that is UINT_MAX) plus the header length This won't fit in 'int' type, hence this patch. That's because this functions tells its caller how much bytes to skip from the beginning of a microcode container file to the first patch section contained in it.","On 14.03.2018 18:58, Borislav Petkov wrote:  There is no limit on CPU equivalence table length in this patch series like it was in the previous version.  The maximum possible value returned by install_equiv_cpu_table() of UINT_MAX + CONTAINER_HDR_SZ comes from the maximum value of this 'size' variable (that is UINT_MAX) plus the header length of CONTAINER_HDR_SZ. This won't fit in 'int' type, hence this patch.  That's because this functions tells its caller how much bytes to skip from the beginning of a microcode container file to the first patch section contained in it.  Maciej",technical,Maciej S. Szmigiero,mail@maciej.szmigiero.name,0,1,451,1.0,0.42857142857142855,0,1.0,0.0,0.0,0.0
226,231231,232222,"OK, will do then.","On 15.03.2018 01:56, Borislav Petkov wrote:  OK, will do then.  Maciej",technical,Maciej S. Szmigiero,mail@maciej.szmigiero.name,0,1,17,0.06896551724137931,1.0,1,1.0,0.0,0.0,0.0
227,231231,232220,"Sure, it leaves the function to deal with the equiv table length only and the caller then adds the header length. Which is actually cleaner","On Thu, Mar 15, 2018 at 01:13:07AM +0100, Maciej S. Szmigiero wrote:  Sure, it leaves the function to deal with the equiv table length only and the caller then adds the header length. Which is actually cleaner.  --  Regards/Gruss,     Boris.  Good mailing practices for 400: avoid top-posting and trim the reply.",technical,Borislav Petkov,bp@alien8.de,1,0,139,0.3103448275862069,0.8571428571428571,0,1.0,0.0,0.0,0.0
228,231231,232210,"This can be done if this function is modified to return only the CPU equivalence table length (without the container header length), leaving its single caller the job of adding the container header length to skip to the fist patch section. Otherwise we introduce a equivalence table length limit of UINT_MAX - CONTAINER_HDR_SZ, as anything more will overflow an unsigned int variable on a 64-bit kernel (on 32-bit this will be caught by the equivalence table truncation check).","On 15.03.2018 00:58, Borislav Petkov wrote:  This can be done if this function is modified to return only the CPU equivalence table length (without the container header length), leaving its single caller the job of adding the container header length to skip to the fist patch section.  Otherwise we introduce a equivalence table length limit of UINT_MAX - CONTAINER_HDR_SZ, as anything more will overflow an unsigned int variable on a 64-bit kernel (on 32-bit this will be caught by the equivalence table truncation check).  Maciej",technical,Maciej S. Szmigiero,mail@maciej.szmigiero.name,0,1,477,0.9885057471264368,0.7142857142857143,0,1.0,0.0,0.0,0.0
229,231363,232244,"Mmm, I'm afraid we can't do this. __sched_setscheduler might be called from interrupt contex by normalize_rt_tasks().","On Tue, Mar 13, 2018 at 8:19 PM, Sinan Kaya <okaya@codeaurora.org> wrote:  So this one missed writel calls in: i40e:   i40e_program_fdir_filter   i40e_clean_rx_irq   i40e_tx_map i40evf:   i40e_clean_rx_irq   i40e_tx_map",technical,Alexander Duyck,alexander.duyck@gmail.com,0,0,117,1.0,1.0,1,0.0,0.0,0.0,0.0
230,232313,234413,"thank you very much, I will resend my patch.BR..","在 2018-03-17六的 12:17 -0700，Randy Dunlap写道：  thank you very much, I will resend my patch.  BR. Ning.",technical,"Zhang, Ning A",ning.a.zhang@intel.com,0,0,48,1.0,1.0,1,1.0,0.0,0.3333333333333333,0.0
231,232512,234791,"Is this commit ready to hit tip/sched/core? I'm looking for an immutable branch that I can use as a basis for the ""dax vs dma vs truncate"" fix series. Thanks in advance.","On Thu, Mar 15, 2018 at 5:19 AM, Peter Zijlstra <peterz@infradead.org> wrote:  Hi Peter,  Is this commit ready to hit tip/sched/core? I'm looking for an immutable branch that I can use as a basis for the dax vs dma vs truncate"" fix series.  Thanks in advance.""",technical,Dan Williams,dan.j.williams@intel.com,1,0,169,1.0,1.0,1,1.0,0.0,0.75,0.0
232,233666,233685,will do. thanks for the feedback,"On 3/16/2018 12:30 PM, Alexander Duyck wrote:  will do. thanks for the feedback.    --  Sinan Kaya Qualcomm Datacenter Technologies, Inc. as an affiliate of Qualcomm Technologies, Inc. Qualcomm Technologies, Inc. is a member of the Code Aurora Forum, a Linux Foundation Collaborative Project.",technical,Sinan Kaya,okaya@codeaurora.org,1,1,32,0.2,1.0,1,0.0,0.0,0.0,0.0
233,233666,233683,You can update the writel call in fm10k_tx_map as well. Of the drivers updated in drivers/net/ethernet/intel/* it looks like this is the only one that still requires any additional changes. Thanks.,"On Fri, Mar 16, 2018 at 9:16 AM, Sinan Kaya <okaya@codeaurora.org> wrote:  You can update the writel call in fm10k_tx_map as well.  Of the drivers updated in drivers/net/ethernet/intel/* it looks like this is the only one that still requires any additional changes.  Thanks.  - Alex",technical,Alexander Duyck,alexander.duyck@gmail.com,0,0,197,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
234,236014,236066,"I don't think this bring anything. However, if you want to fix something you should jump below on error to disable the clock instead of returning 'ret' directly.","Hi Arushi,  On Wed, 21 Mar 2018 11:07:09 +0530, Arushi Singhal <arushisinghal19971997@gmail.com> wrote:   I don't think this bring anything. However, if you want to fix something you should jump below on error to disable the clock instead of returning 'ret' directly.   Thanks, Miquèl  --  Miquel Raynal, Bootlin (formerly Free Electrons) Embedded Linux and Kernel engineering https://bootlin.com",technical,Miquel Raynal,miquel.raynal@bootlin.com,1,0,161,1.0,1.0,1,0.0,0.0,0.0,0.0
235,238260,238297,"I'm still not sure I understand the full extent of the originally-reported error (it's still likely a SPI transport issue?), but I believe this patch is good anyway: I wonder if we should tone down the BUG_ON()'s in drivers/mfd/cros_ec*and drivers/platform/chrome/* too. That's basically a no-no these days, as all of these type of things should be able to gracefully propagate errors, no matter how ""unlikely"" it should be to see a crazy protocol version number or a bad message length.","On Wed, Sep 27, 2017 at 02:35:27PM -0700, Shawn Nematbakhsh wrote:  I'm still not sure I understand the full extent of the originally-reported error (it's still likely a SPI transport issue?), but I believe this patch is good anyway:  Reviewed-by: Brian Norris <briannorris@chromium.org>  I wonder if we should tone down the BUG_ON()'s in drivers/mfd/cros_ec* and drivers/platform/chrome/* too. That's basically a no-no these days, as all of these type of things should be able to gracefully propagate errors, no matter how unlikely"" it should be to see a crazy protocol version number or a bad message length.""",technical,Brian Norris,briannorris@chromium.org,1,0,487,0.3194888178913738,0.2,0,0.0,0.994535519125683,0.0,0.2568306010928962
236,238260,647642,"Jon tracked down the root cause of the originally-reported error, but we should still land this patch, as it fixes error signaling that was previously broken.","On Wed, Sep 27, 2017 at 3:19 PM, Brian Norris <briannorris@chromium.org> wrote:  Jon tracked down the root cause of the originally-reported error, but we should still land this patch, as it fixes error signaling that was previously broken.",technical,Shawn N,shawnn@chromium.org,0,0,158,0.0926517571884984,0.3,0,0.2568306010928962,0.7377049180327869,0.2568306010928962,0.03825136612021858
237,238260,781847,Can you also queue this one as a fix for v4.15?,"Hi Lee,  On 14/11/17 17:00, Shawn N wrote:  Can you also queue this one as a fix for v4.15?  Cheers Jon  --  nvpublic",technical,Jon Hunter,jonathanh@nvidia.com,1,0,47,0.038338658146964855,0.5,0,0.33879781420765026,0.6557377049180327,0.03825136612021858,0.0
238,238260,239641,"Dear all, Cc'ing some more chromium folks. This patch is a bit old and is already applied but I would like to discuss an issue that Tomeu found playing with kernelci and a VeyronJaq attached to our lab. Seems that after this patch, the veyron jaq spits out lots of spitranfer error messages. See full log here. The issue is random, not always happens, but when happens makes cros-ec unusable. Reproduce the issue is easier if CONFIG_SMP is not set. What happens is that the master starts the transmission and the cros-ec returns an spi error event. The difference between after and before the patch is that now the cros-ec does not recover, whereas without this patch after some tries it succeeds (note that before the patch the affected code returned -EAGAIN whereas now returns -EREMOTEIO)I think that reverting this patch is NOT the solution, but I don't have enough knowledge yet to understand why the cros-ec fails. I need to look at the cros-ec firmware to understand better what is happening, but meanwhile, thoughts? clues? An important note is that I did not reproduce the issue on a Veyron Minnie, even with CONFIG_SMP disabled.","Dear all,  Cc'ing some more chromium folks.  2017-11-29 13:11 GMT+01:00 Lee Jones <lee.jones@linaro.org>:  This patch is a bit old and is already applied but I would like to discuss an issue that Tomeu found playing with kernelci and a Veyron Jaq attached to our lab.  Seems that after this patch, the veyron jaq spits out lots of spi tranfer error messages. See full log here [1]   cros-ec-spi spi0.0: spi transfer failed: -121  cros-ec-spi spi0.0: Command xfer error (err:-121)  cros-ec-i2c-tunnel ff110000.spi:ec@0:i2c-tunnel: Error transferring EC i2c message -121  The issue is random, not always happens, but when happens makes cros-ec unusable. Reproduce the issue is easier if CONFIG_SMP is not set.  What happens is that the master starts the transmission and the cros-ec returns an spi error event (EC_SPI_RX_BAD_DATA  - data is 0xfb). The difference between after and before the patch is that now the cros-ec does not recover, whereas without this patch after some tries it succeeds (note that before the patch the affected code returned -EAGAIN whereas now returns -EREMOTEIO)  I think that reverting this patch is NOT the solution, but I don't have enough knowledge yet to understand why the cros-ec fails. I need to look at the cros-ec firmware to understand better what is happening, but meanwhile, thoughts? clues?  An important note is that I did not reproduce the issue on a Veyron Minnie, even with CONFIG_SMP disabled.  [1] https://lava.collabora.co.uk/scheduler/job/1085493#L905  Best regards,   Enric",technical,Enric Balletbo Serra,eballetbo@gmail.com,1,0,1138,0.6996805111821086,0.7,0,0.9781420765027322,0.01639344262295082,0.639344262295082,0.0
239,238260,239660,"Hello Enric Would it be possible for you to run ""ectool version"" on both your machines? Based on the code the EC is running we might have some hints on what the differences are. You can find both ectool and the code the ec runs on Though I would use ectool from the master branch. One thing I suspect is different is that veyron_minnie regularly polls an accelerometer, depending on the userspace you're running it's possible it unwedged itself with a few accelerometer requests.","Hello Enric  On Mon, Mar 26, 2018 at 9:48 AM, Enric Balletbo Serra <eballetbo@gmail.com> wrote:  Would it be possible for you to run ectool version"" on both your machines? Based on the code the EC is running we might have some hints on what the differences are.  You can find both ectool and the code the ec runs on https://chromium.googlesource.com/chromiumos/platform/ec/+/firmware-veyron-6588.B. Though I would use ectool from the master branch.  One thing I suspect is different is that veyron_minnie regularly polls an accelerometer, depending on the userspace you're running it's possible it unwedged itself with a few accelerometer requests.""",technical,Alexandru M Stan,amstan@chromium.org,0,0,479,0.2971246006389776,0.8,0,0.9781420765027322,0.01639344262295082,0.0,0.0
240,238260,240146,"I think that accessing to the ec console should give the same result, right? In such case here is: We're running the RW firmware and I just discovered that our jaq is a mighty (but I guess it's the same?)","Hi Alexandru  2018-03-26 19:26 GMT+02:00 Alexandru M Stan <amstan@chromium.org>:  I think that accessing to the ec console should give the same result, right?  In such case here is:  Veyron Minnie ( ASUS Chromebook Flip C100PA ) ------------------------------------------------------------------- Chip:    stm stm32f07x Board:   0 RO:      minnie_v1.1.2697-faafaa5 RW:      minnie_v1.1.2712-242f6bd Build: minnie_v1.1.2712-242f6bd 2016-11-03 00:34:41 @build196-m2  Veyron Jaq (  Haier Mighty MP ) -------------------------------------------------------------------- Chip:    stm stm32f07x Board:   0 RO:      mighty_v1.1.2680-6727e1d RW:      mighty_v1.1.2712-242f6bd Build: mighty_v1.1.2680-6727e1d 2015-03-24 01:12:48 @build290-m2  We're running the RW firmware and I just discovered that our jaq is a mighty (but I guess it's the same?)  Thanks,   Enric",technical,Enric Balletbo Serra,eballetbo@gmail.com,1,0,204,0.15335463258785942,0.9,0,0.9836065573770492,0.01092896174863388,0.0,0.01092896174863388
241,238260,241978,"Yep, even better. Looks like your mighty is running the RO firmware, whereas your minnie runs RW. Is it possible you have the 0x200 bit in the gbb flags set on mighty? That would prevent the RO->RW transition, and give you an older firmware.6727e1d..242f6bd is quite the change. I see some spi changes too, though i believe it's mostly at power state transitions (suspend/resume). Other changes include battery settings (yeah.. you should really avoid running that RO if you can avoid it) and a ton of accelerometer stuff for minnie. If it's not the gbb flags, and we can't figure it out why you're stuck in RO, you can also use ""sysjump RW"" to force the RW copy on mighty. See if there's any behavior changes in what you care about. They're essentially the same, but they're running slightly different firmware. In practice the only difference is that mighty's firmware reads an extra gpio for the battery presence. Feel free to diff the board/{jaq,mighty} ec folders for yourself for more details/assurances. All in all I'm not sure that the version differences are enough to explain the spi errors you see in the kernel. My bet is back to the accelerometer stuff: Are you running chrome os ui on this device (is there a chrome os-chrome process constantly polling the accelerometer, so asking the cros-ecdriver for transfers)? Another thing to make sure accelerometer is disabled is to run""accelerate 0 0"" on the minnie EC. If none of that accelerometer stuff is enabled, minnie should essentially act like a mighty/jaq.","On Tue, Mar 27, 2018 at 3:49 AM, Enric Balletbo Serra <eballetbo@gmail.com> wrote:  Yep, even better.   Looks like your mighty is running the RO firmware, whereas your minnie runs RW. Is it possible you have the 0x200 bit in the gbb flags set on mighty? That would prevent the RO->RW transition, and give you an older firmware.  6727e1d..242f6bd is quite the change. I see some spi changes too, though i believe it's mostly at power state transitions (suspend/resume). Other changes include battery settings (yeah.. you should really avoid running that RO if you can avoid it) and a ton of accelerometer stuff for minnie.  If it's not the gbb flags, and we can't figure it out why you're stuck in RO, you can also use sysjump RW"" to force the RW copy on mighty. See if there's any behavior changes in what you care about.   They're essentially the same, but they're running slightly different firmware. In practice the only difference is that mighty's firmware reads an extra gpio for the battery presence.  Feel free to diff the board/{jaq,mighty} ec folders for yourself for more details/assurances.   All in all I'm not sure that the version differences are enough to explain the spi errors you see in the kernel.  My bet is back to the accelerometer stuff: Are you running chromeos ui on this device (is there a chromeos-chrome process constantly polling the accelerometer, so asking the cros-ec driver for transfers)? Another thing to make sure accelerometer is disabled is to run ""accelrate 0 0"" on the minnie EC. If none of that accelerometer stuff is enabled, minnie should essentially act like a mighty/jaq.""",technical,Alexandru M Stan,amstan@chromium.org,0,0,1523,1.0,1.0,1,1.0,0.0,0.01092896174863388,0.0
242,239101,239277,Reviewed-by: Gilad Ben-Yossef,"On Sun, Mar 25, 2018 at 9:41 PM, Yael Chemla <yael.chemla@foss.arm.com> wrote:    Reviewed-by: Gilad Ben-Yossef <gilad@benyossef.com>    --  Gilad Ben-Yossef Chief Coffee Drinker  If you take a class in large-scale robotics, can you end up in a situation where the homework eats your dog?""  -- Jean-Baptiste Queru""",technical,Gilad Ben-Yossef,gilad@benyossef.com,1,0,29,0.007905138339920948,0.21428571428571427,0,0.0,1.0,0.0,0.0
243,239101,239279,Reviewed-by: Gilad Ben-Yossef,"On Sun, Mar 25, 2018 at 9:41 PM, Yael Chemla <yael.chemla@foss.arm.com> wrote:  Reviewed-by: Gilad Ben-Yossef <gilad@benyossef.com>  --  Gilad Ben-Yossef Chief Coffee Drinker  If you take a class in large-scale robotics, can you end up in a situation where the homework eats your dog?""  -- Jean-Baptiste Queru""",technical,Gilad Ben-Yossef,gilad@benyossef.com,1,0,29,0.007905138339920948,0.2857142857142857,0,0.0,1.0,0.0,0.0
244,239101,239869,"This one had various issues.  I've fixed most of what I saw and staged in linux-next (purely for build test coverage purposes).  I may drop this patch if others disagree with it (or my sg deallocation in the error path question isn't answered).I've staged the changes here (and in linux-next via 'for-next') switched all the new GFP_KERNEL uses to GFP_NOIO.  The fact that you're doing allocations at all (per IO) is bad enough.  Using GFP_KERNEL is a serious liability (risk of deadlock if dm-verity were to be used for something like.. swap.. weird setup but possible).But the gfp flags aside, the need for additional memory and the expectation of scalable async parallel IO is potentially at odds with changes like this (that I just staged, and had to rebase your 2 patches on top of) So I'm particularly interested to hear from google folks to understand if they are OK with your proposed verity async crypto API use.","On Sun, Mar 25 2018 at  2:41pm -0400, Yael Chemla <yael.chemla@foss.arm.com> wrote:   This one had various issues.  I've fixed most of what I saw and staged in linux-next (purely for build test coverage purposes).  I may drop this patch if others disagree with it (or my sg deallocation in the error path question isn't answered).  I've staged the changes here (and in linux-next via 'for-next'): https://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm.git/log/?h=dm-4.17  I switched all the new GFP_KERNEL uses to GFP_NOIO.  The fact that you're doing allocations at all (per IO) is bad enough.  Using GFP_KERNEL is a serious liability (risk of deadlock if dm-verity were to be used for something like.. swap.. weird setup but possible).  But the gfp flags aside, the need for additional memory and the expectation of scalable async parallel IO is potentially at odds with changes like this (that I just staged, and had to rebase your 2 patches ontop of): https://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm.git/commit/?h=dm-4.17&id=a89f6a2cfec86fba7a115642ff082cb4e9450ea6  So I'm particulalry interested to hear from google folks to understand if they are OK with your proposed verity async crypto API use.  Mike",technical,Mike Snitzer,snitzer@redhat.com,1,0,921,0.3695652173913043,0.35714285714285715,0,0.03333333333333333,0.9666666666666667,0.0,0.0
245,239101,239977,"Out of my curiosity, since I thought whether or not this should use GFP_NOIO during my review but than answered to myself ""Nah, dm-verity is read only, can't swap to that"" - how does one use a read only DM-Verity to host swap partition/file? :-)If by ""scalable async parallel IO"" you mean crypto HW than for what it's worth, my experience is that makers of devices with is less powerful CPUs are the ones that tend to add them, so they stands to benefit  the most of this change. Gilad","On Tue, Mar 27, 2018 at 4:06 AM, Mike Snitzer <snitzer@redhat.com> wrote:  Out of my curiosity, since I thought whether or not this should use GFP_NOIO during my review but than answered to myself Nah, dm-verity is read only, can't swap to that"" - how does one use a read only DM-Verity to host swap partition/file? :-)    If by ""scalable async parallel IO"" you mean crypto HW than for what it's worth, my experience is that makers of devices with is less powerful CPUs are the ones that tend to add them, so they stands to benefit  the most of this change.  Gilad  --  Gilad Ben-Yossef Chief Coffee Drinker  ""If you take a class in large-scale robotics, can you end up in a situation where the homework eats your dog?""  -- Jean-Baptiste Queru""",technical,Gilad Ben-Yossef,gilad@benyossef.com,1,0,485,0.20948616600790515,0.42857142857142855,0,0.03333333333333333,0.9666666666666667,0.0,0.0
246,239101,239980,"Okay, I definitely would like to see dm-verity better support hardware crypto accelerators, but these patches were painful to read. There are lots of smaller bugs, but the high-level problem which you need to address first is that on every bio you are always allocating all the extra memory to hold a hash request and scatter list for every data block.  This will not only hurt performance when the hashing is done in software (I'm skeptical that your performance numbers are representative of that case), but it will also fall apart under memory pressure.  We are trying to get low-end Android devices to start using dm-verity, and such devices often have only 1 GB or even only 512MB of RAM, so memory allocations are at increased risk of failing.  In fact I'm pretty sure you didn't do any proper stress testing of these patches, since the first thing they do for every bio is try to allocate a physically contiguous array that is nearly as long as the full bio data itself (n_blocks *sizeof(struct dm_verity_req_data) = n_blocks * 3264, at least on a 64-bit platform, mostly due to the 'struct dm_verity_fec_io'), so potentially up to about 1 MB, that's going to fail a lot even on systems with gigabytes of RAM...(You also need to verify that your new code is compatible with the forward error correction feature, with the ""ignore_zero_blocks"" option, and with the new""check_at_most_once"" option.  From my reading of the code, all of those seemed broken, the dm_verity_fec_io structures, for example, weren't even being initialized...)I think you need to take a close look at how dm-crypt handles async crypto implementations, since it seems to do it properly without hurting the common case where the crypto happens synchronously.  What it does, is it reserves space in the per-bio data for a single cipher request.  Then, *only* if the cipher implementation actually processes the request asynchronously (as indicated by-EINPROGRESS being returned) is a new cipher request allocated dynamically, using a mempool (not kmalloc, which is prone to fail).  Note that unlike your patches it also properly handles the case where the hardware crypto queue is full, as indicated by the cipher implementation returning -EBUSY, in that case,dm-crypt waits to start another request until there is space in the queue. I think it would be possible to adapt dm-crypt's solution to dm-verity.","[+Cc linux-crypto]  Hi Yael,  On Sun, Mar 25, 2018 at 07:41:30PM +0100, Yael Chemla wrote:  Okay, I definitely would like to see dm-verity better support hardware crypto accelerators, but these patches were painful to read.  There are lots of smaller bugs, but the high-level problem which you need to address first is that on every bio you are always allocating all the extra memory to hold a hash request and scatterlist for every data block.  This will not only hurt performance when the hashing is done in software (I'm skeptical that your performance numbers are representative of that case), but it will also fall apart under memory pressure.  We are trying to get low-end Android devices to start using dm-verity, and such devices often have only 1 GB or even only 512 MB of RAM, so memory allocations are at increased risk of failing.  In fact I'm pretty sure you didn't do any proper stress testing of these patches, since the first thing they do for every bio is try to allocate a physically contiguous array that is nearly as long as the full bio data itself (n_blocks * sizeof(struct dm_verity_req_data) = n_blocks * 3264, at least on a 64-bit platform, mostly due to the 'struct dm_verity_fec_io'), so potentially up to about 1 MB, that's going to fail a lot even on systems with gigabytes of RAM...  (You also need to verify that your new code is compatible with the forward error correction feature, with the ignore_zero_blocks"" option, and with the new ""check_at_most_once"" option.  From my reading of the code, all of those seemed broken, the dm_verity_fec_io structures, for example, weren't even being initialized...)  I think you need to take a close look at how dm-crypt handles async crypto implementations, since it seems to do it properly without hurting the common case where the crypto happens synchronously.  What it does, is it reserves space in the per-bio data for a single cipher request.  Then, *only* if the cipher implementation actually processes the request asynchronously (as indicated by -EINPROGRESS being returned) is a new cipher request allocated dynamically, using a mempool (not kmalloc, which is prone to fail).  Note that unlike your patches it also properly handles the case where the hardware crypto queue is full, as indicated by the cipher implementation returning -EBUSY, in that case, dm-crypt waits to start another request until there is space in the queue.  I think it would be possible to adapt dm-crypt's solution to dm-verity.  Thanks,  Eric """,technical,Eric Biggers,ebiggers3@gmail.com,1,0,2383,0.9051383399209486,0.5,0,0.03333333333333333,0.9666666666666667,0.0,0.0
247,239101,240029,"Mike and others, did anyone even try to run verity setup tests? We have verity-compat-test in our test suite, is has even basic FEC tests included. We just added userspace verification of FEC RS codes to compare if kernel behaves the same. I tried to apply three last dm-verity patches from your tree to Linus mainline. It does even pass the *first* line of the test script and blocks the kernel forever...(Running on 32bit Intel VM.)*NACK* to the last two dm-verity patches.(The ""validate hashes once"" is ok, despite I really do not like this approach...)And comments from Eric are very valid as well, I think all this need to be fixed before it can go to mainline.","Mike and others,  did anyone even try to run veritysetup tests?  We have verity-compat-test in our testsuite, is has even basic FEC tests included.  We just added userspace verification of FEC RS codes to compare if kernel behaves the same.  I tried to apply three last dm-verity patches from your tree to Linus mainline.  It does even pass the *first* line of the test script and blocks the kernel forever... (Running on 32bit Intel VM.)  *NACK* to the last two dm-verity patches.  (The validate hashes once"" is ok, despite I really do not like this approach...)  And comments from Eric are very valid as well, I think all this need to be fixed before it can go to mainline.  Thanks, Milan  On 03/27/2018 08:55 AM, Eric Biggers wrote:""",technical,Milan Broz,gmazyland@gmail.com,0,0,666,0.2845849802371542,0.5714285714285714,0,0.03333333333333333,0.9666666666666667,0.0,0.0
248,239101,240062,"Thanks for the detailed feedback, I'll have a look at how  dm-crypt avoid dynamic allocation per-bio, and also do forward error correction tests. Subject: Re: [dm-devel] [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks I definitely would like to see dm-verity better support hardware crypto accelerators, but these patches were painful to read. There are lots of smaller bugs, but the high-level problem which you need to address first is that on every bio you are always allocating all the extra memory to hold a hash request and scatter list for every data block.  This will not only hurt performance when the hashing is done in software (I'm skeptical that your performance numbers are representative of that case), but it will also fall apart under memory pressure.  We are trying to get low-end Android devices to start using dm-verity, and such devices often have only 1 GB or even only 512 MB of RAM, so memory allocations are at increased risk of failing.  In fact I'm pretty sure you didn't do any proper stress testing of these patches, since the first thing they do for every bio is try to allocate a physically contiguous array that is nearly as long as the full bio data itself (n_blocks * sizeof(struct dm_verity_req_data) = n_blocks * 3264, at least on a 64-bit platform, mostly due to the 'struct dm_verity_fec_io'), so potentially up to about 1 MB, that's going to fail a lot even on systems with gigabytes of RAM...(You also need to verify that your new code is compatible with the forward error correction feature, with the ""ignore_zero_blocks"" option, and with the new ""check_at_most_once"" option.  From my reading of the code, all of those seemed broken, the dm_verity_fec_io structures, for example, weren't even being initialized...)I think you need to take a close look at how dm-crypt handles async crypto implementations, since it seems to do it properly without hurting the common case where the crypto happens synchronously.  What it does, is it reserves space in the per-bio data for a single cipher request.  Then, *only* if the cipher implementation actually processes the request asynchronously (as indicated by -EINPROGRESS being returned) is a new cipher request allocated dynamically, using a mempool (not kmalloc, which is prone to fail).  Note that unlike your patches it also properly handles the case where the hardware crypto queue is full, as indicated by the cipher implementation returning -EBUSY, in that case, dm-crypt waits to start another request until there is space in the queue. I think it would be possible to adapt dm-crypt's solution to dm-verity.","Hi Eric, Thanks for the detailed feedback, I'll have a look at how  dm-crypt avoid dynamic allocation per-bio, and also do forward error correction tests. Yael  -----Original Message----- From: Eric Biggers <ebiggers3@gmail.com>  Sent: Tuesday, 27 March 2018 9:55 To: Yael Chemla <yael.chemla@foss.arm.com> Cc: Alasdair Kergon <agk@redhat.com>, Mike Snitzer <snitzer@redhat.com>, dm-devel@redhat.com, linux-kernel@vger.kernel.org, ofir.drang@gmail.com, Yael Chemla <yael.chemla@arm.com>, linux-crypto@vger.kernel.org, gilad@benyossef.com Subject: Re: [dm-devel] [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks  [+Cc linux-crypto]  Hi Yael,  On Sun, Mar 25, 2018 at 07:41:30PM +0100, Yael Chemla wrote:  Okay, I definitely would like to see dm-verity better support hardware crypto accelerators, but these patches were painful to read.  There are lots of smaller bugs, but the high-level problem which you need to address first is that on every bio you are always allocating all the extra memory to hold a hash request and scatterlist for every data block.  This will not only hurt performance when the hashing is done in software (I'm skeptical that your performance numbers are representative of that case), but it will also fall apart under memory pressure.  We are trying to get low-end Android devices to start using dm-verity, and such devices often have only 1 GB or even only 512 MB of RAM, so memory allocations are at increased risk of failing.  In fact I'm pretty sure you didn't do any proper stress testing of these patches, since the first thing they do for every bio is try to allocate a physically contiguous array that is nearly as long as the full bio data itself (n_blocks * sizeof(struct dm_verity_req_data) = n_blocks * 3264, at least on a 64-bit platform, mostly due to the 'struct dm_verity_fec_io'), so potentially up to about 1 MB, that's going to fail a lot even on systems with gigabytes of RAM...  (You also need to verify that your new code is compatible with the forward error correction feature, with the ignore_zero_blocks"" option, and with the new ""check_at_most_once"" option.  From my reading of the code, all of those seemed broken, the dm_verity_fec_io structures, for example, weren't even being initialized...)  I think you need to take a close look at how dm-crypt handles async crypto implementations, since it seems to do it properly without hurting the common case where the crypto happens synchronously.  What it does, is it reserves space in the per-bio data for a single cipher request.  Then, *only* if the cipher implementation actually processes the request asynchronously (as indicated by -EINPROGRESS being returned) is a new cipher request allocated dynamically, using a mempool (not kmalloc, which is prone to fail).  Note that unlike your patches it also properly handles the case where the hardware crypto queue is full, as indicated by the cipher implementation returning -EBUSY, in that case, dm-crypt waits to start another request until there is space in the queue.  I think it would be possible to adapt dm-crypt's solution to dm-verity.  Thanks,  Eric """,technical,Unkown Name,yael.chemla@foss.arm.com,0,0,2619,1.0,0.6428571428571429,0,0.03333333333333333,0.9666666666666667,0.0,0.0
249,239101,240065,"I need to rewrite these patches according to issues you and Eric Biggers mentioned. Please drop this v1 patch. Subject: Re: [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks This one had various issues.  I've fixed most of what I saw and staged in linux-next (purely for build test coverage purposes).  I may drop this patch if others disagree with it (or my sg deallocation in the error path question isn't answered).I've staged the changes here (and in linux-next via 'for-next') I switched all the new GFP_KERNEL uses to GFP_NOIO.  The fact that you're doing allocations at all (per IO) is bad enough.  Using GFP_KERNEL is a serious liability (risk of deadlock if dm-verity were to be used for something like.. swap.. weird setup but possible).But the gfp flags aside, the need for additional memory and the expectation of scalable async parallel IO is potentially at odds with changes like this (that I just staged, and had to rebase your 2 patches on top of): So I'm particularly interested to hear from google folks to understand if they are OK with your proposed verity async crypto API use.","Hi Mike I need to rewrite these patches according to issues you and Eric Biggers mentioned. please drop this v1 patch. Thank you, Yael  -----Original Message----- From: Mike Snitzer <snitzer@redhat.com>  Sent: Tuesday, 27 March 2018 4:07 To: Yael Chemla <yael.chemla@foss.arm.com> Cc: Alasdair Kergon <agk@redhat.com>, dm-devel@redhat.com, linux-kernel@vger.kernel.org, ofir.drang@gmail.com, Yael Chemla <yael.chemla@arm.com> Subject: Re: [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks  On Sun, Mar 25 2018 at  2:41pm -0400, Yael Chemla <yael.chemla@foss.arm.com> wrote:   This one had various issues.  I've fixed most of what I saw and staged in linux-next (purely for build test coverage purposes).  I may drop this patch if others disagree with it (or my sg deallocation in the error path question isn't answered).  I've staged the changes here (and in linux-next via 'for-next'): https://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm.git/log/?h=dm-4.17  I switched all the new GFP_KERNEL uses to GFP_NOIO.  The fact that you're doing allocations at all (per IO) is bad enough.  Using GFP_KERNEL is a serious liability (risk of deadlock if dm-verity were to be used for something like.. swap.. weird setup but possible).  But the gfp flags aside, the need for additional memory and the expectation of scalable async parallel IO is potentially at odds with changes like this (that I just staged, and had to rebase your 2 patches ontop of): https://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm.git/commit/?h=dm-4.17&id=a89f6a2cfec86fba7a115642ff082cb4e9450ea6  So I'm particulalry interested to hear from google folks to understand if they are OK with your proposed verity async crypto API use.  Mike",technical,Unkown Name,yael.chemla@foss.arm.com,0,0,1115,0.4505928853754941,0.7142857142857143,0,0.03333333333333333,0.9666666666666667,0.0,0.0
250,239101,240077,"I will run verity setup test on next version of these patches and contact you about verity-compat-test test suits. Subject: Re: [dm-devel] [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks Mike and others, did anyone even try to run verity setup tests? We have verity-compat-test in our test suite, is has even basic FEC tests included. We just added userspace verification of FEC RS codes to compare if kernel behaves the same. I tried to apply three last dm-verity patches from your tree to Linus mainline. It does even pass the *first* line of the test script and blocks the kernel forever...(Running on 32bit Intel VM.)*NACK* to the last two dm-verity patches.(The ""validate hashes once"" is ok, despite I really do not like this approach...) And comments from Eric are very valid as well, I think all this need to be fixed before it can go to mainline.","Hi Milan, I will run veritysetup test on next version of these patches and contact you about verity-compat-test testsuits. Thank you, Yael  -----Original Message----- From: Milan Broz <gmazyland@gmail.com>  Sent: Tuesday, 27 March 2018 11:05 To: Eric Biggers <ebiggers3@gmail.com>, Yael Chemla <yael.chemla@foss.arm.com>, Mike Snitzer <snitzer@redhat.com> Cc: Alasdair Kergon <agk@redhat.com>, dm-devel@redhat.com, linux-kernel@vger.kernel.org, ofir.drang@gmail.com, Yael Chemla <yael.chemla@arm.com>, linux-crypto@vger.kernel.org, gilad@benyossef.com Subject: Re: [dm-devel] [PATCH 2/2] md: dm-verity: allow parallel processing of bio blocks  Mike and others,  did anyone even try to run veritysetup tests?  We have verity-compat-test in our testsuite, is has even basic FEC tests included.  We just added userspace verification of FEC RS codes to compare if kernel behaves the same.  I tried to apply three last dm-verity patches from your tree to Linus mainline.  It does even pass the *first* line of the test script and blocks the kernel forever... (Running on 32bit Intel VM.)  *NACK* to the last two dm-verity patches.  (The validate hashes once"" is ok, despite I really do not like this approach...)  And comments from Eric are very valid as well, I think all this need to be fixed before it can go to mainline.  Thanks, Milan  On 03/27/2018 08:55 AM, Eric Biggers wrote:""",technical,Unkown Name,yael.chemla@foss.arm.com,0,0,873,0.36561264822134387,0.7857142857142857,0,0.03333333333333333,0.9666666666666667,0.0,0.0
251,239101,268040,"I have a question regarding scatter list memory: I noticed that all blocks in dm verity end up using two buffers: one for data and other for salt. I'm using function similar to verity_for_io_block to iterate and find the number of buffers, in my case data_dev_block_bits =12, to do=4096, thus the do while will iterate only once. I assume that since it's there there are cases it'll iterate more. I'm trying to figure out which cases will require more than one buffer of data per block? In dm_crypt there is limitation of static 4 scatter list elements per in/out (see struct dm_crypt_request).Is there an upper bound regarding number of buffers per block in dm-verity? I need this for the implementation of  mempool per scatter list buffers.","I have a question regarding scatterlist memory: I noticed that all blocks in dmverity end up using two buffers: one for data and other for salt.  I'm using function similar to verity_for_io_block to iterate and find the number of buffers, in my case data_dev_block_bits =12, todo=4096, thus the do while will iterate only once. I assume that since it's there there are cases it'll iterate more. I'm trying to figure out which cases will require more than one buffer of data per block?  In dm_crypt there is limitation of static 4 scatterlist elements per in/out (see struct dm_crypt_request). Is there an upper bound regarding number of buffers per block in dm-verity?   I need this for the implementation of  mempool per scatterlist buffers. Thanks , Yael",technical,Unkown Name,yael.chemla@foss.arm.com,0,0,742,0.2845849802371542,1.0,1,1.0,0.0,0.9333333333333333,0.0
252,244486,244487,Peter points out that this isn't sufficient...  Let me try again.,"Peter Z points out that this isn't sufficient...  Let me try again.  regards, dan carpenter",technical,Dan Carpenter,dan.carpenter@oracle.com,0,1,65,1.0,1.0,1,0.0,0.0,0.0,0.0
253,245912,245958,"That's a hefty cc list. I can't see Rob Herring though, and he's usually the person who you need to convince to get your bindings accepted. I recommend using it to build your CC list, and then add others you think are relevant. I'm not sure what the guidelines are for generic bindings, so I'll defer to Rob for this patch.","Hi Jae,  On 11 April 2018 at 04:02, Jae Hyun Yoo <jae.hyun.yoo@linux.intel.com> wrote:  That's a hefty cc list. I can't see Rob Herring though, and he's usually the person who you need to convince to get your bindings accepted.  I recommend using ./scripts/get_maintainers.pl to build your CC list, and then add others you think are relevant.  I'm not sure what the guidelines are for generic bindings, so I'll defer to Rob for this patch.  Cheers,  Joel",technical,Joel Stanley,joel@jms.id.au,1,0,323,0.10027472527472528,0.25925925925925924,0,0.0,1.0,0.0,0.0
254,245912,245948,"We try to capitalise ASPEED. Are you sure that this is driven by clkin? Most peripherals on the A speed are attached to the apb, so should reference that clock. Can you explain why you need both the parent clock and this frequency to be specified? Perhaps msg-timing-period? Or just msg-timing?","On 11 April 2018 at 04:02, Jae Hyun Yoo <jae.hyun.yoo@linux.intel.com> wrote:  We try to capitalise ASPEED.   Are you sure that this is driven by clkin? Most peripherals on the Aspeed are attached to the apb, so should reference that clock.   Can you explain why you need both the parent clock and this frequency to be specified?   Perhaps msg-timing-period? Or just msg-timing?",technical,Joel Stanley,joel@jms.id.au,1,0,294,0.07967032967032966,0.2777777777777778,0,0.0,1.0,0.0,0.0
255,245912,245961,The patches to the device trees get merged by the ASPEED maintainer(me). Once you have the bindings reviewed you can send the patches tome and the linux-aspeed list (I've got a pending patch to maintainers that will ensure get_maintainers.pl does the right thing as far as email addresses go). I'd suggest dropping it from your series and re-sending once the bindings and driver are reviewed.,"On 11 April 2018 at 04:02, Jae Hyun Yoo <jae.hyun.yoo@linux.intel.com> wrote:  The patches to the device trees get merged by the ASPEED maintainer (me). Once you have the bindings reviewed you can send the patches to me and the linux-aspeed list (I've got a pending patch to maintainers that will ensure get_maintainers.pl does the right thing as far as email addresses go).  I'd suggest dropping it from your series and re-sending once the bindings and driver are reviewed.  Cheers,  Joel",technical,Joel Stanley,joel@jms.id.au,1,0,392,0.10302197802197802,0.2962962962962963,0,0.0,1.0,0.0,0.0
256,245912,245923,"Thanks a lot for sharing your time. Please see my inline answers. No it isn't. Will drop the line. Okay. I'll use bool instead of int. The two above functions are slightly different but uses the same PECI command which provides both Tthrottle and Tcontrol values in pkg_configarray so it updates the values to reduce duplicate PECI transactions. Probably, combining these two functions into get_ttrottle_and_tcontrol()would look better. I'll rewrite it.Are you pointing out this code? Then I'll rewrite it as a function. If not, please point out the duplication. Core temperature group will be registered only when it detects at least one core checked by check_resolved_cores(), so find_core_index() can be called only when priv->core_mask has a non-zero value. The 'nothing is found' case will not happen.cputemp_read_string() is mapped to read_string member of hwmon_opsstruct, so hwmon subsystem passes the channel parameter based on the registered channel order. Should I modify hwmon subsystem code? As explained above, find_core index() returns a correct index always. This is an attribute of DTS margin temperature which reflects thermal margin to Tcontrol of the CPU package. If it shows '0' means it reached to Tcontrol, the first level of thermal warning. If the CPU keeps getting hot then this DTS margin shows a negative value until it reaches to Tjmax. When the temperature reaches to Tjmax at last then it shows the lower critical value which lcrit indicates as the second level of thermal warning. Both Tjmax and Tcontrol have positive values and Tjmax is greater than Tcontrol always. As explained above, lcrit of DTS margin should show a negative value means the margin goes down across '0'. On the other hand,crit_hyst of Die temperature should show absolute hysteresis value between Tcontrol and Tjmax.This driver provides multiple channels and each channel has its own supplement attributes. As you mentioned, Die temperature channel and Core temperature channel have their individual crit attributes and they reflect the same value, Tjmax. It is not reporting several times but reporting the same value. Each function is called from cputemp_read() which is mapped to read function pointer of hwmon_ops struct. Since each channel has different set of attributes so the cputemp_read() calls an individual channel handler after checking the channel type. Of course, we can handle all attributes of all channels in a single function but the way also needs channel type checking code on each attribute. Sure, will use defines instead. For proper operation of this driver, PECI_CMD_GET_TEMP and PECI_CMD_RD_PKG_CFG have to be supported by a client CPU.PECI_CMD_GET_TEMP is provided as a default command but PECI_CMD_RD_PKG_CFG depends on PECI minor revision of a CPU package so this checking is needed. Got it. I'll remove the error message and will add a proper handling code into PECI core. This driver can't support core temperature monitoring if a CPU doesn't support PECI_CMD_RD_PCI_CFG_LOCAL command. In that case, it skips core temperature group creation and supports only basic temperature monitoring of Die, DTS margin and etc. I'll add this description as a comment. Should I split out hwmon documents and dt bindings too? No. Will drop the line. It is temperature monitoring specific function and it touches module specific variables. Do you really think that this non-generic function should be moved to PECI core? I'm sure it isn't. Sure, I'll rewrite it. Okay. In case of check_cpu_id(), it could be used as a generic PECIfunction. I'll move it into PECI core. I'll rewrite it to.Should I use -EPERM? Any suggestion? Client address range validation will be done inpeci_check_addr_validity() in PECI core before probing a device driver. I'll remove the error message and will add a proper handling code into PECI core on each error type. Yes, it would be safer. Will fix it.","Hi Guenter,  Thanks a lot for sharing your time. Please see my inline answers.  On 4/10/2018 3:28 PM, Guenter Roeck wrote:  No it isn't. Will drop the line.   Okay. I'll use bool instead of int.   The two above functions are slightly different but uses the same PECI  command which provides both Tthrottle and Tcontrol values in pkg_config  array so it updates the values to reduce duplicate PECI transactions.  Probably, combining these two functions into get_ttrottle_and_tcontrol()  would look better. I'll rewrite it.   Are you pointing out this code? /**   * Processors return a value of the core DTS reading in 10.6 format   * (10 bits signed decimal, 6 bits fractional).   * Error codes:   *   0x8000: General sensor error   *   0x8001: Reserved   *   0x8002: Underflow on reading value   *   0x8003-0x81ff: Reserved   */ if (core_dts_margin >= 0x8000 && core_dts_margin <= 0x81ff) 	return -EIO,  Then I'll rewrite it as a function. If not, please point out the  duplication.   Core temperature group will be registered only when it detects at least  one core checked by check_resolved_cores(), so find_core_index() can be  called only when priv->core_mask has a non-zero value. The 'nothing is  found' case will not happen.   cputemp_read_string() is mapped to read_string member of hwmon_ops  struct, so hwmon susbsystem passes the channel parameter based on the  registered channel order. Should I modify hwmon subsystem code?   As explained above, find_core index() returns a correct index always.   This is an attribute of DTS margin temperature which reflects thermal  margin to Tcontrol of the CPU package. If it shows '0' means it reached  to Tcontrol, the first level of thermal warning. If the CPU keeps  getting hot then this DTS margin shows a negative value until it reaches  to Tjmax. When the temperature reaches to Tjmax at last then it shows  the lower critcal value which lcrit indicates as the second level of  thermal warning.   Both Tjmax and Tcontrol have positive values and Tjmax is greater than  Tcontrol always. As explained above, lcrit of DTS margin should show a  negative value means the margin goes down across '0'. On the other hand,  crit_hyst of Die temperature should show absolute hyterisis value  between Tcontrol and Tjmax.   This driver provides multiple channels and each channel has its own  supplement attributes. As you mentioned, Die temperature channel and  Core temperature channel have their individual crit attributes and they  reflect the same value, Tjmax. It is not reporting several times but  reporting the same value.   Each function is called from cputemp_read() which is mapped to read  function pointer of hwmon_ops struct. Since each channel has different  set of attributes so the cputemp_read() calls an individual channel  handler after checking the channel type. Of course, we can handle all  attributes of all channels in a single function but the way also needs  channel type checking code on each attribute.   Sure, will use defines instead.   For proper operation of this driver, PECI_CMD_GET_TEMP and  PECI_CMD_RD_PKG_CFG have to be supported by a client CPU.  PECI_CMD_GET_TEMP is provided as a default command but  PECI_CMD_RD_PKG_CFG depends on PECI minor revision of a CPU package so  this checking is needed.   Got it. I'll remove the error message and will add a proper handling  code into PECI core.   This driver can't support core temperature monitoring if a CPU doesn't  support PECI_CMD_RD_PCI_CFG_LOCAL command. In that case, it skips core  temperature group creation and supports only basic temperature  monitoring of Die, DTS margin and etc. I'll add this description as a  comment.   Should I split out hwmon documents and dt bindings too?   No. Will drop the line.   It is temperature monitoring specific function and it touches module  specific variables. Do you really think that this non-generic function  should be moved to PECI core?   find_dimm_number()? I'm sure it isn't.   Sure, I'll rewrite it.   Okay. In case of check_cpu_id(), it could be used as a generic PECI  function. I'll move it into PECI core.   '&' has a precedence over '!=' but '|' doesn't. I'll rewrite it to:  	if (client->adapter->cmd_mask & 	    (BIT(PECI_CMD_GET_TEMP) | BIT(PECI_CMD_RD_PKG_CFG)) != 	    (BIT(PECI_CMD_GET_TEMP) | BIT(PECI_CMD_RD_PKG_CFG)))   Should I use -EPERM? Any suggestion?   Client address range validation will be done in  peci_check_addr_validity() in PECI core before probing a device driver.   I'll remove the error message and will add a proper handling code into  PECI core on each error type.   Yes, it would be safer. Will fix it.",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,3902,1.0,0.3148148148148148,0,0.07692307692307693,0.9230769230769231,0.0,0.0
257,245912,245924,"There is lots of other duplication. That doesn't guarantee a match. If what you are saying is correct there should always be a well defined match of channel -> idx, and the search should be unnecessary. Huh ? Changing,requires a hwmon core change ? Really ?The hwmon ABI reports chip values, not constants. Even though some drivers do it, reporting a constant is always wrong. The hwmon ABI requires reporting of absolute temperatures in milli-degrees C.Your statements make it very clear that this driver does not report absolute temperatures. This is not acceptable. Then maybe fold the functions accordingly ?I do not question the check. I question the error message and error return value. Why is it an _error_ if the CPU does not support the functionality, and why does it have to be reported in the kernel log ?  The message says ""Failed to ..."". It does not say ""This CPU does not support ..."".Why does this message display the device name twice ? Actually, that is wrong. You refer to address-of. Bit operations do have lower precedence that comparisons. I stand corrected. Is it an _error_ if the CPU does not support this functionality ?","On 04/11/2018 02:59 PM, Jae Hyun Yoo wrote:  There is lots of other duplication.  That doesn't guarantee a match. If what you are saying is correct there should always be a well defined match of channel -> idx, and the search should be unnecessary.   Huh ? Changing 	f(x) { y = x - const, } ... 	f(x),  to 	f(y) { } ... 	f(x - const),  requires a hwmon core change ? Really ?   The hwmon ABI reports chip values, not constants. Even though some drivers do it, reporting a constant is always wrong.  The hwmon ABI requires reporting of absolute temperatures in milli-degrees C. Your statements make it very clear that this driver does not report absolute temperatures. This is not acceptable.  Then maybe fold the functions accordingly ?   I do not question the check. I question the error message and error return value. Why is it an _error_ if the CPU does not support the functionality, and why does it have to be reported in the kernel log ?   The message says Failed to ..."". It does not say ""This CPU does not support ..."".   Why does this message display the device name twice ?   Actually, that is wrong. You refer to address-of. Bit operations do have lower precedence that comparisons. I stand corrected.   Is it an _error_ if the CPU does not support this functionality ? """,technical,Guenter Roeck,linux@roeck-us.net,1,0,1147,0.3118131868131868,0.3333333333333333,0,0.07692307692307693,0.9230769230769231,0.0,0.0
258,245912,245954,"Thanks for sharing your time. Please see my answers inline. Yes, it took a hidden review process between v2 and v3. I know it's an unusual process but it was requested. Hopefully, change logs in cover letter could roughly provide the details. Thanks for your comments. Agreed. I'll change the description. Okay then, better change it now than later. Will change all defines. It doesn't use all but better keep for bug fix or improvement use, I think. Yes, that would be better. I'll rewrite it.Yes, it could be simplified like you pointed out. Will change it.Got it. I'll replace it with print_hex_dump_debug() after removing the define. Intention was that make it run just amount up to the rx_len but it's not efficient. I'll rewrite it like you suggested. No specific reason. regmap makes some overhead as you mentioned but it also provides some advantages on access simplification, endianness handling and register dump at run time. I'd not insist using of regmap if you prefer using of raw readl and writel. Do you? You are right. I'll keep this checking only in _probe() function and remove all redundant error checking codes on memory mapped IO. This code makes changes on the status_ack variable to write back ack bit on each interrupt. Unlike other HW module, PECI uses the 24MHz external clock as its clock source. Should it use clk-aspeed.c in this case? Agreed. I'll make it print out the message only when this.I'll test it again and will remove it if it is not necessary. You are right. I'll remove the flag.","Hello Joel,  Thanks for sharing your time. Please see my answers inline.  On 4/11/2018 4:51 AM, Joel Stanley wrote:  Yes, it took a hidden review process between v2 and v3. I know it's an  unusual process but it was requested. Hopefully, change logs in cover  letter could roughly provide the details. Thanks for your comments.   Agreed. I'll change the description.   Okay then, better change it now than later. Will change all defines.   It doesn't use all but better keep for bug fix or improvement use, I think.   Yes, that would be better. I'll rewrite it.   Yes, it could be simplified like you pointed out. Will change it.   Got it. I'll replace it with print_hex_dump_debug() after removing the  define.   Intention was that make it run just amount up to the rx_len but it's not  efficient. I'll rewrite it like you suggested.   No specific reason. regmap makes some overhead as you mentioned but it  also provides some advantages on access simplification, endianness  handling and register dump at run time. I'd not insist using of regmap  if you prefer using of raw readl and writel. Do you?   You are right. I'll keep this checking only in _probe() function and  remove all redundant error checking codes on memory mapped IO.   This code makes changes on the status_ack variable to write back ack bit  on each interrupt.   Unlike other HW module, PECI uses the 24MHz external clock as its clock  source. Should it use clk-aspeed.c in this case?   Agreed. I'll make it print out the message only when ret == 0 and  msg_timing_nego > PECI_MSG_TIMING_NEGO_MAX.   I'll test it again and will remove it if it is not necessary.   You are right. I'll remove the flag.",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,1521,0.4340659340659341,0.35185185185185186,0,0.07692307692307693,0.9230769230769231,0.0,0.0
259,245912,245949,"Got it. Will capitalize all A speed words. According to the datasheet, PECI controller module is attached to apb but its clock source is the 24MHz external clock. Based on this setting, driver code makes clock divisor value to set operation clock of PECI controller which is adjustable. Will use msg-timing instead.","Hi Joel,  On 4/11/2018 4:52 AM, Joel Stanley wrote:  Got it. Will capitalize all Aspeed words.   According to the datasheet, PECI controller module is attached to apb  but its clock source is the 24MHz external clock.   Based on this setting, driver code makes clock divisor value to set  operation clock of PECI controller which is adjustable.   Will use msg-timing instead.",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,315,0.08104395604395605,0.3888888888888889,0,0.07692307692307693,0.9230769230769231,0.0,0.0
260,245912,245962,Do you mean that bindings and driver of ASPEED peci adapter driver including documents?,"On 4/11/2018 4:52 AM, Joel Stanley wrote:  Do you mean that bindings and driver of ASPEED peci adapter driver  including documents?  Thanks, -Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,87,0.020604395604395604,0.4074074074074074,0,0.07692307692307693,0.9230769230769231,0.0,0.0
261,245912,245925,"Sorry but can you point out the duplication? There could be some disabled cores in the resolved core mask bit sequence also it should remove indexing gap in channel numbering so itis the reason why this search function is needed. Well defined match of channel -> idx would not be always satisfied. Sorry for my misunderstanding. You are right. I'll change the parameter passing of find_core_index() from 'channel' to 'channel -DEFAULT_CHANNEL_NUMS'.Okay. I'll remove the 'DTS margin' temperature. All others are reporting absolute temperatures. I'll use a single function for 'Die temperature' and 'Core temperature' that have the same attributes set. It would simplify this code a bit. Got it. I'll change that to dev_dbg. Got it. Will correct the message. For an example, dev_name(hwmon_dev) shows 'hwmon5' and priv->name shows'peci-cputemp0'.Actually, it returns from this probe() function without making any hwmon info creation so I intended to handle this case as an error. Am I wrong?","On 4/11/2018 5:34 PM, Guenter Roeck wrote:  Sorry but can you point out the duplication?   There could be some disabled cores in the resolved core mask bit  sequence also it should remove indexing gap in channel numbering so it  is the reason why this search function is needed. Well defined match of  channel -> idx would not be always satisfied.   Sorry for my misunderstanding. You are right. I'll change the parameter  passing of find_core_index() from 'channel' to 'channel -  DEFAULT_CHANNEL_NUMS'.   Okay. I'll remove the 'DTS margin' temperature. All others are reporting  absolute temperatures.   I'll use a single function for 'Die temperature' and 'Core temperature'  that have the same attributes set. It would simplify this code a bit.   Got it. I'll change that to dev_dbg.   Got it. Will correct the message.   For an example, dev_name(hwmon_dev) shows 'hwmon5' and priv->name shows  'peci-cputemp0'.   Actually, it returns from this probe() function without making any hwmon  info creation so I intended to handle this case as an error. Am I wrong?",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,990,0.2623626373626374,0.42592592592592593,0,0.07692307692307693,0.9230769230769231,0.0,0.0
262,245912,245926,"write a python script to do a semantic comparison. Are you saying that each call to the function, with the same parameters, can return a different result ?And dev_dbg() shows another device name. So you'll have something like this. If the functionality or HW supported by the driver isn't available, it is customary to return -ENODEV and no error message. Otherwise the kernel log would drown in ""not supported"" error messages. I don't see where it would add any value to handle this driver differently. Invalid argument EPERM.Operation not permitted You'll have to work hard to convince me that any of those makes sense, and that ENODEV No such device doesn't. More specifically, if EINVAL makes sense, the caller did something wrong, meaning there is a problem in the infrastructure which should get fixed. The same is true for EPERM.","On 04/11/2018 07:51 PM, Jae Hyun Yoo wrote: write a python script to do a semantic comparison.  Are you saying that each call to the function, with the same parameters, can return a different result ?  And dev_dbg() shows another device name. So you'll have something like  peci-cputemp0: hwmon5: sensor 'peci-cputemp0'   If the functionality or HW supported by the driver isn't available, it is customary to return -ENODEV and no error message. Otherwise the kernel log would drown in not supported"" error messages. I don't see where it would add any value to handle this driver differently.  EINVAL	Invalid argument EPERM	Operation not permitted  You'll have to work hard to convince me that any of those makes sense, and that  ENODEV	No such device  doesn't. More specifically, if EINVAL makes sense, the caller did something wrong, meaning there is a problem in the infrastructure which should get fixed. The same is true for EPERM. """,technical,Guenter Roeck,linux@roeck-us.net,1,0,836,0.22802197802197802,0.4444444444444444,0,0.07692307692307693,0.9230769230769231,0.0,0.0
263,245912,245927,"Okay. I'll try to simplify this code again. No, the result will be consistent. After reading the priv->core_mask once in check_resolved_cores(), the value will not be changed. I'm saying about this case, for example if core number 2 is unresolved in total 4 cores, then the idx order will be '0, 1, 3' but channel order will be '5, 6, 7' without making any indexing gap. Practically it shows like where 0-30:00 is assigned by peci core. Now I fully understood what you pointed out. Thanks for the detailed explanation. I'll change the error return value to -ENODEV and will use dev_dbg for the message printing. Thanks!","On 4/11/2018 8:40 PM, Guenter Roeck wrote:  Okay. I'll try to simplify this code again.   No, the result will be consistent. After reading the priv->core_mask  once in check_resolved_cores(), the value will not be changed. I'm  saying about this case, for example if core number 2 is unresolved in  total 4 cores, then the idx order will be '0, 1, 3' but channel order  will be '5, 6, 7' without making any indexing gap.   Practically it shows like  peci-cputemp 0-30:00: hwmon10: sensor 'peci_cputemp.cpu0'  where 0-30:00 is assigned by peci core.   Now I fully understood what you pointed out. Thanks for the detailed  explanation. I'll change the error return value to -ENODEV and will use  dev_dbg for the message printing. Thanks!",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,619,0.18681318681318682,0.46296296296296297,0,0.07692307692307693,0.8461538461538461,0.0,0.0
264,245912,245928,"And you yet you claim that this is not well defined ? Or are you concerned about the amount of memory consumed by providing an array for the mapping ? Note that an indexing gap is acceptable and, in many cases, preferred. And what message would you see for cpu1 ?","On Thu, Apr 12, 2018 at 10:09:51AM -0700, Jae Hyun Yoo wrote: [ ... ]  And you yet you claim that this is not well defined ? Or are you concerned about the amount of memory consumed by providing an array for the mapping ?  Note that an indexing gap is acceptable and, in many cases, preferred.  [ ... ]   And what message would you see for cpu1 ?",technical,Guenter Roeck,linux@roeck-us.net,1,0,263,0.07417582417582418,0.48148148148148145,0,0.07692307692307693,0.8461538461538461,0.0,0.0
265,245912,245956,"dt-bindings: ... for the subject prefix please. This should be all one document. No need for these 2 here. Some details on the addressing for PECI would be good. This part of the example is not relevant. Just start with the adapter node. I don't understand what this has to do with PECI? ""simple-bus"" already has a defined meaning. Bindings are for h/w, not client drivers. How are PECI devices defined? 8 devices should be enough for anyone...Where is PECI_OFFSET_MAX defined? Not a valid node name. is what it probably should be. Fora new bus you can define unit-address format you like, but it must be based on the contents of reg. However, it doesn't look like you should create anything special here.","On Tue, Apr 10, 2018 at 11:32:03AM -0700, Jae Hyun Yoo wrote:  dt-bindings: ..."" for the subject prefix please.   This should be all one document.   No need for these 2 here.   Some details on the addressing for PECI would be good.   This part of the example is not relevant. Just start with the adapter  node.   I don't understand what this has to do with PECI? ""simple-bus"" already  has a defined meaning.   Bindings are for h/w, not client drivers.  How are PECI devices defined?   8 devices should be enough for anyone...  Where is PECI_OFFSET_MAX defined?   Not a valid node name. ""function@30"" is what it probably should be. For  a new bus you can define unit-address format you like, but it must be  based on the contents of reg. However, it doesn't look like you should  create anything special here. """,technical,Rob Herring,robh@kernel.org,1,0,705,0.2032967032967033,0.5185185185185185,0,0.38461538461538464,0.5384615384615384,0.23076923076923078,0.0
266,245912,245941,"This is the frequency of the bus or used to derive it? It would be better to specify the bus frequency instead and have the driver calculate its internal freq. And then use ""bus-frequency"" instead/_/-/All these either need vendor prefixes or should be standard properties for PECI adapters. I think probably the latter case. If so, the first2 should probably be in units of clocks (not 4 clocks). And they should then be documented in the common PECI binding doc.No need to show this part in examples.","On Tue, Apr 10, 2018 at 11:32:06AM -0700, Jae Hyun Yoo wrote:  This is the frequency of the bus or used to derive it? It would be  better to specify the bus frequency instead and have the driver  calculate its internal freq. And then use bus-frequency"" instead.   s/_/-/   All these either need vendor prefixes or should be standard properties  for PECI adapters. I think probably the latter case. If so, the first  2 should probably be in units of clocks (not 4 clocks). And they should  then be documented in the common PECI binding doc.   No need to show this part in examples. """,technical,Rob Herring,robh@kernel.org,1,0,501,0.1346153846153846,0.5370370370370371,0,0.38461538461538464,0.5384615384615384,0.0,0.0
267,245912,245931,"dt-bindings: hwmon: ... for the subject. Again, where is PECI_OFFSET_MAX defined? It can't depend on something in the kernel. Unit-address is wrong. It is a different bus from cputemp? Otherwise, you have conflicting addresses. If that's the case, probably should make it clear by showing different host adapters for each example.","On Tue, Apr 10, 2018 at 11:32:09AM -0700, Jae Hyun Yoo wrote:  dt-bindings: hwmon: ..."" for the subject.   Again, where is PECI_OFFSET_MAX defined? It can't depend on something in  the kernel.   unit-address is wrong.  It is a different bus from cputemp? Otherwise, you have conflicting  addresses. If that's the case, probably should make it clear by showing  different host adapters for each example. """,technical,Rob Herring,robh@kernel.org,1,0,330,0.08928571428571429,0.5555555555555556,0,0.38461538461538464,0.5384615384615384,0.0,0.0
268,245912,245957,"Thanks for sharing your time. Please see my answers inline. Sure, I'll change the subject. Okay. I'll combine them into one document. Will drop these 2.It is for the PECI client address. Will add details. Will remove that part. Thanks! Maybe I'm wrong but I intended to show this node is an umbrella node of a PECI bus subsystem. What should I use then? Got it. I'll correct the description. PECI client device is Intel CPU which is connected through a PECI bus.PECI_OFFSET_MAX is defined in include/linux/peci.h based on the maximum CPU numbers of the current IA generation. I'll remove the unnecessary details. A setting out of range would be handled accordingly in kernel. Got it. I'll fix these node name like function@30 and function@31.Thanks a lot for your comments!","Hi Rob,  Thanks for sharing your time. Please see my answers inline.  On 4/16/2018 10:59 AM, Rob Herring wrote:  Sure, I'll change the subject.   Okay. I'll combine them into one document.   Will drop these 2.   It is for the PECI client address. Will add details.   Will remove that part. Thanks!   Maybe I'm wrong but I intended to show this node is an umbrella node of  a PECI bus subsystem. What should I use then?   Got it. I'll correct the description. PECI client device is Intel CPU  which is connected through a PECI bus.   PECI_OFFSET_MAX is defined in include/linux/peci.h based on the maximum  CPU numbers of the current IA generation. I'll remove the unnecessary  details. A setting out of range would be handled accordingly in kernel.   Got it. I'll fix these node name like function@30 and function@31.  Thanks a lot for your comments!  -Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,773,0.2184065934065934,0.5740740740740741,0,0.46153846153846156,0.5384615384615384,0.0,0.0
269,245912,245942,"I agree with you. Actually, it is being used for operation frequency setting of PECI controller module in SoC so it's different from the meaning of ""bus-frequency"". I'll change it to ""operation-frequency"". Will fix it.So far I've checked that these are ASPEED PECI controller specific properties so it should be listed in here. Got it. Will drop the part.","On 4/16/2018 11:10 AM, Rob Herring wrote:  I agree with you. Actually, it is being used for operation frequency  setting of PECI controller module in SoC so it's different from the  meaning of bus-frequency"". I'll change it to ""operation-frequency"".   Will fix it.   So far I've checked that these are ASPEED PECI controller specific  properties so it should be listed in here.   Got it. Will drop the part. """,technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,355,0.10027472527472528,0.5925925925925926,0,0.46153846153846156,0.5384615384615384,0.0,0.0
270,245912,245932,"I'll change the subject. I'll remove the unnecessary description. Will fix it using the reg value. It could be the same bus with cputemp. Also, client address sharing is possible by PECI core if the functionality is different. I mean, cputemp and dimmtemp targeting the same client is possible case like this.peci-cputemp@30peci-dimmtemp@30","On 4/16/2018 11:14 AM, Rob Herring wrote:  I'll change the subject.   I'll remove the unnecessary description.   Will fix it using the reg value.   It could be the same bus with cputemp. Also, client address sharing is  possible by PECI core if the functionality is different. I mean, cputemp  and dimmtemp targeting the same client is possible case like this. peci-cputemp@30 peci-dimmtemp@30",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,340,0.08928571428571429,0.6111111111111112,0,0.46153846153846156,0.5384615384615384,0.0,0.0
271,245912,245933,"Oh, I got your point. Probably, I should change these separate settings into one like peci-client@30,Then cputemp and dimmtemp drivers could refer the same compatible string. Will rewrite it.","On 4/16/2018 4:22 PM, Jae Hyun Yoo wrote:  Oh, I got your point. Probably, I should change these separate settings  into one like  peci-client@30 {      compatible = intel,peci-client"",      reg = <0x30>, },  Then cputemp and dimmtemp drivers could refer the same compatible  string. Will rewrite it. """,technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,191,0.0521978021978022,0.6296296296296297,0,0.46153846153846156,0.5384615384615384,0.0,0.0
272,245912,245943,"No, now you've gone from a standard property name to something custom. Why do you need to set the frequency in DT if it is not related to the interface frequency? Rob","On Mon, Apr 16, 2018 at 6:12 PM, Jae Hyun Yoo <jae.hyun.yoo@linux.intel.com> wrote:  [...]   No, now you've gone from a standard property name to something custom. Why do you need to set the frequency in DT if it is not related to the interface frequency?  Rob",technical,Rob Herring,robh@kernel.org,1,0,166,0.04945054945054945,0.6481481481481481,0,0.46153846153846156,0.5384615384615384,0.0,0.0
273,245912,245951,"Just a drive-by nit:FWIW, <linux/bitfield.h> already provides functionality like this, so it might be worth taking a look at FIELD_{GET,PREP}() to save all these local definitions.","Just a drive-by nit:  On 10/04/18 19:32, Jae Hyun Yoo wrote: [...]  FWIW, <linux/bitfield.h> already provides functionality like this, so it  might be worth taking a look at FIELD_{GET,PREP}() to save all these  local definitions.  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,180,0.054945054945054944,0.6666666666666666,0,0.46153846153846156,0.5384615384615384,0.0,0.0
274,245912,245944,"Actually, the interface frequency is affected by the operation frequency but there is no description of its relationship in datasheet. I'll check again about the detail to ASPEED chip vendor and will use 'bus-frequency' if available.","On 4/17/2018 6:16 AM, Rob Herring wrote:  Actually, the interface frequency is affected by the operation frequency but there is no description of its relationship in datasheet. I'll check again about the detail to ASPEED chip vendor and will use 'bus-frequency' if available.  Thanks,  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,233,0.05631868131868132,0.6851851851851852,0,0.46153846153846156,0.46153846153846156,0.0,0.0
275,245912,245952,"Yes, that looks better. Thanks a lot for your pointing it out.","Hi Robin,  On 4/17/2018 6:37 AM, Robin Murphy wrote:  Yes, that looks better. Thanks a lot for your pointing it out.  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,62,0.020604395604395604,0.7037037037037037,0,0.46153846153846156,0.46153846153846156,0.0,0.0
276,245912,245934,"I've checked it again and realized that it should use function based node name like:peci-cputemp@30peci-dimmtemp@30If it use the same string like 'peci-client@30', the drivers cannot be selectively enabled. The client address sharing way is well handled in PECI core and this way would be better for the future implementations of other PECI functional drivers such as crash dump driver and so on. So I'm going change the unit-address only.","On 4/16/2018 4:51 PM, Jae Hyun Yoo wrote:  [...]   I've checked it again and realized that it should use function based  node name like:  peci-cputemp@30 peci-dimmtemp@30  If it use the same string like 'peci-client@30', the drivers cannot be  selectively enabled. The client address sharing way is well handled in  PECI core and this way would be better for the future implementations of  other PECI functional drivers such as crash dump driver and so on. So  I'm going change the unit-address only.  Thanks,  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,439,0.11675824175824176,0.7222222222222222,0,0.5384615384615384,0.46153846153846156,0.0,0.0
277,245912,245945,"I investigated it more deeply. Basically, by the spec, PECI bus speed cannot be set as a fixed speed. A PECI bus can have a wide speed range from 2Kbps to 2Mbps which is dynamically set by a handshaking sequence between an originator and clients called 'timing negotiation' in spec. This timing negotiation behavior happens on every single transaction so the bus speed also can vary on every transactions. So I'm thinking a custom property name for it, 'peci-clk-frequency' if it is acceptable.","On 4/17/2018 11:16 AM, Jae Hyun Yoo wrote:  I investigated it more deeply. Basically, by the spec, PECI bus speed cannot be set as a fixed speed. A PECI bus can have a wide speed range from 2Kbps to 2Mbps which is dynamically set by a handshaking sequence between an originator and clients called 'timing negotiation' in spec. This timing negotiation behavior happens on every single transaction so  the bus speed also can vary on every transactions. So I'm thinking a  custom property name for it, 'peci-clk-frequency' if it is acceptable.  Thanks,  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,494,0.1304945054945055,0.7407407407407407,0,0.5384615384615384,0.46153846153846156,0.0,0.0
278,245912,245946,"Okay, seems bus-frequency is not appropriate here. So use 'clock-frequency' (note the '-' not '_' as that is the standard property).","On Tue, Apr 17, 2018 at 5:06 PM, Jae Hyun Yoo <jae.hyun.yoo@linux.intel.com> wrote:  Okay, seems bus-frequency is not appropriate here. So use 'clock-frequency' (note the '-' not '_' as that is the standard property).  Rob",technical,Rob Herring,robh@kernel.org,1,0,132,0.04120879120879121,0.7592592592592593,0,0.5384615384615384,0.46153846153846156,0.0,0.0
279,245912,245935,"2 nodes at the same address is wrong (and soon dtc will warn you on this). You have 2 potential options. The first is you need additional address information in the DT if these are in fact 2 independent devices. This could be something like a function number to use something from PCI addressing. From what I found on PECI, it doesn't seem to have anything like that. The 2nd option is you have a single DT node which registers multiple hwmon devices. DT nodes and driver don't have to be 1-1. Don't design your DT nodes from how you want to partition drivers in some OS.","On Tue, Apr 17, 2018 at 3:40 PM, Jae Hyun Yoo <jae.hyun.yoo@linux.intel.com> wrote:  2 nodes at the same address is wrong (and soon dtc will warn you on this). You have 2 potential options. The first is you need additional address information in the DT if these are in fact 2 independent devices. This could be something like a function number to use something from PCI addressing. From what I found on PECI, it doesn't seem to have anything like that. The 2nd option is you have a single DT node which registers multiple hwmon devices. DT nodes and drivers don't have to be 1-1. Don't design your DT nodes from how you want to partition drivers in some OS.  Rob",technical,Rob Herring,robh@kernel.org,1,0,571,0.1662087912087912,0.7777777777777778,0,0.5384615384615384,0.46153846153846156,0.0,0.0
280,245912,245947,Thanks! I'll use 'clock-frequency' for it.,"On 4/18/2018 6:59 AM, Rob Herring wrote:  Thanks! I'll use 'clock-frequency' for it.  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,42,0.013736263736263736,0.7962962962962963,0,0.5384615384615384,0.38461538461538464,0.0,0.0
281,245912,245936,"Please correct me if I'm wrong but I'm still thinking that it is possible. Also, I did compile it but dtc doesn't make a warning. Let me show an another use case which is similar to this case:]This is device tree setting for LPC interface and its child nodes.LPC interface can be used as a multi-functional interface such as snoop 80, KCS, SIO and so on. In this use case are sharing their address range from their individual driver modules and they can be registered quite well through both static dt or dynamic dtoverlay. PECI is also a multi-functional interface which is similar to the above case, I think.","On 4/18/2018 7:32 AM, Rob Herring wrote:  Please correct me if I'm wrong but I'm still thinking that it is possible. Also, I did compile it but dtc doesn't make a warning. Let me show an another use case which is similar to this case:  In arch/arm/boot/dts/aspeed-g5.dtsi [...] lpc_host: lpc-host@80 {          compatible = aspeed,ast2500-lpc-host"", ""simple-mfd"", ""syscon"",          reg = <0x80 0x1e0>,          reg-io-width = <4>,           #address-cells = <1>,          #size-cells = <1>,          ranges = <0x0 0x80 0x1e0>,           lpc_ctrl: lpc-ctrl@0 {                  compatible = ""aspeed,ast2500-lpc-ctrl"",                  reg = <0x0 0x80>,                  clocks = <&syscon ASPEED_CLK_GATE_LCLK>,                  status = ""disabled"",          },           lpc_snoop: lpc-snoop@0 {                  compatible = ""aspeed,ast2500-lpc-snoop"",                  reg = <0x0 0x80>,                  interrupts = <8>,                  status = ""disabled"",          }, } [...]  This is device tree setting for LPC interface and its child nodes. LPC interface can be used as a multi-functional interface such as snoop 80, KCS, SIO and so on. In this use case, lpc-ctrl@0 and lpc-snoop@0 are sharing their address range from their individual driver modules and they can be registered quite well through both static dt or dynamic dtoverlay. PECI is also a multi-functional interface which is similar to the above case, I think.  Thanks,  Jae""",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,610,0.17032967032967034,0.8148148148148148,0,0.6153846153846154,0.38461538461538464,0.0,0.0
282,245912,245937,"I did say *soon*. It's in dtc repo, but not the kernel copy yet. This case too is poor design and should be fixed as well. Simply put, you can have 2 devices on a bus at the same address without some sort of mux or arbitration device in the middle. If you have a device/block with multiple functions provided to the OS, then it is the OS's problem to arbitrate access. It is not a DT problem because OS's can vary in how they handle that both from OS to OS and over time.","On Wed, Apr 18, 2018 at 3:28 PM, Jae Hyun Yoo <jae.hyun.yoo@linux.intel.com> wrote:  I did say *soon*. It's in dtc repo, but not the kernel copy yet.   This case too is poor design and should be fixed as well. Simply put, you can have 2 devices on a bus at the same address without some sort of mux or arbitration device in the middle. If you have a device/block with multiple functions provided to the OS, then it is the OS's problem to arbitrate access. It is not a DT problem because OS's can vary in how they handle that both from OS to OS and over time.  Rob",technical,Rob Herring,robh@kernel.org,1,0,471,0.14972527472527472,0.8333333333333334,0,0.6153846153846154,0.38461538461538464,0.0,0.0
283,245912,245938,"If I change it to a single DT node which registers 2 hwmon devices using the 2nd option above, then I still have 2 devices on a bus at the same address. Does it also make a problem to the OS then?","On 4/18/2018 2:28 PM, Rob Herring wrote:  If I change it to a single DT node which registers 2 hwmon devices using the 2nd option above, then I still have 2 devices on a bus at the same address. Does it also make a problem to the OS then?  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,196,0.061813186813186816,0.8518518518518519,0,0.6153846153846154,0.38461538461538464,0.0,0.0
284,245912,245918,"Thank you for the patch! Yet something to improve:[auto build test ERROR on linus/master][also build test ERROR on v4.17-rc7][cannot apply to next-20180530][if your patch is applied to the wrong git tree, please drop us a note to help improve the system]","Hi Jae,  Thank you for the patch! Yet something to improve:  [auto build test ERROR on linus/master] [also build test ERROR on v4.17-rc1 next-20180419] [if your patch is applied to the wrong git tree, please drop us a note to help improve the system]  url:    https://github.com/0day-ci/linux/commits/Jae-Hyun-Yoo/PECI-device-driver-introduction/20180411-180018 config: x86_64-randconfig-s0-04192349 (attached as .config) compiler: gcc-6 (Debian 6.4.0-9) 6.4.0 20171026 reproduce:         # save the attached .config to linux build tree         make ARCH=x86_64   All errors (new ones prefixed by >>):     drivers//peci/peci-core.c: In function 'peci_device_match':      if (peci_of_match_device(drv->of_match_table, client))          ^~~~~~~~~~~~~~~~~~~~    At top level:    drivers//peci/peci-core.c:840:28: warning: 'peci_new_device' defined but not used [-Wunused-function]     static struct peci_client *peci_new_device(struct peci_adapter *adapter,                                ^~~~~~~~~~~~~~~    drivers//peci/peci-core.c:135:29: warning: 'peci_verify_adapter' defined but not used [-Wunused-function]     static struct peci_adapter *peci_verify_adapter(struct device *dev)                                 ^~~~~~~~~~~~~~~~~~~    cc1: some warnings being treated as errors  vim +/peci_of_match_device +739 drivers//peci/peci-core.c     732	    733	static int peci_device_match(struct device *dev, struct device_driver *drv)    734	{    735		struct peci_client *client = peci_verify_client(dev),    736		struct peci_driver *driver,    737	    738		/* Attempt an OF style match */  > 739		if (peci_of_match_device(drv->of_match_table, client))    740			return 1,    741	    742		driver = to_peci_driver(drv),    743	    744		if (peci_match_id(driver->id_table, client))    745			return 1,    746	    747		return 0,    748	}    749	  --- 0-DAY kernel test infrastructure                Open Source Technology Center https://lists.01.org/pipermail/kbuild-all                   Intel Corporation",technical,kbuild test robot,lkp@intel.com,0,0,254,0.0782967032967033,0.8703703703703703,0,0.6923076923076923,0.3076923076923077,0.0,0.0
285,245912,245939,"Additionally, I need to explain that there is one and only bus host(adapter) and multiple clients on a PECI bus, and PECI spec doesn't allow multiple originators so only the host device can originate message. In this implementation, all message transactions on a bus from client driver modules and user space will be serialized well in the PECI core bus driver so bus occupation and traffic arbitration will be managed well in the PECI core bus driver even in case of a bus has 2client drivers at the same address. I'm sure that this implementation doesn't make that kind of problem to OS.","On 4/18/2018 2:57 PM, Jae Hyun Yoo wrote:  Additionally, I need to explain that there is one and only bus host (adapter) and multiple clients on a PECI bus, and PECI spec doesn't allow multiple originators so only the host device can originate message. In this implementation, all message transactions on a bus from client driver modules and user space will be serialized well in the PECI core bus driver so bus occupation and traffic arbitration will be managed well in the PECI core bus driver even in case of a bus has 2  client drivers at the same address. I'm sure that this implementation  doesn't make that kind of problem to OS.  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,589,0.15796703296703296,0.8888888888888888,0,0.6923076923076923,0.3076923076923077,0.0,0.23076923076923078
286,245912,245917,"Thanks a lot for your review. I think, it should contain actual device resource release code which is being done, or a coupling logic should be added between them.As you suggested, I'll check it again after reading documentation and understanding core.c code more deeply.","On 4/23/2018 3:52 AM, Greg KH wrote:  Hi Greg,  Thanks a lot for your review.  I think, it should contain actual device resource release code which is being done by peci_del_adapter(), or a coupling logic should be added between peci_adapter_dev_release() and peci_del_adapter().  As you suggested, I'll check it again after reading documentation and understanding core.c code more deeply.  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,271,0.06868131868131869,0.9259259259259259,0,0.9230769230769231,0.0,0.0,0.0
287,245912,245920,"Does it make sense one driver per patch? Are we talking about x86 CPU IDs here? If so, why x86 corresponding headers, including intel-family.h are not used?","On Tue, 2018-04-10 at 11:32 -0700, Jae Hyun Yoo wrote:   Does it make sense one driver per patch?     Are we talking about x86 CPU IDs here? If so, why x86 corresponding headers, including intel-family.h are not used?  --  Andy Shevchenko <andriy.shevchenko@linux.intel.com> Intel Finland Oy",technical,Andy Shevchenko,andriy.shevchenko@linux.intel.com,1,0,156,0.04395604395604396,0.9444444444444444,0,1.0,0.0,0.0,0.0
288,245912,245914,"All comments you got for patch 6 are applicable here. And perhaps in the rest of the series. The rule of thumb: when you get even single comment in a certain place,re-check _entire_ series for the same / similar patterns!","On Tue, 2018-04-10 at 11:32 -0700, Jae Hyun Yoo wrote:  All comments you got for patch 6 are applicable here.  And perhaps in the rest of the series.  The rule of thumb: when you get even single comment in a certain place, re-check _entire_ series for the same / similar patterns!  --  Andy Shevchenko <andriy.shevchenko@linux.intel.com> Intel Finland Oy",technical,Andy Shevchenko,andriy.shevchenko@linux.intel.com,1,0,221,0.06318681318681318,0.9629629629629629,0,1.0,0.0,0.0,0.0
289,245912,245921,"Thanks a lot for your review. Please check my inline answers. Yes, I'll separate it into two patches. Yes, that would make more sense. I'll include the intel-family.h and will use these defines instead.","Hi Andy,  Thanks a lot for your review. Please check my inline answers.  On 4/24/2018 8:56 AM, Andy Shevchenko wrote:  Yes, I'll separate it into two patches.   Yes, that would make more sense. I'll include the intel-family.h and  will use these defines instead: INTEL_FAM6_HASWELL_X INTEL_FAM6_BROADWELL_X INTEL_FAM6_SKYLAKE_X  Thanks,  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,202,0.059065934065934064,0.9814814814814815,0,1.0,0.0,0.0,0.0
290,245912,245915,Thanks for your advice. I'll keep that in mind.,"On 4/24/2018 9:01 AM, Andy Shevchenko wrote:  Thanks for your advice. I'll keep that in mind.  Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,47,0.016483516483516484,1.0,1,1.0,0.0,0.0,0.0
291,245912,245959,Thanks a lot for letting me know that. I'll do as you suggested.,"Hi Joel,  On 4/11/2018 4:52 AM, Joel Stanley wrote:  Thanks a lot for letting me know that. I'll do as you suggested.  -Jae",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,64,0.02197802197802198,0.37037037037037035,0,0.07692307692307693,0.9230769230769231,0.0,0.0
292,245912,245929,"If the indexing gap is acceptable, the index search function isn't needed anymore. I'll fix all relating code to make that use direct mapping of channel -> idx then. Thanks! It shows this.","On 4/12/2018 10:37 AM, Guenter Roeck wrote:  If the indexing gap is acceptable, the index search function isn't  needed anymore. I'll fix all relating code to make that use direct  mapping of channel -> idx then. Thanks!   It shows like  peci-cputemp 0-31:00: hwmon10: sensor 'peci_cputemp.cpu1'",technical,Jae Hyun Yoo,jae.hyun.yoo@linux.intel.com,0,1,188,0.05631868131868132,0.5,0,0.15384615384615385,0.8461538461538461,0.0,0.23076923076923078
293,254920,262733,"Hello, so I finally got to this :) I know some people (Matthew Wilcox?) wanted to do something like KSM for file pages - not all virtualization schemes use overlay fs and e.g. if you use reflinks (essentially shared on-disk extents among files) for your container setup, you could save significant amounts of memory with the ability to share pages in page cache among files that are reflinked.It is interesting that you can get rid of page->mapping uses in most places. For page reclaim (vmscan) you'll still need a way to get from a page to an address_space so that you can reclaim the page so you can hardly get rid of page->mapping completely but you're right that with such limited use that transition could be more complex / expensive. What I wonder though is what is the cost of this (in the terms of code size and speed) - propagating the mapping down the stack costs something... Also in terms of maintainability, code readability suffers a bit. This could be helped though. In some cases it seems we just use the mapping because it was easily available but could get away without it. In other case (e.g. lot of fs/buffer.c) we could make bh -> mapping transition easy by storing the mapping in the struct buffer_head - possibly it could replace b_bdev pointer as we could get to that from the mapping with a bit of magic and pointer chasing and accessing b_bdev is not very performance critical. OTOH such optimizations make a rather complex patches from mostly mechanical replacement so I can see why you didn't go that route. Overall I think you'd need to make a good benchmarking comparison showing how much this helps some real workloads (your motivation) and also how other loads on lower end machines are affected.So I'm interested in this write protection mechanism but I didn't find much about it in the series. How does it work? I can see KSM write protects pages in page tables so that works for userspace mappings but what about in-kernel users modifying pages - e.g. pages in page cache carrying filesystem metadata do get modified a lot like this. AFAIK you're right. Auch, the fact that we could share a page as data storage for several inode+offset combinations that are not sharing underlying storage just looks viciously twisted ,) But is it really that useful to warrant complications? In particular I'm afraid that filesystems expect consistency between their internal state (attached to page->private) and page state(e.g. page->flags) and when there are multiple internal states attached to the same page this could go easily wrong..","Hello,  so I finally got to this :)  On Wed 04-04-18 15:17:50, jglisse@redhat.com wrote:  I know some people (Matthew Wilcox?) wanted to do something like KSM for file pages - not all virtualization schemes use overlayfs and e.g. if you use reflinks (essentially shared on-disk extents among files) for your container setup, you could save significant amounts of memory with the ability to share pages in page cache among files that are reflinked.   It is interesting that you can get rid of page->mapping uses in most places. For page reclaim (vmscan) you'll still need a way to get from a page to an address_space so that you can reclaim the page so you can hardly get rid of page->mapping completely but you're right that with such limited use that transition could be more complex / expensive.  What I wonder though is what is the cost of this (in the terms of code size and speed) - propagating the mapping down the stack costs something... Also in terms of maintainability, code readability suffers a bit.  This could be helped though. In some cases it seems we just use the mapping because it was easily available but could get away without it. In other case (e.g. lot of fs/buffer.c) we could make bh -> mapping transition easy by storing the mapping in the struct buffer_head - possibly it could replace b_bdev pointer as we could get to that from the mapping with a bit of magic and pointer chasing and accessing b_bdev is not very performance critical. OTOH such optimizations make a rather complex patches from mostly mechanical replacement so I can see why you didn't go that route.  Overall I think you'd need to make a good benchmarking comparison showing how much this helps some real workloads (your motivation) and also how other loads on lower end machines are affected.   So I'm interested in this write protection mechanism but I didn't find much about it in the series. How does it work? I can see KSM writeprotects pages in page tables so that works for userspace mappings but what about in-kernel users modifying pages - e.g. pages in page cache carrying filesystem metadata do get modified a lot like this.   AFAIK you're right.   Auch, the fact that we could share a page as data storage for several inode+offset combinations that are not sharing underlying storage just looks viciously twisted ,) But is it really that useful to warrant complications? In particular I'm afraid that filesystems expect consistency between their internal state (attached to page->private) and page state (e.g. page->flags) and when there are multiple internal states attached to the same page this could go easily wrong...  								Honza --  Jan Kara <jack@suse.com> SUSE Labs, CR",technical,Jan Kara,jack@suse.cz,1,0,2562,0.8228476821192053,0.86,0,0.8125,0.125,0.8125,0.0
294,254920,262825,"Yes i believe they are still use case where KSM with file back page make senses, i am just not familiar enough with those workload to know how big of a deal this is.Idea for vmscan is that you either have regular mapping pointer store inpage->mapping or you have a pointer to special struct which has a function pointer to a reclaim/walker function (rmap_walk_ksm)I haven't checked that, i will, i was not so concern because in the vast majority of places there is already struct address_space on the stackframe (i.e. local variable in function being call) so moving it to function argument shouldn't impact that. However as i expect this will be merge over multiple kernel release cycle and the intermediary step will see an increase in stack size. The code size should only grow marginally i expect. I will provide numbers with my next posting after LSF/MM.I am willing to do the buffer_head change, i remember considering it but I don't remember why not doing it (i failed to take note of that).Do you have any specific benchmark you would like to see ? My list was:  For workload i care this will be CUDA workload. We are still working on the OpenCL open source stack but i don't expect we will have something that can shows the same performance improvement with OpenCL as soon as with CUDA.So i only care about page which are mmaped into a process address space. At first i only want to intercept CPU write access through mmap of file but i also intend to extend write syscall to also ""fault"" on the write protection i.e. it will call a callback to unprotect the page allowing the write protector to take proper action while write syscall is happening. I am afraid truly generic write protection for metadata pages is bit out of scope of what i am doing. However the mechanism i am proposing can be extended for that too. Issue is that all place that want to write to those page need to be converted to something where write happens between write_begin and write_end section (mmap and CPU pte does give this implicitly through page fault, so does write syscall). Basically there is a need to make sure that write and write protection can be ordered against one another without complex locking. So at first i want to limit to write protect (not KSM) thus page->flags will stay consistent (i.e. page is only ever associated with a single mapping). For KSM yes the page->flags can be problematic, however here we can assume that page is clean (and up to date) and not under writeback. So problematic flags for KSM. Idea again would be to PageFlagsWithMapping(page, mapping) so that for non KSM write protected page you test the usual page->flags and for write protected page you find the flag value using mapping as lookup index. Usually those flag are seldomly changed/accessed. Again the overhead (ignoring code size) would only be for page which are KSM. So maybe KSM will not make sense because perf overhead it has with page->flags access (i don't think so but i haven't tested this).Thank you for taking time to read over all this.","On Wed, Apr 18, 2018 at 04:13:37PM +0200, Jan Kara wrote:  [...]   Yes i believe they are still use case where KSM with file back page make senses, i am just not familiar enough with those workload to know how big of a deal this is.   Idea for vmscan is that you either have regular mapping pointer store in page->mapping or you have a pointer to special struct which has a function pointer to a reclaim/walker function (rmap_walk_ksm)   I haven't checked that, i will, i was not so concern because in the vast majority of places there is already struct address_space on the stack frame (ie local variable in function being call) so moving it to function argument shouldn't impact that. However as i expect this will be merge over multiple kernel release cycle and the intermediary step will see an increase in stack size. The code size should only grow marginaly i expect. I will provide numbers with my next posting after LSF/MM.    I am willing to do the buffer_head change, i remember considering it but i don't remember why not doing it (i failed to take note of that).    Do you have any specific benchmark you would like to see ? My list was:   https://github.com/01org/lkp-tests   https://github.com/gormanm/mmtests   https://github.com/akopytov/sysbench/   http://git.infradead.org/users/dhowells/unionmount-testsuite.git  For workload i care this will be CUDA workload. We are still working on the OpenCL open source stack but i don't expect we will have someting that can shows the same performance improvement with OpenCL as soon as with CUDA.   So i only care about page which are mmaped into a process address space. At first i only want to intercept CPU write access through mmap of file but i also intend to extend write syscall to also fault"" on the write protection ie it will call a callback to unprotect the page allowing the write protector to take proper action while write syscall is happening.  I am affraid truely generic write protection for metadata pages is bit out of scope of what i am doing. However the mechanism i am proposing can be extended for that too. Issue is that all place that want to write to those page need to be converted to something where write happens between write_begin and write_end section (mmap and CPU pte does give this implicitly through page fault, so does write syscall). Basicly there is a need to make sure that write and write protection can be ordered against one another without complex locking.    So at first i want to limit to write protect (not KSM) thus page->flags will stay consistent (ie page is only ever associated with a single mapping). For KSM yes the page->flags can be problematic, however here we can assume that page is clean (and uptodate) and not under write back. So problematic flags for KSM:   - private (page_has_buffers() or PagePrivate (nfs, metadata, ...))   - private_2 (FsCache)   - mappedtodisk   - swapcache   - error  Idea again would be to PageFlagsWithMapping(page, mapping) so that for non KSM write protected page you test the usual page->flags and for write protected page you find the flag value using mapping as lookup index. Usualy those flag are seldomly changed/accessed. Again the overhead (ignoring code size) would only be for page which are KSM. So maybe KSM will not make sense because perf overhead it has with page->flags access (i don't think so but i haven't tested this).   Thank you for taking time to read over all this.  Cheers, Jrme""",technical,Jerome Glisse,jglisse@redhat.com,1,0,3039,1.0,0.88,0,0.8125,0.125,0.0,0.0
295,254920,262844,"Imagine container farms where they deploy the base os image via cp --reflink.This would be a huge win for btrfs/xfs but we've all been too terrified of the memory manager to try it. :)For those following at home, we had a track at LSFMM2017 (and hallwaybofs about this in previous years): starts to look at this big series, having sent his own yesterday. :)","On Wed, Apr 18, 2018 at 11:54:30AM -0400, Jerome Glisse wrote:  Imagine container farms where they deploy the base os image via cp --reflink. This would be a huge win for btrfs/xfs but we've all been too terrified of the memory manager to try it. :)  For those following at home, we had a track at LSFMM2017 (and hallway bofs about this in previous years): https://lwn.net/Articles/717950/  /me starts to look at this big series, having sent his own yesterday. :)  --D",technical,Darrick J. Wong,darrick.wong@oracle.com,1,0,357,0.12582781456953643,0.9,0,0.8125,0.125,0.0,0.0
296,254920,263176,"So e.g. mmtests have a *lot* of different tests so it's probably not realistic for you to run them all. I'd look at bonnie++ (file & dir tests),dbench, reaim - these are crappy IO benchmarks because they mostly fit into page cache but for your purposes this is exactly what you want to see differences in CPU overhead :).I understand metadata pages are not interesting for your use case. However from mm point of view these are page cache pages as any other. So maybe my question should have been: How do we make sure this mechanism will not be used for pages for which it cannot work? Yeah, sure, page->flags could be dealt with in a similar way but at this point I don't think it's worth it. And without page->flags I don't think abstracting page->private makes much sense - or am I missing something why you need page->private depend on the mapping? So what I wanted to suggests that we leave page as is currently and just concentrate on it.","On Wed 18-04-18 11:54:30, Jerome Glisse wrote:  So e.g. mmtests have a *lot* of different tests so it's probably not realistic for you to run them all. I'd look at bonnie++ (file & dir tests), dbench, reaim - these are crappy IO benchmarks because they mostly fit into page cache but for your purposes this is exactly what you want to see differences in CPU overhead :).   I understand metadata pages are not interesting for your use case. However from mm point of view these are page cache pages as any other. So maybe my question should have been: How do we make sure this mechanism will not be used for pages for which it cannot work?   Yeah, sure, page->flags could be dealt with in a similar way but at this point I don't think it's worth it. And without page->flags I don't think abstracting page->private makes much sense - or am I missing something why you need page->private depend on the mapping? So what I wanted to suggest is that we leave page->private as is currently and just concentrate on page->mapping hacks...  								Honza --  Jan Kara <jack@suse.com> SUSE Labs, CR",technical,Jan Kara,jack@suse.cz,1,0,944,0.34105960264900664,0.92,0,0.875,0.0625,0.0,0.0
297,254920,263451,"Oh that one is easy, the API take vma + addr or rather mm struct + addr(i.e. like KSM today kind of). I will change wording in v1 to almost generic write protection :) or process' page write protection (but this would not work for special pfn/vma so not generic their either).Well i wanted to go up to KSM or at least as close as possible to KSM for file back page. But i can focus on page->mapping first, do write protection with that and also do the per page wait queue for page lock. Which i believe are both nice features. This will also make the patchset smaller and easier to review (less scary).KSM can be done on top of that latter and i will be happy to help. I have a bunch of coccinelle patches for page->private, page->index and i can do some for page->flags.","On Thu, Apr 19, 2018 at 12:32:50PM +0200, Jan Kara wrote:  [...]   Oh that one is easy, the API take vma + addr or rather mm struct + addr (ie like KSM today kind of). I will change wording in v1 to almost generic write protection :) or process' page write protection (but this would not work for special pfn/vma so not generic their either).   Well i wanted to go up to KSM or at least as close as possible to KSM for file back page. But i can focus on page->mapping first, do write protection with that and also do the per page wait queue for page lock. Which i believe are both nice features. This will also make the patchset smaller and easier to review (less scary).  KSM can be done on top of that latter and i will be happy to help. I have a bunch of coccinelle patches for page->private, page->index and i can do some for page->flags.  Cheers, Jrme",technical,Jerome Glisse,jglisse@redhat.com,1,0,771,0.2913907284768212,0.94,0,0.875,0.0625,0.0,0.0625
298,254920,265245,"Your approach seems useful if there are lots of locked pages sharing the same wait queue. That said, in the original workload from our customer with the long wait queue problem, there was a single super hot page getting migrated, and itis being accessed by all threads which caused the big log jam while they wait for the migration to get completed. With your approach, we will still likely end up with a long queue in that workload even if we have per page wait queue.","On 04/04/2018 12:17 PM, jglisse@redhat.com wrote:  Your approach seems useful if there are lots of locked pages sharing the same wait queue.    That said, in the original workload from our customer with the long wait queue problem, there was a single super hot page getting migrated, and it is being accessed by all threads which caused the big log jam while they wait for the migration to get completed.   With your approach, we will still likely end up with a long queue  in that workload even if we have per page wait queue.  Thanks.  Tim",technical,Tim Chen,tim.c.chen@linux.intel.com,0,0,469,0.15397350993377484,0.96,0,1.0,0.0,0.0625,0.0
299,254920,265306,"Ok so i re-read the thread, i was writing this cover letter from memory and i had bad recollection of your issue, so sorry. First, do you have a way to reproduce the issue ? Something easy would be nice :)So what i am proposing for per page wait queue would only marginally help you (it might not even be measurable in your workload). It would certainly make the code smaller and easier to understand i believe. Now that i have look back at your issue i think there is 2 things we should do. First keep migration page map read only, this would at least avoid CPU read fault. In trace you captured i wasn't able to ascertain if this were read or write fault. Second idea i have is about NUMA, every time we NUMA migrate a page we could attach a temporary struct to the page (using page->mapping). So if we scan that page again we can inspect information about previous migration and see if we are not over migrating that page (i.e. bouncing it all over). If so we can mark the page (maybe with a page flag if we can find one) to protect it from further migration. That temporary struct would be remove after a while, i.e. autonuma would preallocate a bunch of those and keep an LRU of them and recycle the oldest when it needs a new one to migrate another page.LSF/MM slots:Michal can i get 2 slots to talk about this ? MM only discussion, one to talk about doing migration with page map read only but write protected while migration is happening. The other one to talk about attaching auto NUMA tracking struct to page.","On Fri, Apr 20, 2018 at 12:57:41PM -0700, Tim Chen wrote:  Ok so i re-read the thread, i was writting this cover letter from memory and i had bad recollection of your issue, so sorry.  First, do you have a way to reproduce the issue ? Something easy would be nice :)  So what i am proposing for per page wait queue would only marginaly help you (it might not even be mesurable in your workload). It would certainly make the code smaller and easier to understand i believe.  Now that i have look back at your issue i think there is 2 things we should do. First keep migration page map read only, this would at least avoid CPU read fault. In trace you captured i wasn't able to ascertain if this were read or write fault.  Second idea i have is about NUMA, everytime we NUMA migrate a page we could attach a temporary struct to the page (using page->mapping). So if we scan that page again we can inspect information about previous migration and see if we are not over migrating that page (ie bouncing it all over). If so we can mark the page (maybe with a page flag if we can find one) to protect it from further migration. That temporary struct would be remove after a while, ie autonuma would preallocate a bunch of those and keep an LRU of them and recycle the oldest when it needs a new one to migrate another page.   LSF/MM slots:  Michal can i get 2 slots to talk about this ? MM only discussion, one to talk about doing migration with page map read only but write protected while migration is happening. The other one to talk about attaching auto NUMA tracking struct to page.  Cheers, Jrme",technical,Jerome Glisse,jglisse@redhat.com,1,0,1519,0.5298013245033113,0.98,0,1.0,0.0,0.0,0.0
300,254920,265328,"Unfortunately it is a customer workload that they guard closely and wouldn't let us look at the source code.  We have to profile and backtrace its behavior. Mel made a quick attempt to reproduce the behavior with a hot page migration, but he wasn't quite able to duplicate the pathologic behavior. In certain cases if we have lots of pages sharing a page wait queue, your solution would help, and we wouldn't be wasting time checking waiters not waiting on the page that's being unlocked.  Though I don't have a specific workload that has such behavior. The goal to migrate a hot page with care, or avoid bouncing it around frequently makes sense.  If it is a hot page shared by many threads running on different NUMA nodes, and moving it will only mildly improve NUMA locality, we should avoid the migration.","On 04/20/2018 03:19 PM, Jerome Glisse wrote:  Unfortunately it is a customer workload that they guard closely and wouldn't let us look at the source code.  We have to profile and backtrace its behavior.  Mel made a quick attempt to reproduce the behavior with a hot page migration,  but he wasn't quite able to duplicate the pathologic behavior.   In certain cases if we have lots of pages sharing a page wait queue, your solution would help, and we wouldn't be wasting time checking waiters not waiting on the page that's being unlocked.  Though I don't have a specific workload that has such behavior.   The goal to migrate a hot page with care, or avoid bouncing it around  frequently makes sense.  If it is a hot page shared by many threads running on different NUMA nodes, and moving it will only mildly improve NUMA locality, we should avoid the migration.  Tim",technical,Tim Chen,tim.c.chen@linux.intel.com,0,0,809,0.26490066225165565,1.0,1,1.0,0.0,0.0,0.0
301,254985,256527,"Sorry about that, I actually had three people review my code internally, then I managed to send out an old version. 100% guilty of submitting code when I needed sleep. As for the change, that was in response to a request from Andrew to make the update function less racy. Should I resend a correct v2 now that the thread exists?","Sorry about that, I actually had three people review my code internally, then I managed to send out an old version. 100% guilty of submitting code when I needed sleep. As for the change, that was in response to a request from Andrew to make the update function less racy.  Should I resend a correct v2 now that the thread exists?  —Buddy",technical,Buddy Lumpkin,buddy.lumpkin@oracle.com,0,1,328,1.0,0.6,0,0.2,0.8,0.2,0.0
302,254985,256560,"Let's just discuss open questions for now. Specifics of the code are the least interesting at this stage. If you want some help with the code review, you can put it somewhere in the git tree and send a reference for those who are interested.","On Thu 05-04-18 23:25:14, Buddy Lumpkin wrote:  Let's just discuss open questions for now. Specifics of the code are the least interesting at this stage.  If you want some help with the code review, you can put it somewhere in the git tree and send a reference for those who are interested. --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,241,0.7246376811594203,0.8,0,0.2,0.8,0.0,0.8
303,254985,258778,"Ok, I will go back through the thread and make sure all questions and concerns have been addressed.","  Ok, I will go back through the thread and make sure all questions and concerns have been addressed.",technical,Buddy Lumpkin,buddy.lumpkin@oracle.com,0,1,99,0.2898550724637681,1.0,1,1.0,0.0,0.8,0.0
304,255117,255295,"ok How about include/asm-generic/early_pfn.h ?And could I use it in this case? Currently, arm/arm64 have memblock enable by default. When other arches implement their, they can include this file?","Thanks, Matthew   On 4/5/2018 7:23 PM, Matthew Wilcox Wrote: ok How about include/asm-generic/early_pfn.h ? And could I use CONFIG_HAVE_ARCH_PFN_VALID and CONFIG_HAVE_MEMBLOCKin  this case? Currently, arm/arm64 have memblock enable by default. When other arches  implement their HAVE_MEMBLOCK and HAVE_ARCH_PFN_VALID, they can include this file?  --  Cheers, Jia",technical,Jia He,hejianet@gmail.com,0,1,195,0.14644351464435146,0.6428571428571429,0,0.0,1.0,0.0,0.0
305,255117,255301,"Thanks, thus the binary search in next step can be discarded?","On 4/5/2018 7:34 PM, Matthew Wilcox Wrote: Thanks, thus the binary search in next step can be discarded?  --  Cheers, Jia",technical,Jia He,hejianet@gmail.com,0,1,61,0.05439330543933055,0.7142857142857143,0,0.0,1.0,0.0,0.0
306,255117,255305,I don't know all the circumstances in which this is called.  Maybe a linear search with memo is more appropriate than a binary search.,"On Thu, Apr 05, 2018 at 08:44:12PM +0800, Jia He wrote:  I don't know all the circumstances in which this is called.  Maybe a linear search with memo is more appropriate than a binary search.",technical,Matthew Wilcox,willy@infradead.org,1,0,134,0.11297071129707113,0.7857142857142857,0,0.0,1.0,0.0,0.0
307,255117,256595,"That's been brought up before, and the reasoning appears to be something along the lines of...Academics and published wisdom is that on cached architectures, binary searches are bad because it doesn't operate efficiently due to the overhead from having to load cache lines.  Consequently, there seems to be a knee-jerk reaction that ""all binary searches are bad, we must eliminate them. ""What is failed to be grasped here, though, is that it is typical that the number of entries in this array tend to be small, so the entire array takes up one or two cache lines, maybe a maximum of four lines depending on your cache line length and number of entries. This means that the binary search expense is reduced, and is lower than a linear search for the majority of cases. What is key here as far as performance is concerned is whether the general usage of pfn_valid() by the kernel is optimal.  We should not optimise only for the boot case, which means evaluating the effect of these changes with _real_ workloads, not just ""does my machine boot a milliseconds faster"".--RMK's Patch system: broadband for 0.8mile line in suburbia: sync at 8.8Mbps down 630kbps up According to speedtest.net: 8.21Mbps down 510kbps up","On Thu, Apr 05, 2018 at 05:50:54AM -0700, Matthew Wilcox wrote:  That's been brought up before, and the reasoning appears to be something along the lines of...  Academics and published wisdom is that on cached architectures, binary searches are bad because it doesn't operate efficiently due to the overhead from having to load cache lines.  Consequently, there seems to be a knee-jerk reaction that all binary searches are bad, we must eliminate them.""  What is failed to be grasped here, though, is that it is typical that the number of entries in this array tend to be small, so the entire array takes up one or two cache lines, maybe a maximum of four lines depending on your cache line length and number of entries.  This means that the binary search expense is reduced, and is lower than a linear search for the majority of cases.  What is key here as far as performance is concerned is whether the general usage of pfn_valid() by the kernel is optimal.  We should not optimise only for the boot case, which means evaluating the effect of these changes with _real_ workloads, not just ""does my machine boot a milliseconds faster"".  --  RMK's Patch system: http://www.armlinux.org.uk/developer/patches/ FTTC broadband for 0.8mile line in suburbia: sync at 8.8Mbps down 630kbps up According to speedtest.net: 8.21Mbps down 510kbps up""",technical,Russell King - ARM Linux,linux@armlinux.org.uk,1,0,1213,1.0,0.8571428571428571,0,0.5,0.5,0.0,0.0
308,255117,256622,"This is actually a good point.a) This does not make sense. At least in general case.b) It is not the case here. Here it's really mostly called with sequentially incremented pfns, AFAICT.In this case it hits mostly the last result or eventually the sequentially next one.IIUC, this is only used during early boot (and memory hot plug) and it does not influence regular runtime. Whether the general usage of pfn_valid() by the kernel is optimal is another good question, but that's totally unrelated to this series, IMHO. On the other hand I also wonder if this all really is worth the negligible boot time speedup.","On Fri, Apr 6, 2018 at 11:09 AM, Russell King - ARM Linux <linux@armlinux.org.uk> wrote:  This is actually a good point.   a) This does not make sense. At least in general case. b) It is not the case here. Here it's really mostly called with sequentially incremented pfns, AFAICT.   In this case it hits mostly the last result or eventually the sequentially next one.   IIUC, this is only used during early boot (and memory hotplug) and it does not influence regular runtime. Whether the general usage of pfn_valid() by the kernel is optimal is another good question, but that's totally unrelated to this series, IMHO.  On the other hand I also wonder if this all really is worth the negligible boot time speedup.  --nX",technical,Daniel Vacek,neelx@redhat.com,0,0,613,0.5104602510460251,0.9285714285714286,0,0.5,0.5,0.0,0.5
309,255117,257184,"Thanks for your comments, Russell IIUC, are you opposed to entirely removing the binary search instead of my previous patch set? hmm.. But pfn is linearly increased during the booting time. This assumption is not correct in real workload for pfn_valid out of booting time. So in my patchset, I defined another pfn_valid_region for booting time only. I didn't have many arm/arm64 boxes to verified. What I can do is guaranteeing the improvement in my armv8a (QUALCOMM centriq 2400). Sorry about it.","Thanks for your comments, Russell   On 4/6/2018 5:09 PM, Russell King - ARM Linux Wrote: IIUC, are you opposed to entirely removing the binary search instead of my previous patch set? hmm.. But pfn is linearly increased during the booting time. This assumption is not correct in real workload for pfn_valid out of booting time. So in my patchset, I defined another pfn_valid_region for booting time only.  I didn't have many arm/arm64 boxes to verifed. What I can do is guaranteeing the improvemnet in my armv8a (qualcom centriq 2400). Sorry about it.    -- Cheers, Jia",technical,Jia He,hejianet@gmail.com,0,1,497,0.401673640167364,1.0,1,1.0,0.0,0.5,0.0
310,255117,255250,"arm96_common'?!  No.  Just no. The right way to share common code is to create a header file (or use an existing one), either in asm-generic or linux, with a #ifdef CONFIG_fooblock and then 'select foo' in the arm Kconfig files.  That allows this common code to be shared, maybe with powerpc or x86 or ... in the future.","On Thu, Apr 05, 2018 at 01:04:34AM -0700, Jia He wrote:  'arm96_common'?!  No.  Just no.  The right way to share common code is to create a header file (or use an existing one), either in asm-generic or linux, with a #ifdef CONFIG_foo block and then 'select foo' in the arm Kconfig files.  That allows this common code to be shared, maybe with powerpc or x86 or ... in the future.",technical,Matthew Wilcox,willy@infradead.org,1,0,320,0.301255230125523,0.5,0,0.0,1.0,0.0,0.0
311,255117,255266,"Sure, but I bet if we are >end_pfn, we're almost certainly going to the start_pfn of the next block, so why not test that as well?","On Thu, Apr 05, 2018 at 01:04:35AM -0700, Jia He wrote:  Sure, but I bet if we are >end_pfn, we're almost certainly going to the start_pfn of the next block, so why not test that as well?   		early_region_idx++, 		start_pfn = PFN_DOWN(regions[early_region_idx].base), 		if (pfn >= end_pfn && pfn <= start_pfn) 			return start_pfn,",technical,Matthew Wilcox,willy@infradead.org,1,0,130,0.13389121338912133,0.5714285714285714,0,0.0,1.0,0.0,0.0
312,258997,258997,Please pull from  next to receive the latest Thermal Management updates,"Hi, Linus,  Please pull from   git://git.kernel.org/pub/scm/linux/kernel/git/rzhang/linux.git next  to receive the latest Thermal Management updates for v4.17-rc1 with top-most commit f8837aac36cdc7430422cd65f4466071b42654bb:    Merge branches 'thermal-core' and 'thermal-soc' into next (2018-04-02  21:49:31 +0800)  on top of commit 0c8efd610b58cb23cefdfa12015799079aef94ae:    Linux 4.16-rc5 (2018-03-11 17:25:09 -0700)  Specifics:  - Fix race condition in imx_thermal_probe(). (Mikhail Lappo)  - Add cooling device's statistics in sysfs. (Viresh Kumar)  - add support for i.MX7 thermal sensor in imx_thermal driver. (Anson Huang)  - add support for MT7622 SoC in mtk_thermal driver. (Sean Wang)  - Remove unused min/max cpu cooling DT property. (Viresh Kumar).  - A series of fixes on exynos driver. (Bartlomiej Zolnierkiewicz, Maciej Purski, Marek Szyprowski)  thanks, rui    ---------------------------------------------------------------- Anson Huang (1):       thermal: imx: add i.MX7 thermal sensor support  Bartlomiej Zolnierkiewicz (10):       thermal: exynos: remove unused type"" field from struct exynos_tmu_platform_data       thermal: exynos: remove parsing of samsung, tmu_default_temp_offset property       thermal: exynos: remove parsing of samsung, tmu_[first, second]_point_trim properties       thermal: exynos: remove parsing of samsung, tmu_noise_cancel_mode property       thermal: exynos: remove parsing of samsung, tmu[_min, _max]_efuse_value properties       thermal: exynos: remove parsing of samsung, tmu_reference_voltage property       thermal: exynos: remove parsing of samsung,tmu_gain property       thermal: exynos: remove parsing of samsung, tmu_cal_type property       thermal: exynos: remove separate exynos_tmu.h header file       dt-bindings: thermal: remove no longer needed samsung thermal properties  Maciej Purski (1):       thermal: exynos: Read soc_type from match data  Marek Szyprowski (2):       thermal: exynos: Reading temperature makes sense only when TMU is turned on       thermal: exynos: Propagate error value from tmu_read()  Mikhail Lappo (1):       thermal: imx: Fix race condition in imx_thermal_probe()  Sean Wang (2):       dt-bindings: thermal: add binding for MT7622 SoC       thermal: mediatek: add support for MT7622 SoC  Viresh Kumar (2):       dt-bindings: thermal: Remove ""cooling-{min|max}-level"" properties       thermal: Add cooling device's statistics in sysfs  Zhang Rui (2):       Merge branch 'linus' of git://git.kernel.org/.../evalenti/linux- soc-thermal into thermal-soc       Merge branches 'thermal-core' and 'thermal-soc' into next   .../devicetree/bindings/thermal/exynos-thermal.txt |  23 +-  .../devicetree/bindings/thermal/imx-thermal.txt    |   9 +-  .../bindings/thermal/mediatek-thermal.txt          |   1 +  .../devicetree/bindings/thermal/thermal.txt        |  16 +-  Documentation/thermal/sysfs-api.txt                |  31 +++  drivers/thermal/Kconfig                            |   7 +  drivers/thermal/imx_thermal.c                      | 301 ++++++++++++++++-----  drivers/thermal/mtk_thermal.c                      |  35 +++  drivers/thermal/samsung/exynos_tmu.c               | 268 +++++++++-- -------  drivers/thermal/samsung/exynos_tmu.h               |  75 -----  drivers/thermal/thermal_core.c                     |   3 +-  drivers/thermal/thermal_core.h                     |  10 +  drivers/thermal/thermal_helpers.c                  |   5 +-  drivers/thermal/thermal_sysfs.c                    | 225 +++++++++++++++  include/linux/thermal.h                            |   1 +  15 files changed, 706 insertions(+), 304 deletions(-)  delete mode 100644 drivers/thermal/samsung/exynos_tmu.h""",technical,Zhang Rui,rui.zhang@intel.com,1,1,71,0.1134020618556701,0.04,0,0.0,1.0,-1.25,0.0
313,258997,259904,"These couple of warnings were introduced by:  There should be no functional changes caused by this patch.     After digging into, there is no obvious fix. It returns effectively an uninitialized value and the callers are assuming the value is always correct, so it is also not possible to simply return an error.","On 12/04/2018 18:55, Linus Torvalds wrote:  These couple of warnings were introduced by:  commit 480b5bfc16e17ef51ca1c55bfcebf55db8673ebf Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com> Date:   Tue Mar 6 15:43:45 2018 +0100      thermal: exynos: remove parsing of samsung, tmu_default_temp_offset property      Trimming (one point based or two points based) is always used for     the temperature calibration and the default non-trimming code should     never be reached.      Modify temp_to_code() and code_to_temp() accordingly (WARN_ON(1)     in the default cases) and then remove no longer needed parsing of     samsung,tmu_default_temp_offset property.      There should be no functional changes caused by this patch.      Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>     Signed-off-by: Eduardo Valentin <edubezval@gmail.com>   After digging into, there is no obvious fix. It returns effectively an uninitialized value and the callers are assuming the value is always correct, so it is also not possible to simply return an error.   --   <http://www.linaro.org/> Linaro.org │ Open source software for ARM SoCs  Follow Linaro:  <http://www.facebook.com/pages/Linaro> Facebook | <http://twitter.com/#!/linaroorg> Twitter | <http://www.linaro.org/linaro-blog/> Blog",technical,Daniel Lezcano,daniel.lezcano@linaro.org,1,0,312,0.5979381443298969,0.2,0,0.25,0.5,0.0,0.0
314,258997,260065,"Hello,Yeah, this has also passed my local compilation error. Somehow my gcc4.9 is not catching it. Using an older gcc (gcc4.6) does catch it.Anyways, given that the conversion functions are written to cover for unexpected cal_type, the right way of fixing this is to rewrite the conversion functions to allow for returning error codes and adjusting the callers as expected. please consider the following fix:","Hello,  On Thu, Apr 12, 2018 at 09:55:19AM -0700, Linus Torvalds wrote:  Yeah, this has also passed my local compilation error. Somehow my gcc4.9 is not catching it. Using an older gcc (gcc4.6) does catch it.  Anyways, given that the conversion functions are written to cover for unexpected cal_type, the right way of fixing this is to rewrite the conversion functions to allow for returning error codes and adjusting the callers as expected.  Rui, bzolnier, please consider the following fix:  From: Eduardo Valentin <edubezval@gmail.com> Date: Thu, 12 Apr 2018 21:00:48 -0700 Subject: [PATCH 1/1] thermal: exynos: fix compilation warning around  conversion functions  In order to fix the warns: drivers/thermal/samsung/exynos_tmu.c:931:37: warning: 'temp' may be used uninitialized in this function [-Wmaybe-uninitialized] drivers/thermal/samsung/exynos_tmu.c:304:9: warning: 'temp_code' may be used uninitialized in this function [-Wmaybe-uninitialized]  the conversion functions should allow return error codes and the not mix the converted value with error code.  This patch change the conversion functions to return error code or success and adjusts the callers accordingly.  Signed-off-by: Eduardo Valentin <edubezval@gmail.com> ---  drivers/thermal/samsung/exynos_tmu.c | 120 ++++++++++++++++++++++++-----------  1 file changed, 84 insertions(+), 36 deletions(-)  diff --git a/drivers/thermal/samsung/exynos_tmu.c b/drivers/thermal/samsung/exynos_tmu.c index 2ec8548..b3f0704 100644 --- a/drivers/thermal/samsung/exynos_tmu.c +++ b/drivers/thermal/samsung/exynos_tmu.c @@ -282,52 +282,54 @@ static void exynos_report_trigger(struct exynos_tmu_data *p)   * TMU treats temperature as a mapped temperature code.   * The temperature is converted differently depending on the calibration type.   */ -static int temp_to_code(struct exynos_tmu_data *data, u8 temp) +static int temp_to_code(struct exynos_tmu_data *data, u8 temp, int *temp_code)  { -	int temp_code, +	int ret = 0,    	switch (data->cal_type) {  	case TYPE_TWO_POINT_TRIMMING: -		temp_code = (temp - EXYNOS_FIRST_POINT_TRIM) * +		*temp_code = (temp - EXYNOS_FIRST_POINT_TRIM) *  			(data->temp_error2 - data->temp_error1) /  			(EXYNOS_SECOND_POINT_TRIM - EXYNOS_FIRST_POINT_TRIM) +  			data->temp_error1,  		break,  	case TYPE_ONE_POINT_TRIMMING: -		temp_code = temp + data->temp_error1 - EXYNOS_FIRST_POINT_TRIM, +		*temp_code = temp + data->temp_error1 - EXYNOS_FIRST_POINT_TRIM,  		break,  	default:  		WARN_ON(1), +		ret = -EINVAL,  		break,  	}   -	return temp_code, +	return ret,  }    /*   * Calculate a temperature value from a temperature code.   * The unit of the temperature is degree Celsius.   */ -static int code_to_temp(struct exynos_tmu_data *data, u16 temp_code) +static int code_to_temp(struct exynos_tmu_data *data, u16 temp_code, int *temp)  { -	int temp, +	int ret = 0,    	switch (data->cal_type) {  	case TYPE_TWO_POINT_TRIMMING: -		temp = (temp_code - data->temp_error1) * +		*temp = (temp_code - data->temp_error1) *  			(EXYNOS_SECOND_POINT_TRIM - EXYNOS_FIRST_POINT_TRIM) /  			(data->temp_error2 - data->temp_error1) +  			EXYNOS_FIRST_POINT_TRIM,  		break,  	case TYPE_ONE_POINT_TRIMMING: -		temp = temp_code - data->temp_error1 + EXYNOS_FIRST_POINT_TRIM, +		*temp = temp_code - data->temp_error1 + EXYNOS_FIRST_POINT_TRIM,  		break,  	default:  		WARN_ON(1), +		ret = -EINVAL,  		break,  	}   -	return temp, +	return ret,  }    static void sanitize_temp_error(struct exynos_tmu_data *data, u32 trim_info) @@ -352,7 +354,7 @@ static u32 get_th_reg(struct exynos_tmu_data *data, u32 threshold, bool falling)  	struct thermal_zone_device *tz = data->tzd,  	const struct thermal_trip * const trips =  		of_thermal_get_trip_points(tz), -	unsigned long temp, +	int temp,  	int i,    	if (!trips) { @@ -362,6 +364,8 @@ static u32 get_th_reg(struct exynos_tmu_data *data, u32 threshold, bool falling)  	}    	for (i = 0, i < of_thermal_get_ntrips(tz), i++) { +		int val, ret, +  		if (trips[i].type == THERMAL_TRIP_CRITICAL)  			continue,   @@ -371,7 +375,14 @@ static u32 get_th_reg(struct exynos_tmu_data *data, u32 threshold, bool falling)  		else  			threshold &= ~(0xff << 8 * i),   -		threshold |= temp_to_code(data, temp) << 8 * i, +		ret = temp_to_code(data, temp, &val), +		if (ret) { +			pr_err(%s: Convertion error from temp (%d) to code: %d!\n"", +				__func__, temp, ret), +			return 0, +		} + +		threshold |= val << 8 * i,  	}    	return threshold, @@ -460,11 +471,10 @@ static int exynos4210_tmu_initialize(struct platform_device *pdev)    	/* Write temperature code for threshold */  	reference = trips[0].temperature / MCELSIUS, -	threshold_code = temp_to_code(data, reference), -	if (threshold_code < 0) { -		ret = threshold_code, +	ret = temp_to_code(data, reference, &threshold_code), +	if (ret < 0 || threshold_code < 0)  		goto out, -	} +  	writeb(threshold_code, data->base + EXYNOS4210_TMU_REG_THRESHOLD_TEMP),    	for (i = 0, i < of_thermal_get_ntrips(tz), i++) { @@ -537,7 +547,10 @@ static int exynos4412_tmu_initialize(struct platform_device *pdev)  		goto out,  	}   -	threshold_code = temp_to_code(data, crit_temp / MCELSIUS), +	ret = temp_to_code(data, crit_temp / MCELSIUS, &threshold_code), +	if (ret) +		goto out, +  	/* 1-4 level to be assigned in th0 reg */  	rising_threshold &= ~(0xff << 8 * i),  	rising_threshold |= threshold_code << 8 * i, @@ -620,7 +633,9 @@ static int exynos5433_tmu_initialize(struct platform_device *pdev)  		/* Write temperature code for rising threshold */  		tz->ops->get_trip_temp(tz, i, &temp),  		temp /= MCELSIUS, -		threshold_code = temp_to_code(data, temp), +		ret = temp_to_code(data, temp, &threshold_code), +		if (ret) +			goto out,    		rising_threshold = readl(data->base + rising_reg_offset),  		rising_threshold |= (threshold_code << j * 8), @@ -629,7 +644,9 @@ static int exynos5433_tmu_initialize(struct platform_device *pdev)  		/* Write temperature code for falling threshold */  		tz->ops->get_trip_hyst(tz, i, &temp_hist),  		temp_hist = temp - (temp_hist / MCELSIUS), -		threshold_code = temp_to_code(data, temp_hist), +		ret = temp_to_code(data, temp_hist, &threshold_code), +		if (ret) +			goto out,    		falling_threshold = readl(data->base + falling_reg_offset),  		falling_threshold &= ~(0xff << j * 8), @@ -677,7 +694,12 @@ static int exynos5440_tmu_initialize(struct platform_device *pdev)    	/* if last threshold limit is also present */  	if (!data->tzd->ops->get_crit_temp(data->tzd, &crit_temp)) { -		threshold_code = temp_to_code(data, crit_temp / MCELSIUS), +		int ret, + +		ret = temp_to_code(data, crit_temp / MCELSIUS, &threshold_code), +		if (ret) +			return ret, +  		/* 5th level to be assigned in th2 reg */  		rising_threshold =  			threshold_code << EXYNOS5440_TMU_TH_RISE4_SHIFT, @@ -749,7 +771,10 @@ static int exynos7_tmu_initialize(struct platform_device *pdev)  		temp_hist = temp - (temp_hist / MCELSIUS),    		/* Set 9-bit temperature code for rising threshold levels */ -		threshold_code = temp_to_code(data, temp), +		ret = temp_to_code(data, temp, &threshold_code), +		if (ret) +			goto out, +  		rising_threshold = readl(data->base +  			EXYNOS7_THD_TEMP_RISE7_6 + reg_off),  		rising_threshold &= ~(EXYNOS7_TMU_TEMP_MASK << (16 * bit_off)), @@ -758,7 +783,9 @@ static int exynos7_tmu_initialize(struct platform_device *pdev)  		       data->base + EXYNOS7_THD_TEMP_RISE7_6 + reg_off),    		/* Set 9-bit temperature code for falling threshold levels */ -		threshold_code = temp_to_code(data, temp_hist), +		ret = temp_to_code(data, temp_hist, &threshold_code), +		if (ret) +			goto out,  		falling_threshold &= ~(EXYNOS7_TMU_TEMP_MASK << (16 * bit_off)),  		falling_threshold |= threshold_code << (16 * bit_off),  		writel(falling_threshold, @@ -925,11 +952,18 @@ static int exynos_get_temp(void *p, int *temp)  	clk_enable(data->clk),    	value = data->tmu_read(data), -	if (value < 0) +	if (value < 0) {  		ret = value, -	else -		*temp = code_to_temp(data, value) * MCELSIUS, +		goto out, +	} + +	ret = code_to_temp(data, value, temp), +	if (ret) +		goto out,   +	*temp *= MCELSIUS, + +out:  	clk_disable(data->clk),  	mutex_unlock(&data->lock),   @@ -937,9 +971,11 @@ static int exynos_get_temp(void *p, int *temp)  }    #ifdef CONFIG_THERMAL_EMULATION -static u32 get_emul_con_reg(struct exynos_tmu_data *data, unsigned int val, -			    int temp) +static int get_emul_con_reg(struct exynos_tmu_data *data, unsigned int val, +			    int temp, u32 *con_reg)  { +	int code, ret = 0, +  	if (temp) {  		temp /= MCELSIUS,   @@ -950,27 +986,36 @@ static u32 get_emul_con_reg(struct exynos_tmu_data *data, unsigned int val,  		if (data->soc == SOC_ARCH_EXYNOS7) {  			val &= ~(EXYNOS7_EMUL_DATA_MASK <<  				EXYNOS7_EMUL_DATA_SHIFT), -			val |= (temp_to_code(data, temp) << -				EXYNOS7_EMUL_DATA_SHIFT) | +			ret = temp_to_code(data, temp, &code), +			if (ret) +				goto out, + +			val |= (code << EXYNOS7_EMUL_DATA_SHIFT) |  				EXYNOS_EMUL_ENABLE,  		} else {  			val &= ~(EXYNOS_EMUL_DATA_MASK <<  				EXYNOS_EMUL_DATA_SHIFT), -			val |= (temp_to_code(data, temp) << -				EXYNOS_EMUL_DATA_SHIFT) | +			ret = temp_to_code(data, temp, &code), +			if (ret) +				goto out, + +			val |= (code << EXYNOS_EMUL_DATA_SHIFT) |  				EXYNOS_EMUL_ENABLE,  		}  	} else {  		val &= ~EXYNOS_EMUL_ENABLE,  	}   -	return val, +	*con_reg = val, +out: +	return ret,  }    static void exynos4412_tmu_set_emulation(struct exynos_tmu_data *data,  					 int temp)  {  	unsigned int val, +	int ret,  	u32 emul_con,    	if (data->soc == SOC_ARCH_EXYNOS5260) @@ -983,18 +1028,21 @@ static void exynos4412_tmu_set_emulation(struct exynos_tmu_data *data,  		emul_con = EXYNOS_EMUL_CON,    	val = readl(data->base + emul_con), -	val = get_emul_con_reg(data, val, temp), -	writel(val, data->base + emul_con), +	ret = get_emul_con_reg(data, val, temp, &val), +	if (!ret) +		writel(val, data->base + emul_con),  }    static void exynos5440_tmu_set_emulation(struct exynos_tmu_data *data,  					 int temp)  {  	unsigned int val, +	int ret,    	val = readl(data->base + EXYNOS5440_TMU_S0_7_DEBUG), -	val = get_emul_con_reg(data, val, temp), -	writel(val, data->base + EXYNOS5440_TMU_S0_7_DEBUG), +	ret = get_emul_con_reg(data, val, temp, &val), +	if (!ret) +		writel(val, data->base + EXYNOS5440_TMU_S0_7_DEBUG),  }    static int exynos_tmu_set_emulation(void *drv_data, int temp) --  2.1.4""",technical,Eduardo Valentin,edubezval@gmail.com,1,0,408,0.7835051546391752,0.24,0,0.25,0.5,0.0,0.0
315,258997,260104,"I think there are two problems here1. Actually, this error has been raised by 0-day earlier. Don't know why it still goes into thermal-soc tree.2. After pulled the thermal-soc changes, I also asked 0-day to run build test, but I didn't get any warning report (email attached), to look at this issue.","On 四, 2018-04-12 at 21:08 -0700, Eduardo Valentin wrote:  I think there are two problems here  1. Actually, this error has been raised by 0-day earlier. https://marc.info/?l=linux-pm&m=152107340117077&w=2 Don't know why it still goes into thermal-soc tree.  2. After pulled the thermal-soc changes, I also asked 0-day to run build test, but I didn't get any warning report (email attached), CC Philip and Shun to look at this issue.  thanks, rui",technical,Zhang Rui,rui.zhang@intel.com,1,1,299,0.6597938144329897,0.28,0,0.25,0.5,0.0,0.0
316,258997,260106,"As it is late in this merge window, I'd prefer to1. drop all the thermal-soc material in the first pull request which I will send out soon.2. you can prepare another pull request containing the thermal-socmaterials except the exynos fixes3. exynos fixes with the problem solved can be queued for -rc2 or later.","Hi, Eduardo,  On 四, 2018-04-12 at 21:08 -0700, Eduardo Valentin wrote: as it is late in this merge window, I'd prefer to 1. drop all the thermal-soc material in the first pull request which I will send out soon. 2. you can prepare another pull request containing the thermal-soc materials except the exynos fixes 3. exynos fixes with the problem solved can be queued for -rc2 or later.  thanks, rui",technical,Zhang Rui,rui.zhang@intel.com,1,1,310,0.6082474226804123,0.32,0,0.25,0.5,0.0,0.0
317,258997,260171,"HI, Since this condition cannot happen (the driver makes sure of this during probe) I would prefer much simpler fix from Arnd: I've already ACKed it two weeks ago). Ditto","On Thursday, April 12, 2018 09:08:57 PM Eduardo Valentin wrote:  Hi,   Since this condition cannot happen (the driver makes sure of this during probe) I would prefer much simpler fix from Arnd:  https://patchwork.kernel.org/patch/10313313/  (I've already ACKed it two weeks ago).   ditto   Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,170,0.3917525773195876,0.36,0,0.5,0.5,0.0,0.0
318,258997,260180,"I'm not sure these are correct fixes. The change 480b5bfc16e1 tells: ""There should be no functional changes caused by this patch. ""but the fix above returns 0 as a default value instead of '50' or '25'for the 5440 and that impacts the threshold etc ...IMO, the correct fix would be to define a default value '50', override it at init time to '25' if it is a 5440. And then the variable 'temp 'and 'temp_code' get this value in the default case.--","On 13/04/2018 10:55, Bartlomiej Zolnierkiewicz wrote:  I'm not sure these are correct fixes.  The change 480b5bfc16e1 tells:  There should be no functional changes caused by this patch.""  but the fix above returns 0 as a default value instead of '50' or '25' for the 5440 and that impacts the threshold etc ...  IMO, the correct fix would be to define a default value '50', override it at init time to '25' if it is a 5440. And then the variable 'temp' and 'temp_code' get this value in the default case.       --   <http://www.linaro.org/> Linaro.org │ Open source software for ARM SoCs  Follow Linaro:  <http://www.facebook.com/pages/Linaro> Facebook | <http://twitter.com/#!/linaroorg> Twitter | <http://www.linaro.org/linaro-blog/> Blog""",technical,Daniel Lezcano,daniel.lezcano@linaro.org,1,0,446,1.0,0.44,0,0.5,0.25,0.0,0.0
319,258997,260184,It is okay to return 0 because this code-path (the default one) will be never hit by the driver (probe makes sure of it) - the default case is here is just to silence compilation errors.,"On Friday, April 13, 2018 11:00:43 AM Daniel Lezcano wrote:  It is okay to return 0 because this code-path (the default one) will be never hit by the driver (probe makes sure of it) - the default case is here is just to silence compilation errors..  Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,186,0.422680412371134,0.48,0,0.5,0.25,0.0,0.0
320,258997,260217,"Sent you this I see there is still some discussion around the topic of how to fix this. So, once we get to a point of agreement, I will send the remaining with exynos fixes.","On Fri, Apr 13, 2018 at 03:08:03AM -0700, Eduardo Valentin wrote:  Sent you this https://marc.info/?l=linux-pm&m=152361492711499&w=2   I see there is still some discussion around the topic of how to fix this. So, once we get to a point of agreement, I will send the remaining with exynos fixes.",technical,Eduardo Valentin,edubezval@gmail.com,1,0,173,0.4020618556701031,0.64,0,0.5,0.25,0.0,0.0
321,258997,260218,What should I do now to help resolve the issue?[ There has been no action from you on Arnd's fix for over two weeks and  also you have not commented on it now…,"On Friday, April 13, 2018 03:08:03 AM Eduardo Valentin wrote:  What should I do now to help resolve the issue?  [ There has been no action from you on Arnd's fix for over two weeks and   also you have not commented on it now.. ]  Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,159,0.3711340206185567,0.68,0,0.5,0.25,0.0,0.0
322,258997,260219,"Actually the switch statement was fine until the cleanup. Regarding the latest comment, this can be fixed properly by 'return' (or whatever you want which does not get around of gcc warnings).","On 13/04/2018 11:28, Bartlomiej Zolnierkiewicz wrote:  [ ... ]   Actually the switch statement was fine until the cleanup.   Regarding the latest comment, this can be fixed properly by 'return' (or whatever you want which does not get around of gcc warnings).  --   <http://www.linaro.org/> Linaro.org │ Open source software for ARM SoCs  Follow Linaro:  <http://www.facebook.com/pages/Linaro> Facebook | <http://twitter.com/#!/linaroorg> Twitter | <http://www.linaro.org/linaro-blog/> Blog",technical,Daniel Lezcano,daniel.lezcano@linaro.org,1,0,192,0.3917525773195876,0.72,0,0.5,0.25,0.0,0.0
323,258997,260223,I don't see how it was fine before as the driver has never used the default case (always used).Could you please explain this more? Do you mean that you want the patch with switch statement removal? Is incremental fix OK or do you want something else?,"On Friday, April 13, 2018 12:30:04 PM Daniel Lezcano wrote:  I don't see how it was fine before as the driver has never used the default case (always used TYPE_ONE_POINT_TRIMMING or TYPE_TWO_POINT_TRIMMING).  Could you please explain this more?   Do you mean that you want the patch with switch statement removal?  Is incremental fix OK or do you want something else?  Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,250,0.5463917525773195,0.76,0,0.5,0.25,0.0,0.0
324,258997,260246,this is much better fix.,"On Friday, April 13, 2018 01:00:09 PM Daniel Lezcano wrote:  Thanks Daniel, this is much better fix.  Acked-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>   Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,24,0.061855670103092786,0.84,0,0.5,0.25,0.0,0.0
325,258997,260250,"I'm not saying the code path was fine but from the compiler point of view, it was. By removing the defaulting temp value there is a code path gcc sees the temp variable as not initialized. Your cleanups are relevant.","On 13/04/2018 12:41, Bartlomiej Zolnierkiewicz wrote:   +++ b/drivers/thermal/samsung/exynos_tmu.c @@ -260,7 +260,7 @@ static int temp_to_code(struct exynos_tmu_data *data, u8 temp)                 temp_code = temp + data->temp_error1 - pdata->first_point_trim,                 break,         default: -               temp_code = temp + pdata->default_temp_offset, +               WARN_ON(1),                 break,         }  @@ -287,7 +287,7 @@ static int code_to_temp(struct exynos_tmu_data *data, u16 temp_code)                 temp = temp_code - data->temp_error1 + pdata->first_point_trim,                 break,         default: -               temp = temp_code - pdata->default_temp_offset, +               WARN_ON(1),                 break,         }  I'm not saying the code path was fine but from the compiler point of view, it was. By removing the defaulting temp value there is a code path gcc sees the temp variable as not initialized.  Your cleanups are relevant.     --   <http://www.linaro.org/> Linaro.org │ Open source software for ARM SoCs  Follow Linaro:  <http://www.facebook.com/pages/Linaro> Facebook | <http://twitter.com/#!/linaroorg> Twitter | <http://www.linaro.org/linaro-blog/> Blog",technical,Daniel Lezcano,daniel.lezcano@linaro.org,1,0,216,0.4639175257731959,0.88,0,0.5,0.25,0.0,0.0
326,258997,260251,"He has already posted it, I hope the fix is fine with you. Also sorry for the delay with handling issue - I was on holiday last two days and for some reason I was under (wrong) impression that the previous fix has been in thermal tree (so I was quite surprised today reading this mail thread).","On Friday, April 13, 2018 12:41:18 PM Bartlomiej Zolnierkiewicz wrote:  Danial has already posted it, I hope the fix is fine with you.  Also sorry for the delay with handling issue - I was on holiday last two days and for some reason I was under (wrong) impression that the previous fix has been in thermal tree (so I was quite surprised today reading this mail thread).  Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,293,0.6597938144329897,0.92,0,0.5,0.25,0.0,0.0
327,258997,260260,"should have been. has already posted it, I hope the fix is fine with you.(& sorry for the typo)","On Friday, April 13, 2018 01:12:39 PM Bartlomiej Zolnierkiewicz wrote:  should have been:  Eduardo: Daniel has already posted it, I hope the fix is fine with you.  (& sorry for the typo)   Best regards, -- Bartlomiej Zolnierkiewicz Samsung R&D Institute Poland Samsung Electronics",technical,Bartlomiej Zolnierkiewicz,b.zolnierkie@samsung.com,1,0,95,0.25773195876288657,0.96,0,0.5,0.25,0.0,0.25
328,258997,260714,Please resend your series with the patches without the warnings.,"On Fri, Apr 13, 2018 at 12:27:17PM +0200, Bartlomiej Zolnierkiewicz wrote:  Please resend your series with the patches without the warnings..  Thanks,  Eduardo",technical,Eduardo Valentin,edubezval@gmail.com,1,0,64,0.1134020618556701,1.0,1,1.0,0.0,0.25,0.0
329,259276,259467,Thanks for your reply :)I agree that usleep_range() here will not much affect the real execution of this driver. But I think usleep_range() can more opportunity for other threads to use the CPU core to schedule during waiting. That is why I detect mdelay() that can be replaced with msleep() orusleep_range().,"On 2018/4/12 0:16, James Bottomley wrote:  Hello, James. Thanks for your reply :)  I agree that usleep_range() here will not much affect the real execution  of this driver.  But I think usleep_range() can more opportunity for other threads to use  the CPU core to schedule during waiting. That is why I detect mdelay() that can be replaced with msleep() or  usleep_range().   Best wishes, Jia-Ju Bai",technical,Jia-Ju Bai,baijiaju1990@gmail.com,0,1,309,1.0,0.6,0,0.0,0.0,0.0,0.0
330,259276,259487,"James is right, You have added all usleep_range() during system boot-uptime. During boot-up system will run as single threaded. Where this change will not make much sense. System first priority is match the exact timing on each and every boot-up.","On Thursday 12 April 2018 07:00 AM, Jia-Ju Bai wrote:  James is right, You have added all usleep_range() during system boot-up  time. During boot-up system will run as single threaded. Where this change will not make much sense. System first priority is match the exact timing on each and every boot-up.  ~arvind",technical,arvindY,arvind.yadav.cs@gmail.com,0,0,246,0.7121212121212122,0.8,0,0.0,0.0,0.0,0.0
331,259679,259710,What an understatement :-) Can't we instead improve the error message and turn this into apr_debug2? Isn't it a reasonable scenario that the user expects this topology information to be present and then ends up without it? Perhaps something like:,"Em Thu, Apr 12, 2018 at 01:47:23PM +0200, Thomas Richter escreveu:  What an understatement :-)    Can't we instead improve the error message and turn this into a pr_debug2? Isn't it a reasonable scenario that the user expects this topology information to be present and then ends up without it?  Perhaps something like:  	pr_debug2(%s: could't read %s, does this arch have topology information?\n"", __func__, path),  - Arnaldo  """,technical,Arnaldo Carvalho de Melo,acme@kernel.org,1,0,246,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
332,259679,259719,"Fine with me, I will provide a version 2....","On 04/12/2018 03:05 PM, Arnaldo Carvalho de Melo wrote:  Fine with me, I will provide a version 2....  --  Thomas Richter, Dept 3303, IBM LTC Boeblingen Germany -- Vorsitzende des Aufsichtsrats: Martina Koederitz  Geschäftsführung: Dirk Wittkopp Sitz der Gesellschaft: Böblingen / Registergericht: Amtsgericht Stuttgart, HRB 243294",technical,Thomas-Mich Richter,tmricht@linux.vnet.ibm.com,0,0,44,0.23404255319148937,1.0,1,0.0,0.0,0.0,0.0
333,261377,261397,"Will do in my next email. Apologies. This must be Thunderbird, I've been trying to coax into doing otherwise. Will send the corrected patch shortly.","On 04/16/2018 10:28 PM, Guenter Roeck wrote: Will do in my next email. Appologies. This must be Thunderbird, I've been trying to coax into doing otherwise. Will send the corrected patch shortly.",technical,Ahsan Hussain,ahsan_hussain@mentor.com,0,1,148,0.9117647058823529,0.75,0,0.0,0.0,0.0,0.0
334,261377,261398,"Best is to set up and use git send-email. Anyway, you can see the result of the corruption at, it appears that patchwork doesn't understand your patch either. Guenter","On Mon, Apr 16, 2018 at 10:48:03PM +0500, Ahsan Hussain wrote:  Best is to set up and use git send-email. Anyway, you can see the result of the corruption at https://patchwork.kernel.org/patch/10343535/, it appears that patchwork doesn't understand your patch either.  Guenter",technical,Guenter Roeck,linux@roeck-us.net,1,0,166,1.0,1.0,1,0.0,0.0,0.0,0.0
335,261603,261674,"I'm not sure how the input is freed for managed devices. Either you don't have to destroy it here, or you also need to destroy it in a release() function. No need to unregister managed input devices.","Hi Xiaotong,  On Tue, Apr 17, 2018 at 11:18:24AM +0800, Baolin Wang wrote:  [snip]   I'm not sure how the input_ff is freed for managed devices.  Either you don't have to destroy it here, or you also need to destroy it in a release() function.   No need to unregister managed input devices.   Cheers, Marcus Folkesson",technical,Marcus Folkesson,marcus.folkesson@gmail.com,1,0,199,1.0,0.75,0,0.0,0.0,0.0,0.0
336,261603,261695,"I checked again, we do not need to destroy it manually. Will remove in next version. You are correct. Will remove these in next version. Thanks for your comments.--","Hi Marcus,  On 17 April 2018 at 15:25, Marcus Folkesson <marcus.folkesson@gmail.com> wrote:  I checked again, we do not need to destroy it manually. Will remove in next version.   You are correct. Will remove these in next version. Thanks for your comments.  --  Baolin.wang Best Regards",technical,Baolin Wang,baolin.wang@linaro.org,1,1,164,0.7777777777777778,1.0,1,0.0,0.0,0.0,0.0
337,261866,263848,So are you saying that multi-slot support is a no go in general or it is only applicable to DW MMC (I really doubt that's a case)?BTW there're other controllers that seem to support multi-slot like Atmel etc.,"Hi Ulf,  On Fri, 2018-04-20 at 09:35 +0200, Ulf Hansson wrote:  So are you saying that multi-slot support is a no go in general or it is only applicable to DW MMC (I really doubt that's a case)?  BTW there're other controllers that seem to support multi-slot like Atmel etc.  -Alexey",technical,Alexey Brodkin,Alexey.Brodkin@synopsys.com,1,0,208,0.1875,0.45454545454545453,0,0.25,0.75,0.0,0.0
338,261866,263915,"In general.Yeah, none of those are working - it just bad attempts to try to make*something* work.","On 20 April 2018 at 09:42, Alexey Brodkin <Alexey.Brodkin@synopsys.com> wrote:  In general.   Yeah, none of those are working - it just bad attempts to try to make *something* work.  Kind regards Uffe",technical,Ulf Hansson,ulf.hansson@linaro.org,1,0,97,0.09166666666666666,0.5454545454545454,0,0.25,0.75,0.0,0.0
339,261866,264284,Previous multi slot implementation was removed as nobody used it and nobody tested it. There are lots of mistakes in previous implementation which are not related to request serialization like lack of slot switch / lack of adding slot id to CIU commands / ets...So obviously it was never tested and never used at real multi slot hardware.In current implementation data transfers and commands to different hosts (slots) are serialized internally in the dw_mmc driver. We have request queue and when .request() is called we add new request to the queue. We take new request from the queue only if the previous one has already finished. So although hosts (slots) have separate locks (mmc_claimrelease_host())the requests to different slots are serialized by driver. Isn't that enough? I'm not very familiar with SD/SDIO/(e)MMC specs so my assumptions might be wrong in that case please correct me .Nevertheless we had to deal somehow with existing hardware which has multislot dw mmc controller and both slots are used...This patch at least shouldn't break anything for current users (which use it in single slot mode) Moreover we tested this dual-slot implementation and don't catch any problems (probably yet) except bus performance decrease in dual-slot mode (which is quite expected).,"Hi Ulf,  On Fri, 2018-04-20 at 09:35 +0200, Ulf Hansson wrote:  Previous multi slot implementation was removed as nobody used it and nobody tested it. There are lots of mistakes in previous implementation which are not related to request serialization like lack of slot switch / lack of adding slot id to CIU commands / ets... So obviously it was never tested and never used at real multi slot hardware.   In current implementation data transfers and commands to different hosts (slots) are serialized internally in the dw_mmc driver. We have request queue and when .request() is called we add new request to the queue. We take new request from the queue only if the previous one has already finished.  So although hosts (slots) have separate locks (mmc_claim|release_host()) the requests to different slots are serialized by driver.  Isn't that enough? I'm not very familiar with SD/SDIO/(e)MMC specs so my assumptions might be wrong in that case please correct me.   Nevertheless we had to deal somehow with existing hardware which has multislot dw mmc controller and both slots are used...  This patch at least shouldn't break anything for current users (which use it in single slot mode)  Moreover we tested this dual-slot implementation and don't catch any problems (probably yet) except bus performance decrease in dual-slot mode (which is quite expected).  --   Eugeniy Paltsev",technical,Eugeniy Paltsev,Eugeniy.Paltsev@synopsys.com,1,1,1285,1.0,0.6363636363636364,0,0.375,0.625,0.0,0.25
340,261866,265746,"That isn't sufficient. The core expects all calls to *any* of the hostops to be serialized for one host. It does so to conform to the specs.For example it may call: ->set_ios()->request()->set_ios()->request()->request() Sorry, but no. Well, that kind of explains your simplistic approach. I would suggest you to study the specs and the behavior of the mmccore a bit more carefully, that should give you a better understanding of the problems. Honestly, I don't think efforts of implementing this is worth it! Even if we would be able to solve the problem from an mmc subsystem point of view, we would still have the I/O scheduling problem to address. To solve that, we would need to be able to configure upper block layer code to run one scheduling instance over multiple block devices","On 20 April 2018 at 17:53, Eugeniy Paltsev <Eugeniy.Paltsev@synopsys.com> wrote:  That isn't sufficient. The core expects all calls to *any* of the host ops to be serialized for one host. It does so to conform to the specs.  For example it may call:  ->set_ios() ->request() ->set_ios() ->request() ->request()   Sorry, but no.   Well, that kind of explains your simplistic approach.  I would suggest you to study the specs and the behavior of the mmc core a bit more carefully, that should give you a better understanding of the problems.   Honestly, I don't think efforts of implementing this is worth it!  Even if we would be able to solve the problem from an mmc subsystem point of view, we would still have the I/O scheduling problem to address. To solve that, we would need to be able to configure upper block layer code to run one scheduling instance over multiple block devices...  Kind regards Uffe",technical,Ulf Hansson,ulf.hansson@linaro.org,1,0,786,0.7291666666666666,0.7272727272727273,0,0.625,0.375,0.25,0.25
341,261866,268009,A bit remark for better understanding: All card settings change are serialized too. These settings are applied after slot switch before execution of new request for this slot. So situations like calling any host_0 ops while another host (host_1) is active are handled by current code. This is example of simultaneous ops calls for both slots,"On Mon, 2018-04-23 at 08:47 +0200, Ulf Hansson wrote:  A bit remark for better understanding:  All card settings change are serialized too. These settings are applied after slot switch before execution of new request for this slot.  So situations like calling any host_0 ops while another host (host_1) is active are handled by current code.  This is example of simultaneous ops calls for both slots:  host (slot) 0    | host (slot) 1 ----------------------------------- h0->set_ios()    |    h1->set_ios() h0->request()    |    h1->request() h0->set_ios()    |    h1->set_ios() h0->request()    |    h1->request() h0->request()    | h0->request()    | h0->request()    |  How it will be serialized in the mmc driver:  h0->set_ios()   // h0 settings save h1->set_ios()   // h1 settings save h0->request()   // apply settings for h0 and do request ------ slot switch to h1 ------ h1->request()   // apply settings for h1 and do request h0->set_ios()   // h0 settings save h1->set_ios()   // h1 settings save ------ slot switch to h0 ------ h0->request()   // apply settings for h0 and do request ------ slot switch to h1 ------ h1->request()   // apply settings for h1 and do request ------ slot switch to h0 ------ h0->request()   // do request (no new settings to apply) h0->request()   // do request (no new settings to apply) h0->request()   // do request (no new settings to apply)  --   Eugeniy Paltsev",technical,Eugeniy Paltsev,Eugeniy.Paltsev@synopsys.com,1,1,341,0.25833333333333336,0.8181818181818182,0,1.0,0.0,0.25,0.0
342,261866,268461,This doesn't work as it would mean violation of the specs in some scenarios. Particular during the card initialization and card power off. Ditto. Etc…,[...]   This doesn't work as it would mean violation of the specs in some scenarios. Particular during the card initialization and card power off.   Ditto. Etc...   Kind regards Uffe,technical,Ulf Hansson,ulf.hansson@linaro.org,1,0,150,0.12083333333333333,0.9090909090909091,0,1.0,0.0,0.0,0.0
343,261866,268645,"HI, Sorry for late. :(Well, I will read the other comments. And reply soon.","Hi,  On 04/17/2018 09:11 PM, Eugeniy Paltsev wrote:  Sorry for late. :( Well, I will read the other comments..and reply soon.   Best Regards, Jaehoon Chung",technical,Jaehoon Chung,jh80.chung@samsung.com,1,0,75,0.0875,1.0,1,1.0,0.0,0.0,0.0
344,266017,266033,"Usually perf would help, but even a simple printk() should suffice to see what's going on there :)We know that there are some cases where the codec / controller communication stalls on the recent Coffee Lake or such platforms. But quite not sure how it happens. Moving the stuff into async just moves something ugly, and it's no fix, per se, if such a long delay itself is unexpected","On Mon, 23 Apr 2018 14:30:36 +0200, Paul Menzel wrote:  Usually perf would help, but even a simple printk() should suffice to see what's going on there :)   We know that there are some cases where the codec / controller communication stalls on the recent Coffee Lake or such platforms. But quite not sure how it happens.  Moving the stuff into async just moves something ugly, and it's no fix, per se, if such a long delay itself is unexpected.   thanks,  Takashi",technical,Takashi Iwai,tiwai@suse.de,1,0,383,1.0,0.4,0,0.0,1.0,0.0,0.0
345,266017,266822,"Indeed.  But even from this result, you can have a rough idea. As you can see, the most of time was spent before ""1"" point, which is the very beginning of azx_probe().  That is, the slowness is not in HD-audio driver probe itself.  Rather it's likely because of parallel probing with other multiple devices.","On Tue, 24 Apr 2018 13:59:58 +0200, Paul Menzel wrote:  Indeed.  But even from this result, you can have a rough idea. As you can see, the most of time was spent before 1"" point, which is the very beginning of azx_probe().  That is, the slowness is not in HD-audio driver probe itself.  Rather it's likely because of parallel probing with other multiple devices.   thanks,  Takashi """,technical,Takashi Iwai,tiwai@suse.de,1,0,307,0.8395061728395061,0.8,0,1.0,0.0,0.0,0.0
346,266017,267060,"I agree. But that also makes it clear, that the probe can be done in async task, doesn't it?","Dear Takashi,   On 04/24/18 14:15, Takashi Iwai wrote:   I agree. But that also makes it clear, that the probe can be done in  async task, doesn’t it?   Kind regards,  Paul",technical,Paul Menzel,pmenzel+alsa-devel@molgen.mpg.de,0,1,92,0.2962962962962963,0.9,0,1.0,0.0,0.0,0.0
347,266017,267073,"Yes, but it's no fix, either.  The probe callback itself doesn't take any long time, but the problem is the stage before that.  By declaring the async probe, you can hide it, but it doesn't mean that the whole issue is solved by that.","On Tue, 24 Apr 2018 16:03:53 +0200, Paul Menzel wrote:  Yes, but it's no fix, either.  The probe callback itself doesn't take any long time, but the problem is the stage before that.  By declaring the async probe, you can hide it, but it doesn't mean that the whole issue is solved by that.   thanks,  Takashi",technical,Takashi Iwai,tiwai@suse.de,1,0,234,0.6790123456790124,1.0,1,1.0,0.0,0.0,0.0
348,266279,266337,"Thanks, I 'll mark this series as rejected at patchwork.linuxtv.org.Please feel free to resubmit any patch if they represent a real threat, adding a corresponding description about the threat scenario at the body of the e-mail. Anytime.","Em Mon, 23 Apr 2018 14:11:02 -0500 Gustavo A. R. Silva"" <gustavo@embeddedor.com> escreveu:   Thanks, I 'll mark this series as rejected at patchwork.linuxtv.org.  Please feel free to resubmit any patch if they represent a real threat, adding a corresponding description about the threat scenario at the body of the e-mail.   Anytime.  Thanks, Mauro""",technical,Mauro Carvalho Chehab,mchehab@kernel.org,1,0,236,0.10199004975124377,0.4,0,0.0,1.0,0.0,0.0
349,266279,266346,Yeah. I got it. Much appreciated. :),"On 04/23/2018 02:17 PM, Mauro Carvalho Chehab wrote:  Yeah. I got it.   Much appreciated. :)  Thanks -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,36,0.02736318407960199,0.425,0,0.0,1.0,0.0,0.0
350,266279,266736,"The intent of that comment is to be provocative, in the sense that people would argue against and point flaws (if any) on my rationale. As I explained when reviewing this patch, I don't care much if an automatic tool is saying that there's a vulnerability at the code, as it could be a false positive. So, what I want at the patch description is a threat analysis explaining how an algorithm is exploited. With regards to Spectre, I never tried to write an exploit myself, nor had to study it in detail in order to mitigate it. So, what I know about it is what I read on a few places. From the places where I read, the boundaries for an array exploit are limited to L1 cache, but, as I said before, I can be wrong on that. It will be great to hear to Peter's comment on that, as he knows a lot more than me about it.","Hi Dan,  Em Tue, 24 Apr 2018 12:35:00 +0300 Dan Carpenter <dan.carpenter@oracle.com> escreveu:   The intent of that comment is to be provocative, in the sense that people would argue against and point flaws (if any) on my rationale.  As I explained when reviewing this patch, I don't care much if an automatic tool is saying that there's a vulnerability at the code, as it could be a false positive. So, what I want at the patch description is a threat analysis explaining how an algorithm is exploited.  With regards to Spectre, I never tried to write an exploit myself, nor had to study it in detail in order to mitigate it. So, what I know about it is what I read on a few places. From the places where I read, the  boundaries for an array exploit are limited to L1 cache, but, as I said before, I can be wrong on that.  It will be great to hear to Peter's comment on that, as he knows a lot more than me about it.     Thanks, Mauro",technical,Mauro Carvalho Chehab,mchehab@kernel.org,1,0,816,0.4601990049751244,0.475,0,0.0,1.0,0.0,0.0
351,266279,266747,"TL,DR: read the papers [1] & [2]I suspect you didn't get the gist of Spectre V1 [1], let me explain: Suppose userspace provides f->index > ARRAY_SIZE(format), and we predict the branch to -EINVAL to not be taken. Then the CPU _WILL_ load (out of bounds) format[f->index] int of->pixel format and continue onwards to use this bogus value, all the way until it figures out the branch was mis-predicted. Once it figures out the mispredict, it will throw away the state and start over at the condition site. So far, so basic. The thing is, is will not (and cannot) throw away all state. Suppose our speculation continues into v4l_fill_fmtdesc() and that switch there is compiled as another array lookup, it will then feed our f->pixel format(which contains random kernel memory) into that array to find the requested descr pointer. Now, imagine userspace having flushed cache on the descr pointer array, having trained the branch predictor to mis-predict the branch (see branchscope paper [2]) and doing that out-of-bounds ioctl().It can then speculative do the out-of-bounds array access, followed by the desc array load, then figure out it was wrong and redo. Then usespace probes which part of the descr[] array is now in cache and from that it can infer the initial out-of-bound value. So while format[] is static and bound, it can read random kernel memory up to format+4g, including your crypto keys. As far as V1 goes, this is actually a fairly solid exploit candidate. No false positive about it. Now kernel policy is to kill any and all speculation on user controlled array indexing such that we don't have to go look for subsequent side channels (the above cache side channel is the one described in the Spectre paper and by far the easiest, but there are other possible side channels) and we simply don't want to worry about it. So even from that pov, the proposed patch is good.","On Tue, Apr 24, 2018 at 12:35:00PM +0300, Dan Carpenter wrote:   TL,DR: read the papers [1] & [2]  I suspect you didn't get the gist of Spectre V1 [1], let me explain:  Suppose userspace provides f->index > ARRAY_SIZE(format), and we predict the branch to -EINVAL to not be taken.  Then the CPU _WILL_ load (out of bounds) format[f->index] into f->pixelformat and continue onwards to use this bogus value, all the way until it figures out the branch was mis-predicted.  Once it figures out the mispredict, it will throw away the state and start over at the condition site. So far, so basic.  The thing is, is will not (and cannot) throw away all state. Suppose our speculation continues into v4l_fill_fmtdesc() and that switch there is compiled as another array lookup, it will then feed our f->pixelformat (which contains random kernel memory) into that array to find the requested descr pointer.  Now, imagine userspace having flushed cache on the descr pointer array, having trained the branch predictor to mis-predict the branch (see branchscope paper [2]) and doing that out-of-bounds ioctl().  It can then speculative do the out-of-bounds array access, followed by the desc array load, then figure out it was wrong and redo.  Then usespace probes which part of the descr[] array is now in cache and from that it can infer the initial out-of-bound value.  So while format[] is static and bound, it can read random kernel memory up to format+4g, including your crypto keys.  As far as V1 goes, this is actually a fairly solid exploit candidate. No false positive about it.  Now kernel policy is to kill any and all speculation on user controlled array indexing such that we don't have to go look for subsequent side channels (the above cache side channel is the one described in the Spectre paper and by far the easiest, but there are other possible side channels) and we simply don't want to worry about it.  So even from that pov, the proposed patch is good.   [1] https://spectreattack.com/spectre.pdf [2] www.cs.ucr.edu/~nael/pubs/asplos18.pdf",technical,Peter Zijlstra,peterz@infradead.org,1,0,1886,1.0,0.5,0,0.0,1.0,0.0,0.0
352,266279,266764,"Just had a better look at v4l_fill_fmtdesc() and actually read the comment. The code cannot be compiled as a array because it is big and sparse. But the log(n) condition tree is a prime candidate for the branchscope side-channel, which would be able to reconstruct a significant number of bits of the original value. A denser tree gives more bits etc.","On Tue, Apr 24, 2018 at 12:36:09PM +0200, Peter Zijlstra wrote:  Just had a better look at v4l_fill_fmtdesc() and actually read the comment. The code cannot be compiled as a array because it is big and sparse. But the log(n) condition tree is a prime candidate for the branchscope side-channel, which would be able to reconstruct a significant number of bits of the original value. A denser tree gives more bits etc.",technical,Peter Zijlstra,peterz@infradead.org,1,0,351,0.1791044776119403,0.525,0,0.0,1.0,0.0,0.0
353,266279,291742,"Just to let you know, I was running smatch during the weekend and the tool is still reporting all these Spectre media warnings (and a lot more)","On 05/17/2018 01:08 PM, Gustavo A. R. Silva wrote:  Hi Mauro,  Just to let you know, I was running smatch during the weekend and the  tool is still reporting all these Spectre media warnings (and a lot more):  https://patchwork.linuxtv.org/project/linux-media/list/?submitter=7277  Thanks -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,143,0.07462686567164178,1.0,1,1.0,0.0,0.1111111111111111,0.0
354,266279,288776,"Yep. I get the same warning multiple times. BTW, Mauro, you sent a patch to fix an spectre v1 issue in this file yesterday, but it seems there is another instance of the same issue some lines above","On 05/17/2018 07:13 AM, Mauro Carvalho Chehab wrote:  Yep. I get the same warning multiple times.  BTW, Mauro, you sent a patch to fix an spectre v1 issue in this file  yesterday: dvb_ca_en50221.c:1480, but it seems there is another instance  of the same issue some lines above:  diff --git a/drivers/media/dvb-core/dvb_ca_en50221.c  b/drivers/media/dvb-core/dvb_ca_en50221.c index 1310526..7edd9db 100644 --- a/drivers/media/dvb-core/dvb_ca_en50221.c +++ b/drivers/media/dvb-core/dvb_ca_en50221.c @@ -1398,6 +1398,7 @@ static int dvb_ca_en50221_io_do_ioctl(struct file  *file,                   info->type = CA_CI_LINK,                  info->flags = 0, +               slot = array_index_nospec(slot, ca->slot_count + 1),                  sl = &ca->slot_info[slot],                  if ((sl->slot_state != DVB_CA_SLOTSTATE_NONE) &&                      (sl->slot_state != DVB_CA_SLOTSTATE_INVALID)) {   Thanks -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,197,0.10696517412935323,0.975,0,0.8888888888888888,0.1111111111111111,0.0,0.1111111111111111
355,266279,288471,It seems that something is broken... getting this error/warning,"Em Thu, 17 May 2018 08:43:24 -0300 Mauro Carvalho Chehab <mchehab+samsung@kernel.org> escreveu:   It seems that something is broken... getting this error/warning:  DBD::SQLite::db do failed: unrecognized token: 'end + strlen("" "" at /devel/smatch/smatch_scripts/../smatch_data/db/fill_db_sql.pl line 32, <WARNS> line 2938054.   Thanks, Mauro""",technical,Mauro Carvalho Chehab,mchehab+samsung@kernel.org,1,0,63,0.024875621890547265,0.95,0,0.8518518518518519,0.14814814814814814,0.0,0.0
356,266279,288447,Never mind. Found it using grep. I'm running this:,"Em Thu, 17 May 2018 08:34:40 -0300 Mauro Carvalho Chehab <mchehab+samsung@kernel.org> escreveu:   Never mind. Found it using grep. I'm running this:  	make allyesconfig 	/devel/smatch/smatch_scripts/build_kernel_data.sh 	/devel/smatch/smatch_scripts/build_kernel_data.sh      Thanks, Mauro",technical,Mauro Carvalho Chehab,mchehab+samsung@kernel.org,1,0,50,0.03233830845771144,0.925,0,0.8518518518518519,0.14814814814814814,0.0,0.0
357,266279,288442,"How? Here, I just pull from your git tree and do a ""make"". At most, make clean, make. That makes more sense to me, as the same pattern is used by almost all VIDIOC_ENUM_foo ioctls.","Em Thu, 17 May 2018 05:36:03 -0500 Gustavo A. R. Silva"" <gustavo@embeddedor.com> escreveu:    How? Here, I just pull from your git tree and do a ""make"". At most, make clean, make.   That makes more sense to me, as the same pattern is used by almost all VIDIOC_ENUM_foo ioctls.  Thanks, Mauro""",technical,Mauro Carvalho Chehab,mchehab+samsung@kernel.org,1,0,180,0.11194029850746269,0.9,0,0.8518518518518519,0.14814814814814814,0.0,0.0
358,266279,288379,"Interesting, I've rebuild the db twice and now I get a total of 75 Spectre warnings in drivers/media","On 05/16/2018 08:14 PM, Gustavo A. R. Silva wrote:  Interesting, I've rebuild the db twice and now I get a total of 75  Spectre warnings in drivers/media  -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,100,0.04975124378109453,0.875,0,0.8518518518518519,0.14814814814814814,0.0,0.0
359,266279,288040,"After rebuilding the db (once), these are all the Spectre media warnings I get warn: potential spectre issue I just want to double check if you are getting the same output. In case you are getting the same, then what Mauro commented about these issues: being resolved by commit seems to be correct.","On 05/15/2018 02:39 PM, Dan Carpenter wrote:  Hi Dan,  After rebuilding the db (once), these are all the Spectre media warnings  I get:  drivers/media/pci/ddbridge/ddbridge-core.c:233 ddb_redirect() warn:  potential spectre issue 'ddbs' drivers/media/pci/ddbridge/ddbridge-core.c:243 ddb_redirect() warn:  potential spectre issue 'pdev->port' drivers/media/pci/ddbridge/ddbridge-core.c:252 ddb_redirect() warn:  potential spectre issue 'idev->input' drivers/media/dvb-core/dvb_ca_en50221.c:1400  dvb_ca_en50221_io_do_ioctl() warn: potential spectre issue  'ca->slot_info' (local cap) drivers/media/dvb-core/dvb_ca_en50221.c:1479 dvb_ca_en50221_io_write()  warn: potential spectre issue 'ca->slot_info' (local cap) drivers/media/dvb-core/dvb_net.c:252 handle_one_ule_extension() warn:  potential spectre issue 'p->ule_next_hdr' drivers/media/dvb-core/dvb_net.c:1483 dvb_net_do_ioctl() warn: potential  spectre issue 'dvbnet->device' (local cap) drivers/media/cec/cec-pin-error-inj.c:170 cec_pin_error_inj_parse_line()  warn: potential spectre issue 'pin->error_inj_args'  I just want to double check if you are getting the same output. In case  you are getting the same, then what Mauro commented about these issues:  https://patchwork.linuxtv.org/project/linux-media/list/?submitter=7277  being resolved by commit 3ad3b7a2ebaefae37a7eafed0779324987ca5e56 seems  to be correct.  Thanks -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,298,0.1517412935323383,0.85,0,0.8518518518518519,0.14814814814814814,0.0,0.0
360,266279,287636,"Yeah, I was thinking that is would be harder to clean this up on smatch. I proposed a patch to the ML that simplifies the logic, making easier for both humans and Smatch to better understand how  the arrays are indexed.","Em Wed, 16 May 2018 16:11:08 +0300 Dan Carpenter <dan.carpenter@oracle.com> escreveu:   Yeah, I was thinking that is would be harder to clean this up on smatch. I proposed a patch to the ML that simplifies the logic, making easier for both humans and Smatch to better understand how the arrays are indexed.   Thanks!  Regards, Mauro",technical,Mauro Carvalho Chehab,mchehab+samsung@kernel.org,1,0,219,0.11194029850746269,0.825,0,0.8148148148148148,0.18518518518518517,0.0,0.0
361,266279,287611,"It's hard to silence this because Smatch stores the current user controlled range list, not what it was initially.  I wrote all this code to detect bounds checking errors, so there wasn't any need to save the range list before the bounds check.  Since ""op"" is a u32, I can't even go by the type of the index....Oh...  Huh.  This is a bug in smatch.  That line looks like: Smatch see the ntohs() and marks everything inside it as untrusted network data.  I'll fix this.","On Tue, May 15, 2018 at 04:00:33PM -0300, Mauro Carvalho Chehab wrote:  It's hard to silence this because Smatch stores the current user controlled range list, not what it was initially.  I wrote all this code to detect bounds checking errors, so there wasn't any need to save the range list before the bounds check.  Since op"" is a u32, I can't even go by the type of the index....   Oh...  Huh.  This is a bug in smatch.  That line looks like:  	p->ule_sndu_type = ntohs(*(__be16 *)(p->ule_next_hdr + ((p->ule_dbit ? 2 : 3) * ETH_ALEN))),  Smatch see the ntohs() and marks everything inside it as untrusted network data.  I'll fix this.  regards, dan carpenter""",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,468,0.263681592039801,0.8,0,0.8148148148148148,0.18518518518518517,0.0,0.0
362,266279,286997,You'd need to rebuild the db (possibly twice but definitely once),"On Tue, May 15, 2018 at 12:29:10PM -0500, Gustavo A. R. Silva wrote:  You'd need to rebuild the db (possibly twice but definitely once).  regards, dan carpenter",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,65,0.03482587064676617,0.775,0,0.8148148148148148,0.18518518518518517,0.0,0.0
363,266279,286987,"Yeah, that's the same I'm getting from media upstream. This one seems a false positive, as the index var is u8 and the array has 256 elements, as the userspace input from 'op' is initialized with: u8 v, u32 op, if (!kstrtou8(token, 0, &v))  op = this one seems a real issue to me. Sent a patch for  failed to see what's wrong here, or if this is exploited. Here, I'm at this commit: commit  db: make call_implies rows unique Plus the diff below (that won't affect Spectre errors)","Em Tue, 15 May 2018 12:29:10 -0500 Gustavo A. R. Silva"" <gustavo@embeddedor.com> escreveu:   Yeah, that's the same I'm getting from media upstream.   This one seems a false positive, as the index var is u8 and the array has 256 elements, as the userspace input from 'op' is  initialized with:  	u8 v, 	u32 op,  	if (!kstrtou8(token, 0, &v)) 		op = v,   This one seems a real issue to me. Sent a patch for it.   I failed to see what's wrong here, or if this is exploited.    Here, I'm at this commit:  commit 2f66d40cbf57b0bd581fe75447d2a8625fc7bb1d (origin/master, origin/HEAD) Author: Dan Carpenter <dan.carpenter@oracle.com> Date:   Tue May 15 16:35:20 2018 +0300      db: make call_implies rows unique  Plus the diff below (that won't affect Spectre errors).  Regards, Mauro   diff --git a/check_missing_break.c b/check_missing_break.c index 434b7283fc94..5bba6e919521 100644 --- a/check_missing_break.c +++ b/check_missing_break.c @@ -73,7 +73,7 @@ static void print_missing_break(struct expression *expr)  	last_print_expr = get_switch_expr(),    	name = expr_to_var(expr), -	sm_msg(""warn: missing break? reassigning '%s'"", name), +//	sm_msg(""warn: missing break? reassigning '%s'"", name),  	free_string(name),  }   diff --git a/smatch_flow.c b/smatch_flow.c index dc0e78824370..cd72a9ded375 100644 --- a/smatch_flow.c +++ b/smatch_flow.c @@ -1005,8 +1005,7 @@ void __split_stmt(struct statement *stmt)    		__bail_on_rest_of_function = 1,  		final_pass = 1, -		sm_msg(""Function too hairy.  Giving up. %lu seconds"", -		       stop.tv_sec - fn_start_time.tv_sec), +		sm_msg(""__split_smt: function too hairy.  Giving up.""),  		fake_a_return(),  		final_pass = 0,  /* turn off sm_msg() from here */  		return, diff --git a/smatch_implied.c b/smatch_implied.c index 3588816361fe..f3ccd4b6d79e 100644 --- a/smatch_implied.c +++ b/smatch_implied.c @@ -594,7 +594,7 @@ static void separate_and_filter(struct sm_state *sm, int comparison, struct rang    	gettimeofday(&time_after, NULL),  	sec = time_after.tv_sec - time_before.tv_sec, -	if (sec > 20) { +	if (sec > 60) {  		sm->nr_children = 4000,  		sm_msg(""Function too hairy.  Ignoring implications after %d seconds."", sec),  	} diff --git a/smatch_slist.c b/smatch_slist.c index e1eb1b999b2a..2f8ba34a4b9a 100644 --- a/smatch_slist.c +++ b/smatch_slist.c @@ -237,12 +237,14 @@ char *alloc_sname(const char *str)  int out_of_memory(void)  {  	/* -	 * I decided to use 50M here based on trial and error. +	 * I decided to use 6GB here based on trial and error.  	 * It works out OK for the kernel and so it should work  	 * for most other projects as well.  	 */ -	if (sm_state_counter * sizeof(struct sm_state) >= 100000000) +	if (sm_state_counter * sizeof(struct sm_state) >= 6000000000) { +		sm_msg(""Out of memory""),  		return 1, +	}  	return 0,  }       Thanks, Mauro""",technical,Mauro Carvalho Chehab,mchehab+samsung@kernel.org,1,0,479,0.2960199004975124,0.75,0,0.8148148148148148,0.18518518518518517,0.0,0.0
364,266279,286923,"Thanks, Mauro. These are all the Spectre media issues I see smatch is reporting in I pulled the latest changes from the smatch repository and compiled it. I'm running smatch now. Is this the latest version? I wonder if there is anything I might be missing.","On 05/15/2018 09:16 AM, Dan Carpenter wrote:  Thanks, Mauro.   Dan,  These are all the Spectre media issues I see smatch is reporting in  linux-next-20180515:  drivers/media/cec/cec-pin-error-inj.c:170 cec_pin_error_inj_parse_line()  warn: potential spectre issue 'pin->error_inj_args' drivers/media/dvb-core/dvb_ca_en50221.c:1479 dvb_ca_en50221_io_write()  warn: potential spectre issue 'ca->slot_info' (local cap) drivers/media/dvb-core/dvb_net.c:252 handle_one_ule_extension() warn:  potential spectre issue 'p->ule_next_hdr'  I pulled the latest changes from the smatch repository and compiled it.  I'm running smatch v0.5.0-4459-g2f66d40 now. Is this the latest version?  I wonder if there is anything I might be missing.  Thanks -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,256,0.1318407960199005,0.725,0,0.7777777777777778,0.18518518518518517,0.0,0.0
365,266279,286753,Possibly...  There was an ancient bug in Smatch's function pointer handling.  I just pushed a fix for it now so the warning is there on linux-next.,"On Tue, May 15, 2018 at 08:59:53AM -0300, Mauro Carvalho Chehab wrote:  Possibly...  There was an ancient bug in Smatch's function pointer handling.  I just pushed a fix for it now so the warning is there on linux-next.  regards, dan carpenter",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,147,0.07462686567164178,0.7,0,0.7777777777777778,0.2222222222222222,0.0,0.0
366,266279,286660,"There was no direct fix for it, but maybe this patch has something to do with the smatch error report cleanup: commit with stub functions    This change removes  and adds stub functions where    needed using the DEFINE_V4L_STUB_FUNC macro. This fixes indirect call    mismatches with Control-Flow Integrity, caused by calling standard    ioctls using a function pointer that doesn't match the function type.","Em Mon, 14 May 2018 22:31:37 -0500 Gustavo A. R. Silva"" <gustavo@embeddedor.com> escreveu:   There was no direct fix for it, but maybe this patch has something to do with the smatch error report cleanup:  commit 3ad3b7a2ebaefae37a7eafed0779324987ca5e56 Author: Sami Tolvanen <samitolvanen@google.com> Date:   Tue May 8 13:56:12 2018 -0400      media: v4l2-ioctl: replace IOCTL_INFO_STD with stub functions          This change removes IOCTL_INFO_STD and adds stub functions where     needed using the DEFINE_V4L_STUB_FUNC macro. This fixes indirect call     mismatches with Control-Flow Integrity, caused by calling standard     ioctls using a function pointer that doesn't match the function type.          Signed-off-by: Sami Tolvanen <samitolvanen@google.com>     Signed-off-by: Hans Verkuil <hansverk@cisco.com>     Signed-off-by: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>       Thanks, Mauro""",technical,Mauro Carvalho Chehab,mchehab+samsung@kernel.org,1,0,407,0.16666666666666666,0.675,0,0.7777777777777778,0.2222222222222222,0.0,0.0
367,266279,286284,I'm curious about how you finally resolved to handle these issues.I noticed Smatch is no longer reporting them.,"Hi Mauro,  On 04/26/2018 06:42 PM, Mauro Carvalho Chehab wrote:   I'm curious about how you finally resolved to handle these issues.  I noticed Smatch is no longer reporting them.  Thanks -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,111,0.04975124378109453,0.65,0,0.7777777777777778,0.2222222222222222,0.6666666666666666,0.0
368,266279,269258,"Well, the issues will be there everywhere on all media drivers. I marked your patches because I need to study it carefully, after Peter's explanations. My plan is to do it next week. Still not sure if the approach you took is the best one or not. As I said, one possibility is to change the way v4l2-core handles VIDIOC_ENUM_foo ioctls, but that would be make harder to -stable backports.I need a weekend to sleep on it.","Em Thu, 26 Apr 2018 16:41:56 -0500 Gustavo A. R. Silva"" <gustavo@embeddedor.com> escreveu:   Yes.   Well, the issues will be there everywhere on all media drivers.  I marked your patches because I need to study it carefully, after Peter's explanations. My plan is to do it next week. Still not sure if the approach you took is the best one or not.  As I said, one possibility is to change the way v4l2-core handles  VIDIOC_ENUM_foo ioctls, but that would be make harder to -stable backports.  I need a weekend to sleep on it.     Thanks, Mauro""",technical,Mauro Carvalho Chehab,mchehab+samsung@kernel.org,1,0,420,0.21641791044776118,0.625,0,0.1111111111111111,0.8888888888888888,0.0,0.6666666666666666
369,266279,267349,"Sadly no, the whole crux is the array bound check itself. You could maybe pass around the array size to the core code and then do something like: in generic code, and have all the drivers use f->index as usual, but even that would be quite a bit of code churn I guess.","On Tue, Apr 24, 2018 at 02:47:55PM -0300, Mauro Carvalho Chehab wrote:  Sadly no, the whole crux is the array bound check itself. You could maybe pass around the array size to the core code and then do something like:  	if (f->index >= f->array_size) 		return -EINVAL,  	f->index = nospec_array_index(f->index, f->array_size),  in generic code, and have all the drivers use f->index as usual, but even that would be quite a bit of code churn I guess.",technical,Peter Zijlstra,peterz@infradead.org,1,0,268,0.1517412935323383,0.575,0,0.037037037037037035,0.9629629629629629,0.0,0.07407407407407407
370,266279,266706,I saw your comment on LWN.  You argue on LWN that since the format array is static the CPU won't speculatively read past the L1 cache? I don't know if that's true.  It should be easy enough to filter out the reads into static arrays.  Peter do you know the answer here?,"Hi Mauro,  I saw your comment on LWN.  You argue on LWN that since the format array is static the CPU won't speculatively read past the L1 cache?  I don't know if that's true.  It should be easy enough to filter out the reads into static arrays.  Peter do you know the answer here?  regards, dan carpenter  On Mon, Apr 23, 2018 at 03:24:55PM -0300, Mauro Carvalho Chehab wrote:",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,269,0.14925373134328357,0.45,0,0.0,1.0,0.0,0.0
371,266279,269189,"I noticed you changed the status of this series from rejected to new. Also, there are other similar issues in media/pci/I can write proper patches for all of them if you agree those are not False Positives","Hi Mauro,  On 04/23/2018 02:17 PM, Mauro Carvalho Chehab wrote:  I noticed you changed the status of this series from rejected to new.  Also, there are other similar issues in media/pci/  I can write proper patches for all of them if you agree those are not  False Positives:  diff --git a/drivers/media/pci/cx18/cx18-ioctl.c  b/drivers/media/pci/cx18/cx18-ioctl.c index 80b902b..63f4388 100644 --- a/drivers/media/pci/cx18/cx18-ioctl.c +++ b/drivers/media/pci/cx18/cx18-ioctl.c @@ -36,6 +36,8 @@   #include <media/tveeprom.h>   #include <media/v4l2-event.h>  +#include <linux/nospec.h> +   u16 cx18_service2vbi(int type)   {          switch (type) { @@ -488,8 +490,9 @@ static int cx18_enum_fmt_vid_cap(struct file *file,  void *fh,                  },          },  -       if (fmt->index > ARRAY_SIZE(formats) - 1) +       if (fmt->index >= ARRAY_SIZE(formats))                  return -EINVAL, +       fmt->index = array_index_nospec(fmt->index, ARRAY_SIZE(formats)),          *fmt = formats[fmt->index],          return 0,   } diff --git a/drivers/media/pci/saa7134/saa7134-video.c  b/drivers/media/pci/saa7134/saa7134-video.c index 1a50ec9..d93cf09 100644 --- a/drivers/media/pci/saa7134/saa7134-video.c +++ b/drivers/media/pci/saa7134/saa7134-video.c @@ -30,6 +30,8 @@   #include <media/v4l2-event.h>   #include <media/i2c/saa6588.h>  +#include <linux/nospec.h> +   /* ------------------------------------------------------------------ */    unsigned int video_debug, @@ -1819,6 +1821,8 @@ static int saa7134_enum_fmt_vid_cap(struct file  *file, void  *priv,          if (f->index >= FORMATS)                  return -EINVAL,  +       f->index = array_index_nospec(f->index, FORMATS), +          strlcpy(f->description, formats[f->index].name,                  sizeof(f->description)),  diff --git a/drivers/media/pci/tw68/tw68-video.c  b/drivers/media/pci/tw68/tw68-video.c index 8c1f4a0..a6cfb4b 100644 --- a/drivers/media/pci/tw68/tw68-video.c   #include <media/v4l2-event.h>   #include <media/videobuf2-dma-sg.h>  +#include <linux/nospec.h> +   #include tw68.h""   #include ""tw68-reg.h""  @@ -789,6 +791,8 @@ static int tw68_enum_fmt_vid_cap(struct file *file,  void  *priv,          if (f->index >= FORMATS)                  return -EINVAL,  +       f->index = array_index_nospec(f->index, FORMATS), +          strlcpy(f->description, formats[f->index].name,                  sizeof(f->description)),  diff --git a/drivers/media/pci/tw686x/tw686x-video.c  b/drivers/media/pci/tw686x/tw686x-video.c index c3fafa9..281d722 100644 --- a/drivers/media/pci/tw686x/tw686x-video.c +++ b/drivers/media/pci/tw686x/tw686x-video.c @@ -25,6 +25,8 @@   #include ""tw686x.h""   #include ""tw686x-regs.h""  +#include <linux/nospec.h> +   #define TW686X_INPUTS_PER_CH           4   #define TW686X_VIDEO_WIDTH             720   #define TW686X_VIDEO_HEIGHT(id)                ((id & V4L2_STD_525_60)  ? 480 : 576) @@ -981,6 +983,7 @@ static int tw686x_enum_fmt_vid_cap(struct file  *file, void *priv,   {          if (f->index >= ARRAY_SIZE(formats))                  return -EINVAL, +       f->index = array_index_nospec(f->index, ARRAY_SIZE(formats)),          f->pixelformat = formats[f->index].fourcc,          return 0,   }   Thanks -- Gustavo""",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,205,0.09701492537313433,0.6,0,0.1111111111111111,0.8888888888888888,0.07407407407407407,0.0
372,269898,270650,"These files are generated from other files, so any future update to them will bring back the spelling mistakes, so, as Andi pointed out  previously, we better fix the spellings in the original files, maintained by Intel.","Em Fri, Apr 27, 2018 at 07:52:06PM +0100, Colin King escreveu:  These files are generated from other files, so any future update to them will bring back the spelling mistakes, so, as Andi pointed out previously, we better fix the spellings in the original files, maintained by Intel.  - Arnaldo",technical,Arnaldo Carvalho de Melo,acme@kernel.org,1,0,220,1.0,0.6666666666666666,0,1.0,0.0,1.0,0.0
373,269898,270715,Thanks. I forwarded to the right people.-,"On Mon, Apr 30, 2018 at 10:52:56AM -0300, Arnaldo Carvalho de Melo wrote:  Thanks. I forwarded to the right people. -Andi",technical,Andi Kleen,andi@firstfloor.org,0,0,41,0.18604651162790697,1.0,1,1.0,0.0,0.0,0.0
374,270330,280545,"Looks like part of the problem was introduced in that patch, part well predated it (BDU).As you way, this needs to be a bit different to take into account the change to regmap.  We'll need to have that upstream before we look at a back port.  One element inline surprises me and needs further explanation. Why drop the enable setting?  Seems that we want to do this 'as well', if the device was previous enabled.","On Mon, 30 Apr 2018 12:25:46 +0800 Shrirang Bagul <shrirang.bagul@canonical.com> wrote:  Looks like part of the problem was introduced in that patch, part well predated it (BDU).  As you way, this needs to be a bit different to take into account the change to regmap.  We'll need to have that upstream before we look at a back port.  One element inline surprises me and needs further explanation.    Why drop the enable setting?  Seems that we want to do this 'as well', if the device was previous enabled.",technical,Jonathan Cameron,jic23@kernel.org,1,0,412,1.0,0.6666666666666666,0,1.0,0.0,1.0,0.0
375,270330,280612,"I have sent a patch based on iio-for-4.17b [1], Lorenzo and I are still discussing our findings. It's not just the CTRL1 reg, but also the AV_CONF(0x10) rightish loses it's contents coming out of suspend. Yes, will cover this in v2.","On Sun, 2018-05-06 at 17:19 +0100, Jonathan Cameron wrote: I have sent a patch based on iio-for-4.17b [1], Lorenzo and I are still  discussing our findings. It's not just the CTRL1 reg, but also the AV_CONF(0x10) reg. which loses it's contents coming out of suspend.  [1] https://marc.info/?l=linux-iio&m=152534455701742&w=2 Yes, will cover this in v2.",technical,Shrirang Bagul,shrirang.bagul@canonical.com,0,1,232,0.6136363636363636,1.0,1,1.0,0.0,0.0,0.0
376,275347,275348,Greg doesn't accept patches without a commit message.  Just say which tool warned for example.,"On Mon, May 14, 2018 at 10:57:25PM +0200, Samuel Thibault wrote:  Greg doesn't accept patches without a commit message.  Just say which tool warned for example.  regards, dan carpenter  _______________________________________________ devel mailing list devel@linuxdriverproject.org http://driverdev.linuxdriverproject.org/mailman/listinfo/driverdev-devel",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,94,1.0,1.0,1,0.0,0.0,0.0,0.0
377,278837,286959,"Is your divider using the CLK_DIVIDER_ROUND_CLOSEST flag? When divider_get_val() is called, the incoming rate should already be rounded to something that is what the actual frequency would be so the rounding here hopefully doesn't matter. Maybe something is lost in translation though, because we do round_rate() to figure out some divider based on a requested rate, and then we return that rate we calculated that we can achieve to the framework. In turn, that rate comes back into this divider_get_val()function to get the divider out of the rate again. It's sort of annoying how circular this is. Perhaps your example can show this call sequence and the associated math gymnastics that go on and then I'll be convinced that we need to update this part of the code.","Quoting Tejas Patel (2018-05-03 02:25:35)  Is your divider using the CLK_DIVIDER_ROUND_CLOSEST flag? When divider_get_val() is called, the incoming rate should already be rounded to something that is what the actual frequency would be so the rounding here hopefully doesn't matter.  Maybe something is lost in translation though, because we do round_rate() to figure out some divider based on a requested rate, and then we return that rate we calculated that we can achieve to the framework. In turn, that rate comes back into this divider_get_val() function to get the divider out of the rate again. It's sort of annoying how circular this is. Perhaps your example can show this call sequence and the associated math gymnastics that go on and then I'll be convinced that we need to update this part of the code.",technical,Stephen Boyd,sboyd@kernel.org,1,0,767,1.0,1.0,1,1.0,0.0,1.0,0.0
378,279573,280009,"gen_tunnel() may not ever return a value larger than is a u8 and therefore can never take on a value outside of the range of 0 to 255.I'm not applying this patch, sorry.","From: Wenwen Wang <wang6495@umn.edu> Date: Fri,  4 May 2018 02:05:05 -0500   gen_tunnel() may not ever return a value larger than 255.  data->tgenerator is a u8 and therefore can never take on a value outside of the range of 0 to 255.  I'm not applying this patch, sorry.",technical,David Miller,davem@davemloft.net,1,0,169,1.0,1.0,1,0.0,0.0,0.0,0.0
379,281311,282919,"I only read the code and find this possible data race. It is not found in real driver execution. I am not sure of it, so I use ""may"" and ""possible"" here.","On 2018/5/9 4:17, Rafael J. Wysocki wrote:  I only read the code and find this possible data race. It is not found in real driver execution. I am not sure of it, so I use may"" and ""possible"" here.   Best wishes, Jia-Ju Bai""",technical,Jia-Ju Bai,baijiaju1990@gmail.com,0,1,153,1.0,0.75,0,1.0,0.0,0.0,0.0
380,281783,363947,"Guys, any comments? That is a kinda useful feature, in worst case only some of memory could get corrupted instead of trashing the whole memory. In my experience with T20/30, the interrupt handling latency is low and blocking happens immediately after the first page fault.","On Tuesday, 8 May 2018 19:58:41 MSK Dmitry Osipenko wrote:  Guys, any comments? That is a kinda useful feature, in worst case only some of  memory could get corrupted instead of trashing the whole memory. In my  experience with T20/30, the interrupt handling latency is low and blocking  happens immediately after the first page fault.",technical,Dmitry Osipenko,digetx@gmail.com,1,1,272,0.8793103448275862,0.5,0,0.9886363636363636,0.0,0.9886363636363636,0.0
381,281783,364118,"One potential issue is with host1x clients where userspace processes can submit jobs with invalid memory accesses (addresses not mapped to IOMMU). If when such a failure happens, we disable the DMA for the whole host1x client, unrelated userspace processes may see failures even though there is no problem with their jobs.","One potential issue is with host1x clients where userspace processes can  submit jobs with invalid memory accesses (addresses not mapped to  IOMMU). If when such a failure happens, we disable the DMA for the whole  host1x client, unrelated userspace processes may see failures even  though there is no problem with their jobs.  Mikko  On 08/04/2018 02:53 PM, Dmitry Osipenko wrote:",technical,Mikko Perttunen,cyndis@kapsi.fi,0,0,322,1.0,0.75,0,1.0,0.0,0.0,0.0
382,281783,364135,"Good point, I'll take a look at partial resurrection of patch [0]. Anyway I think it's still better to fail even the unrelated jobs, rather than to have corrupted memory.","On Sunday, 5 August 2018 03:11:57 MSK Mikko Perttunen wrote:  Good point, I'll take a look at partial resurrection of patch [0]. Anyway I  think it's still better to fail even the unrelated jobs, rather than to have  corrupted memory.  [0] https://github.com/grate-driver/linux/commit/ c0351a9d01491af2b2fefc162de1c2e4fcfaa94c#diff-20226be476307cb7ec9f60e26a9c30deR378",technical,Dmitry Osipenko,digetx@gmail.com,1,1,170,0.6551724137931034,1.0,1,1.0,0.0,0.0,0.0
383,289026,289740,"Tsukada-san,I am not familiar with memcg so can't comment about whether the patchset is the right way to solve the problem outlined in the cover letter but had a couple of comments about this patch. Please move this before Patch 1/7. This is to prevent wrong accounting of pages to memcg for size != PMD_SIZE. Instead of replacing calls to hpage_nr_pages(), is it possible to modify it to do the calculation?","Tsukada-san,  I am not familiar with memcg so can't comment about whether the patchset is the right way to solve the problem outlined in the cover letter but had a couple of comments about this patch.  TSUKADA Koutaro <tsukada@ascade.co.jp> writes:   Please move this before Patch 1/7. This is to prevent wrong accounting of pages to memcg for size != PMD_SIZE.   Instead of replacing calls to hpage_nr_pages(), is it possible to modify it to do the calculation?  Thanks, Punit",technical,Punit Agrawal,punit.agrawal@arm.com,0,0,408,0.1544256120527307,0.32142857142857145,0,0.0,1.0,0.0,0.0
384,289026,289745,I just noticed that the default state is off so the change isn't enabled until the sysfs node is exposed in the next patch. Please ignore this comment. One below still applies.,Punit Agrawal <punit.agrawal@arm.com> writes:   I just noticed that the default state is off so the change isn't enabled until the sysfs node is exposed in the next patch. Please ignore this comment.  One below still applies. ,technical,Punit Agrawal,punit.agrawal@arm.com,0,0,176,0.06779661016949153,0.35714285714285715,0,0.0,1.0,0.0,0.3333333333333333
385,289026,290447,"Thank you for review my code and please just call me Tsukada.I think it is possible to modify the inside of itself rather than replacing the call to hpage_nr_pages().Inferring from the processing that hpage_nr_pages() desires, I thought that the definition of hpage_nr_pages() could be moved outside the CONFIG_TRANSPARENT_HUGEPAGE. It seems that THP and HugeTLBfs can be handled correctly because compound_order() is judged by seeing whether itis PageHead or not. Also, I would like to use compound_order() inside hpage_nr_pages(), but since huge_mm.h is included before mm.h where compound_order() is defined, move hpage_nr_pages to mm.h. Instead of patch 3/7, are the following patches implementing what you intended?","On 2018/05/19 2:51, Punit Agrawal wrote:  Thank you for review my code and please just call me Tsukada.  I think it is possible to modify the inside of itself rather than replacing the call to hpage_nr_pages().  Inferring from the processing that hpage_nr_pages() desires, I thought that the definition of hpage_nr_pages() could be moved outside the CONFIG_TRANSPARENT_HUGEPAGE. It seems that THP and HugeTLBfs can be handled correctly because compound_order() is judged by seeing whether it is PageHead or not.  Also, I would like to use compound_order() inside hpage_nr_pages(), but since huge_mm.h is included before mm.h where compound_order() is defined, move hpage_nr_pages to mm.h.  Instead of patch 3/7, are the following patches implementing what you intended?  diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h index a8a1262..1186ab7 100644 --- a/include/linux/huge_mm.h +++ b/include/linux/huge_mm.h @@ -204,12 +204,6 @@ static inline spinlock_t *pud_trans_huge_lock(pud_t *pud,   	else   		return NULL,   } -static inline int hpage_nr_pages(struct page *page) -{ -	if (unlikely(PageTransHuge(page))) -		return HPAGE_PMD_NR, -	return 1, -}    struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,   		pmd_t *pmd, int flags), @@ -254,8 +248,6 @@ static inline bool thp_migration_supported(void)   #define HPAGE_PUD_MASK ({ BUILD_BUG(), 0, })   #define HPAGE_PUD_SIZE ({ BUILD_BUG(), 0, })  -#define hpage_nr_pages(x) 1 -   static inline bool transparent_hugepage_enabled(struct vm_area_struct *vma)   {   	return false, diff --git a/include/linux/mm.h b/include/linux/mm.h index 1ac1f06..082f2ee 100644 --- a/include/linux/mm.h +++ b/include/linux/mm.h @@ -673,6 +673,12 @@ static inline unsigned int compound_order(struct page *page)   	return page[1].compound_order,   }  +static inline int hpage_nr_pages(struct page *page) +{ +	VM_BUG_ON_PAGE(PageTail(page), page), +	return (1 << compound_order(page)), +} +   static inline void set_compound_order(struct page *page, unsigned int order)   {   	page[1].compound_order = order,  --  Thanks, Tsukada",technical,TSUKADA Koutaro,tsukada@ascade.co.jp,0,1,720,0.24293785310734464,0.39285714285714285,0,0.3333333333333333,0.5,0.3333333333333333,0.0
386,289026,291682,"I was staring at memcg code to better understand your changes and had the below thought. Instead of tying the surplus huge page charging control per-hstate,could the control be made per-memcg? This can be done by introducing a per-memory controller file in sysfs(memory.charge_surplus_hugepages?) that indicates whether surplus hugepages are to be charged to the controller and forms part of the total limit. IIUC, the limit already accounts for page and swap cache pages. This would allow the control to be enabled per-cgroup and also keep the userspace control interface in one place. As said earlier, I'm not familiar with memcg so the above might not be a feasible but think it'll lead to a more coherent user interface. Hopefully, more knowledgeable folks on the thread can chime in.","Hi Tsukada,  I was staring at memcg code to better understand your changes and had the below thought.  TSUKADA Koutaro <tsukada@ascade.co.jp> writes:  [...]   Instead of tying the surplus huge page charging control per-hstate, could the control be made per-memcg?  This can be done by introducing a per-memory controller file in sysfs (memory.charge_surplus_hugepages?) that indicates whether surplus hugepages are to be charged to the controller and forms part of the total limit. IIUC, the limit already accounts for page and swap cache pages.  This would allow the control to be enabled per-cgroup and also keep the userspace control interface in one place.  As said earlier, I'm not familiar with memcg so the above might not be a feasible but think it'll lead to a more coherent user interface. Hopefully, more knowledgeable folks on the thread can chime in.  Thanks, Punit ",technical,Punit Agrawal,punit.agrawal@arm.com,0,0,788,0.2749529190207156,0.42857142857142855,0,0.5,0.5,0.0,0.0
387,289026,291684,That looks a lot better. Thanks for giving it a go.,TSUKADA Koutaro <tsukada@ascade.co.jp> writes:   That looks a lot better. Thanks for giving it a go.,technical,Punit Agrawal,punit.agrawal@arm.com,0,0,51,0.02448210922787194,0.4642857142857143,0,0.5,0.5,0.0,0.0
388,289026,292507,"Thank you for good advise. As you mentioned, it is better to be able to control by per-memcg. After organizing my thoughts, I will develop the next version patch-set that can solve issues and challenge again.","Hi Punit,  On 2018/05/21 23:52, Punit Agrawal wrote:  Thank you for good advise. As you mentioned, it is better to be able to control by per-memcg. After organizing my thoughts, I will develop the next version patch-set that can solve issues and challenge again.  Thanks, Tsukada",technical,TSUKADA Koutaro,tsukada@ascade.co.jp,0,1,208,0.07721280602636535,0.5357142857142857,0,0.6666666666666666,0.3333333333333333,0.0,0.0
389,289026,292515,"Thank you for your feedback. That makes sense, surplus hugepages are charged to both memcg and hugetlb cgroup, which may be contrary to cgroup design philosophy. Based on the above advice, I have considered the following improvements, what do you think about? The 'charge_surplus_hugepages' of v2 patch-set was an option to switch ""whether to charge memcg in addition to hugetlb cgroup"", but it will be abolished. Instead, change to ""switch only to memcg instead of hugetlb cgroup"" option. This is called 'surplus_charge_to_memcg'.The surplus_charge_to_memcg option is created in per hugetlb cgroup. If it is false(default), charge destination cgroup of various page types is the same as the current kernel version. If it become true, hugetlbc group stops accounting for surplus hugepages, and memcg starts accounting instead. A table showing which cgroups are charged: I stared at the commit log of mm/hugetlb_cgroup.c, but it did not seem to have specially considered of surplus hugepages. Later, I will send a mail to hugetlb cgroup's committer to ask about surplus hugepages charge specifications.","On 2018/05/22 3:07, Mike Kravetz wrote:  Thank you for your feedback. That makes sense, surplus hugepages are charged to both memcg and hugetlb cgroup, which may be contrary to cgroup design philosophy.  Based on the above advice, I have considered the following improvements, what do you think about?  The 'charge_surplus_hugepages' of v2 patch-set was an option to switch whether to charge memcg in addition to hugetlb cgroup"", but it will be abolished. Instead, change to ""switch only to memcg instead of hugetlb cgroup"" option. This is called 'surplus_charge_to_memcg'.  The surplus_charge_to_memcg option is created in per hugetlb cgroup. If it is false(default), charge destination cgroup of various page types is the same as the current kernel version. If it become true, hugetlb cgroup stops accounting for surplus hugepages, and memcg starts accounting instead.  A table showing which cgroups are charged:  page types          | current  v2(off)  v2(on)   v3(off)   v3(on) ------------------------------------------------------------------- normal + THP        |       m       m       m         m        m hugetlb(persistent) |       h       h       h         h        h hugetlb(surplus)    |       h       h     m+h         h        m -------------------------------------------------------------------   v2: charge_surplus_hugepages option  v3: next version, surplus_charge_to_memcg option   m: memory cgroup   h: hugetlb cgroup   I stared at the commit log of mm/hugetlb_cgroup.c, but it did not seem to have specially considered of surplus hugepages. Later, I will send a mail to hugetlb cgroup's committer to ask about surplus hugepages charge specifications.  --  Thanks, Tsukada""",technical,TSUKADA Koutaro,tsukada@ascade.co.jp,0,1,1101,0.3766478342749529,0.5714285714285714,0,0.6666666666666666,0.3333333333333333,0.0,0.0
390,289026,292930,"I went back and looked at surplus huge page allocation.  Previously, I made a statement that the hugetlb controller accounts for surplus huge pages. Turns out that may not be 100% correct. Thanks to Michal, all surplus huge page allocation is performed via the alloc_surplus_huge_page() routine.  This will ultimately call into the buddy allocator without any cgroup charges.  Calls to alloc_surplus_huge_page are made from:- alloc_huge_page() when allocating a huge page to a mapping/file.  In this  case, appropriate calls to the hugetlb controller are in place.  So, any  limits are enforced here.- gather_surplus_pages() when allocating and setting aside 'reserved' huge  pages. No accounting is performed here.  Do note that in this case the  allocated huge pages are not assigned to the mapping/file.  Even though  'reserved', they are deposited into the global pool and also counted as  'free'.  When these reserved pages are ultimately used to populate a  file/mapping, the code path goes through alloc_huge_page() where appropriate  calls to the hugetlb controller are in place. So, the bottom line is that surplus huge pages are not accounted for when they are allocated as 'reserves'.  It is not until these reserves are actually used that accounting limits are checked.  This 'seems' to align with general allocation of huge pages within the pool.  No accounting is done until they are actually allocated to a mapping/file.","On 05/22/2018 06:04 AM, TSUKADA Koutaro wrote:  I went back and looked at surplus huge page allocation.  Previously, I made a statement that the hugetlb controller accounts for surplus huge pages. Turns out that may not be 100% correct.  Thanks to Michal, all surplus huge page allocation is performed via the alloc_surplus_huge_page() routine.  This will ultimately call into the buddy allocator without any cgroup charges.  Calls to alloc_surplus_huge_page are made from: - alloc_huge_page() when allocating a huge page to a mapping/file.  In this   case, appropriate calls to the hugetlb controller are in place.  So, any   limits are enforced here. - gather_surplus_pages() when allocating and setting aside 'reserved' huge   pages. No accounting is performed here.  Do note that in this case the   allocated huge pages are not assigned to the mapping/file.  Even though   'reserved', they are deposited into the global pool and also counted as   'free'.  When these reserved pages are ultimately used to populate a   file/mapping, the code path goes through alloc_huge_page() where appropriate   calls to the hugetlb controller are in place.  So, the bottom line is that surplus huge pages are not accounted for when they are allocated as 'reserves'.  It is not until these reserves are actually used that accounting limits are checked.  This 'seems' to align with general allocation of huge pages within the pool.  No accounting is done until they are actually allocated to a mapping/file.  --  Mike Kravetz",technical,Mike Kravetz,mike.kravetz@oracle.com,1,0,1435,0.4896421845574388,0.6785714285714286,0,0.6666666666666666,0.3333333333333333,0.0,0.16666666666666666
391,289026,294072,"Thank you for your time. I do not know if it is really a strong use case, but I will explain my motive in detail. English is not my native language, so please pardon my poor English. I am one of the developers for software that managing the resource used from user job at HPC-Cluster with Linux. The resource is memory mainly. The HPC-Cluster may be shared by multiple people and used. Therefore, the memory used by each user must be strictly controlled, otherwise the user's job will runaway, not only will it hamper the other users, it will crash the entire system in OOM. Some users of HPC are very nervous about performance. Jobs are executed while synchronizing with MPI communication using multiple compute nodes. Since CPU wait time will occur when synchronizing, they want to minimize the variation in execution time at each node to reduce waiting times as much as possible. We call this variation a noise.THP does not guarantee to use the Huge Page, but may use the normal page. This mechanism is one cause of variation (noise).The users who know this mechanism will be hesitant to use THP. However, the users also know the benefits of the Huge Page's TLB hit rate performance, and the Huge Page seems to be attractive. It seems natural that these users are interested in HugeTLBfs, I do not know at all whether it is the right approach or not. At the very least, our HPC system is pursuing high versatility and we have to consider whether we can provide it if users want to use HugeTLBfs. In order to use HugeTLBfs we need to create a persistent pool, but in our use case sharing nodes, it would be impossible to create, delete or resize the pool. One of the answers I have reached is to use HugeTLBfs by overcommitting without creating a pool (this is the surplus hugepage).Surplus hugepages is hugetlb page, but I think at least that consuming buddy pool is a decisive difference from hugetlb page of persistent pool. If nr_overcommit_hugepages is assumed to be infinite, allocating pages for surplus hugepages from buddy pool is all unlimited even if being limited by memcg. In extreme cases, overcommitment will allow users to exhaust the entire memory of the system. Of course, this can be prevented by the hugetlb cgroup, but even if we set the limit for memcg and hugetlb cgroup respectively, as I asked in the first mail(set limit to 10GB), the control will not work. I thought I could charge surplus hugepages to memcg, but maybe I did not have enough knowledge about memcg. I would like to reply to another mail for details. Actually, I am opposed to the 64KB page, but the proposal to change the page size is expected to be dismissed as a problem.","On 2018/05/22 22:51, Michal Hocko wrote:  Thank you for your time.  I do not know if it is really a strong use case, but I will explain my motive in detail. English is not my native language, so please pardon my poor English.  I am one of the developers for software that managing the resource used from user job at HPC-Cluster with Linux. The resource is memory mainly. The HPC-Cluster may be shared by multiple people and used. Therefore, the memory used by each user must be strictly controlled, otherwise the user's job will runaway, not only will it hamper the other users, it will crash the entire system in OOM.  Some users of HPC are very nervous about performance. Jobs are executed while synchronizing with MPI communication using multiple compute nodes. Since CPU wait time will occur when synchronizing, they want to minimize the variation in execution time at each node to reduce waiting times as much as possible. We call this variation a noise.  THP does not guarantee to use the Huge Page, but may use the normal page. This mechanism is one cause of variation(noise).  The users who know this mechanism will be hesitant to use THP. However, the users also know the benefits of the Huge Page's TLB hit rate performance, and the Huge Page seems to be attractive. It seems natural that these users are interested in HugeTLBfs, I do not know at all whether it is the right approach or not.  At the very least, our HPC system is pursuing high versatility and we have to consider whether we can provide it if users want to use HugeTLBfs.  In order to use HugeTLBfs we need to create a persistent pool, but in our use case sharing nodes, it would be impossible to create, delete or resize the pool.  One of the answers I have reached is to use HugeTLBfs by overcommitting without creating a pool(this is the surplus hugepage).  Surplus hugepages is hugetlb page, but I think at least that consuming buddy pool is a decisive difference from hugetlb page of persistent pool. If nr_overcommit_hugepages is assumed to be infinite, allocating pages for surplus hugepages from buddy pool is all unlimited even if being limited by memcg. In extreme cases, overcommitment will allow users to exhaust the entire memory of the system. Of course, this can be prevented by the hugetlb cgroup, but even if we set the limit for memcg and hugetlb cgroup respectively, as I asked in the first mail(set limit to 10GB), the control will not work.  I thought I could charge surplus hugepages to memcg, but maybe I did not have enough knowledge about memcg. I would like to reply to another mail for details.   Actually, I am opposed to the 64KB page, but the proposal to change the page size is expected to be dismissed as a problem.  --  Thanks, Tsukada",technical,TSUKADA Koutaro,tsukada@ascade.co.jp,0,1,2668,1.0,0.7142857142857143,0,0.8333333333333334,0.0,0.16666666666666666,0.0
392,289026,294205,"Because they have already allocated from the buddy allocator so the end result is very same. But this is simply not correct. Surplus pages are fluid. If you increase the hugetlb size they will become regular persistent hugetlb pages. Not really. Memcg accounts primarily for reclaimable memory. We do account for some non-reclaimable slabs but the life time should be at least bound to a process life time. Otherwise the memcg oom killer behavior is not guaranteed to unclutter the situation. Hugetlb pages are simply persistent. Well, to be completely honest tmpfs pages have a similar problem but lacking the swap space for them is kind a configuration bug.Ohh, it is very much interested. The primary goal of memcg is to enforce the limit. How are you going to do that in an absence of the reclaimable memory? And quite a lot of it because hugetlb pages usually consume a lot of memory. It does change when ou change the hugetlb pool size, migrate pages between per-numa pools (have a look at adjust_pool_surplus).","On Thu 24-05-18 13:39:59, TSUKADA Koutaro wrote: [...]  Because they have already allocated from the buddy allocator so the end result is very same.   But this is simply not correct. Surplus pages are fluid. If you increase the hugetlb size they will become regular persistent hugetlb pages.    Not really. Memcg accounts primarily for reclaimable memory. We do account for some non-reclaimable slabs but the life time should be at least bound to a process life time. Otherwise the memcg oom killer behavior is not guaranteed to unclutter the situation. Hugetlb pages are simply persistent. Well, to be completely honest tmpfs pages have a similar problem but lacking the swap space for them is kinda configuration bug.   Ohh, it is very much interested. The primary goal of memcg is to enforce the limit. How are you going to do that in an absence of the reclaimable memory? And quite a lot of it because hugetlb pages usually consume a lot of memory.   It does change when ou change the hugetlb pool size, migrate pages between per-numa pools (have a look at adjust_pool_surplus). --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,1017,0.3615819209039548,0.7857142857142857,0,1.0,0.0,0.0,0.0
393,289026,294210,"Sure, asking for guarantee makes hugetlb pages attractive. But nothing is really for free, especially any resource _guarantee_, and you have to pay an additional configuration price usually. Why? I can see this would be quite a PITA but not really impossible. Not really, you can specify how much you can overcommit hugetlb pages","On Thu 24-05-18 13:26:12, TSUKADA Koutaro wrote: [...]  Sure, asking for guarantee makes hugetlb pages attractive. But nothing is really for free, especially any resource _guarantee_, and you have to pay an additional configuration price usually.    Why? I can see this would be quite a PITA but not really impossible.   Not really, you can specify how much you can overcommit hugetlb pages.  --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,329,0.1167608286252354,0.8214285714285714,0,1.0,0.0,0.0,0.0
394,289026,271212,"I really can not understand what's wrong with this. That page is obviously released before being added to the persistent pool, and at that time it is uncharged from memcg to which the task belongs(This assumes my patch-set).After that, the same page obtained from the pool is not surplus hugepage so it will not be charged to memcg again. Absolutely you are saying the right thing, but, for example, can mlock(2)edpages be swapped out by reclaim?(What is the difference between mlock(2)edpages and hugetlb page?) Simply kill any of the tasks belonging to that memcg. Maybe, no one wants reclaim at the time of account of with surplus hugepages.[...]As I looked at, what kind of fatal problem is caused by charging surplus hugepages to memcg by just manipulating counter of statistical information?","On 2018/05/24 17:20, Michal Hocko wrote:  I really can not understand what's wrong with this. That page is obviously released before being added to the persistent pool, and at that time it is uncharged from memcg to which the task belongs(This assumes my patch-set). After that, the same page obtained from the pool is not surplus hugepage so it will not be charged to memcg again.   Absolutely you are saying the right thing, but, for example, can mlock(2)ed pages be swapped out by reclaim?(What is the difference between mlock(2)ed pages and hugetlb page?)   Simply kill any of the tasks belonging to that memcg. Maybe, no one wants reclaim at the time of account of with surplus hugepages.  [...]  As I looked at, what kind of fatal problem is caused by charging surplus hugepages to memcg by just manipulating counter of statistical information?  --  Thanks, Tsukada",technical,TSUKADA Koutaro,tsukada@ascade.co.jp,0,1,797,0.3088512241054614,0.8571428571428571,0,1.0,0.0,0.0,0.0
395,289026,294522,"I do not see anything like that. adjust_pool_surplus is simply and accounting thing. At least the last time I've checked. Maybe your patchset handles that? No mlocked pages cannot be reclaimed and that is why we restrict them to a relatively small amount . But that will not release the hugetlb memory, does it? Fatal? Not sure. It simply tries to add an alien memory to the memcg concept so I would presume an unexpected behavior (e.g. not being able to reclaim memcg or, over reclaim, trashing etc.).","On Thu 24-05-18 21:58:49, TSUKADA Koutaro wrote:  I do not see anything like that. adjust_pool_surplus is simply and accounting thing. At least the last time I've checked. Maybe your patchset handles that?    No mlocked pages cannot be reclaimed and that is why we restrict them to a relatively small amount.    But that will not release the hugetlb memory, does it?    Fatal? Not sure. It simply tries to add an alien memory to the memcg concept so I would pressume an unexpected behavior (e.g. not being able to reclaim memcg or, over reclaim, trashing etc.). --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,502,0.1977401129943503,0.8928571428571429,0,1.0,0.0,0.0,0.0
396,289026,271211,"Yes. If do not support multiple size hugetlb pages such as x86, because number of pages between THP and hugetlb is same, the failure rate of obtaining a compound page is same, as you said. I think that what you say is absolutely right. I understand the superiority of THP, but there are scenes where k hugepaged occupies cpu due to page fragmentation. Instead of overcommit, setup a persistent pool once, I think that hugetlb can be superior, such as memory allocation performance exceeding THP. I will try to find a good way to use hugetlb page. I sincerely thank you for your help.","On 2018/05/25 2:45, Mike Kravetz wrote: [...]  [...]  Yes. If do not support multiple size hugetlb pages such as x86, because number of pages between THP and hugetlb is same, the failure rate of obtaining a compound page is same, as you said.   I think that what you say is absolutely right.   I understand the superiority of THP, but there are scenes where khugepaged occupies cpu due to page fragmentation. Instead of overcommit, setup a persistent pool once, I think that hugetlb can be superior, such as memory allocation performance exceeding THP. I will try to find a good way to use hugetlb page.  I sincerely thank you for your help.  --  Thanks, Tsukada",technical,TSUKADA Koutaro,tsukada@ascade.co.jp,0,1,583,0.2222222222222222,1.0,1,1.0,0.0,0.0,0.0
397,289026,294650,"Note.  You do not want to use THP because ""THP does not guarantee"". Using hugetlbfs overcommit also does not provide a guarantee.  Without doing much research, I would say the failure rate for obtaining a hugepage via THP and hugetlbfs overcommit is about the same.  The most difficult issue in both cases will be obtaining a ""huge page"" number of pages from the buddy allocator.I really do not think hugetlbfs overcommit will provide any benefit over THP for your use case.  Also, new user space code is required to ""fall back"" to normal pages in the case of hugetlbfs page allocation failure.  This is not needed in the THP case.","On 05/23/2018 09:26 PM, TSUKADA Koutaro wrote:  Note.  You do not want to use THP because THP does not guarantee"".   Using hugetlbfs overcommit also does not provide a guarantee.  Without doing much research, I would say the failure rate for obtaining a huge page via THP and hugetlbfs overcommit is about the same.  The most difficult issue in both cases will be obtaining a ""huge page"" number of pages from the buddy allocator.  I really do not think hugetlbfs overcommit will provide any benefit over THP for your use case.  Also, new user space code is required to ""fall back"" to normal pages in the case of hugetlbfs page allocation failure.  This is not needed in the THP case. --  Mike Kravetz""",technical,Mike Kravetz,mike.kravetz@oracle.com,1,0,631,0.23540489642184556,0.9285714285714286,0,1.0,0.0,0.0,0.0
398,289431,271305,"This is inconsistent with the ext2 and xfs implementations ...I'm worried this is too much complexity to push down to the filesystems. When should errors get reported through the return value, when should they be reported through the errseq_t?Can we hide all of this?  Maybe ext4 could do: (we'd need to make calling errseq_set(x, 0) be a no-op instead of an error) and then the caller is the one who takes care of calling errseq_check_and_advance() so we don't have to pass 'since' into each filesystem.","On Fri, May 18, 2018 at 08:34:11AM -0400, Jeff Layton wrote:  This is inconsistent with the ext2 and xfs implementations ...  I'm worried this is too much complexity to push down to the filesystems. When should errors get reported through the return value, when should they be reported through the errseq_t?  Can we hide all of this?  Maybe ext4 could do:  	errseq_set(&sb->s_wb_err, __sync_blockdev(sb->s_bdev, wait)), 	return ret,  (we'd need to make calling errseq_set(x, 0) be a no-op instead of an error)  and then the caller is the one who takes care of calling errseq_check_and_advance() so we don't have to pass 'since' into each filesystem.",technical,Matthew Wilcox,willy@infradead.org,1,0,504,1.0,0.7222222222222222,0,0.0,1.0,0.0,0.0
399,289431,271303,"Thanks, I wasn't sure about xfs. I'll drop this hunk.FWIW, I think pushing this call down into the sync_fs routines is still probably the right thing to do, regardless of the state of the later patches.I patterned the name after the call_mmap (and now-defunct call_fsync) helpers. I'll rename it and change it to be non-inclined.","On Fri, 2018-05-18 at 08:56 -0700, Christoph Hellwig wrote:  Thanks, I wasn't sure about xfs. I'll drop this hunk.  FWIW, I think pushing this call down into the sync_fs routines is still probably the right thing to do, regardless of the state of the later patches.   I patterned the name after the call_mmap (and now-defunct call_fsync) helpers. I'll rename it and change it to be non-inlined.  Thanks, --  Jeff Layton <jlayton@kernel.org>",technical,Jeff Layton,jlayton@kernel.org,1,1,329,0.6285714285714286,0.8888888888888888,0,0.0,1.0,0.0,1.0
400,289431,292008,"An earlier patch that pushed this down into the sync_fs routines. We call this today for all filesystems, and I wasn't sure about xfs.Christoph already pointed out that it's not needed so it's removed from my current branch.Ok, sounds good. I'll fix that too.FWIW, we'll actually want to advance the cursor even if xfs_log_force returns an error to ensure that we don't end up reporting errors twice, but that's simple enough to do.","On Tue, 2018-05-22 at 09:01 +1000, Dave Chinner wrote:  An earlier patch that pushed this down into the sync_fs routines. We call this today for all filesystems, and I wasn't sure about xfs.  Christoph already pointed out that it's not needed so it's removed from my current branch.    Ok, sounds good. I'll fix that too.  FWIW, we'll actually want to advance the cursor even if xfs_log_force returns an error to ensure that we don't end up reporting errors twice, but that's simple enough to do.  Thanks! --  Jeff Layton <jlayton@kernel.org>",technical,Jeff Layton,jlayton@kernel.org,1,1,432,0.8285714285714286,1.0,1,1.0,0.0,0.0,0.0
401,289431,271302,"XFS never uses the block device mapping for anything, so this is not needed. The proper name for this would be vfs_sync_fs.  And I don't think it warrants an inline."," XFS never uses the block device mapping for anything, so this is not needed.   The proper name for this would be vfs_sync_fs.  And I don't think it warrants an inline.",technical,Christoph Hellwig,hch@infradead.org,1,0,165,0.3333333333333333,0.7777777777777778,0,0.0,1.0,0.0,0.0
402,291816,292254,fs/udf/unicode.c also uses char2uni and uni2char but evades your SMPLpatch. So you'll need to fix that up manually. Also note that I have some changes to that area queued in my tree.,"On Mon 21-05-18 14:36:03, Gabriel Krisman Bertazi wrote:  fs/udf/unicode.c also uses char2uni and uni2char but evades your SMPL patch. So you'll need to fix that up manually. Also note that I have some changes to that area queued in my tree.  								Honza  --  Jan Kara <jack@suse.com> SUSE Labs, CR",technical,Jan Kara,jack@suse.cz,1,0,182,0.631578947368421,0.8947368421052632,0,0.0,0.8888888888888888,0.0,0.0
403,291816,292257,"Ah, OK, now I see that you've handled that in patch 3. Sorry for the noise.","On Tue 22-05-18 10:37:53, Jan Kara wrote:  Ah, OK, now I see that you've handled that in patch 3. Sorry for the noise.  --  Jan Kara <jack@suse.com> SUSE Labs, CR",technical,Jan Kara,jack@suse.cz,1,0,75,0.3684210526315789,0.9473684210526315,0,0.0,0.8888888888888888,0.0,0.8888888888888888
404,291816,301580,"Thank you for the patch! Yet something to improve:[auto build test ERROR on linus/master][also build test ERROR on v4.17-rc7][cannot apply to next-20180530][if your patch is applied to the wrong git tree, please drop us a note to help improve the system]","Hi Olaf,  Thank you for the patch! Yet something to improve:  [auto build test ERROR on linus/master] [also build test ERROR on v4.17-rc7] [cannot apply to next-20180530] [if your patch is applied to the wrong git tree, please drop us a note to help improve the system]  url:    https://github.com/0day-ci/linux/commits/Gabriel-Krisman-Bertazi/NLS-refactor-and-UTF-8-normalization/20180522-234546 config: openrisc-allmodconfig (attached as .config) compiler: or1k-linux-gcc (GCC) 6.0.0 20160327 (experimental) reproduce:         wget https://raw.githubusercontent.com/intel/lkp-tests/master/sbin/make.cross -O ~/bin/make.cross         chmod +x ~/bin/make.cross         # save the attached .config to linux build tree         make.cross ARCH=openrisc   All errors (new ones prefixed by >>):     make[3]: Target '__build' not remade because of errors.  --- 0-DAY kernel test infrastructure                Open Source Technology Center https://lists.01.org/pipermail/kbuild-all                   Intel Corporation",technical,kbuild test robot,lkp@intel.com,0,0,254,1.0,1.0,1,1.0,0.0,0.8888888888888888,0.0
405,294520,294550,Hmm.  Don't we also need to cover suspend-to-idle?,"On Thu, May 24, 2018 at 4:24 PM, Sebastian Andrzej Siewior <bigeasy@linutronix.de> wrote:  Hmm.  Don't we also need to cover suspend-to-idle?",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,50,0.5238095238095238,0.5,0,0.0,0.0,0.0,0.0
406,294520,294573,"Well, if you agree with the approach then I would look into it.","On 2018-05-24 17:07:16 [+0200], Rafael J. Wysocki wrote:  Well, if you agree with the approach then I would look into it.  Sebastian",technical,Sebastian Andrzej Siewior,bigeasy@linutronix.de,0,1,63,0.7142857142857143,0.75,0,0.0,0.0,0.0,0.0
407,294520,294578,"As long as the SYSTEM_SUSPEND system state is defined unambiguously, I don't have a problem with doing this.","On Thu, May 24, 2018 at 5:45 PM, Sebastian Andrzej Siewior <bigeasy@linutronix.de> wrote:  As long as the SYSTEM_SUSPEND system state is defined unambiguously, I don't have a problem with doing this.",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,108,1.0,1.0,1,0.0,0.0,0.0,0.0
408,295726,295756,Should be fixed in my for-next branch,"On Fri, May 25, 2018 at 11:09:46PM +0200, Arnd Bergmann wrote:  Should be fixed in my for-next branch:  https://git.kernel.org/pub/scm/linux/kernel/git/wsa/linux.git/commit/drivers/i2c/busses/i2c-i801.c?h=i2c/for-next&id=4b2f9bd5e39fb47011074c9a26b64b616acc18f0",technical,Wolfram Sang,wsa@the-dreams.de,1,0,37,1.0,1.0,1,0.0,0.0,0.0,0.0
409,297430,297921,"The help text above should be indented (as below), with one tab + 2 spaces.","On 05/28/2018 06:29 AM, Ladvine D Almeida wrote:  The help text above should be indented (as below), with one tab + 2 spaces.    --  ~Randy",technical,Randy Dunlap,rdunlap@infradead.org,0,0,75,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
410,297430,298385,"Sure, I will send v2 patch with the corrected indentation.","On Monday 28 May 2018 04:49 PM, Randy Dunlap wrote:    Sure, I will send v2 patch with the corrected indentation.  Best Regards, Ladvine",technical,Ladvine D Almeida,Ladvine.DAlmeida@synopsys.com,0,1,58,0.631578947368421,1.0,1,0.0,0.0,0.0,0.0
411,297951,299228,"Thanks, I've picked this up.","On Mon, May 28 2018 at 11:54am -0400, Arnd Bergmann <arnd@arndb.de> wrote:   Thanks, I've picked this up.",technical,Mike Snitzer,snitzer@redhat.com,1,0,28,0.1,0.3333333333333333,0,0.14285714285714285,0.8571428571428571,0.14285714285714285,0.0
412,297951,300869,"__builtin_expect returns always long, see the GCC documentation (it used to return int in very old gcc versions such as 2.96).I think this is a bug in the macro __branch_check__. The variable ______r should be long, but it is int. This bug may cause misbehavior of other kernel parts (i.e. truncation of long value to int), so it should be fixed in __branch_check__ - not in dm-write cache.","On Mon, 28 May 2018, Arnd Bergmann wrote:   __builtin_expect returns always long, see the GCC documentation (it used  to return int in very old gcc versions such as 2.96).  I think this is a bug in the macro __branch_check__. The variable ______r  should be long, but it is int. This bug may cause misbehavior of other  kernel parts (i.e. truncation of long value to int), so it should be fixed  in __branch_check__ - not in dm-writecache.  Mikulas",technical,Mikulas Patocka,mpatocka@redhat.com,1,0,390,1.0,0.5,0,0.14285714285714285,0.7142857142857143,0.0,0.0
413,297951,304811,"Nice catch. I'm curious to what that bug was. Anyway, I can pull this in my tree and test it.","On Wed, 30 May 2018 08:19:22 -0400 (EDT) Mikulas Patocka <mpatocka@redhat.com> wrote:   Nice catch.   I'm curious to what that bug was.   Anyway, I can pull this in my tree and test it.  -- Steve",technical,Steven Rostedt,rostedt@goodmis.org,1,0,93,0.3125,0.8333333333333334,0,1.0,0.0,0.7142857142857143,0.0
414,297951,304812,2 sinks 3 LED strings.  How do you know which LED string is which and what bank it belongs to when setting the brightness.  Each Bank has a separate register for brightness control.,"On Mon, 4 Jun 2018, Steven Rostedt wrote:   printk(%ld"", writecache_has_error(wc))  ... and writecache_has_error was defined as #define writecache_has_error(wc)	(unlikely(READ_ONCE((wc)->error)))  Mikulas""",technical,Mikulas Patocka,mpatocka@redhat.com,1,0,181,0.45,1.0,1,1.0,0.0,0.0,0.0
415,298390,299552,Eric points out this wont initialize the rest of the dest if src if less than size.,"On Mon, May 28, 2018 at 11:37 PM, Nick Desaulniers <nick.desaulniers@gmail.com> wrote:  Eric points out this wont initialize the rest of the dest if src if less than size.",technical,Nick Desaulniers,nick.desaulniers@gmail.com,1,1,83,1.0,1.0,1,0.0,0.0,0.0,0.0
416,303621,304146,"Hi,This should be the very first line.And this is redundant with the SPDX header. These two pinctrl property aren't needed. Each step should increase the perceived brightness by roughly 1/Nth, N being the number of steps. Usually PWM backlights don't work like that. You can drop these two pinctrl properties as well And all these nodes. Thanks!","Hi,  On Sat, Jun 02, 2018 at 05:03:13PM +0100, Bob Ham wrote:  This should be the very first line.   And this is redundant with the SPDX header.   These two pinctrl property aren't needed.   Each step should increase the perceived brightness by roughly 1/Nth, N being the number of steps. Usually PWM backlights don't work like that.   You can drop these two pinctrl properties as well   And all these nodes.  Thanks! Maxime  --  Maxime Ripard, Bootlin (formerly Free Electrons) Embedded Linux and Kernel engineering https://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,345,0.37777777777777777,0.2857142857142857,0,0.1,0.9,0.1,0.0
417,303621,304654,"The X11 license notice states explicitly that the notice has to be included in the file.  Wouldn't removing it be a violation of the license?FYI, this was copied from another .dts file.  All of the other brightness-levels settings in sun{4,5,7}i .dts files follow similar patterns","On 04/06/18 09:13, Maxime Ripard wrote:     The X11 license notice states explicitly that the notice has to be included in the file.  Wouldn't removing it be a violation of the license?    FYI, this was copied from another .dts file.  All of the other brightness-levels settings in sun{4,5,7}i .dts files follow similar patterns:  sun4i-a10-dserve-dsrv9703c.dts:               brightness-levels = <0 10 20 30 40 50 60 70 80 90 100>, sun4i-a10-inet1.dts:          brightness-levels = <0 10 20 30 40 50 60 70 80 90 100>, sun4i-a10-pov-protab2-ips9.dts:               brightness-levels = <0 10 20 30 40 50 60 70 80 90 100>, sun5i-a13-empire-electronix-d709.dts:         brightness-levels = <0 10 20 30 40 50 60 70 80 90 100>, sun5i-a13-utoo-p66.dts:       brightness-levels = <0 30 40 50 60 70 80 90 100>, sun5i-gr8-evb.dts:            brightness-levels = <0 10 20 30 40 50 60 70 80 90 100>, sun7i-a20-wexler-tab7200.dts:         brightness-levels = <0 10 20 30 40 50 60 70 80 90 100>,  I'll take the brightness-levels from sun8i-a83t-tbs-a711.dts which follows a more appropriate pattern:  sun8i-a83t-tbs-a711.dts:              brightness-levels = <0 1 2 4 8 16 32 64 128 255>,   Thanks,  Bob  --  Bob Ham <rah@settrans.net>  for (,,) { ++pancakes, }",technical,Bob Ham,rah@settrans.net,0,1,280,0.3055555555555556,0.42857142857142855,0,0.2,0.8,0.0,0.0
418,303621,305798,"Well, the top bit that I quoted above says that the licenses refer to only that one file in particular and not the project as a whole.  Then the X11 license states that the notice can't be removed from 'this software and associated documentation files (the ""Software"")' which would seem to refer to the single file.  Therefore, removing the notice from the single file and replacing it with an SPDX header would seem to violate the license.It's a fine point but it makes me nervous.  I originally based my .dtson sun4i-a10-inet1.dts.  I've CC'd the original copyright holder, Hansde Goede.  Hans, are you willing to give permission for the license notice to be replaced with just an SPDX header indicating the dual licensing? While we're at it, there are a number of other files with the same license text.  Hans, are you prepared to give permission for your other license notices to be replaced with SPDX headers?","On 05/06/18 15:50, Maxime Ripard wrote:  Well, the top bit that I quoted above says that the licenses refer to only that one file in particular and not the project as a whole.  Then the X11 license states that the notice can't be removed from 'this software and associated documentation files (the Software"")' which would seem to refer to the single file.  Therefore, removing the notice from the single file and replacing it with an SPDX header would seem to violate the license.  It's a fine point but it makes me nervous.  I originally based my .dts on sun4i-a10-inet1.dts.  I've CC'd the original copyright holder, Hans de Goede.  Hans, are you willing to give permission for the license notice to be replaced with just an SPDX header indicating the dual licensing?  While we're at it, there are a number of other files with the same license text.  Hans, are you prepared to give permission for your other license notices to be replaced with SPDX headers?  Thanks,  Bob  --  Bob Ham <rah@settrans.net>  for (,,) { ++pancakes, }  """,technical,Bob Ham,rah@settrans.net,0,1,914,1.0,0.7142857142857143,0,0.3,0.7,0.0,0.7
419,303621,312142,"HI, Yes that is fine by me and you've my permission to switch to using just the SPDX header.FWIW I do not believe the ""can't be removed from 'this software and associated documentation files (the ""Software"")'"" language applies to the software as a whole and not individual files. Yes you may make the same change to all files with my copyright.","Hi,  On 05-06-18 20:18, Bob Ham wrote:  Yes that is fine by me and you've my permission to switch to using just the SPDX header.  FWIW I do not believe the can't be removed from 'this software and associated documentation files (the ""Software"")'"" language applies to the software as a whole and not individual files.   Yes you may make the same change to all files with my copyright.  Regards,  Hans""",technical,Hans de Goede,hdegoede@redhat.com,1,0,344,0.40555555555555556,0.8571428571428571,0,1.0,0.0,0.7,0.0
420,303621,312258,"Excellent, thanks! :-)","On 13/06/18 08:28, Hans de Goede wrote:   Excellent, thanks! :-)   --  Bob Ham <rah@settrans.net>  for (,,) { ++pancakes, }",technical,Bob Ham,rah@settrans.net,0,1,22,0.03888888888888889,1.0,1,1.0,0.0,0.0,0.0
421,306145,306134,This looks a lot like phy_write_mmd().,"On Wed, Jun 06, 2018 at 10:03:18AM +0200, Alexander Onnasch wrote:  Hi Alexander  This looks a lot like phy_write_mmd().       Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,38,0.0743801652892562,0.3333333333333333,0,0.0,1.0,0.0,0.24074074074074073
422,306145,317094,"thanks for the hint. But actually I cannot confirm - or I don't see it yet. Without having tested, just from the code, the struct phy_driver instance for PHY_ID_KSZ8061 in micrel.c does not have a .write_mmd function assigned, thus phy_write_mmd should evaluate to its else-clause (see below) and not to mdiobus_write (as in phy_write).Also the function which I have added uses the same principle as already existing HW-specific functions in micrel.c for similar reasons (kszphy_extended_write and ksz9031_extended_write).They use phy_write all over the place in that file and never phy_write_mmd - for whatever reason they had. Thus I thought it would be a good idea ...","Hi Andrew  thanks for the hint. But actually I cannot confirm - or I don't see it yet.     Without having tested, just from the code, the struct phy_driver instance for PHY_ID_KSZ8061 in micrel.c does not have a .write_mmd function assigned, thus phy_write_mmd should evaluate to its else-clause (see below) and not to mdiobus_write (as in phy_write).    Also the ksz8061_extended_write() function which I have added uses the same principle as already existing HW-specific functions in micrel.c for simular reasons (kszphy_extended_write and ksz9031_extended_write).  They use phy_write all over the place in that file and never phy_write_mmd - for whatever reason they had.  Thus I thought it would be a good idea ...    best regards, Alex      static inline int phy_write(struct phy_device *phydev, u32 regnum, u16 val)  {  	return mdiobus_write(phydev->mdio.bus, phydev->mdio.addr, regnum, val),  }        int phy_write_mmd(struct phy_device *phydev, int devad, u32 regnum, u16 val)  {  	int ret,    	if (regnum > (u16)~0 || devad > 32)  		return -EINVAL,    	if (phydev->drv->write_mmd) {  		ret = phydev->drv->write_mmd(phydev, devad, regnum, val),  	} else if (phydev->is_c45) {  		u32 addr = MII_ADDR_C45 | (devad << 16) | (regnum & 0xffff),    		ret = mdiobus_write(phydev->mdio.bus, phydev->mdio.addr,  				    addr, val),  	} else {  		struct mii_bus *bus = phydev->mdio.bus,  		int phy_addr = phydev->mdio.addr,    		mutex_lock(&bus->mdio_lock),  		mmd_phy_indirect(bus, phy_addr, devad, regnum),    		/* Write the data into MMD's selected register */  		bus->write(bus, phy_addr, MII_MMD_DATA, val),  		mutex_unlock(&bus->mdio_lock),    		ret = 0,  	}  	return ret,  }    -----Original Message-----  From: Andrew Lunn <andrew@lunn.ch>   Sent: Mittwoch, 6. Juni 2018 14:40  To: Onnasch, Alexander (EXT) <Alexander.Onnasch@landisgyr.com>  Cc: Florian Fainelli <f.fainelli@gmail.com>, netdev@vger.kernel.org, linux-kernel@vger.kernel.org  Subject: Re: [PATCH] net/phy: Micrel KSZ8061 PHY link failure after cable connect    On Wed, Jun 06, 2018 at 10:03:18AM +0200, Alexander Onnasch wrote:    Hi Alexander    This looks a lot like phy_write_mmd().         Andrew",technical,"Onnasch, Alexander (EXT)",Alexander.Onnasch@landisgyr.com,0,0,671,1.0,0.4444444444444444,0,0.24074074074074073,0.7407407407407407,0.24074074074074073,0.0
423,306145,357674,"thanks for your feedback. I was on holiday, thus just delayed, not forgotten...Sorry for top-posting - odd company default mail setup. I checked again phy_write_mmd(), you are right !Patch with changed implementation will follow. PHY link failure after cable connect Please don't top post. And wrap your lines at around 75 characters Look closely at the two implementations. Look at what mmd_phy_indirect() does. I _think_ these are identical. So don't add your own helper, please use the core code.","Hi Andrew  thanks for your feedback. I was on holiday, thus just delayed, not forgotten... Sorry for top-posting - odd company default mail setup. I checked again phy_write_mmd(), you are right !  Patch with changed implementation will follow.  Best regards, Alex   -----Original Message----- From: Andrew Lunn <andrew@lunn.ch>  Sent: Dienstag, 19. Juni 2018 17:29 To: Onnasch, Alexander (EXT) <Alexander.Onnasch@landisgyr.com> Cc: Florian Fainelli <f.fainelli@gmail.com>, netdev@vger.kernel.org, linux-kernel@vger.kernel.org Subject: Re: [PATCH] net/phy: Micrel KSZ8061 PHY link failure after cable connect  On Tue, Jun 19, 2018 at 02:23:41PM +0000, Onnasch, Alexander (EXT) wrote:  Hi Alexander  Please don't top post. And wrap your lines at around 75 characters      Look closely at the two implementations. Look at what mmd_phy_indirect() does. I _think_ these are identical. So don't add your own helper, please use the core code.       Andrew",technical,"Onnasch, Alexander (EXT)",Alexander.Onnasch@landisgyr.com,0,0,499,0.8264462809917356,0.7777777777777778,0,1.0,0.0,0.7407407407407407,0.0
424,307410,308344,"That was an extensive changelog, thanks for the details and for working on this!","On Fri, Jun 8, 2018 at 8:08 PM, Logan Gunthorpe <logang@deltatee.com> wrote:  Thanks Logan.  Series: Acked-by: Allen Hubbe <allenbh@gmail.com>",technical,Allen Hubbe,allenbh@gmail.com,1,0,80,0.0367816091954023,0.3870967741935484,0,0.024390243902439025,0.975609756097561,0.024390243902439025,0.024390243902439025
425,307410,310831,"I just checked, this can be removed for this mode. I'll update the patch. Thanks!","On 06/08/2018 05:08 PM, Logan Gunthorpe wrote:  Acked-by: Dave Jiang <dave.jiang@intel.com> for the Intel parts and the generic parts",technical,Dave Jiang,dave.jiang@intel.com,1,0,81,0.04597701149425287,0.41935483870967744,0,0.04878048780487805,0.926829268292683,0.024390243902439025,0.0
426,307410,311516,"is more of a glue layer, and this is more device specific. While I like adding it here for more common code, it should probably reside in the ntb_hw_*.c files to enforce the hw specific code all reside in that layer.  So, this probably needs to be replaced with a patch which adds the setting of the mask to the switchtec driver.","On Fri, Jun 8, 2018 at 8:08 PM, Logan Gunthorpe <logang@deltatee.com> wrote:  ntb.c is more of a glue layer, and this is more device specific. While I like adding it here for more common code, it should probably reside in the ntb_hw_*.c files to enforce the hw specific code all reside in that layer.  So, this probably needs to be replaced with a patch which adds the setting of the mask to the switchtec driver.  Thanks, Jon",technical,Jon Mason,jdmason@kudzu.us,1,0,329,0.16091954022988506,0.45161290322580644,0,0.07317073170731707,0.926829268292683,0.0,0.0
427,307410,311525,"This is a very long way of saying ""no clients are checking the error codes,  so removing them"". :)I think the history and references to follow-on patches are not necessary in the commit message and belong more in a 0/X.This is more of a feature than a bug fix.  Can you break this (and the pingpong and perf changes caused by this) off into a separate series, as I'll want to apply this to the ntb-next and not bugfixes branch?","On Fri, Jun 8, 2018 at 8:08 PM, Logan Gunthorpe <logang@deltatee.com> wrote:  This is a very long way of saying no clients are checking the error codes, so removing them"". :) I think the history and references to follow-on patches are not necessary in the commit message and belong more in a 0/X.  This is more of a feature than a bug fix.  Can you break this (and the pingpong and perf changes caused by this) off into a separate series, as I'll want to apply this to the ntb-next and not bugfixes branch?  Thanks, Jon """,technical,Jon Mason,jdmason@kudzu.us,1,0,427,0.21149425287356322,0.4838709677419355,0,0.07317073170731707,0.926829268292683,0.0,0.0
428,307410,311538,"I disagree strongly. You can tell it's not device specific seeing we have 4 devices that need the exact same code. In fact, there is nothing device specific in those lines of code as the device specific part comes when a driver sets the PCI parent device's DMA mask. These lines just initialize the dma_mask for the new NTB device with its parent's mask. This is just sensible given that nothing now works if it is not done and trusting driver writers to get it right is not a good idea seeing we already screwed it up once. Furthermore, it violates DRY (do not repeat yourself).If there is something driver specific that must be done (although I can't actually imagine what this would be) the drivers are free to change the mask after calling ntb_register but getting the common case setup in common code is just good design.","On 12/06/18 09:48 AM, Jon Mason wrote:  I disagree strongly. You can tell it's not device specific seeing we have 4 devices that need the exact same code. In fact, there is nothing device specific in those lines of code as the device specific part comes when a driver sets the PCI parent device's DMA mask. These lines just initialize the dma_mask for the new NTB device with its parent's mask. This is just sensible given that nothing now works if it is not done and trusting driver writers to get it right is not a good idea seeing we already screwed it up once. Furthermore, it violates DRY (do not repeat yourself).  If there is something driver specific that must be done (although I can't actually imagine what this would be) the drivers are free to change the mask after calling ntb_register but getting the common case setup in common code is just good design.  Logan",technical,Logan Gunthorpe,logang@deltatee.com,1,1,826,0.3793103448275862,0.5161290322580645,0,0.07317073170731707,0.926829268292683,0.0,0.0
429,307410,314412,"Good day,  .Thanks for the patchset you submitted. My hopefully useful comments are under the corresponding patches. Regards,","On Fri, Jun 08, 2018 at 06:08:10PM -0600, Logan Gunthorpe <logang@deltatee.com> wrote:  Good day, Logan. Thanks for the patchset you submitted. My hopefully useful comments are under the corresponding patches.  Regards, -Sergey",technical,Serge Semin,fancer.lancer@gmail.com,1,0,125,0.05057471264367816,0.5806451612903226,0,0.14634146341463414,0.8292682926829268,0.07317073170731707,0.0
430,307410,314413,"This is weird. Neither me nor the folks' who tested the script saw this warning. I tried it on my laptop with bash and on a target device with busy box-shell. The warning never occurred. I even tried a simple command like: It might be that your bash is more modern than mine. Anyway if this patch solves the problem you see, that's great. Thanks for it.","On Fri, Jun 08, 2018 at 06:08:11PM -0600, Logan Gunthorpe <logang@deltatee.com> wrote:  This is weird. Neither me nor the folks' who tested the script saw this warning. I tried it on my laptop with bash and on a target device with busybox-shell. The warning never occurred. I even tried a simple command like: [[ $(echo -ne \x4e\x0a\00"") == ""N"" ]] && echo ""True""  It might be that your bash is more modern than mine. Anyway if this patch solves the problem you see, that's great. Thanks for it.  -Sergey """,technical,Serge Semin,fancer.lancer@gmail.com,1,0,353,0.1793103448275862,0.6129032258064516,0,0.14634146341463414,0.8292682926829268,0.0,0.0
431,307410,314415,"As a part of the multi-port NTB API the port-index interface was freshly introduced. The main idea was to somehow address local/peer domains within one NTB device, since from now there can be more than one peer domain to send message to or to set MWs up with. For this we invented the two-spaces interface which mapped in general non-linear ports space to the locally linear ports indexes space, and vise-versa. That mapping was implemented by new.Even though it perfectly fitted the IDT NTB functions, the Intel/AMD devices didn't have explicit ports numbering. Instead we decided to assign the numbers by using the topology type. So the Primary and B2B US sides got port, Secondary and B2B DS sides got port NTB_PORT_SEC_DSD. In order to make it being default for all pure two-ports devices like Intel/AMD the new methods ntb_default_port_number() and were developed and utilized in the API functions (see ntb.h header file).So to speak the main purpose of the default methods is to assign some unique port number to the NTB devices based on the topology at current implementation. Please note, that it is essential for the NTB API to have each port uniquely enumerated within one device. This is the way the multi-port NTB API has been designed in the first place. That was the reason we altered the Intel/AMD and IDT drivers about two years ago. Based on this I redeveloped the ntb_tool/ntb_perf/ntb_pingpong drivers. Needless to say that I was sure all the NTB devices followed the API convention regarding the port numbers. Since the Switchtec driver doesn't provide the explicit port-index API callbacks, the NTB API internals uses the default methods, which as you can see don't know anything about SWITCH and CROSSLINK topologies. That's why the methods return -EINVAL so the test drivers don't work properly. Concerning the fix of the discovered issues and fixes introduced by this patchset. I'd suggest to add the ports-index callbacks to the Switch tecdriver, which identify local and peer ports. After this the current version of all the test drivers shall perfectly work. As far as I can see the PFX family switches documentation operates with the definitions like Ports/Partitions (similar to the IDT switches) as well as the switchtec management driver. It might be a clue to the switch functionality, which can be used to find something similar to the ports numbering.","On Fri, Jun 08, 2018 at 06:08:13PM -0600, Logan Gunthorpe <logang@deltatee.com> wrote:  As a part of the multi-port NTB API the port-index interface was freshly introduced. The main idea was to somehow address local/peer domains within one NTB device, since from now there can be more than one peer domain to send message to or to set MWs up with. For this we invented the two-spaces interface which mapped in general non-linear ports space to the locally linear ports indexes space, and vise-versa. That mapping was implemented by new callbacks: ntb_port*()/ntb_peer_port*().  Even though it perfectly fitted the IDT NTB functions, the Intel/AMD devices didn't have explicit ports numbering. Instead we decided to assign the numbers by using the topology type. So the Primary and B2B US sides got port NTB_PORT_PRI_USD, Secondary and B2B DS sides got port NTB_PORT_SEC_DSD. In order to make it being default for all pure two-ports devices like Intel/AMD the new methods ntb_default_port_number() and ntb_default_peer_port_number() were developed and utilized in the ntb_port*()/ntb_peer_port*() API functions (see ntb.h header file).  So to speak the main purpose of the default methods is to assign some unique port number to the NTB devices based on the topology at current implementation. Please note, that it is essential for the NTB API to have each port uniquely enumerated within one device. This is the way the multi-port NTB API has been designed in the first place. That was the reason we altered the Intel/AMD and IDT drivers about two years ago.  Based on this I redeveloped the ntb_tool/ntb_perf/ntb_pingpong drivers. Needless to say that I was sure all the NTB devices followed the API convention regarding the port numbers. Since the Switchtec driver doesn't provide the explicit port-index API callbacks, the NTB API internals uses the default methods, which as you can see don't know anything about SWITCH and CROSSLINK topologies. That's why the methods return -EINVAL so the test drivers don't work properly.  Concerning the fix of the discovered issues and fixes introduced by this patchset. I'd suggest to add the ports-index callbacks to the Switchtec driver, which identify local and peer ports. After this the current version of all the test drivers shall perfectly work.  As far as I can see the PFX family switches documentation operates with the definitions like Ports/Partitions (similar to the IDT switches) as well as the switchtec management driver. It might be a clue to the switch functionality, which can be used to find something similar to the ports numbering.  Regards, -Sergey",technical,Serge Semin,fancer.lancer@gmail.com,1,0,2385,1.0,0.6451612903225806,0,0.14634146341463414,0.8292682926829268,0.0,0.0
432,307410,314416,"Thanks for the patch. It was the original version of the ping-pong driver, I was going to submit. But I've decided to develop it a bit different. And here is why. My goal was to create the multi-port version of the ping-pong test. The idea of the new driver was to implement the cyclic port-to-portping-pong algorithm. Simply speaking each port selects two partner-ports, one partner would be used as the source of pings and another one would be target of pongs sent to with the defined delay. Since IDT got a global Doorbell register, which is shared between all the ports, I had to assign an unique doorbell bit to each port. I created a simple algorithm, which linearised in general non-linear port numbers. Then I used the globally unique port index to select the corresponding doorbell bit. pp_init_flds() methods implements the corresponding algorithm, while performs the next port selection to convey the pong to. Regarding the patch. The idea of using the port number instead of linearised unique index should also work for Intel/AMD/IDT drivers. But the ports-space linearization algorithm was created for the case if the real port numbers would exceed the available Doorbell bits. I thought this might be the case of multi-ports version of the switchtec driver. Needless to say, that if Switchtec driver had the ports-index API implementation, this patch wouldn't be needed.","On Fri, Jun 08, 2018 at 06:08:14PM -0600, Logan Gunthorpe <logang@deltatee.com> wrote:  Thanks for the patch. It was the original version of the ping-pong driver, I was going to submit. But I've decided to develop it a bit different. And here is why.  My goal was to create the multi-port version of the ping-pong test. The idea of the new driver was to implement the cyclic port-to-port ping-pong algorithm. Simply speaking each port selects two partner-ports, one partner would be used as the source of pings and another one would be target of pongs sent to with the defined delay.  Since IDT got a global Doorbell register, which is shared between all the ports, I had to assign an unique doorbell bit to each port. I created a simple algorithm, which linearised in general non-linear port numbers. Then I used the globally unique port index to select the corresponding doorbell bit. pp_init_flds() methods implements the corresponding algorithm, while pp_find_next_peer() performs the next port selection to convey the pong to.  Regarding the patch. The idea of using the port number instead of linearised unique index should also work for Intel/AMD/IDT drivers. But the ports-space linearization algorithm was created for the case if the real port numbers would exceed the available Doorbell bits. I thought this might be the case of multi-ports version of the switchtec driver.  Needless to say, that if Switchtec driver had the ports-index API implementation, this patch wouldn't be needed.  Regards, -Sergey",technical,Serge Semin,fancer.lancer@gmail.com,1,0,1384,0.5885057471264368,0.6774193548387096,0,0.14634146341463414,0.8292682926829268,0.0,0.0
433,307410,314417,Good catch. Thanks. IDT got a lot of MWs especially if LookUpTables are enabled. That's why I didn't find the effect of this error.,"On Fri, Jun 08, 2018 at 06:08:15PM -0600, Logan Gunthorpe <logang@deltatee.com> wrote:  Good catch. Thanks. IDT got a lot of MWs especially if LookUpTables are enabled. That's why I didn't find the effect of this error.  Regards, -Sergey",technical,Serge Semin,fancer.lancer@gmail.com,1,0,131,0.06896551724137931,0.7096774193548387,0,0.14634146341463414,0.8292682926829268,0.0,0.0
434,307410,314418,"Please, see the comment to the patch 3/8. I explained everything there including the fact, that the Intel/AMD drivers do have unique port numbers assigned.","On Fri, Jun 08, 2018 at 06:08:17PM -0600, Logan Gunthorpe <logang@deltatee.com> wrote:  Please, see the comment to the patch 3/8. I explained everything there including the fact, that the Intel/AMD drivers do have unique port numbers assigned.  Regards, -Sergey",technical,Serge Semin,fancer.lancer@gmail.com,1,0,155,0.06666666666666667,0.7419354838709677,0,0.14634146341463414,0.8292682926829268,0.0,0.0
435,307410,314420,Well that will work for the simple switchtec case. The crosslink topology CAN NOT produce port numbers like you ask. It is perfectly symmetric and the two hosts cannot reliably figure out which is port 0 and which is port 1. So these patches support this case.,"On 15/06/18 01:48 PM, Serge Semin wrote:  Well that will work for the simple switchtec case. The crosslink topology CAN NOT produce port numbers like you ask. It is perfectly symmetric and the two hosts cannot reliably figure out which is port 0 and which is port 1. So these patches support this case.  Logan",technical,Logan Gunthorpe,logang@deltatee.com,1,1,260,0.11954022988505747,0.7741935483870968,0,0.14634146341463414,0.8292682926829268,0.0,0.0
436,307410,314419,"Hmm, this behavior is the feature of the driver and isn't a bug or race to be fixed. ntb_perf driver returns -ENOLINK until the link is actually established, when the memory windows are properly initialized so the test can be performed. What do you think of leaving the algorithm as is, but instead to develop the polling scheme in the ntb_test.sh script and break the script execution if the link isn't established after sometime? At least we won't need to wait forever in case if the peer hanged up or crashed while the NTB link negotiation algorithm was in-progress.","On Fri, Jun 08, 2018 at 06:08:18PM -0600, Logan Gunthorpe <logang@deltatee.com> wrote:  Hmm, this behavior is the feature of the driver and isn't a bug or race to be fixed. ntb_perf driver returns -ENOLINK until the link is actually established, when the memory windows are properly initialized so the test can be performed. What do you think of leaving the algorithm as is, but instead to develop the polling scheme in the ntb_test.sh script and break the script execution if the link isn't established after sometime? At least we won't need to wait forever in case if the peer hanged up or crashed while the NTB link negotiation algorithm was in-progress.  Regards, -Sergey",technical,Serge Semin,fancer.lancer@gmail.com,1,0,569,0.25057471264367814,0.8064516129032258,0,0.14634146341463414,0.8292682926829268,0.0,0.0
437,307410,314421,"Good catch. Thanks for the patch. I discovered this problem myself a few days before you sent this patchset. So was going to submit the fix, but you were faster. I also tested this script in the looped-back setup. It is the case when two NTB-device ports are available at the same RootComplex. So the NTB can be configured from the single executional context. In this case the REMOTE_HOST is left empty, so the colon is left prepended to the corresponding paths and causes multiple errors including the one fixed by this patch. In order to fix it, we need to discard the colon for remote-less case, for instance, by the next patch so on for REMOTE_PP and REMOTE_PERF. It is necessary for NTB devices, which ports are looped-back to the same Root-Port. Would you be amenable if you resent this patch together with the fix I suggested?","On Fri, Jun 08, 2018 at 06:08:19PM -0600, Logan Gunthorpe <logang@deltatee.com> wrote:  Good catch. Thanks for the patch. I discovered this problem myself a few days before you sent this patchset. So was going to submit the fix, but you were faster.  I also tested this script in the looped-back setup. It is the case when two NTB-device ports are available at the same RootComplex. So the NTB can be configured from the single executional context. In this case the REMOTE_HOST is left empty, so the colon is left prepended to the corresponding paths and causes multiple errors including the one fixed by this patch. In order to fix it, we need to discard the colon for remote-less case, for instance, by the next patch:  @@ -482,7 +495,11 @@ function perf_test()  function ntb_tool_tests()  {  	LOCAL_TOOL=$DEBUGFS/ntb_tool/$LOCAL_DEV"" -	REMOTE_TOOL=""$REMOTE_HOST:$DEBUGFS/ntb_tool/$REMOTE_DEV"" +	if [[ ""${REMOTE_HOST}"" != "" ]], then +		REMOTE_TOOL=""$REMOTE_HOST:$DEBUGFS/ntb_tool/$REMOTE_DEV"" +	else +		REMOTE_TOOL=""$DEBUGFS/ntb_tool/$REMOTE_DEV"" +	fi    	echo ""Starting ntb_tool tests...""   And so on for REMOTE_PP and REMOTE_PERF. It is necessary for NTB devices, which ports are looped-back to the same Root-Port. Would you be amenable if you resent this patch together with the fix I suggested?  Regards, -Sergey """,technical,Serge Semin,fancer.lancer@gmail.com,1,0,833,0.3793103448275862,0.8387096774193549,0,0.14634146341463414,0.8292682926829268,0.0,0.0
438,307410,314422,"Well, the switchtec driver splits its 64 doorbells in two sets, one for each port right now. That will likely have to change when we go to a multi-port implementation. In that case we will have 64 doorbells and a maximum 48 ports. So I don't think we have to concern ourselves with more ports than doorbells. As I've said, it's impossible to write for the crosslink topology so the clients must support that case.","On 15/06/18 01:49 PM, Serge Semin wrote:  Well, the switchtec driver splits its 64 doorbells in two sets, one for each port right now. That will likely have to change when we go to a multi-port implementation. In that case we will have 64 doorbells and a maximum 48 ports. So I don't think we have to concern ourselves with more ports than doorbells.   As I've said, it's impossible to write for the crosslink topology so the clients must support that case.  Logan",technical,Logan Gunthorpe,logang@deltatee.com,1,1,413,0.19770114942528735,0.8709677419354839,0,0.14634146341463414,0.8292682926829268,0.0,0.0
439,307410,314423,"I think polling is really ugly and doesn't really address solve the issue of waiting forever. It's pretty easy to interrupt out of the wait and provides a much better clue to what's going on than an error. If we want to be more explicit, it would be pretty easy to start a timer in the bash script and use SIGALRM to exit if the link doesn't come up after 30 seconds or something.","On 15/06/18 01:51 PM, Serge Semin wrote:  I think polling is really ugly and doesn't really address solve the issue of waiting forever. It's pretty easy to interrupt out of the wait and provides a much better clue to whats going on than an error.  If we want to be more explicit, it would be pretty easy to start a timer in the bash script and use SIGALRM to exit if the link doesn't come up after 30 seconds or something.  Logan",technical,Logan Gunthorpe,logang@deltatee.com,1,1,380,0.18850574712643678,0.9032258064516129,0,0.14634146341463414,0.8292682926829268,0.0,0.0
440,307410,314424,See my comments as to why this is impossible.,"On 15/06/18 01:50 PM, Serge Semin wrote:  See my comments as to why this is impossible.  Logan",technical,Logan Gunthorpe,logang@deltatee.com,1,1,45,0.022988505747126436,0.9354838709677419,0,0.14634146341463414,0.8292682926829268,0.0,0.0
441,307410,314425,Thanks. It would be good to get Acked-bys or Reviewed-bys on the patches you approved.,"On 15/06/18 01:51 PM, Serge Semin wrote:  Thanks. It would be good to get Acked-bys or Reviewed-bys on the patches you approved.  Logan",technical,Logan Gunthorpe,logang@deltatee.com,1,1,86,0.03908045977011494,0.967741935483871,0,0.14634146341463414,0.8292682926829268,0.0,0.8292682926829268
442,310985,311019,SLAB_PANIC was added to not worry about error handling.,"On Tue, Jun 12, 2018 at 12:23:52PM +0800, Zhouyang Jia wrote:   SLAB_PANIC was added to not worry about error handling.",technical,Alexey Dobriyan,adobriyan@gmail.com,0,0,55,0.17543859649122806,0.6666666666666666,0,0.0,0.0,0.0,0.0
443,310985,311030,"Thank you for the patch! Yet something to improve:[auto build test ERROR on linus/master][also build test ERROR on v4.17-rc7][cannot apply to next-20180530][if your patch is applied to the wrong git tree, please drop us a note to help improve the system]","Hi,  Thank you for the patch! Perhaps something to improve:  [auto build test WARNING on linus/master] [also build test WARNING on v4.17 next-20180608] [if your patch is applied to the wrong git tree, please drop us a note to help improve the system]  url:    https://github.com/0day-ci/linux/commits/linux-kernel-owner-vger-kernel-org/proc-add-error-handling-for-kmem_cache_create/20180612-122737 config: i386-randconfig-x012-201823 (attached as .config) compiler: gcc-7 (Debian 7.3.0-16) 7.3.0 reproduce:         # save the attached .config to linux build tree         make ARCH=i386   All warnings (new ones prefixed by >>):     fs/proc/inode.c: In function 'proc_init_kmemcache':       return -ENOMEM,              ^    fs/proc/inode.c:96:13: note: declared here     void __init proc_init_kmemcache(void)                 ^~~~~~~~~~~~~~~~~~~  vim +/return +108 fs/proc/inode.c      95	     96	void __init proc_init_kmemcache(void)     97	{     98		proc_inode_cachep = kmem_cache_create(proc_inode_cache"",     99						     sizeof(struct proc_inode),    100						     0, (SLAB_RECLAIM_ACCOUNT|    101							SLAB_MEM_SPREAD|SLAB_ACCOUNT|    102							SLAB_PANIC),    103						     init_once),    104		pde_opener_cache =    105			kmem_cache_create(""pde_opener"", sizeof(struct pde_opener), 0,    106					  SLAB_ACCOUNT|SLAB_PANIC, NULL),    107		if (!proc_inode_cachep || !pde_opener_cache)  > 108			return -ENOMEM,    109	    110		proc_dir_entry_cache = kmem_cache_create_usercopy(    111			""proc_dir_entry"", sizeof(struct proc_dir_entry), 0, SLAB_PANIC,    112			offsetof(struct proc_dir_entry, inline_name),    113			sizeof_field(struct proc_dir_entry, inline_name), NULL),    114	}    115	  --- 0-DAY kernel test infrastructure                Open Source Technology Center https://lists.01.org/pipermail/kbuild-all                   Intel Corporation """,technical,kbuild test robot,lkp@intel.com,0,0,254,1.0,1.0,1,0.0,0.0,0.0,0.0
444,312440,312709,"Thank you for the patch! Yet something to improve:[auto build test ERROR on linus/master][also build test ERROR on v4.17-rc7][cannot apply to next-20180530][if your patch is applied to the wrong git tree, please drop us a note to help improve the system]","Hi Borys,  Thank you for the patch! Perhaps something to improve:  [auto build test WARNING on iio/togreg] [also build test WARNING on v4.17 next-20180613] [if your patch is applied to the wrong git tree, please drop us a note to help improve the system]  url:    https://github.com/0day-ci/linux/commits/Borys-Movchan/dt-bindings-iio-light-add-ISL29033-device-bindings/20180613-220725 base:   https://git.kernel.org/pub/scm/linux/kernel/git/jic23/iio.git togreg reproduce:         # apt-get install sparse         make ARCH=x86_64 allmodconfig         make C=1 CF=-D__CHECK_ENDIAN__   sparse warnings: (new ones prefixed by >>)   vim +235 drivers/iio/light/isl29033.c     220	    221	static int isl29033_read_sensor_input(struct isl29033_chip *chip)    222	{    223		int ret,    224		u16 val,    225		struct device *dev = regmap_get_device(chip->regmap),    226	    227		ret = regmap_bulk_read(chip->regmap, ISL29033_REG_ADD_DATA_LSB,    228					(u8 *)&val, 2),    229		if (ret < 0) {    230			dev_err(dev,    231				Data bulk read error %d\n"", ret),    232			return ret,    233		}    234	  > 235		val = be16_to_cpu(val),    236		dev_vdbg(dev, ""Data read: %x\n"", val),    237	    238		return val,    239	}    240	  --- 0-DAY kernel test infrastructure                Open Source Technology Center https://lists.01.org/pipermail/kbuild-all                   Intel Corporation""",technical,kbuild test robot,lkp@intel.com,0,0,254,1.0,0.75,0,0.0,0.8571428571428571,0.0,0.8571428571428571
445,312440,320072,Commit msg missing Preferred name if there only one compatible is the full compatible string plus '.txt'.-ohms and -micro-ohms are the documented units. Please use '-ohms'unless there is a compelling reason not to. Light-sensor is the documented node name for ALSs.,"On Wed, Jun 13, 2018 at 04:00:15PM +0200, Borys Movchan wrote:  Commit msg missing   Preferred name if there only one compatible is the full compatible  string plus '.txt'.   -ohms and -micro-ohms are the documented units. Please use '-ohms'  unless there is a compelling reason not to.   light-sensor is the documented node name for ALSs.",technical,Rob Herring,robh@kernel.org,1,0,265,0.7719298245614035,1.0,1,1.0,0.0,0.8571428571428571,0.0
446,312759,312766,The grammar is now incorrect :(Shouldn't we just have a SPDX line at the top here and this whole boilerplate text be removed?,"On Wed, Jun 13, 2018 at 04:40:15PM -0400, Javier Martinez wrote:  The grammer is now incorrect :(  Shouldn't we just have a SPDX line at the top here and this whole boiler plate text be removed?  thanks,  greg k-h",technical,Greg KH,gregkh@linuxfoundation.org,1,0,125,0.75,0.6666666666666666,0,0.0,0.0,0.0,0.0
447,312759,312768,"Sure, will get right to it. I am actually newbie to all this. I watched your tutorial on YouTube when you were at FOSDEM, Huge help, thank you for that.","On Wed, Jun 13, 2018 at 10:52:55PM +0200, Greg KH wrote:  Sure, will get right to it. I am actually newbie to all this. I watched your tutorial on youtube when you were at FOSDEM, Huge help, thank you for that.",technical,Javier Martinez,javiermlinux@gmail.com,0,1,152,1.0,1.0,1,0.0,0.0,0.0,0.0
448,314565,314673,Are you sure you are not changing the logic here? I think it'd be nicer to refactor the code instead. Something like:,"On Sat, 2018-06-16 at 15:03 +0900, Joonhwan Kim wrote:  Are you sure you are not changing the logic here?  I think it'd be nicer to refactor the code instead.  Something like: ---  drivers/staging/rtl8712/rtl871x_mlme.c | 73 +++++++++++++++++-----------------  1 file changed, 36 insertions(+), 37 deletions(-)  diff --git a/drivers/staging/rtl8712/rtl871x_mlme.c b/drivers/staging/rtl8712/rtl871x_mlme.c index ac547ddd72d1..d711305b33e1 100644 --- a/drivers/staging/rtl8712/rtl871x_mlme.c +++ b/drivers/staging/rtl8712/rtl871x_mlme.c @@ -552,6 +552,19 @@ void r8712_survey_event_callback(struct _adapter *adapter, u8 *pbuf)  	spin_unlock_irqrestore(&pmlmepriv->lock2, flags),  }   +static bool r8712_under_linking_then_join(struct mlme_priv *pmlmepriv) +{ +	set_fwstate(pmlmepriv, _FW_UNDER_LINKING), + +	if (r8712_select_and_join_from_scan(pmlmepriv) != _SUCCESS) +		return false, + +	mod_timer(&pmlmepriv->assoc_timer, +		  jiffies + msecs_to_jiffies(MAX_JOIN_TIMEOUT)), + +	return true, +} +  void r8712_surveydone_event_callback(struct _adapter *adapter, u8 *pbuf)  {  	unsigned long irqL, @@ -565,45 +578,31 @@ void r8712_surveydone_event_callback(struct _adapter *adapter, u8 *pbuf)  		_clr_fwstate_(pmlmepriv, _FW_UNDER_SURVEY),  	}   -	if (pmlmepriv->to_join) { -		if (check_fwstate(pmlmepriv, WIFI_ADHOC_STATE)) { -			if (!check_fwstate(pmlmepriv, _FW_LINKED)) { -				set_fwstate(pmlmepriv, _FW_UNDER_LINKING), +	if (!pmlmepriv->to_join) +		goto exit, + +	if (check_fwstate(pmlmepriv, WIFI_ADHOC_STATE)) { +		if (check_fwstate(pmlmepriv, _FW_LINKED) || +		    r8712_under_linking_then_join(pmlmepriv)) +			goto exit, + +		pmlmepriv->fw_state ^= _FW_UNDER_SURVEY, +		memcpy(&adapter->registrypriv.dev_network.Ssid, +		       &pmlmepriv->assoc_ssid, +		       sizeof(struct ndis_802_11_ssid)), +		r8712_update_registrypriv_dev_network(adapter), +		r8712_generate_random_ibss(adapter->registrypriv.dev_network.MacAddress), +		pmlmepriv->fw_state = WIFI_ADHOC_MASTER_STATE, +		pmlmepriv->to_join = false, +	} else { +		pmlmepriv->to_join = false, +		if (r8712_under_linking_then_join(pmlmepriv)) +			goto exit,   -				if (r8712_select_and_join_from_scan(pmlmepriv) -				    == _SUCCESS) { -					mod_timer(&pmlmepriv->assoc_timer, jiffies + -						  msecs_to_jiffies(MAX_JOIN_TIMEOUT)), -				} else { -					struct wlan_bssid_ex *pdev_network = -					  &(adapter->registrypriv.dev_network), -					u8 *pibss = -						 adapter->registrypriv. -							dev_network.MacAddress, -					pmlmepriv->fw_state ^= _FW_UNDER_SURVEY, -					memcpy(&pdev_network->Ssid, -						&pmlmepriv->assoc_ssid, -						sizeof(struct -							 ndis_802_11_ssid)), -					r8712_update_registrypriv_dev_network -						(adapter), -					r8712_generate_random_ibss(pibss), -					pmlmepriv->fw_state = -						 WIFI_ADHOC_MASTER_STATE, -					pmlmepriv->to_join = false, -				} -			} -		} else { -			pmlmepriv->to_join = false, -			set_fwstate(pmlmepriv, _FW_UNDER_LINKING), -			if (r8712_select_and_join_from_scan(pmlmepriv) == -			    _SUCCESS) -				mod_timer(&pmlmepriv->assoc_timer, jiffies + -					  msecs_to_jiffies(MAX_JOIN_TIMEOUT)), -			else -				_clr_fwstate_(pmlmepriv, _FW_UNDER_LINKING), -		} +		_clr_fwstate_(pmlmepriv, _FW_UNDER_LINKING),  	} + +exit:  	spin_unlock_irqrestore(&pmlmepriv->lock, irqL),  }",technical,Joe Perches,joe@perches.com,1,0,117,1.0,1.0,1,0.0,0.0,0.0,0.0
449,325533,325542,"Actually this binding already exists for mediatek timers, it is useless to add a new one. I note the binding. However the existing driver does only use <&system_clk> AFAICT, I'm questioning if <&rtc_clk> is really needed. So, I suggest you sort out and fixup the rtc_clk thing (drop it) and then just add your new platform in the list in this binding.","On 27/06/2018 09:53, Stanley Chu wrote:    Actually this binding already exists for mediatek timers, it is useless to add a new one.  I note the binding in  Documentation/devicetree/bindings/timer/mediatek,mtk-timer.txt  contains:   clocks = <&system_clk>, <&rtc_clk>  However the existing driver does only use <&system_clk> AFAICT, I'm questioning if <&rtc_clk> is really needed.  So, I suggest you sort out and fixup the rtc_clk thing (drop it) and then just add your new platform in the list in this binding.   --   <http://www.linaro.org/> Linaro.org │ Open source software for ARM SoCs  Follow Linaro:  <http://www.facebook.com/pages/Linaro> Facebook | <http://twitter.com/#!/linaroorg> Twitter | <http://www.linaro.org/linaro-blog/> Blog",technical,Daniel Lezcano,daniel.lezcano@linaro.org,1,0,351,0.975,0.4,0,0.0,1.0,0.0,0.0
450,325533,325598,Recent platforms have the arch_arm_timer and it will be always selected. What is the benefit of adding this timer ?,"On 27/06/2018 09:53, Stanley Chu wrote:  Recent platforms have the arch_arm_timer and it will be always selected.  What is the benefit of adding this timer ?   --   <http://www.linaro.org/> Linaro.org │ Open source software for ARM SoCs  Follow Linaro:  <http://www.facebook.com/pages/Linaro> Facebook | <http://twitter.com/#!/linaroorg> Twitter | <http://www.linaro.org/linaro-blog/> Blog",technical,Daniel Lezcano,daniel.lezcano@linaro.org,1,0,115,0.2625,0.5,0,0.0,1.0,0.0,0.0
451,325533,325606,"To save power as much as possible, our platform enables""arch_timer_c3stop"" in arch_arm_timer, and thus another always-on timer is required for tick-broadcasting. System Timer is introduced for above purpose. Thanks.","On Wed, 2018-06-27 at 11:39 +0200, Daniel Lezcano wrote: Hi Daniel,  To save power as much as possible, our platform enables arch_timer_c3stop"" in arch_arm_timer, and thus another always-on timer is required for tick-broadcasting. System Timer is introduced for above purpose.  Thanks. Stanley Chu""",technical,Stanley Chu,stanley.chu@mediatek.com,1,1,215,0.4625,0.6,0,0.0,1.0,0.0,0.0
452,325533,325612,Please merge mtk_systimer.c and mtk_timer.c into a single file:timer-mediatek.c. Change the name in MakefilePatch 2: Change function prefix name to _gpt_Patch 2.1 : Move the gpt's init code to timer-ofPatch 3: Add code for syst in timer-mediatek.c A couple of comments below. Wouldn't make sense to clear the interrupt after disabling the timer ?Why do you need IRQF_TRIGGER_HIGH ?Why IRQF_PERCPU ?No it is consistent with the DT binding.,"On 27/06/2018 09:53, Stanley Chu wrote:  Please merge mtk_systimer.c and mtk_timer.c into a single file:  timer-mediatek.c  Patch 1:   git mv mtk_timer.c timer-mediatek.c  Change the name in Makefile  Patch 2:   Change function prefix name to _gpt_  Patch 2.1 [optional but recommended] :   Move the gpt's init code to timer-of  Patch 3:   Add code for syst in timer-mediatek.c    A couple of comments below.   Wouldn't make sense to clear the interrupt after disabling the timer ?   Why do you need IRQF_TRIGGER_HIGH ?   Why IRQF_PERCPU ?   No mediatek,sys_timer"" but eg. ""mediatek,mt6765"", so it is consistent with the DT binding.   --   <http://www.linaro.org/> Linaro.org │ Open source software for ARM SoCs  Follow Linaro:  <http://www.facebook.com/pages/Linaro> Facebook | <http://twitter.com/#!/linaroorg> Twitter | <http://www.linaro.org/linaro-blog/> Blog""",technical,Daniel Lezcano,daniel.lezcano@linaro.org,1,0,438,1.0,0.8,0,0.0,1.0,0.0,1.0
453,325533,326506,OK! We'll fix it and merge two timers into single document file in v3.Thanks.,"On Wed, 2018-06-27 at 10:20 +0200, Daniel Lezcano wrote: Hi Daniel,  OK! We'll fix it and merge two timers into single document file in v3.  Thanks. Stanley Chu",technical,Stanley Chu,stanley.chu@mediatek.com,1,1,77,0.2125,0.9,0,1.0,0.0,1.0,0.0
454,325533,326512,"Thanks for suggestion. Will do above all in v3.The comment may mislead readers. The first step, we do both things in the same time,1. Clear interrupt status.2. Disable interrupt engine in timer hardware, so the interrupt cannot come repeatedly. After that, we shall be safe enough to do followings. Both flags are wrong and will be removed. We will sort out bindings of these two timers in v3.Thanks.","On Wed, 2018-06-27 at 12:01 +0200, Daniel Lezcano wrote:  Thanks for suggestion. Will do above all in v3.  The comment may mislead readers.  The first step, we do both things in the same time, 1. Clear interrupt status. 2. Disable interrupt engine in timer hardware, so the interrupt cannot come repeatedly.  After that, we shall be safe enough to do followings.   Both flags are wrong and will be removed.   We will sort out bindings of these two timers in v3.   Thanks. Stanley Chu",technical,Stanley Chu,stanley.chu@mediatek.com,1,1,400,1.0,1.0,1,1.0,0.0,0.0,0.0
455,326460,326469,"Thanks for the patch, but Andrew Jones has also posted a patch[1] which I had a look but was not sure what is the best approach to fix it yet. I will think about it and respond to that.","Hi Shunyong,  On 28/06/18 10:18, Shunyong Yang wrote:  Thanks for the patch, but Andrew Jones has also posted a patch[1] which I had a look but was not sure what is the best approach to fix it yet. I will think about it and respond to that.  -- Regards, Sudeep  [1] https://patchwork.kernel.org/patch/10482261",technical,Sudeep Holla,sudeep.holla@arm.com,1,0,185,0.2571428571428571,0.16666666666666666,0,0.0,1.0,0.0,0.0
456,326460,326573,"I'll send a v1 yet today. The RFC version was actually OK, as the concern with ACPI nodes not being in the expected order wasn't actually a problem. The thread-id or core-id would only be reset to zero when a yet to be remapped core-id (and all its peers) was found when iterating the PEs. Since all peers were handled at the same time, the counter reset was correct, even when the ACPI nodes were out-of-order. The code didn't make that very obvious, though, and there was some room for other cleanups, so I've reworked it. Once I run it through a couple more rounds of testing I'll repost. FYI, I'm able to easily test a variety of configs using a KVM guest and this QEMU branch[*]. To use threads it's necessary to revert the last QEMU patch and to hack KVM to set MPIDR.MT for the VCPUs.","On Thu, Jun 28, 2018 at 10:38:24AM +0100, Sudeep Holla wrote:  I'll send a v1 yet today. The RFC version was actually OK, as the concern with ACPI nodes not being in the expected order wasn't actually a problem. The thread-id or core-id would only be reset to zero when a yet to be remapped core-id (and all its peers) was found when iterating the PEs. Since all peers were handled at the same time, the counter reset was correct, even when the ACPI nodes were out-of-order. The code didn't make that very obvious, though, and there was some room for other cleanups, so I've reworked it. Once I run it through a couple more rounds of testing I'll repost.  FYI, I'm able to easily test a variety of configs using a KVM guest and this QEMU branch[*]. To use threads it's necessary to revert the last QEMU patch and to hack KVM to set MPIDR.MT for the VCPUs.  Thanks, drew  [*] https://github.com/rhdrjones/qemu/commits/virt-cpu-topology",technical,Andrew Jones,drjones@redhat.com,0,0,791,1.0,0.25,0,0.0,0.9285714285714286,0.0,0.0
457,326460,326580,OK sure. I liked the approach in Shunyong's patch. I was thinking if we can avoid the list and dynamic allocation on each addition and make it more simpler.,"On 28/06/18 12:57, Andrew Jones wrote:  OK sure. I liked the approach in Shunyong's patch. I was thinking if we can avoid the list and dynamic allocation on each addition and make it more simpler.  --  Regards, Sudeep",technical,Sudeep Holla,sudeep.holla@arm.com,1,0,156,0.18857142857142858,0.3333333333333333,0,0.0,0.9285714285714286,0.0,0.0
458,326460,326657,"This one reads simpler, but yes I agree we should try to avoid the dynamic allocation. OTOH, I think that dropping the dynamic allocation leads to an algorithm that picks a value and replaces all the matches. Which of course is Andrew's patch, although I did have to read it a couple times to get a grasp how it works. I'm guessing that is due to the fact that he seems to have optimized 3 double loops into a single loop with two individual nested loops. AKA its probably more efficient than the naive implementation, but readability seems to have suffered a bit in the initial version he posted. I'm not sure the optimization is worth it, but I'm guessing there is a middle ground which makes it more readable. Finally, thanks for putting the effort into this...","On 28/06/18 14:19, Jeremy Linton wrote:  [...]   Completely agree. RFC from Andrew is not so readable and easy to understand.  --  Regards, Sudeep",technical,Jeremy Linton,jeremy.linton@arm.com,0,0,764,0.8857142857142857,0.4166666666666667,0,0.0,0.9285714285714286,0.0,0.0
459,326460,326700,Completely agree. RFC from Andrew is not so readable and easy to understand.,,technical,Sudeep Holla,sudeep.holla@arm.com,1,0,76,0.08571428571428572,0.5,0,0.0,0.9285714285714286,0.0,0.0
460,326460,326737,"Middle ground coming up. At the expense of a triple-nested loop (which will never be N^3 iterations due to conditions at the start of each loop),we can avoid dynamic allocations and list iterations and still gain readability.","On Thu, Jun 28, 2018 at 03:09:19PM +0100, Sudeep Holla wrote:  Middle ground coming up. At the expense of a triple-nested loop (which will never be N^3 iterations due to conditions at the start of each loop), we can avoid dynamic allocations and list iterations and still gain readability.  Thanks, drew",technical,Andrew Jones,drjones@redhat.com,0,0,225,0.24571428571428572,0.5833333333333334,0,0.0,0.9285714285714286,0.0,0.0
461,326460,326804,"Hi, All I have a new approach. As we've already got the offset of the node with physical package bit set, which is the parent of the cpu we are querying. We can iterate from the beginning of PPTT to count the nodes with physical package bit set till we reach the offset we've got. Then, the count value is the package id. This avoid list and dynamic allocation. And PPTT provides length for each node, iteration should be easy. I think this may implemented in pptt.c.I am writing this mail on my phone. Maybe I should try to write a patch to test in office tomorrow if you think it's feasible.","Hi, All   I have a new approach. As we've already got the offset of the node with physical package bit set, which is the parent of the cpu we are querying. We can iterate from the begining of PPTT to count the nodes with physical package bit set till we reach the offset we've got. Then, the count value is the package id. This avoid list and dynamic allocation. And PPTT provides length for each node, iteration should be easy. I think this may implemented in pptt.c.  I am writing this mail on my phone. Maybe I should try to write a patch to test in office tomorrow if you think it's feasible.  Thanks. Shunyong.",technical,"Yang, Shunyong",shunyong.yang@hxt-semitech.com,0,0,593,0.7257142857142858,0.6666666666666666,0,0.0,0.9285714285714286,0.0,0.0
462,326460,326830,"I was thinking of simple solution like add the offset to sorted array and assign the index to that. In this way if ACPI_PROCESSOR_ID_VALID flag is set at the package level too and they start and increase linearly from 0, we are matching them(requires 1 line change I posted in the other thread)--Regards, Sudeep IMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium. Thank you.","On 28/06/18 16:44, Yang, Shunyong wrote:  I was thinking of simple solution like add the offset to sorted array and assign the index to that. In this way if ACPI_PROCESSOR_ID_VALID flag is set at the package level too and they start and increase linearly from 0, we are matching them(requires 1 line change I posted in the other thread)  -- Regards, Sudeep IMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium. Thank you.",technical,Sudeep Holla,sudeep.holla@arm.com,1,0,634,0.7085714285714285,0.75,0,0.0,0.9285714285714286,0.0,0.0
463,326460,340893,I'm assuming this is no longer needed now that we have queued the series from Sudeep?,"On Thu, Jun 28, 2018 at 05:18:28PM +0800, Shunyong Yang wrote:  I'm assuming this is no longer needed now that we have queued the series from Sudeep?  Will",technical,Will Deacon,will.deacon@arm.com,1,0,85,0.10285714285714286,0.9166666666666666,0,1.0,0.0,0.9285714285714286,0.0
464,326460,340899,"The series relating to topology/numa that you have queued helps us tore-introduces numa mask check that was reverted partial in v4.18 and is not related to the issue reported or addressed by this patch. However, there's no proper solution to the issue reported in this thread and the one from Andrew Jones[1]. The only solution is to rely on ACPI firmware for that instead of trying to fix in OS and we have already merged the patch to fix that in acpi/pptt.c. In short, all the know issues are addressed so far and nothing else needs to be queued for now.","Hi Will,  On 12/07/18 12:06, Will Deacon wrote:  [..]   The series relating to topology/numa that you have queued helps us to re-introduces numa mask check that was reverted partial in v4.18 and is not related to the issue reported or addressed by this patch.  However, there's no proper solution to the issue reported in this thread and the one from Andrew Jones[1]. The only solution is to rely on ACPI firmware for that instead of trying to fix in OS and we have already merged the patch to fix that in acpi/pptt.c (Commit 30998033f62a (ACPI / PPTT: use ACPI ID whenever ACPI_PPTT_ACPI_PROCESSOR_ID_VALID is set""))  In short, all the know issues are addressed so far and nothing else needs to be queued for now.  --  Regards, Sudeep  [1] https://lore.kernel.org/lkml/20180629180308.zdl4taihzv2zwarc@kamzik.brq.redhat.com/T/#t""",technical,Sudeep Holla,sudeep.holla@arm.com,1,0,556,0.6342857142857142,1.0,1,1.0,0.0,0.0,0.0
465,331266,331349,Same problem here :(,"On Tue, Jul 03, 2018 at 05:04:11PM +1000, Andrew Jeffery wrote:  Same problem here :(",technical,Greg KH,gregkh@linuxfoundation.org,1,0,20,0.03205128205128205,0.4375,0,0.0,0.0,0.0,0.0
466,331266,332460,"I wasn't expecting you to put them into your tree - the general concept/implementation is still too immature for that, let alone the commit messages :) However, I'll address this before sending another spin of the patches. Sorry for the noise and thanks for the quick response .","Hi Greg,  On Tue, 3 Jul 2018, at 17:20, Greg KH wrote:  I wasn't expecting you to put them into your tree - the general concept/implementation is still too immature for that, let alone the commit messages :) However, I'll address this before sending another spin of the patches.  Sorry for the noise and thanks for the quick response.  Andrew",technical,Andrew Jeffery,andrew@aj.id.au,1,1,278,0.34615384615384615,0.8125,0,0.0,0.0,0.0,0.0
467,331266,332510,"It got a bit perverse from attempting to separate the devicetree handling from everything else. I'll fix the issues you've pointed out and rework the sysfs/kobj stuff. However, the patch (and the series) was intended as a straw man. I should have put more effort in to avoid some of the distraction (sorry), but I was also hoping to get feedback on the general approach (so devicetree design and how to expose the bits to userspace in a useful manner).Regarding the class, yeah, it's probably not the right choice and I'm not going to double down on it, but it collates the fields in an easy to discover location. Not doing something like this means a lot of grubbing around in /sys/device. Is there a better approach? Thanks for the feedback so far.","On Tue, 3 Jul 2018, at 17:24, Greg KH wrote:  It got a bit perverse from attempting to separate the devicetree handling from everything else. I'll fix the issues you've pointed out and rework the sysfs/kobj stuff.  However, the patch (and the series) was intended as a straw man. I should have put more effort in to avoid some of the distraction (sorry), but I was also hoping to get feedback on the general approach (so devicetree design and how to expose the bits to userspace in a useful manner).  Regarding the class, yeah, it's probably not the right choice and I'm not going to double down on it, but it collates the fields in an easy to discover location. Not doing something like this means a lot of grubbing around in /sys/device. Is there a better approach?  Thanks for the feedback so far.  Andrew",technical,Andrew Jeffery,andrew@aj.id.au,1,1,750,1.0,1.0,1,1.0,0.0,0.0,0.0
468,331266,331356,No changelog :(,"On Tue, Jul 03, 2018 at 05:04:13PM +1000, Andrew Jeffery wrote:  No changelog :(",technical,Greg KH,gregkh@linuxfoundation.org,1,0,15,0.02564102564102564,0.5625,0,0.0,0.0,0.0,0.0
469,331920,332141,"Yes, the cpu locks should be irq safe too, however, as irq is always disabled in that function, save/restore is redundant, no? We at least used to do this in the kernel - manipulating irq safe locks with spin_lock/unlock() if the irq state is known, whether enabled or disabled, and ISTR lockdep being smart enough to track actual irq state to determine irq safety.  Am I misremembering or is this different on RT kernels?","(cc'ing Peter and Ingo for lockdep)  Hello, Sebastian.  On Tue, Jul 03, 2018 at 06:45:44PM +0200, Sebastian Andrzej Siewior wrote:  So, irq is always disabled in cgroup_rstat_flush_locked().   Yes, the cpu locks should be irqsafe too, however, as irq is always disabled in that function, save/restore is redundant, no?   We at least used to do this in the kernel - manipulating irqsafe locks with spin_lock/unlock() if the irq state is known, whether enabled or disabled, and ISTR lockdep being smart enough to track actual irq state to determine irq safety.  Am I misremembering or is this different on RT kernels?  Thanks.  --  tejun",technical,Tejun Heo,tj@kernel.org,1,0,422,0.7049180327868853,0.4,0,0.0,0.875,0.0,0.0
470,331920,332179,"On not RT enabled kernels. On RT enabled kernels spin_lock_irq.*() is turned into a sleeping spinlock which do not disable interrupts. as I pointed out above only the raw_spin_lock_t really disables interrupts on -RT. That is the difference between those two. No, this is correct. So on !RT kernels the spin_lock_irq() disables interrupts and the raw_spin_lock() has the interrupts already disabled, everything is good. On RT kernels the spin_lock_irq() does not disable interrupts and the raw_spin_lock() acquires the lock with enabled interrupts and lockdep complains properly. Lockdep sees the hardirq path via: {IN-HARDIRQ-W} state was registered at","On 2018-07-03 13:24:24 [-0700], Tejun Heo wrote: Hi Tejun,   on not RT enabled kernels. On RT enabled kernels spin_lock_irq.*() is turned into a sleeping spinlock which do not disable interrupts.   as I pointed out above only the raw_spin_lock_t really disables interrupts on -RT. That is the difference between those two.   No, this is correct. So on !RT kernels the spin_lock_irq() disables interrupts and the raw_spin_lock() has the interrupts already disabled, everything is good. On RT kernels the spin_lock_irq() does not disable interrupts and the raw_spin_lock() acquires the lock with enabled interrupts and lockdep complains properly. lockdep sees the hardirq path via:   {IN-HARDIRQ-W} state was registered at:    lock_acquire+0x9e/0x250    _raw_spin_lock_irqsave+0x38/0x50    cgroup_rstat_updated+0x57/0x100    cgroup_base_stat_cputime_account_end.isra.6+0x17/0x60    __cgroup_account_cputime_field+0x49/0x60    account_system_index_time+0xdb/0x1f0    account_system_time+0x3f/0x70    account_process_tick+0x59/0x80    update_process_times+0x1d/0x50    tick_sched_handle+0x20/0x60    tick_sched_timer+0x37/0x80    __hrtimer_run_queues+0x12c/0x6d0    hrtimer_interrupt+0xed/0x240    smp_apic_timer_interrupt+0x89/0x3c0    apic_timer_interrupt+0xf/0x20    pin_current_cpu+0xa/0x120    migrate_disable+0x9a/0x200    rt_spin_lock+0x1d/0x60    put_unused_fd+0x2c/0x50    do_sys_open+0x23a/0x250    __x64_sys_openat+0x1b/0x20    do_syscall_64+0x50/0x190    entry_SYSCALL_64_after_hwframe+0x49/0xbe   Sebastian",technical,Sebastian Andrzej Siewior,bigeasy@linutronix.de,0,1,653,1.0,0.6,0,0.0,0.875,0.0,0.875
471,331920,340238,"I feel weary about applying a patch which isn't needed in mainline, especially without annotations or at least comments.  I suppose it may not be too common but this can't be the only place which needs this and using irqsave/restore spuriously in all those sites doesn't sound like a good solution.  Is there any other way of handling this?","Hello, Sebastian.  On Wed, Jul 11, 2018 at 01:05:13PM +0200, Sebastian Andrzej Siewior wrote:  I feel weary about applying a patch which isn't needed in mainline, especially without annotations or at least comments.  I suppose it may not be too common but this can't be the only place which needs this and using irqsave/restore spuriously in all those sites doesn't sound like a good solution.  Is there any other way of handling this?  Thanks.  --  tejun",technical,Tejun Heo,tj@kernel.org,1,0,340,0.5409836065573771,1.0,1,1.0,0.0,0.0,0.0
472,333988,334009,"This looks much better than the previous version. Yes, I think we need device links between the users of the power domains and the PGC devices. Since this is one of the use-cases for which device links were conceived I would guess that implementing this should be fairly straight-forward. If possible the regulator should also be disabled in the system suspend path, as this might reduce power consumption some more. But then this might have issues with suspend ordering of the regulator. As I don't want to send you down the rabbit hole too much with adding ordering to this stuff, I won't object to this going in as-is. If this is in fact due to a ordering limitation I would like to see this documented in a code comment.","Am Donnerstag, den 05.07.2018, 18:12 +0300 schrieb Leonard Crestez:  This looks much better than the previous version.   Yes, I think we need device links between the users of the power domains and the PGC devices. Since this is one of the use-cases for which device links were conceived I would guess that implementing this should be fairly straight-forward.   If possible the regulator should also be disabled in the system suspend path, as this might reduce power consumption some more. But then this might have issues with suspend ordering of the regulator.  As I don't want to send you down the rabbit hole too much with adding ordering to this stuff, I won't object to this going in as-is. If this is in fact due to a ordering limitation I would like to see this documented in a code comment.  Regards, Lucas",technical,Lucas Stach,l.stach@pengutronix.de,1,0,724,1.0,1.0,1,0.0,0.0,0.0,0.0
473,335504,335511,"I think anything is useful, thanks. The truth is that nobody is left that seems to really understand this code and syzkaller has shown it is full of various bugs. If there is someone out there that would like to tackle it, let me know. There might be a possibility to support such work.","On Sat, Jul 07, 2018 at 03:41:30AM +0200, Tomas Bortoli wrote:  I think anything is useful, thanks..  The truth is that nobody is left that seems to really understand this code and syzkaller has shown it is full of various bugs..  If there is someone out there that would like to tackle it, let me know. There might be a possibility to support such work.  Jason",technical,Jason Gunthorpe,jgg@ziepe.ca,1,0,286,1.0,1.0,1,0.0,0.0,0.0,0.0
474,336636,350535,"You should use prefix ""nl80211: "" in the title","Bernd Edlinger <bernd.edlinger@hotmail.de> writes:   You should use prefix nl80211: "" in the title.  https://wireless.wiki.kernel.org/en/developers/documentation/submittingpatches#subject  --  Kalle Valo""",technical,Kalle Valo,kvalo@codeaurora.org,1,0,46,1.0,1.0,1,1.0,0.0,1.0,0.0
475,336637,337817,"Thanks for cleaning this up! Correct. Yeah, I think this is fine.","On Sun, Jul 08, 2018 at 06:59:42PM +0900, Masahiro Yamada wrote:  Thanks for cleaning this up!   Correct.   Yeah, I think this is fine.  Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,65,0.36363636363636365,0.6666666666666666,0,1.0,0.0,1.0,0.0
476,336637,337986,"Thanks for your review! I have one question. The package information is contained in the warning/error message. Is it better to keep this? If so, I will send v2to move the information to the help, like this.","2018-07-10 4:21 GMT+09:00 Josh Poimboeuf <jpoimboe@redhat.com>:   Thanks for your review!   I have one question.   The package information is contained in the warning/error message. Is it better to keep this?  If so, I will send v2 to move the information to the help, like this:     config STACK_VALIDATION          bool Compile-time stack metadata validation""          depends on HAVE_STACK_VALIDATION          depends on $(success,echo ""int main() {}"" | $(HOSTCC) -xc -o /dev/null -lelf -)          help           Add compile-time checks to validate stack metadata, including frame           pointers (if CONFIG_FRAME_POINTER is enabled).  This helps ensure           that runtime stack traces are more reliable.            This is also a prerequisite for generation of ORC unwind data, which           is needed for CONFIG_UNWINDER_ORC.  +         To enable this, the host compiler needs to be able to link libelf. +         If it is missing, please install libelf-dev, libelf-devel or +         elfutils-libelf-devel.            For more information, see           tools/objtool/Documentation/stack-validation.txt.   --  Best Regards Masahiro Yamada""",technical,Masahiro Yamada,yamada.masahiro@socionext.com,1,1,207,1.0,1.0,1,1.0,0.0,0.0,0.0
477,336942,336998,"We would not even enter this path without matching compatible, so I think a check here is not really necessary.","On Mon, 9 Jul 2018 14:05:02 +0800 Mars Cheng <mars.cheng@mediatek.com> wrote:   Acked-by: Marc Zyngier <marc.zyngier@arm.com>  	M. --  Without deviation from the norm, progress is not possible.",technical,Marc Zyngier,marc.zyngier@arm.com,1,0,111,0.08835341365461848,0.375,0,0.0,1.0,0.0,0.0
478,336942,337162,"As you can see, we have a long list of SoCs which are poorly supported. I'm not very keen to just add another SoC which supports booting into a ramdisk using the serial console. Do you have a roadmap adding mainline support for this SoC?","On 09/07/18 08:05, Mars Cheng wrote:  As you can see, we have a long list of SoCs which are poorly supported. I'm not very keen to just add another SoC which supports booting into a ramdisk using the serial console. Do you have a roadmap adding mainline support for this SoC?  Regards, Matthias",technical,Matthias Brugger,matthias.bgg@gmail.com,1,0,237,0.20080321285140562,0.5,0,0.0,1.0,0.0,0.0
479,336942,337664,"Yes, that's a valid concern.mt6755 and mt6795 are in a similar state, the latter after three years. I'm all for supporting new SoCs, but this feels looks a box-ticking exercise (""hey, look, our SoC is supported in mainline"") which doesn't help anyone. My Ack still stands, but I'd definitely like to see some more complete support before this patch goes in.","On 09/07/18 11:20, Matthias Brugger wrote:  Yes, that's a valid concern.  mt6755 and mt6795 are in a similar state, the latter after three years. I'm all for supporting new SoCs, but this feels looks a box-ticking exercise (hey, look, our SoC is supported in mainline"") which doesn't help anyone.  My Ack still stands, but I'd definitely like to see some more complete support before this patch goes in.  Thanks,  	M. --  Jazz is not dead. It just smells funny...""",technical,Marc Zyngier,marc.zyngier@arm.com,1,0,357,0.3132530120481928,0.625,0,0.0,1.0,0.0,0.0
480,336942,337979,"Yes, we do arrange more resources to do upstream task for mt6765,clk/pinctrl drivers are almost ready to submit. systimer is under reviewing (v9). other drivers including. We have dedicated owners to handle them and will cowork tightly with members to make sure things happen in the following weeks. For previous chips, we did have no enough support after shell. It is due to fast pace of smartphone SoC and other resource issues. We also know that is no excuse so that we already confirmed owners and their schedules for this.If there is any suggestion, please let us know. Thanks.","Hi Matthias/Marc  On Mon, 2018-07-09 at 17:43 +0100, Marc Zyngier wrote:  Yes, we do arrange more resources to do upstream task for mt6765, clk/pinctrl drivers are almost ready to submit. systimer is under reviewing (v9). http://lists.infradead.org/pipermail/linux-mediatek/2018-July/013989.html  other drivers including pmic/pwrap/i2c/rtc/kpd/spi/wdt/cqdma/auxadc/pwm/cmdq/disp. We have dedicated owners to handle them and will cowork tightly with members to make sure things happen in the following weeks.  For previous chips, we did have no enough support after shell. It is due to fast pace of smartphone SoC and other resource issues. We also know that is no excuse so that we already confirmed owners and their schedules for mt6765.  If there is any suggestion, please let us know.  Thanks.",technical,Mars Cheng,mars.cheng@mediatek.com,0,1,582,0.4578313253012048,0.75,0,0.0,1.0,0.0,0.0
481,336942,338350,"Ok, so let's wait until pinctrl driver is submitted. I'd prefer if you could add the clk driver to this series. This way we can get rid of the dummy clocks in the device tree. I know that smartphone SoC is a fast paced business. Never the less I'm convinced that the basic building blocks won't change much from one version to another. And that mainline support for the previous version of your SoC will help you to get your new drivers faster upstream. For me the best example is the mt7622 which got to a reasonable upstream support quite fast, thanks to a good foundation of mt7623 in mainline. I'd love to see that happen on the smartphone SoCs as well. Not to mention that upstream support will help you internally when you have to rebase your BSP code-base to a new kernel version. That said I think it is good news that you have already defined owner for the different devices and hope to see submissions for them in the near future :)As a suggestion I would say that upstream submission takes time and effort and it will help your engineers if they can allocate some time to do so. But that's most probably a management decision and all engineers know that management bases it's decision on some hard-to-understandable abbreviations like EBITDA etc. ,)","On 10/07/18 01:04, Mars Cheng wrote:  Ok, so let's wait until pinctrl driver is submitted. I'd prefer if you could add the clk driver to this series. This way we can get rid of the dummy clocks in the device tree.   I know that smartphone SoC is a fast paced business. Never the less I'm convinced that the basic building blocks won't change much from one version to another. And that mainline support for the previous version of your SoC will help you to get your new drivers faster upstream.  For me the best example is the mt7622 which got to a reasonable upstream support quite fast, thanks to a good foundation of mt7623 in mainline. I'd love to see that happen on the smartphone SoCs as well.  Not to mention that upstream support will help you internally when you have to rebase your BSP code-base to a new kernel version.  That said I think it is good news that you have already defined owner for the different devices and hope to see submissions for them in the near future :) As a suggestion I would say that upstream submission takes time and effort and it will help your engineers if they can allocate some time to do so. But that's most probably a management decision and all engineers know that management bases it's decision on some hard-to-understandable abbreviations like EBITDA etc. ,)  Best regards, Matthias",technical,Matthias Brugger,matthias.bgg@gmail.com,1,0,1260,1.0,0.875,0,0.5,0.5,0.0,0.5
482,336942,340511,"Got it, I will submit this series with clk support in v5. and pinctrl after that. Thanks for your suggestions. We will try to catch up on this mission :-)","Hi Matthias  On Tue, 2018-07-10 at 12:52 +0200, Matthias Brugger wrote: [...]   Got it, I will submit this series with clk support in v5. and pinctrl after that.   Thanks for your suggestions. We will try to catch up on this mission :-)",technical,Mars Cheng,mars.cheng@mediatek.com,0,1,154,0.14457831325301204,1.0,1,1.0,0.0,0.5,0.0
483,346223,347961,"In fact, it's not just trying to avoid confusing users. Kexec loading and kexec_file loading are just do the same thing in essence. Just we need do kernel image verification on uefi system, have to port kexec loading code to kernel. Kexec has been a formal feature in our distro, and customers owning those kind of very large machine can make use of this feature to speed up the reboot process. On uefi machine, the kexec_file loading will search place to put kernel under 4G from top to down. As we know, the1st 4G space is DMA32 ZONE, dma, pci mmcfg, bios etc all try to consume it. It may have possibility to not be able to find a usable space for kernel/initrd. From the top down of the whole memory space, we don't have this worry. And at the first post, I just posted below with AKASHI's version. Later you suggested to use list_head to link child sibling of resource, see what the code change looks like. Then I posted v2 Rob Herring mentioned that other components which has this tree struct have planned to do the same thing, replacing the singly linked list with list_head to link resource child sibling. Just quote Rob's words as below. I think this could be another reason. From Rob The DT struct device_node also has the same tree structure withparent, child, sibling pointers and converting to list_head had been on the to do list for a while. ACPI also has some tree walking functions. Perhaps there should be a common tree struct and helpers defined either on top of list_head or a new struct if that saves some size.Kexec was invented for kernel developer to speed up their kernel rebooting. Now high end sever admin, kernel developer and QE are also keen to use it to reboot large box for faster feature testing, bug debugging. Kernel dev could know this well, about kernel loading position, admin or QE might not be aware of it very well. Understood. The list_head replacing patch truly invokes too many code changes, it's risky. I am willing to try any idea from reviewers, won't per suit they have to be accepted finally. If don't have a try, we don't know what it looks like, and what impact it may have. I am fine to take AKASHI's simple version of walk_system_ram_res_rev() to lower risk, even though it could be a little bit low efficient","Hi Andrew,  On 07/18/18 at 03:33pm, Andrew Morton wrote:   In fact, it's not just trying to avoid confusing users. Kexec loading and kexec_file loading are just do the same thing in essence. Just we need do kernel image verification on uefi system, have to port kexec loading code to kernel.   Kexec has been a formal feature in our distro, and customers owning those kind of very large machine can make use of this feature to speed up the reboot process. On uefi machine, the kexec_file loading will search place to put kernel under 4G from top to down. As we know, the 1st 4G space is DMA32 ZONE, dma, pci mmcfg, bios etc all try to consume it. It may have possibility to not be able to find a usable space for kernel/initrd. From the top down of the whole memory space, we don't have this worry.   And at the first post, I just posted below with AKASHI's walk_system_ram_res_rev() version. Later you suggested to use list_head to link child sibling of resource, see what the code change looks like. http://lkml.kernel.org/r/20180322033722.9279-1-bhe@redhat.com  Then I posted v2 http://lkml.kernel.org/r/20180408024724.16812-1-bhe@redhat.com Rob Herring mentioned that other components which has this tree struct have planned to do the same thing, replacing the singly linked list with list_head to link resource child sibling. Just quote Rob's words as below. I think this could be another reason.  ~~~~~ From Rob The DT struct device_node also has the same tree structure with parent, child, sibling pointers and converting to list_head had been on the todo list for a while. ACPI also has some tree walking functions (drivers/acpi/acpica/pstree.c). Perhaps there should be a common tree struct and helpers defined either on top of list_head or a ~~~~~ new struct if that saves some size.   Kexec was invented for kernel developer to speed up their kernel rebooting. Now high end sever admin, kernel developer and QE are also keen to use it to reboot large box for faster feature testing, bug debugging. Kernel dev could know this well, about kernel loading position, admin or QE might not be aware of it very well.    Understood. The list_head replacing patch truly involes too many code changes, it's risky. I am willing to try any idea from reviewers, won't persuit they have to be accepted finally. If don't have a try, we don't know what it looks like, and what impact it may have. I am fine to take AKASHI's simple version of walk_system_ram_res_rev() to lower risk, even though it could be a little bit low efficient.  Thanks Baoquan",technical,Baoquan He,bhe@redhat.com,1,1,2263,1.0,0.3888888888888889,0,0.125,0.75,0.0,0.0
484,346223,348158,"Please let's get all this into the changelogs? The larger patch produces a better result.  We can handle it ,)","On Thu, 19 Jul 2018 23:17:53 +0800 Baoquan He <bhe@redhat.com> wrote:   Please let's get all this into the changelogs?   The larger patch produces a better result.  We can handle it ,)",technical,Andrew Morton,akpm@linux-foundation.org,1,0,110,0.05194805194805195,0.4444444444444444,0,0.125,0.75,0.0,0.375
485,346223,351112,I do not have the full context here but let me note that you should be careful when doing top-down reservation because you can easily get into hot pluggable memory and break the hot remove use case. We even warn when this is done. See memblock_find_in_range_node,"On Thu 19-07-18 23:17:53, Baoquan He wrote:  I do not have the full context here but let me note that you should be careful when doing top-down reservation because you can easily get into hotplugable memory and break the hotremove usecase. We even warn when this is done. See memblock_find_in_range_node --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,262,0.1038961038961039,0.5,0,0.625,0.375,0.375,0.125
486,346223,352752,"Sorry for late reply because of some urgent customer hot plug issues.I am rewriting all change logs, and cover letter. Then found I was wrong about the 2nd reason. The current kexec_file_load calls kexec_locate_mem_hole() to go through all system RAM region, if one region is larger than the size of kernel or initrd, it will search a position in that region from top to down. Since kexec will jump to 2nd kernel and don't need to care the 1st kernel's data, we can always find a usable space to load kexec kernel/initrd under 4G.So the only reason for this patch is keeping consistent with kexec_load and avoid confusion. And since x86 5-level paging mode has been added, we have another issue for top-down searching in the whole system RAM. That is we support dynamic 4-level to 5-level changing. Namely a kernel compiled with 5-level support, we can add 'no5lvl' to force 4-level. Then jumping from a 5-level kernel to 4-level kernel, e.g. we load kernel at the top of system RAM in 5-level paging mode which might be bigger than 64TB, then try to jump to 4-level kernel with the upper limit of 64TB. For this case, we need add limit for kexec kernel loading if in 5-level kernel. All this mess makes me hesitate to choose a delegate method. Maybe I should drop this patchset. For this issue, if we stop changing the kexec top down searching code, I am not sure if we should post this replacing with list_head patches separately.","Hi Andrew,  On 07/19/18 at 12:44pm, Andrew Morton wrote:  Sorry for late reply because of some urgent customer hotplug issues.  I am rewriting all change logs, and cover letter. Then found I was wrong about the 2nd reason. The current kexec_file_load calls kexec_locate_mem_hole() to go through all system RAM region, if one region is larger than the size of kernel or initrd, it will search a position in that region from top to down. Since kexec will jump to 2nd kernel and don't need to care the 1st kernel's data, we can always find a usable space to load kexec kernel/initrd under 4G.  So the only reason for this patch is keeping consistent with kexec_load and avoid confusion.  And since x86 5-level paging mode has been added, we have another issue for top-down searching in the whole system RAM. That is we support dynamic 4-level to 5-level changing. Namely a kernel compiled with 5-level support, we can add 'no5lvl' to force 4-level. Then jumping from a 5-level kernel to 4-level kernel, e.g we load kernel at the top of system RAM in 5-level paging mode which might be bigger than 64TB, then try to jump to 4-level kernel with the upper limit of 64TB. For this case, we need add limit for kexec kernel loading if in 5-level kernel.  All this mess makes me hesitate to choose a deligate method. Maybe I should drop this patchset.   For this issue, if we stop changing the kexec top down searching code, I am not sure if we should post this replacing with list_head patches separately.  Thanks Baoquan",technical,Baoquan He,bhe@redhat.com,1,1,1432,0.6103896103896104,0.5555555555555556,0,0.75,0.125,0.125,0.0
487,346223,352854,"Kexec read kernel/initrd file into buffer, just search usable positions for them to do the later copying. You can see below struct kexec_segment,for the old kexec_load, kernel/initrd are read into user space buffer, the stores the user space buffer address, stores the position where kernel/initrd will be put. In kernel, it calls kimage_load_normal_segment() to copy user space buffer to intermediate pages which are allocated with flag GFP_KERNEL. These intermediate pages are recorded as entries, later when user execute ""kexec -e"" to trigger kexec jumping, it will do the final copying from the intermediate pages to the real destination pages which @mem pointed. Because we can't touch the existed data in 1st kernel when do kexec kernel loading. With my understanding, GFP_KERNEL will make those intermediate pages be allocated inside immovable area, it won't impact hot plugging. But the@mem we searched in the whole system RAM might be lost along with hot plug. Hence we need do kexec kernel again when hot plug event is detected.","On 07/23/18 at 04:34pm, Michal Hocko wrote:  Kexec read kernel/initrd file into buffer, just search usable positions for them to do the later copying. You can see below struct kexec_segment,  for the old kexec_load, kernel/initrd are read into user space buffer, the @buf stores the user space buffer address, @mem stores the position where kernel/initrd will be put. In kernel, it calls kimage_load_normal_segment() to copy user space buffer to intermediate pages which are allocated with flag GFP_KERNEL. These intermediate pages are recorded as entries, later when user execute kexec -e"" to trigger kexec jumping, it will do the final copying from the intermediate pages to the real destination pages which @mem pointed. Because we can't touch the existed data in 1st kernel when do kexec kernel loading. With my understanding, GFP_KERNEL will make those intermediate pages be allocated inside immovable area, it won't impact hotplugging. But the @mem we searched in the whole system RAM might be lost along with hotplug. Hence we need do kexec kernel again when hotplug event is detected.  #define KEXEC_CONTROL_MEMORY_GFP (GFP_KERNEL | __GFP_NORETRY)   struct kexec_segment {         /*          * This pointer can point to user memory if kexec_load() system          * call is used or will point to kernel memory if          * kexec_file_load() system call is used.          *          * Use ->buf when expecting to deal with user memory and use ->kbuf          * when expecting to deal with kernel memory.          */         union {                 void __user *buf,                 void *kbuf,         },         size_t bufsz,                                                                                                                                      unsigned long mem,         size_t memsz, },  Thanks Baoquan""",technical,Baoquan He,bhe@redhat.com,1,1,1038,0.41774891774891776,0.6111111111111112,0,0.875,0.125,0.0,0.125
488,346223,355042,"I am not sure I am following. If @mem is placed at movable node then the memory hot remove simply won't work, because we are seeing reserved pages and do not know what to do about them. They are not migrateable. Allocating intermediate pages from other nodes doesn't really help. The memblock code warns exactly for that reason.","On Wed 25-07-18 14:48:13, Baoquan He wrote:  I am not sure I am following. If @mem is placed at movable node then the memory hotremove simply won't work, because we are seeing reserved pages and do not know what to do about them. They are not migrateable. Allocating intermediate pages from other nodes doesn't really help.  The memblock code warns exactly for that reason. --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,328,0.14502164502164502,0.6666666666666666,0,1.0,0.0,0.125,0.0
489,346223,355052,"OK, I forgot the 2nd kernel which kexec jump into. It won't impact hot remove in 1st kernel, it does impact the kernel which kexec jump into if kernel is at top of system RAM and the top RAM is in movable node.","On 07/26/18 at 02:59pm, Michal Hocko wrote:  OK, I forgot the 2nd kernel which kexec jump into. It won't impact hotremove in 1st kernel, it does impact the kernel which kexec jump into if kernel is at top of system RAM and the top RAM is in movable node.",technical,Baoquan He,bhe@redhat.com,1,1,210,0.1038961038961039,0.7222222222222222,0,1.0,0.0,0.0,0.0
490,346223,355055,It will affect the 1st kernel (which does the memblock allocation top-down) as well. For reasons mentioned above.,"On Thu 26-07-18 21:09:04, Baoquan He wrote:  It will affect the 1st kernel (which does the memblock allocation top-down) as well. For reasons mentioned above. --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,113,0.047619047619047616,0.7777777777777778,0,1.0,0.0,0.0,0.0
491,346223,355058,"And btw. in the ideal world, we would restrict the memblock allocation top-down from the non-movable nodes. But I do not think we have that information ready at the time when the reservation is done.","On Thu 26-07-18 15:12:42, Michal Hocko wrote:  And btw. in the ideal world, we would restrict the memblock allocation top-down from the non-movable nodes. But I do not think we have that information ready at the time when the reservation is done. --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,199,0.08441558441558442,0.8333333333333334,0,1.0,0.0,0.0,0.0
492,346223,355081,"Oh, you could mix kexec loading up with kdump kernel loading. For kdump kernel, we need reserve memory region during bootup with memblock allocator. For kexec loading, we just operate after system up, and do not need to reserve any memory region. About memory used to load them, it's quite different way.","On 07/26/18 at 03:14pm, Michal Hocko wrote:  Oh, you could mix kexec loading up with kdump kernel loading. For kdump kernel, we need reserve memory region during bootup with memblock allocator. For kexec loading, we just operate after system up, and do not need to reserve any memmory region. About memory used to load them, it's quite different way.  Thanks Baoquan",technical,Baoquan He,bhe@redhat.com,1,1,304,0.1341991341991342,0.8888888888888888,0,1.0,0.0,0.0,0.0
493,346223,355106,I didn't know about that. I thought both use the same underlying reservation mechanism. My bad and sorry for the noise.,"On Thu 26-07-18 21:37:05, Baoquan He wrote:  I didn't know about that. I thought both use the same underlying reservation mechanism. My bad and sorry for the noise. --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,119,0.05411255411255411,0.9444444444444444,0,1.0,0.0,0.0,0.0
494,346223,355160,Not at all. It's truly confusing. I often need take time to recall those details.,"On 07/26/18 at 04:01pm, Michal Hocko wrote:  Not at all. It's truly confusing. I often need take time to recall those details.",technical,Baoquan He,bhe@redhat.com,1,1,81,0.04112554112554113,1.0,1,1.0,0.0,0.0,0.0
495,347119,347209,"The patch looks correct to me. In fact I have a patch [1], which does the same thing and switches to using per-cpu variable for the paths. I think it is safe to use cpu_online_mask outside the get/put_online_cpus(). Using present_mask may fail as we could fail to create a path for a CPU that is not online. Please could you check if [1] fixes the problem for you ?","Hi Mathieu,  On 07/18/2018 08:43 PM, Mathieu Poirier wrote:   The patch looks correct to me. In fact I have a patch [1], which does the same thing and switches to using per-cpu variable for the paths.    I think it is safe to use cpu_online_mask outside the get/put_online_cpus(). Using present_mask may fail as we could fail to create a path for a CPU that is not online.   Please could you check if [1] fixes the problem for you ?  [1]  http://lists.infradead.org/pipermail/linux-arm-kernel/2018-July/591606.html  Cheers Suzuki",technical,Suzuki K Poulose,suzuki.poulose@arm.com,1,0,365,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
496,347119,347248,"Not only did you beat me to the punch but I think using a per-cpu variable is cleaner, so let's go with yours.","On Wed, 18 Jul 2018 at 15:31, Suzuki K Poulose <suzuki.poulose@arm.com> wrote:  Not only did you beat me to the punch but I think using a per-cpu variable is cleaner, so let's go with yours.  Mathieu",technical,Mathieu Poirier,mathieu.poirier@linaro.org,1,1,110,0.325,1.0,1,0.0,0.0,0.0,0.0
497,347183,347491,"This will still deadlock if ->runtime_suspend commences before the hot plug event and the hot plug event occurs before polling has been disabled by ->runtime_suspend.The correct fix is to call pm_runtime_get_sync() *conditionally* in the atomic commit which enables the display, using the same conditional as, i.e. if.Now I realize I sent you down the wrong path when I suggested to introduce a DRM helper here.  My apologies, I didn't fully appreciate what is going awry here! Anything that happens in nouveau's poll worker never needs to acquire a runtime PM ref because polling is only enabled while runtime active, and ->runtime_suspend waits for an ongoing poll to finish. Thinking a bit more about this, our mistake is to acquire runtime PMrefs too far down in the call stack.  As a fix that can be backported to stable, adding if conditionals seems fine to me, but the long term fix is to push acquisition of refs further up in the call stack.E.g., if the user forces connector probing via sysfs, a runtime PM ref should be acquired in status_store() in drm_sysfs.c before invoking connector->funcs->fill_modes().  That way, if the user forces connector probing while the GPU is powering down, rpm_resume() will correctly wait for rpm_suspend() to finish before resuming the card.  So if we architect it like this, we're actually using the functionality provided by the PM core in the way that it's supposed to be used. The problem is that adding pm_runtime_get_sync() to status_store() conflicts with the desire to have a library of generic DRM functions: Some GPUs may be able to probe connectors without resuming to runtime active state, others don't use runtime PM at all.  One solution that comes to mind is a driver_features flag which tells the DRM core whether to acquire a runtime PM ref in various places. In your original patches 4 and 5, what exactly was the call stack which led to i2c being accessed while runtime suspended?  Was it sysfs access?  If so, acquisition of the runtime PM ref needs to likewise happen in that sysfs entry point, rather than deep down in the call stack upon accessing the i2c bus.","On Wed, Jul 18, 2018 at 04:56:39PM -0400, Lyude Paul wrote:  This will still deadlock if ->runtime_suspend commences before the hotplug event and the hotplug event occurs before polling has been disabled by ->runtime_suspend.  The correct fix is to call pm_runtime_get_sync() *conditionally* in the atomic commit which enables the display, using the same conditional as d61a5c106351, i.e. if (!drm_kms_helper_is_poll_worker()).  Now I realize I sent you down the wrong path when I suggested to introduce a DRM helper here.  My apologies, I didn't fully appreciate what is going awry here!  Anything that happens in nouveau's poll worker never needs to acquire a runtime PM ref because polling is only enabled while runtime active, and ->runtime_suspend waits for an ongoing poll to finish.  Thinking a bit more about this, our mistake is to acquire runtime PM refs too far down in the call stack.  As a fix that can be backported to stable, adding if (!drm_kms_helper_is_poll_worker()) conditionals seems fine to me, but the long term fix is to push acquisition of refs further up in the call stack.  E.g., if the user forces connector probing via sysfs, a runtime PM ref should be acquired in status_store() in drm_sysfs.c before invoking connector->funcs->fill_modes().  That way, if the user forces connector probing while the GPU is powering down, rpm_resume() will correctly wait for rpm_suspend() to finish before resuming the card.  So if we architect it like this, we're actually using the functionality provided by the PM core in the way that it's supposed to be used.  The problem is that adding pm_runtime_get_sync() to status_store() conflicts with the desire to have a library of generic DRM functions: Some GPUs may be able to probe connectors without resuming to runtime active state, others don't use runtime PM at all.  One solution that comes to mind is a driver_features flag which tells the DRM core whether to acquire a runtime PM ref in various places.  In your original patches 4 and 5, what exactly was the call stack which led to i2c being accessed while runtime suspended?  Was it sysfs access via /sys/class/i2c-adapter/* ?  If so, acquisition of the runtime PM ref needs to likewise happen in that sysfs entry point, rather than deep down in the call stack upon accessing the i2c bus.  Thanks,  Lukas",technical,Lukas Wunner,lukas@wunner.de,1,0,2129,1.0,0.4,0,0.0,1.0,0.0,0.0
498,347183,347502,"Hm, a runtime PM ref is already acquired in nouveau_connector_detect().I'm wondering why that's not sufficient?","On Wed, Jul 18, 2018 at 04:56:40PM -0400, Lyude Paul wrote:  Hm, a runtime PM ref is already acquired in nouveau_connector_detect(). I'm wondering why that's not sufficient?  Thanks,  Lukas",technical,Lukas Wunner,lukas@wunner.de,1,0,111,0.05263157894736842,0.5,0,0.0,1.0,0.0,0.0
499,347183,348395,"As an additional note, I realized this might seem wrong but it isn't pm_runtime_put_sync() calls down to nouveau's runtime idle callback, which does this: So, it doesn't actually synchronously suspend the GPU, it just starts up the auto suspend thread Just wanted to make sure there wasn't any confusion :)","On Thu, 2018-07-19 at 20:08 -0400, Lyude Paul wrote:                        ^^^^^^ As an additional note, I realized this might seem wrong but it isn't  pm_runtime_put_sync() calls down to nouveau's runtime idle callback, which does this:  static int nouveau_pmops_runtime_idle(struct device *dev) { 	if (!nouveau_pmops_runtime()) { 		pm_runtime_forbid(dev), 		return -EBUSY, 	}  	pm_runtime_mark_last_busy(dev), 	pm_runtime_autosuspend(dev), 	/* we don't want the main rpm_idle to call suspend - we want to autosuspend */ 	return 1, }  So, it doesn't actually synchronously suspend the GPU, it just starts up the autosuspend thread  Just wanted to make sure there wasn't any confusion :)  --  Cheers, 	Lyude Paul",technical,Lyude Paul,lyude@redhat.com,1,1,306,0.14832535885167464,0.7,0,0.05555555555555555,0.9444444444444444,0.0,0.05555555555555555
500,347183,364473,"Why do we need all this? i915 has full rpm support, including i2c and dpaux correctly working, and everything else too. Why is nouveau special? Also, there's a metric pile of other drivers using the existing helpers an infrastructure, with full rpm support, all seemingly being happy too. Given all that it smells a bit like nouveau has it's rpm design backwards, which would indicate that these new helpers should be nouveau-specific wrappers and not generic. If that's not the case, then I'd like to first understand why nouveau needs them and why no one else seems to need these.","On Wed, Jul 18, 2018 at 04:56:38PM -0400, Lyude Paul wrote:  Why do we need all this? i915 has full rpm support, including i2c and dp aux correctly working, and everything else too. Why is nouveau special?  Also, there's a metric pile of other drivers using the existing helpers an infrastructure, with full rpm support, all seemingly being happy too.  Given all that it smells a bit like nouveau has it's rpm design backwards, which would indicate that these new helpers should be nouveau-specific wrappers and not generic. If that's not the case, then I'd like to first understand why nouveau needs them and why no one else seems to need these. -Daniel   --  Daniel Vetter Software Engineer, Intel Corporation http://blog.ffwll.ch",technical,Daniel Vetter,daniel@ffwll.ch,1,0,582,0.2799043062200957,1.0,1,1.0,0.0,0.7222222222222222,0.0
501,347183,351313,"Because there's a difference between DPMST connectors and encoders vs. the rest of the device's encoders. Every DP MST topology will take up a single ""physical"" DP connector on the device that will be marked as disconnected. This connector also owns the ""mstm"" (MST manager, referred to as the drm_dp_mst_topology_mgr in DRM), which through the callbacks nouveau provides is responsible for creating the fake DP MST ports and encoders. All of these fake ports will have DPMST encoders as opposed to the physical DP ports, which will have TMDS encoders. Hence-mstms are only on physical connectors with TMDS, not fake connectors with DPMST.--","On Sat, 2018-07-21 at 11:39 +0200, Lukas Wunner wrote: Because there's a difference between DPMST connectors and encoders vs. the rest of the device's encoders. Every DP MST topology will take up a single physical"" DP connector on the device that will be marked as disconnected. This connector also owns the ""mstm"" (MST manager, referred to as the drm_dp_mst_topology_mgr in DRM), which through the callbacks nouveau provides is responsible for creating the fake DP MST ports and encoders. All of these fake ports will have DPMST encoders as opposed to the physical DP ports, which will have TMDS encoders. Hence-mstms are only on physical connectors with TMDS, not fake connectors with DPMST.   --  Cheers, 	Lyude Paul""",technical,Lyude Paul,lyude@redhat.com,1,1,641,0.28708133971291866,0.9,0,0.2222222222222222,0.7222222222222222,0.1111111111111111,0.7222222222222222
502,349495,357124,"Hoping for some review from Gergory, Ralph or Richard who all seem to use this driver!","Hoping for some review from Gergory, Ralph or Richard who all seem to use this driver!  Yours, Linus Walleij  On Fri, Jul 20, 2018 at 9:24 PM Aditya Prayoga <aditya@kobol.io> wrote:",technical,Linus Walleij,linus.walleij@linaro.org,1,0,86,0.16981132075471697,0.5714285714285714,0,0.6923076923076923,0.3076923076923077,0.6923076923076923,0.3076923076923077
503,349495,363137,"Would it be possible to resend the series adding me in CC?  I would like to comment the patches, but unfortunately it seems that I was in none of the mailing list where the series had been sent. I found the patches on patchwork, and started to have a look on it for example, in the patch ""gpio: mvebu: Add support for multiple PWM lines per GPIO chip"", I wonder why the id is stored as it was not used at all. Having the patch inclined in an email would male the review easier.","Hi Linus and Adita    On dim., juil. 29 2018, Linus Walleij <linus.walleij@linaro.org> wrote:   Would it be possible to resend the series adding me in CC?  I would like to comment the patches, but unfortunately it seems that I was in none of the mailing list where the series had been sent.  I found the patches on patchwork, and started to have a look on it for example, in the patch gpio: mvebu: Add support for multiple PWM lines per GPIO chip"", I wonder why the id is stored as it was not used at all.  Having the patch inlined in an email would male the review easier.  Thanks,  Gregory   --  Gregory Clement, Bootlin (formerly Free Electrons) Embedded Linux and Kernel engineering http://bootlin.com""",technical,Gregory CLEMENT,gregory.clement@bootlin.com,1,0,477,1.0,0.7142857142857143,0,1.0,0.0,0.3076923076923077,0.0
504,349495,363143,Same for me :),"On 03/08/2018 11:36, Gregory CLEMENT wrote: Same for me :)",technical,Richard Genoud,richard@sorico.fr,1,0,14,0.04716981132075472,0.8571428571428571,0,1.0,0.0,0.0,0.0
505,349495,363146,"Sorry, for the noise, but please use my Gmail address. Thanks !","On 03/08/2018 11:42, Richard Genoud wrote:  Sorry, for the noise, but please use my gmail address.  Thanks !  Richard",technical,Richard Genoud,richard.genoud@gmail.com,1,0,63,0.14150943396226415,1.0,1,1.0,0.0,0.0,0.0
506,349646,350179,"Thanks for all your catches here. return unsigned long.(hm, arch/arc/include/asm/bitops.h is a little different.)(and unicore32)I'll send a v2 patch (for hexagon).That will still fix the printk format warning above.","On 07/22/2018 02:25 AM, Geert Uytterhoeven wrote:  Hi Geert,  Thanks for all your catches here.  So ffs() and fls() return int, while __ffs() and __fls() return unsigned long.  (hm, arch/arc/include/asm/bitops.h is a little different.) (and unicore32)   I'll send a v2 patch (for hexagon). That will still fix the printk format warning above.    --  ~Randy",technical,Randy Dunlap,rdunlap@infradead.org,0,1,215,1.0,1.0,1,1.0,0.0,0.0,0.0
507,358863,359037,"Do you want me to try to write a patch that does that change? Yeah, true. Should I just remove the method name from the pr_err_once? Or should I also use one of the _rate limited things and print multiple messages? I guess that would conform to normal kernel coding standards a bit better. Thanks for the quick feedback!","On Tue, Jul 31, 2018 at 6:51 PM Al Viro <viro@zeniv.linux.org.uk> wrote:  Do you want me to try to write a patch that does that change?   Yeah, true. Should I just remove the method name from the pr_err_once? Or should I also use one of the _ratelimited things and print multiple messages?   I guess that would conform to normal kernel coding standards a bit better.  Thanks for the quick feedback!",technical,Jann Horn,jannh@google.com,0,1,320,1.0,1.0,1,0.0,0.0,0.0,0.0
508,359320,360784,"Protect posix clock array access against speculation Documentation/process/submitting-patches.rst:  For these reasons, the ``summary`` must be no more than 70-75  characters, and it must describe both what the patch changes, as well  as why the patch might be necessary.  It is challenging to be both  succinct and descriptive, but that is what a well-written summary  should do. The above subject violates all of these rules. We should? Changelogs are about facts. So having:  The ""array_index_mask_nospec"" code has been updated to allow index  argument to have const-qualified type.  So the stack variable ""idx"" which was introduced in commit 19b558db12f9  (""posix-timers: Protect posix clock array access against speculation"") to  cast the const argument 'id' is not longer required. is factual and precise. Hmm?This SOB chain is wrong. Please read and follow the Documentation.","On Wed, 1 Aug 2018, Aashish Lakhwara wrote:      Protect posix clock array access against speculation  Documentation/process/submitting-patches.rst:    For these reasons, the ``summary`` must be no more than 70-75   characters, and it must describe both what the patch changes, as well   as why the patch might be necessary.  It is challenging to be both   succinct and descriptive, but that is what a well-written summary   should do.  The above $subject violates all of these rules.    We should? Changelogs are about facts. So having:    The array_index_mask_nospec"" code has been updated to allow index   argument to have const-qualified type.    So the stack variable ""idx"" which was introduced in commit 19b558db12f9   (""posix-timers: Protect posix clock array access against speculation"") to   cast the const argument 'id' is not longer required.  is factual and precise. Hmm?   This SOB chain is wrong. Please read and follow the Docuentation.  Thanks,  	tglx""",technical,Thomas Gleixner,tglx@linutronix.de,1,0,881,1.0,1.0,1,0.0,0.0,0.0,0.0
509,359897,359920,"How does the caller know whether or not a particular attribute has multiple values?  Is that a fundamental attribute of a particular attribute?  (e.g., the documentation for each attribute must state whether or not that attribute returns multiple attributes or not)","On Wed, Aug 01, 2018 at 05:14:01PM +0100, David Howells wrote:  How does the caller know whether or not a particular attribute has multiple values?  Is that a fundamental attribute of a particular attribute?  (e.g., the documentation for each attribute must state whether or not that attribute returns multiple attributes or not)  	       	    	      	      - Ted",technical,Theodore Y. Ts'o,tytso@mit.edu,1,0,265,1.0,0.875,0,0.0,0.0,0.0,0.0
510,364168,371955,Adding fall through isn't wrong but its reasonable to ask why there is a complex hand unrolled loop here in the first place (and doubly so without a comment). The whole switch statement would be much clear expressed as,"On 05/08/18 05:14, Gustavo A. R. Silva wrote:  Adding fall through isn't wrong but its reasonable to ask why there is a  complex hand unrolled loop here in the first place (and doubly so  without a comment). The whole switch statement would be much clear  expressed as:  	for (j=0, j<bytesperword, j++) 		*c++ = printable_char(*cp++), 	addr += bytesperword,   Daniel.",technical,Daniel Thompson,daniel.thompson@linaro.org,1,0,218,1.0,0.6666666666666666,0,1.0,0.0,1.0,0.0
511,364168,372013,"Yeah, I agree. I can send a patch for that. Thanks for the feedback","Hi Daniel,  On 8/15/18 9:34 AM, Daniel Thompson wrote:  Yeah, I agree. I can send a patch for that.  Thanks for the feedback. -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,67,0.3953488372093023,1.0,1,1.0,0.0,0.0,0.0
512,364236,364359,"Thanks for your patch. Since erofs and gasket are different feature, it is better to separate into two patches. and could you please cc linux-erofs mailing list as well? Yes, there is an extra semicolon in z_erofs_vle_unzip_all, it was reported by Julia Lawall several days ago. Actually, there is a patch in linux-erofs mailing list, but it seems that Chao hasn't reviewed it yet. to the original patch if if you don't mind, do you?","Hi Jiang,  On 2018/8/5 21:57, zhong jiang wrote:  Thanks for your patch. Since erofs and gasket are different feature, it is better to seperate into two patches. and could you please cc linux-erofs mailing list <linux-erofs@lists.ozlabs.org> as well?  Yes, there is an extra semicolon in z_erofs_vle_unzip_all, it was reported by Julia Lawall several days ago. Actually, there is a patch in linux-erofs mailing list, but it seems that Chao hasn't reviewed it yet...  https://lists.ozlabs.org/pipermail/linux-erofs/2018-August/000303.html  I will add Signed-off-by: zhong jiang <zhongjiang@huawei.com> to the original patch if if you don't mind, do you?  Thanks, Gao Xiang",technical,Gao Xiang,gaoxiang25@huawei.com,1,0,433,1.0,0.3333333333333333,0,0.0,0.0,0.0,0.0
513,364236,364366,"Oh, sorry, I missed this one, let me check/review all recent patches again and update them in tree later.","Hi Xiang,  On 2018/8/6 9:27, Gao Xiang wrote:  Oh, sorry, I missed this one, let me check/review all recent patches again and update them in tree later.  Thanks,",technical,Chao Yu,yuchao0@huawei.com,1,0,105,0.25842696629213485,0.5,0,0.0,0.0,0.0,0.0
514,364236,364369,It is nothing. :) I will send v2 version of this patch if doesn't mind.,"Hi Chao,  On 2018/8/6 9:58, Chao Yu wrote:  It is nothing. :) I will send v2 version of this patch if zhong jiang doesn't mind.  Thanks, Gao Xiang",technical,Gao Xiang,gaoxiang25@huawei.com,1,0,71,0.21348314606741572,0.6666666666666666,0,0.0,0.0,0.0,0.0
515,364236,364371,do it.  Thanks.  Then I will just resend the gasket feature separately.,"On 2018/8/6 9:27, Gao Xiang wrote:  do it.  Thanks.  Then I will just resend the gasket feature separately.   Thanks,  zhong jiang",technical,zhong jiang,zhongjiang@huawei.com,0,1,71,0.16853932584269662,0.8333333333333334,0,0.0,0.0,0.0,0.0
516,364236,364373,Thanks for your understanding :),"On 2018/8/6 10:11, zhong jiang wrote:  Thanks for your understanding :)  Thanks, Gao Xiang",technical,Gao Xiang,gaoxiang25@huawei.com,1,0,32,0.06741573033707865,1.0,1,0.0,0.0,0.0,0.0
517,364550,366559,Why not just one call with: The comment applies for the next patch as well.,"Hi Stefan,  Stefan Agner <stefan@agner.ch> wrote on Mon,  6 Aug 2018 11:29:08 +0200:   Why not just one call with:  	vf610_nfc_clear(nfc, NFC_IRQ_STATUS, WERR_EN_BIT | DONE_EN_BIT | IDLE_EN_BIT),  The comment applies for the next patch as well.   Thanks, Miquèl",technical,Miquel Raynal,miquel.raynal@bootlin.com,1,0,75,1.0,1.0,1,1.0,0.0,1.0,0.0
518,365796,366466,it seems like you missed to fix part of the subject as Jacek suggested. I think it should be,"On 7.8.2018 18:04, Dan Murphy wrote:  Hi Dan, it seems like you missed to fix part of the subject as Jacek suggested.  I think it should be: dt-bindings: leds: Add bindings for lm3697 driver""  Best regards, Michal""",technical,Michal Vokáč,michal.vokac@ysoft.com,0,0,92,0.0947867298578199,0.12,0,0.0,1.0,0.0,0.0
519,365796,366557,"Thanks for the heads-up.""dt-"" prefix is indeed more preferred than ""dt: "" .Dan - I will fix it by myself, no need to resend.","Hi Michal.  Thanks for the heads-up. dt-"" prefix is indeed more preferred than ""dt: "".  Dan - I will fix it by myself, no need to resend.  Best regards, Jacek Anaszewski  On 08/08/2018 09:56 AM, Michal Vokáč wrote:""",technical,Jacek Anaszewski,jacek.anaszewski@gmail.com,1,0,124,0.15165876777251186,0.2,0,0.0,1.0,0.0,0.0
520,365796,367098,"This is quite long way to describe a bitmask, no? Could we make it so that control-bank-cfg is not needed? Can we compute control-bank-cfg from this? Does the example show correct config? AFAICT all controls go to bank A according to control-bank-cfg, yet led@1 describes bank B...","On Tue 2018-08-07 11:04:41, Dan Murphy wrote:  This is quite long way to describe a bitmask, no? Could we make it so that control-bank-cfg is not needed?   Can we compute control-bank-cfg from this?   Does the example show correct config? AFAICT all controls go to bank A according to control-bank-cfg, yet led@1 describes bank B...  									Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,281,0.26540284360189575,0.24,0,0.16666666666666666,0.8333333333333334,0.0,0.0
521,365796,367099,"HI! That's rather long and verbose way to describe a bitmap, right? Is rbtree good idea? You don't have that many registers.....No error checking required here? This checks if we have just one bank, I see it. Should it also check the led actually uses the correct bank? Is not the fwnode_handle_put() done twice for non-error case? The if is not needed here. Misleading, this does nothing with regulators.","Hi!    That's rather long and verbose way to describe a bitmap, right?   Is rbtree good idea? You don't have that many registers.  ....  No error checking required here?   This checks if we have just one bank, I see it. Should it also check the led actually uses the correct bank?   Is not the fwnode_handle_put() done twice for non-error case?   The if is not needed here.   Misleading, this does nothing with regulators.  									Pavel  --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html ",technical,Pavel Machek,pavel@ucw.cz,1,0,405,0.41232227488151657,0.28,0,0.16666666666666666,0.8333333333333334,0.0,0.0
522,365796,367136,"HI! Can we forget about the LED strings, and just expose the sinks as Linux LED devices?","Hi!   Can we forget about the LED strings, and just expose the sinks as Linux LED devices? 									Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html ",technical,Pavel Machek,pavel@ucw.cz,1,0,88,0.0947867298578199,0.36,0,0.16666666666666666,0.8333333333333334,0.0,0.0
523,365796,367138,2 sinks 3 LED strings.  How do you know which LED string is which and what bank it belongs to when setting the brightness.  Each Bank has a separate register for brightness control.,"On 08/08/2018 04:02 PM, Pavel Machek wrote:  2 sinks 3 LED strings.  How do you know which LED string is which and what bank it belongs to when setting the brightness.  Each Bank has a separate register for brightness control.  Dan    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,181,0.17061611374407584,0.4,0,0.16666666666666666,0.8333333333333334,0.0,0.0
524,365796,367140,"Yes, and LED strings are statically assigned to banks, right? So why not simply forget about LED strings for sake of hw abstractions, and work just with banks?","On Wed 2018-08-08 16:04:43, Dan Murphy wrote:  Yes, and LED strings are statically assigned to banks, right?  So why not simply forget about LED strings for sake of hw abstractions, and work just with banks? 									Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,159,0.15639810426540285,0.44,0,0.16666666666666666,0.8333333333333334,0.0,0.0
525,365796,367153,How would you set the control bank register for the correct LED string configuration?,"On 08/08/2018 04:09 PM, Pavel Machek wrote:  How would you set the control bank register for the correct LED string configuration?  Dan    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,85,0.07109004739336493,0.52,0,0.16666666666666666,0.8333333333333334,0.0,0.0
526,365796,367155,JacekI could change the name to led-sources.  But this part does not really follow the 1 output to a1 LED string topology.,"Jacek  On 08/08/2018 04:09 PM, Jacek Anaszewski wrote:  I could change the name to led-sources.  But this part does not really follow the 1 output to a 1 LED string topology.   --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,122,0.11374407582938388,0.56,0,0.16666666666666666,0.8333333333333334,0.0,0.0
527,365796,367156,Have property at each LED saying which  HVLEDs it controls?,"On Wed 2018-08-08 16:41:16, Dan Murphy wrote:  Have property at each LED saying which which HVLEDs it controls?  									Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,59,0.052132701421800945,0.6,0,0.16666666666666666,0.8333333333333334,0.0,0.0
528,365796,367161,"Hi! Yes. No, I don't want that. I'd like 2 child nodes, each specifying which HVLEDs it controls. Let me edit the original proposal.","Hi!    Yes.   No, I don't want that.  I'd like 2 child nodes, each specifying which HVLEDs it controls.  Let me edit the original proposal.  									Pavel   --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,132,0.15639810426540285,0.68,0,0.16666666666666666,0.8333333333333334,0.0,0.0
529,365796,367162,What I'd like to see: Remove control-bank-cfg. Add required child propertyset of outputs this child controls.,"On Tue 2018-08-07 11:04:41, Dan Murphy wrote:  What I'd like to see:   Remove control-bank-cfg.   Add required child property:         - hvleds = <list> -- set of outputs this child controls.  									Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,109,0.0947867298578199,0.72,0,0.16666666666666666,0.8333333333333334,0.0,0.0
530,365796,367623,"led-sources was designed for describing the topology where one LED can be connected to more then one output, see bindings of max77693-led.Here the topology is a bit different - more than one LED (string) can be connected to a single bank, but this is accomplished inside the chip. Logically LEDs configured that way can be treated as a single LED(string) connected to two outputs, and what follows they should be described by a single DT child node.led-sources will fit very well for this purpose. You could do the following mapping. Then, in the child DT nodes you would use these identifiers to describe the topology: Following node would describe strings connected to the outputsHVLED1 and HVLED2 controlled by bank. I agree with Pavel, but I propose to use already documented common DT LED property.","Dan,  On 08/08/2018 11:45 PM, Dan Murphy wrote:  led-sources was designed for describing the topology where one LED can be connected to more then one output, see bindings of max77693-led (in Documentation/devicetree/bindings/mfd/max77693.txt).  Here the topology is a bit different - more than one LED (string) can be connected to a single bank, but this is accomplished inside the chip. Logically LEDs configured that way can be treated as a single LED (string) connected to two outputs, and what follows they should be described by a single DT child node.  led-sources will fit very well for this purpose. You could do the following mapping:  0 - HVLED1 1 - HVLED2 2 - HVLED3  Then, in the child DT nodes you would use these identifiers to describe the topology:  Following node would describe strings connected to the outputs HVLED1 and HVLED2 controlled by bank A.  led@0 { 	reg = <0>, 	led-sources = <0>. <1>, 	label = white:first_backlight_cluster"", 	linux,default-trigger = ""backlight"", },   IOW I agree with Pavel, but I propose to use already documented common DT LED property.  --  Best regards, Jacek Anaszewski""",technical,Jacek Anaszewski,jacek.anaszewski@gmail.com,1,0,803,0.7109004739336493,0.76,0,0.16666666666666666,0.8333333333333334,0.0,0.0
531,365796,367698,"Jacek and PavelI agree to use the led-sources but I still believe this approach may be confusing to other sw devas and will lead to configuration issues by users. This implementation requires the sw dev to know which strings are controlled by which bank. And this method may produce a misconfiguration like something below where HVLED2 is declared in both bank A and bank Bled. The driver will need to be intelligent and declare a miss configuration on the above. Not saying this cannot be done but I am not sure why we want to add all of these extra LoC and intelligence in the kernel driver. The driver cannot make assumptions on the intention.  And the device tree documentation will need to pretty much need a lengthy explanation on how to configure the child nodes. The implementation I suggested removes that ambiguity.  It is a simple integer that is written to the device as part of the device configuration, which the config is a setting for the device. The child nodes denote which bank the exposed LED node will control.  Removing any need for the sw developers new or old to know the specific device configurations.","Jacek and Pavel  On 08/09/2018 07:09 AM, Jacek Anaszewski wrote:  I agree to use the led-sources but I still believe this approach may be confusing to other sw devs and will lead to configuration issues by users.  This implementation requires the sw dev to know which strings are controlled by which bank. And this method may produce a misconfiguration like something below where HVLED2 is declared in both bank A and bank B  led@0 { 	reg = <0>, 	led-sources = <0>. <1>, 	label = white:first_backlight_cluster"", 	linux,default-trigger = ""backlight"", },  led@1 { 	reg = <1>, 	led-sources = <1>. <2>, 	label = ""white:keypad_cluster"", 	linux,default-trigger = ""backlight"", },  The driver will need to be intelligent and declare a miss configuration on the above. Not saying this cannot be done but I am not sure why we want to add all of these extra LoC and intelligence in the kernel driver. The driver cannot make assumptions on the intention.  And the device tree documentation will need to pretty much need a lengthy explanation on how to configure the child nodes.  The implementation I suggested removes that ambiguity.  It is a simple integer that is written to the device as part of the device configuration, which the config is a setting for the device.  The child nodes denote which bank the exposed LED node will control.  Removing any need for the sw developers new or old to know the specific device configurations.   Dan --  ------------------ Dan Murphy""",technical,Dan Murphy,dmurphy@ti.com,0,1,1127,1.0,0.84,0,0.16666666666666666,0.8333333333333334,0.0,0.0
532,365796,367804,Yes I know that the driver can check the string but if the same string is declared by another child then the driver must exit with -EINVAL.  Again a lot of code for little pay off. I believe we should keep drivers as simple as possible. I will add the changes. Unfortunately in either case this high level of documentation will need to be done. I believe both solutions will raise questions and concerns. There does not seem to be a good way to describe this device. Both solutions are wrought with issues and concerns. But like I said I will re-write the code with the above suggestion.,"On 08/09/2018 09:48 AM, Jacek Anaszewski wrote:  Yes I know that the driver can check the string but if the same string is declared by another child then the driver must exit with -EINVAL.  Again a lot of code for little pay off. I believe we should keep drivers as simple as possible.  I will add the changes.   Unfortunately in either case this high level of documentation will need to be done. I believe both solutions will raise questions and concerns.  There does not seem to be a good way to describe this device. Both solutions are wrought with issues and concerns.  But like I said I will re-write the code with the above suggestion.   Dan --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,587,0.5545023696682464,0.92,0,0.16666666666666666,0.6666666666666666,0.0,0.0
533,365796,368113,"HI! Yes. But please do it that way, it is still better than being different from all the other drivers.","Hi!   Yes. But please do it that way, it is still better than being different from all the other drivers.  Thanks, 									Pavel  --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html ",technical,Pavel Machek,pavel@ucw.cz,1,0,103,0.11374407582938388,0.96,0,0.3333333333333333,0.6666666666666666,0.0,0.6666666666666666
534,365796,370174,Thanks for the review It will be removed with v3 ack Ack It will be removed with v3.,"Pavel  Thanks for the review  On 08/08/2018 02:59 PM, Pavel Machek wrote:  It will be removed with v3   ack   Ack   It will be removed with v3   Ack   Ack   Ack    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,84,0.09004739336492891,1.0,1,1.0,0.0,0.6666666666666666,0.0
535,365796,367158,Isn't that what I have already using the reg property? Then we would have to aggregate the configuration and make a determination in the driver. But that does not follow the LED child node ideology. Each output of the LED driver should have a child node. In this case the outputs are the sinks (inputs) and there are only 2 sinks so having 3 LED child nodes would be confusing and there are required properties for each child like label. Each child node would then need to present 1 LED node to the user space to control the LED string.  Which would be technically incorrect because you would have 2 LED nodes controlling the same control bank sink.,"Pavel  On 08/08/2018 04:45 PM, Pavel Machek wrote:  Isn't that what I have already using the reg property?  Then we would have to aggregate the configuration and make a determination in the driver.  But that does not follow the LED child node ideology. Each output of the LED driver should have a child node.  In this case the outputs are the sinks(inputs) and there are only 2 sinks so having 3 LED child nodes would be confusing and there are required properties for each child like label.  Each child node would then need to present 1 LED node to the user space to control the LED string.  Which would be technically incorrect because you would have 2 LED nodes controlling the same control bank sink.    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,649,0.6066350710900474,0.64,0,0.16666666666666666,0.8333333333333334,0.0,0.0
536,372597,373109,"Besides this, please split it into two patches. The RCU change does not belong to ""comment fix"" for sure. Thanks","On 2018年08月17日 03:30, David Miller wrote:  Besides this, please split it into two patches. The RCU change does not  belong to comment fix"" for sure.  Thanks""",technical,Jason Wang,jasowang@redhat.com,1,0,112,1.0,0.75,0,0.0,0.0,0.0,0.0
537,372597,373169,"Thanks for the reminder. Because this change is trivial, I change the subject.","Thanks for the reminder. Because this change is trivial, I change the subject. On Fri, Aug 17, 2018 at 12:29 PM Jason Wang <jasowang@redhat.com> wrote:   --  Regards, Wang Jian",technical,Wang Jian,jianjian.wang1@gmail.com,0,1,78,0.64,1.0,1,0.0,0.0,0.0,0.0
538,373262,374131,"Why ""bridge"" and not ""host"" or even something to stand for ""root complex""? Or maybe it can still be ""host_bridge""?","On Fri, Aug 17, 2018 at 12:33 PM Arnd Bergmann <arnd@arndb.de> wrote:  Why bridge"" and not ""host"" or even something to stand for ""root complex""?  Or maybe it can still be ""host_bridge""? """,technical,Rafael J. Wysocki,rafael@kernel.org,1,0,114,0.2127659574468085,0.6071428571428571,0,0.043478260869565216,0.9347826086956522,0.043478260869565216,0.0
539,373262,374135,"This looks fishy, as bridge->private is not set at this point AFAICS, unless one of the previous patches changes that.","On Fri, Aug 17, 2018 at 12:32 PM Arnd Bergmann <arnd@arndb.de> wrote:  This looks fishy, as bridge->private is not set at this point AFAICS, unless one of the previous patches changes that.",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,118,0.1773049645390071,0.6428571428571429,0,0.043478260869565216,0.9347826086956522,0.0,0.0
540,373262,374242,"bridge->private what comes after the bridge structure, and it's allocated by pci_alloc_host_bridge() passing the size of the structure we want for this private area.","On Mon, Aug 20, 2018 at 10:31 AM Rafael J. Wysocki <rafael@kernel.org> wrote:   bridge->private what comes after the bridge structure, and it's allocated by pci_alloc_host_bridge() passing the size of the structure we want for this private area.           Arnd",technical,Arnd Bergmann,arnd@arndb.de,1,1,165,0.2198581560283688,0.6785714285714286,0,0.06521739130434782,0.9347826086956522,0.0,0.0
541,373262,374244,"I did this for consistency with the naming in drivers/pci/probe.c,which always declares the local variable as 'struct pci_host_bridge *bridge'.It's easy to change here if you feel strongly about it (I don't).","On Mon, Aug 20, 2018 at 10:23 AM Rafael J. Wysocki <rafael@kernel.org> wrote:  I did this for consistency with the naming in drivers/pci/probe.c, which always declares the local variable as 'struct pci_host_bridge *bridge'. It's easy to change here if you feel strongly about it (I don't).          Arnd",technical,Arnd Bergmann,arnd@arndb.de,1,1,208,0.2765957446808511,0.7142857142857143,0,0.06521739130434782,0.9347826086956522,0.0,0.0
542,373262,374246,I would leave host_bridge here.  It would make the patch smaller too I think.,"On Mon, Aug 20, 2018 at 1:20 PM Arnd Bergmann <arnd@arndb.de> wrote:  I would leave host_bridge here.  It would make the patch smaller too I think.",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,77,0.11347517730496454,0.75,0,0.06521739130434782,0.9347826086956522,0.0,0.0
543,373262,374248,"I see, sorry for the noise.","On Mon, Aug 20, 2018 at 1:17 PM Arnd Bergmann <arnd@arndb.de> wrote:  I see, sorry for the noise.",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,27,0.05673758865248227,0.7857142857142857,0,0.06521739130434782,0.9347826086956522,0.0,0.0
544,373262,374250,"Ok, I've changed my local copy as you suggested now.","On Mon, Aug 20, 2018 at 1:24 PM Rafael J. Wysocki <rafael@kernel.org> wrote:  Ok, I've changed my local copy as you suggested now.        Arnd",technical,Arnd Bergmann,arnd@arndb.de,1,1,52,0.09219858156028368,0.8214285714285714,0,0.06521739130434782,0.9347826086956522,0.0,0.0
545,373262,374710,"I really like the idea behind this series. I wonder if there is a way to avoid some of that by adding a few more helpers, but even without the helpers that approach looks ok to me. Do you have a git tree somewhere to play around with the changes?","On Fri, Aug 17, 2018 at 12:26:30PM +0200, Arnd Bergmann wrote:  I really like the idea behind this series.   I wonder if there is a way to avoid some of that by adding a few more helpers, but even without the helpers that approach looks ok to me.  Do you have a git tree somewhere to play around with the changes?",technical,Christoph Hellwig,hch@infradead.org,1,0,246,0.3829787234042553,0.8571428571428571,0,0.06521739130434782,0.9130434782608695,0.0,0.0
546,373262,374977,"Ok, thanks for taking a first look. One core part that gets duplicated a lot (also in existing drivers) is the chunk that could be handled by this: That would probably help, but we should think carefully about the set of fields that we want pass here, specifically because the idea of splitting the probing into two parts was to avoid having to come up with a new interface every time that list changes due to some rework. For instance, the numa node is something that might get passed here, and if we decide to split out the operations into a separate pci_host_bridge_ops structure, the pointer to that would also be something we'd want to pass this way. I now uploaded it (with fixes incorporated) to","On Tue, Aug 21, 2018 at 8:14 AM Christoph Hellwig <hch@infradead.org> wrote:  Ok, thanks for taking a first look.  One core part that gets duplicated a lot (also in existing drivers) is the chunk that could be handled by this:  int pci_host_bridge_init(struct pci_host_bridge *bridge,                    struct device *parent, int bus,                    struct pci_ops *ops, void *sysdata,                    struct list_head *resource_list) {        if (resources)               list_splice_init(resources, &bridge->windows),        bridge->dev.parent = parent,        bridge->sysdata = sysdata,        bridge->busnr = bus,        bridge->ops = ops, }  That would probably help, but we should think carefully about the set of fields that we want pass here, specifically because the idea of splitting the probing into two parts was to avoid having to come up with a new interface every time that list changes due to some rework.  For instance, the numa node is something that might get passed here, and if we decide to split out the operations into a separate pci_host_bridge_ops structure, the pointer to that would also be something we'd want to pass this way.   I now uploaded it (with fixes incorporated) to  https://git.kernel.org/pub/scm/linux/kernel/git/arnd/playground.git pci-probe-rework         arnd",technical,Arnd Bergmann,arnd@arndb.de,1,1,702,1.0,0.8928571428571429,0,0.06521739130434782,0.9130434782608695,0.0,0.0
547,373262,375055,Hm... are you turning direct calls into retpolined indirect calls?,"On Mon, 2018-08-20 at 23:14 -0700, Christoph Hellwig wrote:  Hm... are you turning direct calls into retpolined indirect calls?",technical,David Woodhouse,dwmw2@infradead.org,1,0,66,0.0851063829787234,0.9285714285714286,0,0.08695652173913043,0.9130434782608695,0.0,0.0
548,373262,375072,He does.  But not anywhere near the fast path.,"On Tue, Aug 21, 2018 at 12:30:50PM +0100, David Woodhouse wrote:  He does.  But not anywhere near the fast path.",technical,Christoph Hellwig,hch@infradead.org,1,0,46,0.07801418439716312,0.9642857142857143,0,0.08695652173913043,0.9130434782608695,0.0,0.9130434782608695
549,373262,418139,"Sorry for the late response to this. I think I'm generally on-board with this.  I admit I'm a little hesitant about adding 200 lines of code when this is really more ""cleanup"" than new functionality, but I think a lot of that is because this series contains costs (e.g., duplicating code) for everybody but only has the corresponding benefits for a few (ACPI, x86, xenfront).Those cases are much closer to parity in terms of lines added/removed. I saw some minor comments that suggested you had some updates, so I'll watch for an updated posting.","On Fri, Aug 17, 2018 at 12:26:30PM +0200, Arnd Bergmann wrote:  Sorry for the late response to this.  I think I'm generally on-board with this.  I admit I'm a little hesitant about adding 200 lines of code when this is really more cleanup"" than new functionality, but I think a lot of that is because this series contains costs (e.g., duplicating code) for everybody but only has the corresponding benefits for a few (ACPI, x86, xenfront). Those cases are much closer to parity in terms of lines added/removed.  I saw some minor comments that suggested you had some updates, so I'll watch for an updated posting.  Bjorn""",technical,Bjorn Helgaas,helgaas@kernel.org,1,0,546,0.8014184397163121,1.0,1,1.0,0.0,0.9130434782608695,0.0
550,373468,586193,"I'm inclined to apply this -- because it will not break anything, and it would at least enable testing by people, who have this hardware. Thoughts?","Hi Calvin, I'm inclined to apply this -- because it will not break anything, and it would at least enable testing by people, who have this hardware.  thoughts? -Len  On Fri, Aug 17, 2018 at 12:35 PM Calvin Walton <calvin.walton@kepstin.ca> wrote:   --  Len Brown, Intel Open Source Technology Center",technical,Len Brown,lenb@kernel.org,1,0,147,0.24031007751937986,0.7142857142857143,0,1.0,0.0,0.0,0.0
551,373468,586262,"(Whoops, resending this message. Forgot that Gmail defaulted to HTML...) By all means go ahead and apply the package power patch as well. I've tested it on both Summit (single-die desktop) and Raven (desktop/laptopapu) with good results. The worst case as it stands is that if the multi-die packages (EPYC/TR) don't aggregate the value per package, then package power will be under-reported.(I previously wrote that this would be easier to test with the patch applied, because the package power is reported with the -Dump option, but I'm not sure that's actually the case - the value might still only be collected once per package?)","(Whoops, resending this message. Forgot that Gmail defaulted to HTML...)  On Wed, 2019-03-20 at 19:36 -0400, Len Brown wrote:  Hi Len,  By all means go ahead and apply the package power patch as well. I've tested it on both Summit (single-die desktop) and Raven (desktop/laptop apu) with good results. The worst case as it stands is that if the multi-die packages (EPYC/TR) don't aggregate the value per package, then package power will be under-reported.  (I previously wrote that this would be easier to test with the patch applied, because the package power is reported with the -Dump option, but I'm not sure that's actually the case - the value might still only be collected once per package?)  Calvin.",technical,Calvin Walton,calvin.walton@kepstin.ca,0,1,632,1.0,0.8571428571428571,0,1.0,0.0,0.0,0.0
552,373468,586316,"Right -- we'd have to move it to a per-core or per-CPU data structure to capture it on a finer granularity than per package. Of course, if you want to see if you get different values on the cpus, you can always dump the raw values from each cpu using rdmsr(1).Unfortunately, I don't have that AMD hardware -- somebody with an interest in it can test it out.","Right -- we'd have to move it to a per-core or per-CPU data structure to capture it on a finer granularity than per package.  Of course, if you want to see if you get different values on the cpus, you can always dump the raw values from each cpu using rdmsr(1).  Unfortunately, I don't have that AMD hardware -- somebody with an interest in it can test it out.  thanks, Len Brown, Intel Open Source Technology Center",technical,Len Brown,lenb@kernel.org,1,0,357,0.6124031007751938,1.0,1,1.0,0.0,0.0,0.0
553,373468,586191,"yes, thanks for the suggestion. Try  release branch at [1].","Applied, thanks!  On Fri, Aug 17, 2018 at 12:35 PM Calvin Walton <calvin.walton@kepstin.ca> wrote:   --  Len Brown, Intel Open Source Technology Center",technical,Len Brown,lenb@kernel.org,1,0,59,0.11627906976744186,0.5714285714285714,0,1.0,0.0,1.0,0.0
554,373889,374612,"As enable is required and direction is optional, enable should come first. So fix the pwms property instead. (And perhaps make the binding more explicit as to what the order should be.","On Sat, Aug 18, 2018 at 03:58:55PM -0400, Brian Masney wrote:  As enable is required and direction is optional, enable should come  first. So fix the pwms property instead. (And perhaps make the binding  more explicit as to what the order should be.",technical,Rob Herring,robh@kernel.org,1,0,184,1.0,1.0,1,1.0,0.0,1.0,0.0
555,377230,378176,"You are right. The XFS filesystem was created on a small ramfs device and so the disk space was tiny. The micro benchmark that I used exposes some extreme cases that may not be normally observed. Those were part of the perf trace that I marked down: A spurious wakeup shouldn't change the behavior as the waiter should find that it is still being queued (list not empty) and so will sleep again. However, if the waiter is rightfully waken but find that there is not enough log space somehow, it will put itself back to the end of the queue. That is a change in behavior that I think could be an issue. I am just wondering why the code doesn't reserve the actual log space at the time a task is being waken. Instead, it has to try to get it itself after being waken. Also I think the current code allows consecutive waiters to come in and wake up the same set of tasks again and again. That may be where a part of the performance slowdown come from. As I said above, spurious wakeup shouldn't cause problem. However, consecutive waiters will probably wake up more tasks than there is logspace available. In this case, some of the tasks will be put back to the queue. Maybe a counter to record the log space temporarily reserved for the waken-up task can help to prevent over-subscription. The wake_q should work correctly as it is used by mutexes and rwsems. We will see a lot of problem reports if that is not the case. That is probably true. I am using a old git (1.8). I should probably upgrade to a newer one. OK It is because the wake_q uses a field in the task structure for queuing. So a task can only be in one wake_q at a time. Leaving the ticket in the queue may cause the task to be put into multiple wake_q causing missed wakeup, perhaps. Yes, you are right. The xlog_grant_head_wake_all() need to be modified as well.OK, I need more time to think about some of the questions that you raise.  Thanks for reviewing the patch.","On 08/23/2018 08:30 PM, Dave Chinner wrote:  You are right. The XFS filesystem was created on a small ramfs device and so the disk space was tiny. The microbenchmark that I used exposes some extreme cases that may not be normally observed.   Those were part of the perf trace that I marked down:  Before patch:  +   69.69%    69.17%  reaim            [kernel.vmlinux]            [k] native_queued_spin_lock_ -   54.48%     0.01%  reaim            [kernel.vmlinux]            [k] do_sys_open    - 54.46% do_sys_open       - 54.35% do_filp_open          - 54.34% path_openat             - 42.18% do_truncate                - 42.17% notify_change                   - 42.12% xfs_vn_setattr                      - 42.09% xfs_setattr_size                         - 42.08% xfs_setattr_nonsize                            - 27.31% xfs_trans_alloc                               - 27.28% xfs_trans_reserve                                  - xfs_log_reserve                                     - 27.20% xlog_grant_head_check                                        + 13.79% queued_spin_lock_slowpath                                        + 12.95% xlog_grant_head_wait                            + 14.72% __xfs_trans_commit             - 8.90% xfs_generic_create                - 5.81% security_inode_init_security                   - 5.72% xfs_initxattrs                      - xfs_attr_set                         - 2.90% xfs_bmap_add_attrfork                            + 1.88% xfs_trans_alloc                            + 1.02% __xfs_trans_commit                         + 1.78% xfs_trans_alloc                         + 1.01% __xfs_trans_commit                + 3.08% xfs_create             + 2.62% down_write  1500    99.36    3982.81 351.51  91485.51   60.99      15.87    20.05    79  After patch:  -   41.68%     0.07%  reaim  [kernel.vmlinux] [k] do_sys_open    - 41.62% do_sys_open       - 41.23% do_filp_open          - 41.20% path_openat             - 30.32% do_truncate                - 30.27% notify_change                   - 30.06% xfs_vn_setattr                      - 29.90% xfs_setattr_size                         - 29.88% xfs_setattr_nonsize                            - 20.61% xfs_trans_alloc                               - 20.46% xfs_trans_reserve                                  - 20.43% xfs_log_reserve                                     - 19.96% xlog_grant_head_check                                        + 10.24% queued_spin_lock_slowpath                                        + 9.04% xlog_grant_head_wait                            + 9.06% __xfs_trans_commit             - 7.16% xfs_generic_create                + 4.27% security_inode_init_security                + 2.87% xfs_create             + 1.44% down_write +   39.59%    38.98%  reaim  [kernel.vmlinux] [k] native_queued_spin_lock  1500    22.88    414.82  365.48  397290.21  264.86     3.40     18.41    81   A spurious wakeup shouldn't change the behavior as the waiter should find that it is still being queued (list not empty) and so will sleep again. However, if the waiter is rightfully waken but find that there is not enough log space somehow, it will put itself back to the end of the queue. That is a change in behavior that I think could be an issue.  I am just wondering why the code doesn't reserve the actual log space at the time a task is being waken. Instead, it has to try to get it itself after being waken.  Also I think the current code allows consecutive waiters to come in and wake up the same set of tasks again and again. That may be where a part of the performance slowdown come from.   As I said above, spurious wakeup shouldn't cause problem. However, consecutive waiters will probably wake up more tasks than there is log space available. In this case, some of the tasks will be put back to the queue. Maybe a counter to record the log space temporarily reserved for the waken-up task can help to prevent over-subscription.    The wake_q should work correctly as it is used by mutexes and rwsems. We will see a lot of problem reports if that is not the case.   That is probably true.   I am using a old git (1.8). I should probably upgrade to a newer one.   OK   It is because the wake_q uses a field in the task structure for queuing. So a task can only be in one wake_q at a time. Leaving the ticket in the queue may cause the task to be put into multiple wake_q causing missed wakeup, perhaps.    Yes, you are right. The xlog_grant_head_wake_all() need to be modified as well.   OK, I need more time to think about some of the questions that you raise.  Thanks for reviewing the patch.  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,1935,1.0,0.8333333333333334,0,0.3333333333333333,0.3333333333333333,0.0,0.3333333333333333
556,378204,378266,The original is better than the new version.  Please leave it as is. Regards,"On Sat, Aug 25, 2018 at 02:07:01AM +0300, GuyLuz wrote:  The original is better than the new version.  Please leave it as is.  regards, dan carpenter",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,76,1.0,1.0,1,0.0,0.0,0.0,0.0
557,378505,378568,"Oh, I just noticed you are using a ramfs for this benchmark,tl, dr: Once you pass a certain point, ramdisks can be *much* slower than SSDs on journal intensive workloads like AIM7. Hence it would be useful to see if you have the same problems on, say, high performance nvme SSDs.-----Ramdisks have substantially different means log IO completion and wakeup behaviour compared to real storage on real production systems. Basically, ramdisks are synchronous and real storage is a synchronous. That is, on a ramdisk the IO completion is run synchronously in the same task as the IO submission because the IO is just a mercy().Hence a single dispatch thread can only drive an IO queue depth of 1IO - there is no concurrency possible. This serialises large parts of the XFS journal - the journal is really an asynchronous IO engine that gets it's performance from driving deep IO queues and batching commits while IO is in flight. Ramdisks also have very low IO latency, which means there's only a very small window for ""IO in flight"" batching optimisations to be made effectively. It effectively stops such algorithms from working completely. This means the XFS journal behaves very differently on ramdisks when compared to normal storage. The submission batching techniques reduces log IOs by a factor of 10-20 under heavy synchronous transaction loads when there is any noticeable journal IO delay - a few tens of microseconds is enough for it to function effectively, but a ramdisk doesn't even have this delay on journal IO.  The submission batching also has the effect of reducing log space wakeups by the same factor there are less IO completions signalling that space has been made available. Further, when we get async IO completions from real hardware, they get processed in batches by a completion work queue - this leads to there typically only being a single reservation space update from all batched IO completions. This tends to reduce log space wakeups due to log IO completion by a factor of 6-8 as the log can have upto 8 concurrent IOs in flight at a time. And when we throw in the lack of batching, merging and IO completion aggregation of metadata writeback because ramdisks are synchronous and don't queue or merge adjacent IOs, we end up with lots more contention on the AIL lock and much more frequent log space wakeups (i.e. from log tail movement updates). This further exacerbates the problems the log already has with synchronous IO.IOWs, log space wakeups on real storage are likely to be 50-100xlower than on a ramdisk for the same metadata and journal intensive workload, and as such those workloads often run faster on real storage than they do on ramdisks. This can be trivially seen with dbench, a simple IO benchmark that hammers the journal. On a ramdisk, I can only get 2-2.5GB/s throughput from the benchmark before the log bottlenecks at about 20,000 log tiny IOs per second. In comparison, on an old, badly abused Samsung 850EVO SSD, I see 5-6GB/s in 2,000 log IOs per second because of the pipelining and IO batching in the XFS journal async IO engine and the massive reduction in metadata IO due to merging of adjacent IOs in the block layer. i.e. the journal and metadata writeback design allows the filesystem to operate at a much higher synchronous transaction rate than would otherwise be possible by taking advantage of the IO concurrency that storage provides us with. So if you use proper storage hardware (e.g. nvme SSD) and/or an appropriately sized log, does the slow path wakeup contention go away? Can you please test both of these things and report the results so we can properly evaluate the impact of these changes?","On Sun, Aug 26, 2018 at 04:53:14PM -0400, Waiman Long wrote:  Oh, I just noticed you are using a ramfs for this benchmark,  tl, dr: Once you pass a certain point, ramdisks can be *much* slower than SSDs on journal intensive workloads like AIM7. Hence it would be useful to see if you have the same problems on, say, high performance nvme SSDs.  -----  Ramdisks have substantially different means log IO completion and wakeup behaviour compared to real storage on real production systems. Basically, ramdisks are synchronous and real storage is asynchronous.  That is, on a ramdisk the IO completion is run synchronously in the same task as the IO submission because the IO is just a memcpy(). Hence a single dispatch thread can only drive an IO queue depth of 1 IO - there is no concurrency possible. This serialises large parts of the XFS journal - the journal is really an asynchronous IO engine that gets it's performance from driving deep IO queues and batching commits while IO is in flight.  Ramdisks also have very low IO latency, which means there's only a very small window for IO in flight"" batching optimisations to be made effectively. It effectively stops such algorithms from working completely. This means the XFS journal behaves very differently on ramdisks when compared to normal storage.  The submission batching techniques reduces log IOs by a factor of 10-20 under heavy synchrnous transaction loads when there is any noticeable journal IO delay - a few tens of microseconds is enough for it to function effectively, but a ramdisk doesn't even have this delay on journal IO.  The submission batching also has the effect of reducing log space wakeups by the same factor there are less IO completions signalling that space has been made available.  Further, when we get async IO completions from real hardware, they get processed in batches by a completion workqueue - this leads to there typically only being a single reservation space update from all batched IO completions. This tends to reduce log space wakeups due to log IO completion by a factor of 6-8 as the log can have up to 8 concurrent IOs in flight at a time.  And when we throw in the lack of batching, merging and IO completion aggregation of metadata writeback because ramdisks are synchrnous and don't queue or merge adjacent IOs, we end up with lots more contention on the AIL lock and much more frequent log space wakeups (i.e. from log tail movement updates). This futher exacerbates the problems the log already has with synchronous IO.  IOWs, log space wakeups on real storage are likely to be 50-100x lower than on a ramdisk for the same metadata and journal intensive workload, and as such those workloads often run faster on real storage than they do on ramdisks.  This can be trivially seen with dbench, a simple IO benchmark that hammers the journal. On a ramdisk, I can only get 2-2.5GB/s throughput from the benchmark before the log bottlenecks at about 20,000 log tiny IOs per second. In comparison, on an old, badly abused Samsung 850EVO SSD, I see 5-6GB/s in 2,000 log IOs per second because of the pipelining and IO batching in the XFS journal async IO engine and the massive reduction in metadata IO due to merging of adjacent IOs in the block layer. i.e. the journal and metadata writeback design allows the filesystem to operate at a much higher synchronous transaction rate than would otherwise be possible by taking advantage of the IO concurrency that storage provides us with.  So if you use proper storage hardware (e.g. nvme SSD) and/or an appropriately sized log, does the slowpath wakeup contention go away? Can you please test both of these things and report the results so we can properly evaluate the impact of these changes?  Cheers,  Dave. --  Dave Chinner david@fromorbit.com""",technical,Dave Chinner,david@fromorbit.com,0,0,3667,1.0,0.6,0,0.0,1.0,0.0,0.0
558,378505,378684,"Note that all these ramdisk issues you mentioned below will also apply to using the pmem driver on nvdimms, which might be a more realistic version.  Even worse at least for cases where the nvdimms aren't actually powerfail dram of some sort with write through caching and ADR the latency is going to be much higher than the ramdisk as well.","On Mon, Aug 27, 2018 at 10:21:34AM +1000, Dave Chinner wrote:  Note that all these ramdisk issues you mentioned below will also apply to using the pmem driver on nvdimms, which might be a more realistic version.  Even worse at least for cases where the nvdimms aren't actually powerfail dram of some sort with write through caching and ADR the latency is going to be much higher than the ramdisk as well.",technical,Christoph Hellwig,hch@infradead.org,1,0,341,0.09420289855072464,0.7,0,0.0,0.0,0.0,0.0
559,378505,379012,"Oh sorry, I made a mistake. There were some problems with my test configuration. I was actually running the test on a regular enterprise-class disk device mount on. It was not an SSD, nor ramdisk. I reran the test on ramdisk, the performance of the patched kernel was 679,880 jobs/min which was a bit more than double the 285,221 score that I got on a regular disk. So the filesystem used wasn't tiny, though it is still not very large. The test was supposed to create 16 ramdisks and distribute the test tasks to the ramdisks. Instead, they were all pounding on the same filesystem worsening the spinlock contention problem.","On 08/26/2018 08:21 PM, Dave Chinner wrote:  Oh sorry, I made a mistake.  There were some problems with my test configuration. I was actually running the test on a regular enterprise-class disk device mount on /.  Filesystem                              1K-blocks     Used Available Use% Mounted on /dev/mapper/rhel_hp--xl420gen9--01-root  52403200 11284408  41118792  22% /  It was not an SSD, nor ramdisk. I reran the test on ramdisk, the performance of the patched kernel was 679,880 jobs/min which was a bit more than double the 285,221 score that I got on a regular disk.  So the filesystem used wasn't tiny, though it is still not very large. The test was supposed to create 16 ramdisks and distribute the test tasks to the ramdisks. Instead, they were all pounding on the same filesystem worsening the spinlock contention problem.  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,625,0.17971014492753623,0.8,0,0.0,0.0,0.0,0.0
560,378505,379318,"Yes, I realise that. I am expecting that when it comes to optimising for pmem, we'll actually rewrite the journal to map pmem and mercy() directly rather than go through the buffering and IO layers we currently do so we can minimise write latency and control concurrency ourselves. Hence I'm not really concerned by performance issues with pmem at this point - most of our still users have traditional storage and will for a long time to come...","On Mon, Aug 27, 2018 at 12:39:06AM -0700, Christoph Hellwig wrote:  Yes, I realise that.  I am expecting that when it comes to optimising for pmem, we'll actually rewrite the journal to map pmem and memcpy() directly rather than go through the buffering and IO layers we currently do so we can minimise write latency and control concurrency ourselves. Hence I'm not really concerned by performance issues with pmem at this point - most of our still users have traditional storage and will for a long time to come....  Cheers,  Dave. --  Dave Chinner david@fromorbit.com",technical,Dave Chinner,david@fromorbit.com,0,0,445,0.12608695652173912,0.9,0,1.0,0.0,0.0,0.0
561,379138,379794,Is it ok to share VTune GUI screenshots I sent you the last time to demonstrate the advantage of AIO trace streaming?,"Hi Andi,  On 28.08.2018 11:59, Jiri Olsa wrote:  Is it ok to share VTune GUI screenshots I sent you the last time  to demonstrate the advantage of AIO trace streaming?  Thanks, Alexey",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,1,117,0.359375,0.6,0,0.0,0.0,0.0,0.0
562,379138,379931,"HI, I am not sure it would be representative in perf stat data however that could be visible there as well. I could share screenshots of VTune GUI that demonstrate the advantage of Perf implementing AIO streaming in comparison with the serial streaming. Data loss metrics can be easily understood from that. Is it ok to post it here?","Hi,  On 28.08.2018 11:59, Jiri Olsa wrote:  I am not sure it would be representative in perf stat data however  that could be visible there as well. I could share screenshots of  VTune GUI that demonstrate the advantage of Perf implementing AIO  streaming in comparison with the serial streaming. Data loss metrics  can be easily understood from that. Is it ok to post it here?",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,1,333,1.0,0.8,0,0.0,0.0,0.0,0.0
563,379138,379938,"Hi, VTune release manager permitted to share it, well, sorry for bothering.","Hi,  On 28.08.2018 14:58, Alexey Budankov wrote:  VTune release manager permitted to share it, well, sorry for bothering.",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,1,75,0.25,1.0,1,0.0,0.0,0.0,0.0
564,380945,381003,"Which branch is this works based on? I don't see any out label in current code. Please add same test in ovl_can_decode_fh(). Problem: none of the ovl_export_operations functions override creds.I guess things are working now because nfsd is privileged enough. IOW, the capability check you added doesn't check mounter creds when coming from nfs export ops - I guess that is not what you want although you probably don't enable nfs export.","On Tue, Aug 28, 2018 at 7:53 PM Mark Salyzyn <salyzyn@android.com> wrote:  Which branch is this works based on? I don't see any out label in current code.   Please add same test in ovl_can_decode_fh().  Problem: none of the ovl_export_operations functions override creds. I guess things are working now because nfsd is privileged enough. IOW, the capability check you added doesn't check mounter creds when coming from nfs export ops - I guess that is not what you want although you probably don'r enable nfs export.  Thanks, Amir.",technical,Amir Goldstein,amir73il@gmail.com,1,0,437,1.0,0.5,0,0.0,0.0,0.0,0.0
565,380945,381010,<sigh> I can only truly test this on 4.14 (android's current top of tree) and on Hikey with that. Lack of due diligence for Top of Linux. Ahhhh NFS export/Import blocked on Android devices.,"On 08/28/2018 10:34 AM, Amir Goldstein wrote:  <sigh> I can only truly test this on 4.14 (android's current top of  tree) and on Hikey with that. Lack of due diligence for Top of Linux.  Ahhhh NFS export/import blocked on Android devices.",technical,Mark Salyzyn,salyzyn@android.com,0,1,189,0.5,0.75,0,0.0,0.0,0.0,0.0
566,380945,381036,"Well, not sure how that review is going to work out. Anyway, this case should not return an error. Returning NULL should be just fine.","On Tue, Aug 28, 2018 at 8:44 PM Mark Salyzyn <salyzyn@android.com> wrote:  Well, not sure how that review is going to work out. anyway, this case should not return an error. returning NULL should be just fine.",technical,Amir Goldstein,amir73il@gmail.com,1,0,134,0.35714285714285715,1.0,1,0.0,0.0,0.0,0.0
567,383283,383407,I have picked this up Dave., I have picked this up Dave.,technical,"Kirsher, Jeffrey T",jeffrey.t.kirsher@intel.com,0,0,27,1.0,0.6666666666666666,0,0.0,1.0,0.0,1.0
568,383283,384394,"Thanks, Jeffrey.","On 8/30/18 4:09 PM, Kirsher, Jeffrey T wrote:  Thanks, Jeffrey. -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,16,0.5714285714285714,1.0,1,1.0,0.0,1.0,0.0
569,384448,384456,"Are you sure you did this right?  With the clock source set to TSC(which is the only reasonable choice unless KVM has seriously cleaned up its act), with retpolines enabled, I get 24ns for CLOCK_MONOTONIC without your patch and 32ns with your patch.  And there is indeed a retpoline in the disassembled output:  You're probably going to have to set -fno-jump-tables or do something clever like adding a whole array of (seconds, nsec) in gtod and indexing that array by the clock id. Meanwhile, I wrote the following trivial patch to add a__vdso_clock_gettime_monotonic export.  It runs in 21ns, and I suspect that the speedup is even a bit bigger when cache-cold because it avoids some branches.  What do you all think?  Florian, do you think glibc would be willing to add some magic to turnclock_gettime when CLOCK_MONOTONIC is a constant?","(Hi, Florian!)  On Fri, Aug 31, 2018 at 6:59 PM, Matt Rickard <matt@softrans.com.au> wrote:  Are you sure you did this right?  With the clocksource set to TSC (which is the only reasonable choice unless KVM has seriously cleaned up its act), with retpolines enabled, I get 24ns for CLOCK_MONOTONIC without your patch and 32ns with your patch.  And there is indeed a retpoline in the disassembled output:    e5:   e8 07 00 00 00          callq  f1 <__vdso_clock_gettime+0x31>   ea:   f3 90                   pause   ec:   0f ae e8                lfence   ef:   eb f9                   jmp    ea <__vdso_clock_gettime+0x2a>   f1:   48 89 04 24             mov    %rax,(%rsp)   f5:   c3                      retq  You're probably going to have to set -fno-jump-tables or do something clever like adding a whole array of (seconds, nsec) in gtod and indexing that array by the clock id.  Meanwhile, I wrote the following trivial patch to add a __vdso_clock_gettime_monotonic export.  It runs in 21ns, and I suspect that the speedup is even a bit bigger when cache-cold because it avoids some branches.  What do you all think?  Florian, do you think glibc would be willing to add some magic to turn clock_gettime(CLOCK_MONOTONIC, t) into __vdso_clock_gettime_monotonic(t) when CLOCK_MONOTONIC is a constant?",technical,Andy Lutomirski,luto@kernel.org,1,0,840,1.0,0.125,0,0.0,1.0,0.0,0.0
570,384448,384522,"What's the goal here?  Turn the indirect call/conditional jump/indirect call sequence into a single indirect call, purely for performance reasons?","On 09/01/2018 05:39 AM, Andy Lutomirski wrote:  What's the goal here?  Turn the indirect call/conditional jump/indirect  call sequence into a single indirect call, purely for performance reasons?  Thanks, Florian",technical,Florian Weimer,fweimer@redhat.com,0,0,146,0.15,0.1875,0,0.0,1.0,0.0,0.0
571,384448,384609,"Almost.  It's to bypass some of the branches in__vdso_clock_gettime(), which is supposed to be very fast.  AFAIK most user code that uses clock_gettime() passes a constant for the first argument, and we can squeeze out some performance by optimizing that case.  The indirect branches internal to the vDSO are a separate issue and should be solved separately.(It's really too bad that x86 doesn't have a 64-bit call instruction.If it did, then the PLT could get rewritten at dynamic link time to avoid indirect calls entirely, and presumably glibc could use the same technique to call into the vDSO without indirect calls.)","On Sat, Sep 1, 2018 at 2:33 AM, Florian Weimer <fweimer@redhat.com> wrote:  Almost.  It's to bypass some of the branches in __vdso_clock_gettime(), which is supposed to be very fast.  AFAIK most user code that uses clock_gettime() passes a constant for the first argument, and we can squeeze out some performance by optimizing that case.  The indirect branches internal to the vDSO are a separate issue and should be solved separately.  (It's really too bad that x86 doesn't have a 64-bit call instruction. If it did, then the PLT could get rewritten at dynamic link time to avoid indirect calls entirely, and presumably glibc could use the same technique to call into the vDSO without indirect calls.)",technical,Andy Lutomirski,luto@kernel.org,1,0,622,0.75,0.25,0,0.0,1.0,0.0,0.6666666666666666
572,384448,392428,See the patch below. It's integrating TAI without slowing down everything and it definitely does not result in indirect calls. On a HSW it slows down clock_gettime() by ~0.5ns. On a SKL I get a speedup by ~0.5ns. On a AMD Epyc server it's 1.2ns speedup. So it somehow depends on the uarch and I also observed compiler version dependent variations.,"On Fri, 31 Aug 2018, Andy Lutomirski wrote:   See the patch below. It's integrating TAI without slowing down everything and it definitely does not result in indirect calls.  On a HSW it slows down clock_gettime() by ~0.5ns. On a SKL I get a speedup by ~0.5ns. On a AMD Epyc server it's 1.2ns speedup. So it somehow depends on the uarch and I also observed compiler version dependend variations.  Thanks,  	tglx ---- --- a/arch/x86/entry/vdso/vclock_gettime.c +++ b/arch/x86/entry/vdso/vclock_gettime.c @@ -203,39 +203,18 @@ notrace static inline u64 vgetsns(int *m  	return v * gtod->mult,  }   -/* Code size doesn't matter (vdso is 4k anyway) and this is faster. */ -notrace static int __always_inline do_realtime(struct timespec *ts) +notrace static int __always_inline do_hres(struct timespec *ts, clockid_t clk)  { -	unsigned long seq, -	u64 ns, +	struct vgtod_ts *base = &gtod->basetime[clk & VGTOD_HRES_MASK], +	unsigned int seq,  	int mode, - -	do { -		seq = gtod_read_begin(gtod), -		mode = gtod->vclock_mode, -		ts->tv_sec = gtod->wall_time_sec, -		ns = gtod->wall_time_snsec, -		ns += vgetsns(&mode), -		ns >>= gtod->shift, -	} while (unlikely(gtod_read_retry(gtod, seq))), - -	ts->tv_sec += __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns), -	ts->tv_nsec = ns, - -	return mode, -} - -notrace static int __always_inline do_monotonic(struct timespec *ts) -{ -	unsigned long seq,  	u64 ns, -	int mode,    	do {  		seq = gtod_read_begin(gtod),  		mode = gtod->vclock_mode, -		ts->tv_sec = gtod->monotonic_time_sec, -		ns = gtod->monotonic_time_snsec, +		ts->tv_sec = base->sec, +		ns = base->nsec,  		ns += vgetsns(&mode),  		ns >>= gtod->shift,  	} while (unlikely(gtod_read_retry(gtod, seq))), @@ -246,58 +225,50 @@ notrace static int __always_inline do_mo  	return mode,  }   -notrace static void do_realtime_coarse(struct timespec *ts) +notrace static void do_coarse(struct timespec *ts, clockid_t clk)  { +	struct vgtod_ts *base = &gtod->basetime[clk],  	unsigned long seq, -	do { -		seq = gtod_read_begin(gtod), -		ts->tv_sec = gtod->wall_time_coarse_sec, -		ts->tv_nsec = gtod->wall_time_coarse_nsec, -	} while (unlikely(gtod_read_retry(gtod, seq))), -}   -notrace static void do_monotonic_coarse(struct timespec *ts) -{ -	unsigned long seq,  	do {  		seq = gtod_read_begin(gtod), -		ts->tv_sec = gtod->monotonic_time_coarse_sec, -		ts->tv_nsec = gtod->monotonic_time_coarse_nsec, +		ts->tv_sec = base->sec, +		ts->tv_nsec = base->nsec,  	} while (unlikely(gtod_read_retry(gtod, seq))),  }    notrace int __vdso_clock_gettime(clockid_t clock, struct timespec *ts)  { -	switch (clock) { -	case CLOCK_REALTIME: -		if (do_realtime(ts) == VCLOCK_NONE) -			goto fallback, -		break, -	case CLOCK_MONOTONIC: -		if (do_monotonic(ts) == VCLOCK_NONE) -			goto fallback, -		break, -	case CLOCK_REALTIME_COARSE: -		do_realtime_coarse(ts), -		break, -	case CLOCK_MONOTONIC_COARSE: -		do_monotonic_coarse(ts), -		break, -	default: -		goto fallback, -	} +	unsigned int msk,   -	return 0, -fallback: +	/* Sort out negative (CPU/FD) and invalid clocks */ +	if (unlikely((unsigned int) clock >= MAX_CLOCKS)) +		return vdso_fallback_gettime(clock, ts), + +	/* +	 * Convert the clockid to a bitmask and use it to check which +	 * clocks are handled in the VDSO directly. +	 */ +	msk = 1U << clock, +	if (likely(msk & VGTOD_HRES)) { +		if (do_hres(ts, clock) != VCLOCK_NONE) +			return 0, +	} else if (msk & VGTOD_COARSE) { +		do_coarse(ts, clock), +		return 0, +	}  	return vdso_fallback_gettime(clock, ts),  } +  int clock_gettime(clockid_t, struct timespec *)  	__attribute__((weak, alias(__vdso_clock_gettime""))),    notrace int __vdso_gettimeofday(struct timeval *tv, struct timezone *tz)  {  	if (likely(tv != NULL)) { -		if (unlikely(do_realtime((struct timespec *)tv) == VCLOCK_NONE)) +		struct timespec *ts = (struct timespec *) tv, + +		if (unlikely(do_hres(ts, CLOCK_REALTIME) == VCLOCK_NONE))  			return vdso_fallback_gtod(tv, tz),  		tv->tv_usec /= 1000,  	} @@ -318,7 +289,7 @@ int gettimeofday(struct timeval *, struc  notrace time_t __vdso_time(time_t *t)  {  	/* This is atomic on x86 so we don't need any locks. */ -	time_t result = READ_ONCE(gtod->wall_time_sec), +	time_t result = READ_ONCE(gtod->basetime[CLOCK_REALTIME].sec),    	if (t)  		*t = result, --- a/arch/x86/entry/vsyscall/vsyscall_gtod.c +++ b/arch/x86/entry/vsyscall/vsyscall_gtod.c @@ -31,6 +31,8 @@ void update_vsyscall(struct timekeeper *  {  	int vclock_mode = tk->tkr_mono.clock->archdata.vclock_mode,  	struct vsyscall_gtod_data *vdata = &vsyscall_gtod_data, +	struct vgtod_ts *base, +	u64 nsec,    	/* Mark the new vclock used. */  	BUILD_BUG_ON(VCLOCK_MAX >= 32), @@ -45,34 +47,37 @@ void update_vsyscall(struct timekeeper *  	vdata->mult		= tk->tkr_mono.mult,  	vdata->shift		= tk->tkr_mono.shift,   -	vdata->wall_time_sec		= tk->xtime_sec, -	vdata->wall_time_snsec		= tk->tkr_mono.xtime_nsec, - -	vdata->monotonic_time_sec	= tk->xtime_sec -					+ tk->wall_to_monotonic.tv_sec, -	vdata->monotonic_time_snsec	= tk->tkr_mono.xtime_nsec -					+ ((u64)tk->wall_to_monotonic.tv_nsec -						<< tk->tkr_mono.shift), -	while (vdata->monotonic_time_snsec >= -					(((u64)NSEC_PER_SEC) << tk->tkr_mono.shift)) { -		vdata->monotonic_time_snsec -= -					((u64)NSEC_PER_SEC) << tk->tkr_mono.shift, -		vdata->monotonic_time_sec++, +	base = &vdata->basetime[CLOCK_REALTIME], +	base->sec = (gtod_long_t)tk->xtime_sec, +	base->nsec = tk->tkr_mono.xtime_nsec, + +	base = &vdata->basetime[VGTOD_TAI], +	base->sec = (gtod_long_t)(tk->xtime_sec + (s64)tk->tai_offset), +	base->nsec = tk->tkr_mono.xtime_nsec, + +	base = &vdata->basetime[CLOCK_MONOTONIC], +	base->sec = (gtod_long_t)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec), +	nsec = tk->tkr_mono.xtime_nsec, +	nsec +=	((u64)tk->wall_to_monotonic.tv_nsec << tk->tkr_mono.shift), +	while (nsec >= (((u64)NSEC_PER_SEC) << tk->tkr_mono.shift)) { +		nsec -= ((u64)NSEC_PER_SEC) << tk->tkr_mono.shift, +		base->sec++,  	} +	base->nsec = nsec,   -	vdata->wall_time_coarse_sec	= tk->xtime_sec, -	vdata->wall_time_coarse_nsec	= (long)(tk->tkr_mono.xtime_nsec >> -						 tk->tkr_mono.shift), - -	vdata->monotonic_time_coarse_sec = -		vdata->wall_time_coarse_sec + tk->wall_to_monotonic.tv_sec, -	vdata->monotonic_time_coarse_nsec = -		vdata->wall_time_coarse_nsec + tk->wall_to_monotonic.tv_nsec, - -	while (vdata->monotonic_time_coarse_nsec >= NSEC_PER_SEC) { -		vdata->monotonic_time_coarse_nsec -= NSEC_PER_SEC, -		vdata->monotonic_time_coarse_sec++, +	base = &vdata->basetime[CLOCK_REALTIME_COARSE], +	base->sec = (gtod_long_t)tk->xtime_sec, +	base->nsec = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift, + +	base = &vdata->basetime[CLOCK_MONOTONIC_COARSE], +	base->sec = (gtod_long_t)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec), +	nsec = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift, +	nsec += tk->wall_to_monotonic.tv_nsec, +	while (nsec >= NSEC_PER_SEC) { +		nsec -= NSEC_PER_SEC, +		base->sec++,  	} +	base->nsec = nsec,    	gtod_write_end(vdata),  } --- a/arch/x86/include/asm/vgtod.h +++ b/arch/x86/include/asm/vgtod.h @@ -5,33 +5,41 @@  #include <linux/compiler.h>  #include <linux/clocksource.h>   +#include <uapi/linux/time.h> +  #ifdef BUILD_VDSO32_64  typedef u64 gtod_long_t,  #else  typedef unsigned long gtod_long_t,  #endif + +struct vgtod_ts { +	gtod_long_t	sec, +	u64		nsec, +}, + +#define VGTOD_BASES	(CLOCK_MONOTONIC_COARSE + 1) +#define VGTOD_HRES	(BIT(CLOCK_REALTIME) | BIT(CLOCK_MONOTONIC) | BIT(CLOCK_TAI)) +#define VGTOD_COARSE	(BIT(CLOCK_REALTIME_COARSE) | BIT(CLOCK_MONOTONIC_COARSE)) + +/* Abuse CLOCK_THREAD_CPUTIME_ID for VGTOD CLOCK TAI */ +#define VGTOD_HRES_MASK	0x3 +#define VGTOD_TAI	(CLOCK_TAI & VGTOD_HRES_MASK) +  /*   * vsyscall_gtod_data will be accessed by 32 and 64 bit code at the same time   * so be carefull by modifying this structure.   */  struct vsyscall_gtod_data { -	unsigned seq, +	unsigned int	seq, + +	int		vclock_mode, +	u64		cycle_last, +	u64		mask, +	u32		mult, +	u32		shift,   -	int vclock_mode, -	u64	cycle_last, -	u64	mask, -	u32	mult, -	u32	shift, - -	/* open coded 'struct timespec' */ -	u64		wall_time_snsec, -	gtod_long_t	wall_time_sec, -	gtod_long_t	monotonic_time_sec, -	u64		monotonic_time_snsec, -	gtod_long_t	wall_time_coarse_sec, -	gtod_long_t	wall_time_coarse_nsec, -	gtod_long_t	monotonic_time_coarse_sec, -	gtod_long_t	monotonic_time_coarse_nsec, +	struct vgtod_ts	basetime[VGTOD_BASES],    	int		tz_minuteswest,  	int		tz_dsttime, @@ -44,9 +52,9 @@ static inline bool vclock_was_used(int v  	return READ_ONCE(vclocks_used) & (1 << vclock),  }   -static inline unsigned gtod_read_begin(const struct vsyscall_gtod_data *s) +static inline unsigned int gtod_read_begin(const struct vsyscall_gtod_data *s)  { -	unsigned ret, +	unsigned int ret,    repeat:  	ret = READ_ONCE(s->seq),""",technical,Thomas Gleixner,tglx@linutronix.de,1,0,347,0.44375,0.3125,0,0.6666666666666666,0.25,0.6666666666666666,0.0
573,384448,392847,"and actually this wants to become u64 unconditionally as we need to provide the full seconds even on 32bit for the upcoming y2038 support. We still have to truncate it for the current 32bit interface, but the core code can be made ready now.","On Sun, 9 Sep 2018, Thomas Gleixner wrote:  and actually this wants to become u64 unconditionally as we need to provide the full seconds even on 32bit for the upcoming y2038 support. We still have to truncate it for the current 32bit interface, but the core code can be made ready now.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,241,0.29375,0.375,0,0.75,0.25,0.0,0.16666666666666666
574,384448,395078,"Does this mean glibc can keep using a single vDSO entry point, the one we have today?","On 09/09/2018 10:05 PM, Thomas Gleixner wrote:  Does this mean glibc can keep using a single vDSO entrypoint, the one we  have today?  Thanks, Florian",technical,Florian Weimer,fweimer@redhat.com,0,0,85,0.11875,0.4375,0,0.9166666666666666,0.08333333333333333,0.16666666666666666,0.0
575,384448,395171,We have no intention to change that. But we surely could provide separate entry points as an extra to avoid a bunch of conditionals.,"On Wed, 12 Sep 2018, Florian Weimer wrote:  We have no intention to change that.  But we surely could provide separate entry points as an extra to avoid a bunch of conditionals.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,132,0.1625,0.5,0,0.9166666666666666,0.08333333333333333,0.0,0.0
576,384448,395174,"Okay, I was wondering because Andy seemed to have proposed just that. We could adjust to that, but the benefit would be long-term because it's an ABI change for glibc, and they tend to take a long time to propagate. But I must say that clock_gettime is an odd place to start.  I would have expected any of the type-polymorphic multiplexer interfaces (fcntl,ioctl, ptrace, futex) to be a more natural starting point. 8-)","On 09/12/2018 04:17 PM, Thomas Gleixner wrote:  Okay, I was wondering because Andy seemed to have proposed just that.   We could adjust to that, but the benefit would be long-term because it's  an ABI change for glibc, and they tend to take a long time to propagate.  But I must say that clock_gettime is an odd place to start.  I would  have expected any of the type-polymorphic multiplexer interfaces (fcntl,  ioctl, ptrace, futex) to be a more natural starting point. 8-)  Thanks, Florian",technical,Florian Weimer,fweimer@redhat.com,0,0,419,0.55,0.5625,0,0.9166666666666666,0.08333333333333333,0.0,0.0
577,384448,395190,"Well, the starting point of this was to provide clock_tai support in thevdso. clock_gettime() in the vdso vs. the real syscall is a factor of 10 in speed. clock_gettime() is a pretty hot function in some workloads. Andy then noticed that some conditionals could be avoided entirely by using a different entry point and offered one along with a 10% speedup. We don't have to go there, we can. The multiplexer interfaces need much more surgery and talking about futex, we'd need to sit down with quite some people and identify the things they actually care about before just splitting it up and keeping the existing overloaded trainwreck the same.","On Wed, 12 Sep 2018, Florian Weimer wrote:  Well, the starting point of this was to provide clock_tai support in the vdso. clock_gettime() in the vdso vs. the real syscall is a factor of 10 in speed. clock_gettime() is a pretty hot function in some workloads.  Andy then noticed that some conditionals could be avoided entirely by using a different entry point and offered one along with a 10% speedup. We don't have to go there, we can.  The multiplexer interfaces need much more surgery and talking about futex, we'd need to sit down with quite some people and identify the things they actually care about before just splitting it up and keeping the existing overloaded trainwreck the same.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,645,0.7875,0.625,0,0.9166666666666666,0.08333333333333333,0.0,0.0
578,384448,395418,"There's also the issue of how much the speedup matters. For futex, maybe a better interface saves 3ns, but a futex syscall is hundreds of ns. clock_gettime() is called at high frequency and can be ~25ns. Saving a few ns is a bigger deal.","There’s also the issue of how much the speedup matters. For futex, maybe a better interface saves 3ns, but a futex syscall is hundreds of ns. clock_gettime() is called at high frequency and can be ~25ns. Saving a few ns is a bigger deal.",technical,Andy Lutomirski,luto@amacapital.net,1,0,237,0.33125,0.6875,0,0.9166666666666666,0.08333333333333333,0.0,0.0
579,384448,395902,"My concern is that the userspace system call wrappers currently do not know how many arguments the individual operations take and what types the arguments have (hence the  type-polymorphic nature I mentioned). This could be a problem for on-stack argument passing (where you might read values beyond the end of the stack, and glibc avoids that most of the time by having enough cruft on the stack), and for architectures which pass pointers and integers in different registers (like some m68kABIs do for the return value).","On 09/12/2018 07:11 PM, Andy Lutomirski wrote:  My concern is that the userspace system call wrappers currently do not  know how many arguments the individual operations take and what types  the arguments have (hence the “type-polymorphic” nature I mentioned).  This could be a problem for on-stack argument passing (where you might  read values beyond the end of the stack, and glibc avoids that most of  the time by having enough cruft on the stack), and for architectures  which pass pointers and integers in different registers (like some m68k  ABIs do for the return value).  Thanks, Florian",technical,Florian Weimer,fweimer@redhat.com,0,0,522,0.6,0.75,0,1.0,0.0,0.0,0.0
580,384448,396652,"Isn't clock_gettime already special because of the vDSO entry point, though?","   Isn’t clock_gettime already special because of the vDSO entry point, though?",technical,Andy Lutomirski,luto@amacapital.net,1,0,76,0.0875,0.8125,0,1.0,0.0,0.0,0.0
581,384448,396833,"Somewhat special, yes, but not overly so, and not in the type-polymorphic sense.  We can't give direct access of the vDSO implementation to applications because the kernel does not know about the userspace errno variable.  We do that for time on x86_64, where applications call into the vDSO directly, bypassing glibc completely after binding. I suspect most Linux libcs that know about the vDSO at all have generic vsyscall support, just like they have generic support for plain system calls.","On 09/13/2018 05:22 PM, Andy Lutomirski wrote:   Somewhat special, yes, but not overly so, and not in the  type-polymorphic sense.  We can't give direct access of the vDSO  implementation to applications because the kernel does not know about  the userspace errno variable.  We do that for time on x86_64, where  applications call into the vDSO directly, bypassing glibc completely  after binding.  I suspect most Linux libcs that know about the vDSO at all have generic  vsyscall support, just like they have generic support for plain system  calls.  Thanks, Florian",technical,Florian Weimer,fweimer@redhat.com,0,0,493,0.56875,0.875,0,1.0,0.0,0.0,0.0
582,384448,396860,"If the vDSO adds special helpers for CLOCK_MONOTONIC and CLOCK_REALTIME, I think we can reasonably safely promise that they never fail. (seccomp can obviously break that promise if there's no TSC, but I think that seccomp users who do that get to keep both pieces.)","   If the vDSO adds special helpers for CLOCK_MONOTONIC and CLOCK_REALTIME, I think we can reasonably safely promise that they never fail. (seccomp can obviously break that promise if there’s no TSC, but I think that seccomp users who do that get to keep both pieces.)",technical,Andy Lutomirski,luto@amacapital.net,1,0,265,0.325,0.9375,0,1.0,0.0,0.0,0.0
583,384448,396866,"I agree, I thought about the same thing.  We already do not return EFAULT for invalid pointers, for obvious reasons.  And if the clock ID is fixed, the EINVAL error is impossible. That would shave off a few nanoseconds more if the calling convention is identical to what glibc exposes to applications.  If the vDSO is not available or the symbol is missing, we can provide an implementation based on the current clock_gettime in glibc.","On 09/13/2018 09:35 PM, Andy Lutomirski wrote:   I agree, I thought about the same thing.  We already do not return  EFAULT for invalid pointers, for obvious reasons.  And if the clock ID  is fixed, the EINVAL error is impossible.  That would shave off a few nanoseconds more if the calling convention is  identical to what glibc exposes to applications.  If the vDSO is not  available or the symbol is missing, we can provide an implementation  based on the current clock_gettime in glibc.  Thanks, Florian",technical,Florian Weimer,fweimer@redhat.com,0,0,435,0.525,1.0,1,1.0,0.0,0.0,0.0
584,387234,387531,"Now tfm could be removed from the macro arguments, no?","On Tuesday, September 4, 2018, 8:16:29 PM CEST Kees Cook wrote:  Now tfm could be removed from the macro arguments, no?  Best regards, Alexander",technical,Alexander Stein,alexander.stein@systec-electronic.com,0,0,54,0.09302325581395349,0.21052631578947367,0,0.0,0.5,0.0,0.0
585,387234,387701,"Are you sure this is a representative sampling? I haven't double checked myself, but we have plenty of drivers for peripherals in drivers/crypto that implement block ciphers, and they would not turnup in tcrypt unless you are running on a platform that provides the hardware in question.","On 4 September 2018 at 20:16, Kees Cook <keescook@chromium.org> wrote:  Are you sure this is a representative sampling? I haven't double checked myself, but we have plenty of drivers for peripherals in drivers/crypto that implement block ciphers, and they would not turn up in tcrypt unless you are running on a platform that provides the hardware in question.",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,287,0.40310077519379844,0.2631578947368421,0,0.0,0.5,0.0,0.0
586,387234,388338,"Hrm, excellent point. Looking at this again: The core part of the VLA is using this in the ON_STACK macro. I don't find any and the initial reqsize is here: And in my earlier skcipher wrapper analysis, lrw was the largest skcipher wrapper is an extreme outlier, with cvm_req_ctx at a bit less than half. Making this a 2920 byte fixed array doesn't seem sensible at all (though that's what's already possible to use with existing SKCIPHER_REQUEST_ON_STACK users).What's the right path forward here?-","On Wed, Sep 5, 2018 at 2:18 AM, Ard Biesheuvel <ard.biesheuvel@linaro.org> wrote:  Hrm, excellent point. Looking at this again:  The core part of the VLA is using this in the ON_STACK macro:  static inline unsigned int crypto_skcipher_reqsize(struct crypto_skcipher *tfm) {         return tfm->reqsize, }  I don't find any struct crypto_skcipher .reqsize static initializers, and the initial reqsize is here:  static int crypto_init_skcipher_ops_ablkcipher(struct crypto_tfm *tfm) { ...         skcipher->reqsize = crypto_ablkcipher_reqsize(ablkcipher) +                             sizeof(struct ablkcipher_request),  with updates via crypto_skcipher_set_reqsize().  So I have to examine ablkcipher reqsize too:  static inline unsigned int crypto_ablkcipher_reqsize(         struct crypto_ablkcipher *tfm) {         return crypto_ablkcipher_crt(tfm)->reqsize, }  And of the crt_ablkcipher.reqsize assignments/initializers, I found:  ablkcipher reqsize: 1       struct dcp_aes_req_ctx 8       struct atmel_tdes_reqctx 8       struct cryptd_blkcipher_request_ctx 8       struct mtk_aes_reqctx 8       struct omap_des_reqctx 8       struct s5p_aes_reqctx 8       struct sahara_aes_reqctx 8       struct stm32_cryp_reqctx 8       struct stm32_cryp_reqctx 16      struct ablk_ctx 24      struct atmel_aes_reqctx 48      struct omap_aes_reqctx 48      struct omap_aes_reqctx 48      struct qat_crypto_request 56      struct artpec6_crypto_request_context 64      struct chcr_blkcipher_req_ctx 80      struct spacc_req 80      struct virtio_crypto_sym_request 136     struct qce_cipher_reqctx 168     struct n2_request_context 328     struct ccp_des3_req_ctx 400     struct ccp_aes_req_ctx 536     struct hifn_request_context 992     struct cvm_req_ctx 2456    struct iproc_reqctx_s  The base ablkcipher wrapper is: 80      struct ablkcipher_request  And in my earlier skcipher wrapper analysis, lrw was the largest skcipher wrapper: 384     struct rctx  iproc_reqctx_s is an extreme outlier, with cvm_req_ctx at a bit less than half.  Making this a 2920 byte fixed array doesn't seem sensible at all (though that's what's already possible to use with existing SKCIPHER_REQUEST_ON_STACK users).  What's the right path forward here?  -Kees  --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,1,498,0.7829457364341085,0.3157894736842105,0,0.5,0.5,0.0,0.0
587,387234,388487,"The skcipher implementations based on crypto IP blocks are typically asynchronous, and I wouldn't be surprised if a fair number of SKCIPHER_REQUEST_ON_STACK() users are limited to synchronous skciphers. So we could formalize this and limit SKCIPHER_REQUEST_ON_STACK() to synchronous skciphers, which implies that the reqsize limit only has to apply synchronous skciphers as well. But before we can do this, we have to identify the remaining occurrences that allow asynchronous skciphers to be used, and replace them with heap allocations.","On 5 September 2018 at 23:05, Kees Cook <keescook@chromium.org> wrote:  The skcipher implementations based on crypto IP blocks are typically asynchronous, and I wouldn't be surprised if a fair number of SKCIPHER_REQUEST_ON_STACK() users are limited to synchronous skciphers.  So we could formalize this and limit SKCIPHER_REQUEST_ON_STACK() to synchronous skciphers, which implies that the reqsize limit only has to apply synchronous skciphers as well. But before we can do this, we have to identify the remaining occurrences that allow asynchronous skciphers to be used, and replace them with heap allocations.",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,538,0.7054263565891473,0.3684210526315789,0,0.5,0.0,0.0,0.0
588,387234,388521,"Looks similar to ahash vs shash. :) Yes, so nearly all crypto_alloc_skcipher() users explicitly mask away ASYNC. What's left appears to be: I'll cross-reference this with SKCIPHER_REQUEST_ON_STACK...Sounds good, thanks!","On Wed, Sep 5, 2018 at 3:49 PM, Ard Biesheuvel <ard.biesheuvel@linaro.org> wrote:  Looks similar to ahash vs shash. :) Yes, so nearly all crypto_alloc_skcipher() users explicitly mask away ASYNC. What's left appears to be:  crypto/drbg.c:  sk_tfm = crypto_alloc_skcipher(ctr_name, 0, 0), crypto/tcrypt.c:        tfm = crypto_alloc_skcipher(algo, 0, async ? 0 : CRYPTO_ALG_ASYNC), drivers/crypto/omap-aes.c:      ctx->ctr = crypto_alloc_skcipher(ecb(aes)"", 0, 0), drivers/md/dm-crypt.c:          cc->cipher_tfm.tfms[i] = crypto_alloc_skcipher(ciphermode, 0, 0), drivers/md/dm-integrity.c:              ic->journal_crypt = crypto_alloc_skcipher(ic->journal_crypt_alg.alg_string, 0, 0), fs/crypto/keyinfo.c:    struct crypto_skcipher *tfm = crypto_alloc_skcipher(""ecb(aes)"", 0, 0), fs/crypto/keyinfo.c:    ctfm = crypto_alloc_skcipher(mode->cipher_str, 0, 0), fs/ecryptfs/crypto.c:   crypt_stat->tfm = crypto_alloc_skcipher(full_alg_name, 0, 0),  I'll cross-reference this with SKCIPHER_REQUEST_ON_STACK...   Sounds good, thanks!  -Kees  --  Kees Cook Pixel Security""",technical,Kees Cook,keescook@chromium.org,1,1,219,0.32558139534883723,0.42105263157894735,0,0.5,0.0,0.0,0.0
589,387234,388618,"According to Herbert, SKCIPHER_REQUEST_ON_STACK() may only be used for invoking synchronous ciphers. In fact, due to the way the crypto API is built, if you try using it with any transformation that uses DMA you would most probably end up trying to DMA to/from the stack which as we all know is not a great idea. Any such occurrences are almost for sure broken already due to the DMA issue I've mentioned.","On Thu, Sep 6, 2018 at 1:49 AM, Ard Biesheuvel <ard.biesheuvel@linaro.org> wrote:  According to Herbert, SKCIPHER_REQUEST_ON_STACK() may only be used for invoking synchronous ciphers.  In fact, due to the way the crypto API is built, if you try using it with any transformation that uses DMA you would most probably end up trying to DMA to/from the stack which as we all know is not a great idea.   Any such occurrences are almost for sure broken already due to the DMA issue I've mentioned.  Gilad  --  Gilad Ben-Yossef Chief Coffee Drinker  values of β will give rise to dom!",technical,Gilad Ben-Yossef,gilad@benyossef.com,1,0,405,0.627906976744186,0.47368421052631576,0,0.5,0.0,0.0,0.0
590,387234,388671,"Ah yes, I found which contains that quote. So that means that Kees can disregard the occurrences that are async only, but it still implies that we cannot limit the reqsize like he proposes unless we take the sync/async nature into account. It also means we should probably BUG() or WARN() in SKCIPHER_REQUEST_ON_STACK() when used with an async algo. I am not convinced of this. The skcipher request struct does not contain any payload buffers, and whether the algo specific ctx struct is used for DMA is completely up to the driver. So I am quite sure there are plenty of async algos that work fine with SKCIPHER_REQUEST_ON_STACK() and vmapped stacks.","On 6 September 2018 at 06:53, Gilad Ben-Yossef <gilad@benyossef.com> wrote:  Ah yes, I found [0] which contains that quote.  So that means that Kees can disregard the occurrences that are async only, but it still implies that we cannot limit the reqsize like he proposes unless we take the sync/async nature into account. It also means we should probably BUG() or WARN() in SKCIPHER_REQUEST_ON_STACK() when used with an async algo.   I am not convinced of this. The skcipher request struct does not contain any payload buffers, and whether the algo specific ctx struct is used for DMA is completely up to the driver. So I am quite sure there are plenty of async algos that work fine with SKCIPHER_REQUEST_ON_STACK() and vmapped stacks.   [0] https://www.redhat.com/archives/dm-devel/2018-January/msg00087.html",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,651,1.0,0.5263157894736842,0,0.5,0.0,0.0,0.0
591,387234,388722,"Something like this should do the trick: DOC: Symmetric Key Cipher API That way, we will almost certainly oops on a NULL pointer dereference right after, but we at least the stack corruption.","On 6 September 2018 at 09:21, Ard Biesheuvel <ard.biesheuvel@linaro.org> wrote:  Something like this should do the trick:  diff --git a/include/crypto/skcipher.h b/include/crypto/skcipher.h index 2f327f090c3e..70584e0f26bc 100644 --- a/include/crypto/skcipher.h +++ b/include/crypto/skcipher.h @@ -142,7 +142,9 @@ struct skcipher_alg {  #define SKCIPHER_REQUEST_ON_STACK(name, tfm) \         char __##name##_desc[sizeof(struct skcipher_request) + \                 crypto_skcipher_reqsize(tfm)] CRYPTO_MINALIGN_ATTR, \ -       struct skcipher_request *name = (void *)__##name##_desc +       struct skcipher_request *name = WARN_ON( \ +               crypto_skcipher_alg(tfm)->base.cra_flags & CRYPTO_ALG_ASYNC) \ +               ? NULL : (void *)__##name##_desc   /**   * DOC: Symmetric Key Cipher API  That way, we will almost certainly oops on a NULL pointer dereference right after, but we at least the stack corruption.",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,191,0.29457364341085274,0.5789473684210527,0,0.5,0.0,0.0,0.0
592,387234,388739,"You are right that it is up to the driver but the cost is an extra memory allocation and release*per request* for any per request data that needs to be DMA able beyond the actual plain and cipher text buffers such as the IV, so driver writers have an incentive against doing that :-)","On Thu, Sep 6, 2018 at 10:21 AM, Ard Biesheuvel <ard.biesheuvel@linaro.org> wrote:    You are right that it is up to the driver but the cost is an extra memory allocation and release *per request* for any per request data that needs to be DMAable beyond the actual plain and cipher text buffers such as the IV, so driver writers have an incentive against doing that :-)  Gilad   --  Gilad Ben-Yossef Chief Coffee Drinker  values of β will give rise to dom!",technical,Gilad Ben-Yossef,gilad@benyossef.com,1,0,283,0.46511627906976744,0.631578947368421,0,0.5,0.0,0.0,0.0
593,387234,388767,A crash is just as bad as a BUG_ON.Is this even a real problem? Do we have any users of this construct that is using it on async algorithms?,"On Thu, Sep 06, 2018 at 10:11:59AM +0200, Ard Biesheuvel wrote:  A crash is just as bad as a BUG_ON.  Is this even a real problem? Do we have any users of this construct that is using it on async algorithms?  Cheers, --  Email: Herbert Xu <herbert@gondor.apana.org.au> Home Page: http://gondor.apana.org.au/~herbert/ PGP Key: http://gondor.apana.org.au/~herbert/pubkey.txt",technical,Herbert Xu,herbert@gondor.apana.org.au,1,0,140,0.24031007751937986,0.6842105263157895,0,0.5,0.0,0.0,0.0
594,387234,389636,"Perhaps not, but it is not enforced atm. In any case, limiting the reqsize is going to break things, so that needs to occur based on the sync/async nature of the algo. That also means we'll corrupt the stack if we ever end up using SKCIPHER_REQUEST_ON_STACK() with an async algo whose reqsize is greater than the sync reqsize limit, so I do think some additional sanity check is appropriate.","On 6 September 2018 at 10:51, Herbert Xu <herbert@gondor.apana.org.au> wrote:  Perhaps not, but it is not enforced atm.  In any case, limiting the reqsize is going to break things, so that needs to occur based on the sync/async nature of the algo. That also means we'll corrupt the stack if we ever end up using SKCIPHER_REQUEST_ON_STACK() with an async algo whose reqsize is greater than the sync reqsize limit, so I do think some additional sanity check is appropriate.",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,391,0.6124031007751938,0.7368421052631579,0,0.5,0.0,0.0,0.0
595,387234,389855,"I'd prefer compile-time based checks.  Perhaps we can introduce a wrapper around crypto_skcipher, say crypto_skcipher_sync which could then be used by SKCIPHER_REQUEST_ON_STACK to ensure that only sync algorithms can use this construct.","On Thu, Sep 06, 2018 at 11:29:41AM +0200, Ard Biesheuvel wrote:  I'd prefer compile-time based checks.  Perhaps we can introduce a wrapper around crypto_skcipher, say crypto_skcipher_sync which could then be used by SKCIPHER_REQUEST_ON_STACK to ensure that only sync algorithms can use this construct.  Cheers, --  Email: Herbert Xu <herbert@gondor.apana.org.au> Home Page: http://gondor.apana.org.au/~herbert/ PGP Key: http://gondor.apana.org.au/~herbert/pubkey.txt",technical,Herbert Xu,herbert@gondor.apana.org.au,1,0,236,0.27906976744186046,0.7894736842105263,0,0.5,0.0,0.0,0.0
596,387234,390037,"That would require lots of changes in the callers, including ones that already take care to use sync algos only. How about we do something like the below instead?","On 6 September 2018 at 15:11, Herbert Xu <herbert@gondor.apana.org.au> wrote:  That would require lots of changes in the callers, including ones that already take care to use sync algos only.  How about we do something like the below instead?  diff --git a/include/crypto/skcipher.h b/include/crypto/skcipher.h index 2f327f090c3e..ace707d59cd9 100644 --- a/include/crypto/skcipher.h +++ b/include/crypto/skcipher.h @@ -19,6 +19,7 @@   /**   *     struct skcipher_request - Symmetric key cipher request + *     @__onstack: 1 if the request was allocated by SKCIPHER_REQUEST_ON_STACK   *     @cryptlen: Number of bytes to encrypt or decrypt   *     @iv: Initialisation Vector   *     @src: Source SG list @@ -27,6 +28,7 @@   *     @__ctx: Start of private context data   */  struct skcipher_request { +       unsigned char __onstack,         unsigned int cryptlen,          u8 *iv, @@ -141,7 +143,7 @@ struct skcipher_alg {   #define SKCIPHER_REQUEST_ON_STACK(name, tfm) \         char __##name##_desc[sizeof(struct skcipher_request) + \ -               crypto_skcipher_reqsize(tfm)] CRYPTO_MINALIGN_ATTR, \ +               crypto_skcipher_reqsize(tfm)] CRYPTO_MINALIGN_ATTR = { 1 }, \         struct skcipher_request *name = (void *)__##name##_desc   /** @@ -437,6 +439,10 @@ static inline int crypto_skcipher_encrypt(struct skcipher_request *req)  {         struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req),  +       if (req->__onstack && +           (crypto_skcipher_alg(tfm)->base.cra_flags & CRYPTO_ALG_ASYNC)) +               return -EINVAL, +         if (crypto_skcipher_get_flags(tfm) & CRYPTO_TFM_NEED_KEY)                 return -ENOKEY,  @@ -458,6 +464,10 @@ static inline int crypto_skcipher_decrypt(struct skcipher_request *req)  {         struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req),  +       if (req->__onstack && +           (crypto_skcipher_alg(tfm)->base.cra_flags & CRYPTO_ALG_ASYNC)) +               return -EINVAL, +         if (crypto_skcipher_get_flags(tfm) & CRYPTO_TFM_NEED_KEY)                 return -ENOKEY,",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,162,0.24806201550387597,0.8421052631578947,0,0.5,0.0,0.0,0.0
597,387234,390245,You mean in the original code? I strongly suspect this was to not take it for each page.,"On Thu, Sep 6, 2018 at 7:49 AM, Ard Biesheuvel <ard.biesheuvel@linaro.org> wrote:  Oh, I like this, thanks!  -Kees  --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,1,88,0.15503875968992248,0.8947368421052632,0,1.0,0.0,0.0,0.0
598,387234,390283,"All of these are ASYNC (they're all crt_ablkcipher), so IIUC, I can ignore them. None of these use that I can find. crypto_init_skcipher_ops_blkcipher() doesn't touch reqsize at all, so the only places I can find it gets changed are with direct callers of crypto_skcipher_set_reqsize(), which, when wrapping a sync blkcipher start with a reqsize == 0. So, the remaining non-ASYNC callers ask for So, following your patch to encrypt/decrypt, I can add reqsize check there. How does this look, on top of your patch","On Wed, Sep 5, 2018 at 5:43 PM, Kees Cook <keescook@chromium.org> wrote:  All of these are ASYNC (they're all crt_ablkcipher), so IIUC, I can ignore them.   None of these use SKCIPHER_REQUEST_ON_STACK that I can find.   crypto_init_skcipher_ops_blkcipher() doesn't touch reqsize at all, so the only places I can find it gets changed are with direct callers of crypto_skcipher_set_reqsize(), which, when wrapping a sync blkcipher start with a reqsize == 0. So, the remaining non-ASYNC callers ask for:  4       struct sun4i_cipher_req_ctx 96      struct crypto_rfc3686_req_ctx 375     sum:                 160     crypto_skcipher_blocksize(cipher) (max)                 152     struct crypto_cts_reqctx                 63      align_mask (max) 384     struct rctx  So, following your patch to encrypt/decrypt, I can add reqsize check there. How does this look, on top of your patch?  --- a/include/crypto/skcipher.h +++ b/include/crypto/skcipher.h @@ -144,9 +144,10 @@ struct skcipher_alg {  /*   * This must only ever be used with synchronous algorithms.   */ +#define MAX_SYNC_SKCIPHER_REQSIZE      384  #define SKCIPHER_REQUEST_ON_STACK(name, tfm) \         char __##name##_desc[sizeof(struct skcipher_request) + \ -               crypto_skcipher_reqsize(tfm)] CRYPTO_MINALIGN_ATTR = { 1 } \ +               MAX_SYNC_SKCIPHER_REQSIZE] CRYPTO_MINALIGN_ATTR = { 1 } \         struct skcipher_request *name = (void *)__##name##_desc   /** @@ -442,10 +443,14 @@ static inline int crypto_skcipher_encrypt(struct skcipher_request *req)  {         struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req),  -       if (req->__onstack && -           WARN_ON(crypto_skcipher_alg(tfm)->base.cra_flags & -                       CRYPTO_ALG_ASYNC)) -               return -EINVAL, +       if (req->__onstack) { +               if (WARN_ON(crypto_skcipher_alg(tfm)->base.cra_flags & +                               CRYPTO_ALG_ASYNC)) +                       return -EINVAL, +               if (WARN_ON(crypto_skcipher_reqsize(tfm) > +                               MAX_SYNC_SKCIPHER_REQSIZE)) +                       return -ENOSPC, +       } ...etc  --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,1,512,0.813953488372093,0.9473684210526315,0,1.0,0.0,0.0,0.0
599,387234,390370,"If the lack of named initializer is too ugly, we could do something crazy like","On Thu, Sep 6, 2018 at 1:22 PM, Kees Cook <keescook@chromium.org> wrote:  If the lack of named initializer is too ugly, we could do something crazy like:  #define MAX_SYNC_SKCIPHER_REQSIZE       384 struct skcipher_request_on_stack {         union {                 struct skcipher_request req,                 char bytes[sizeof(struct skcipher_request) +                            MAX_SYNC_SKCIPHER_REQSIZE],         }, },  /*  * This must only ever be used with synchronous algorithms.  */ #define SKCIPHER_REQUEST_ON_STACK(name)                         \         struct skcipher_request_on_stack __##name##_req =       \                 { req.__onstack = 1 },                          \         struct skcipher_request *name = &(__##name##_req.req)   -Kees  --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,1,78,0.12403100775193798,1.0,1,1.0,0.0,0.0,0.0
600,388675,427616,Any comments about this patch?,"Hi Alexander,  Any comments about this patch?  BRs Zhi Jin ",technical,"Jin, Zhi",zhi.jin@intel.com,0,0,30,0.048,0.5,0,0.8974358974358975,0.07692307692307693,0.8974358974358975,0.07692307692307693
601,388675,429426,"The description is slightly confusing, but the patch looks correct and the original code is clearly wrong. Thank you for finding this! Basically, if you request 1 channel 3 times, release the first two and then request 4 channels, you'll be stuck, right?","Zhi Jin <zhi.jin@intel.com> writes:   The description is slightly confusing, but the patch looks correct and the original code is clearly wrong. Thank you for finding this!  Basically, if you request 1 channel 3 times, release the first two and then request 4 channels, you'll be stuck, right?  Thanks, -- Alex",technical,Alexander Shishkin,alexander.shishkin@linux.intel.com,1,0,254,0.416,0.75,0,1.0,0.0,0.07692307692307693,0.0
602,388675,429454,"Yes, you are right. But the real case that I reproduced the issue is a little different: I have 2 stp-policy: ""console"": masters ""256 259""  channels ""7 10""user""   : masters ""256 1024"" channels ""0 127""I understand the policies should not be overlapped, which is caused by some other issues. So if someone uses ""console"" to request a channel (who will get Channel #7) and then another uses ""user"" to request more than 8 channels, it will be stuck. The commit message is what I trying to abstract the above case, sorry for the confusion.","Yes, you are right. But the real case that I reproduced the issue is a little different: I have 2 stp-policy: console"": masters ""256 259""  channels ""7 10"" ""user""   : masters ""256 1024"" channels ""0 127"" I understand the policies should not be overlapped, which is caused by some other issues. So if someone uses ""console"" to request a channel (who will get Channel #7) and then another uses ""user"" to request more than 8 channels, it will be stuck. The commit message is what I trying to abstract the above case, sorry for the confusion. """,technical,"Jin, Zhi",zhi.jin@intel.com,0,0,534,1.0,1.0,1,1.0,0.0,0.0,0.0
603,390394,390815,"OK, so given that all SKCIPHER_REQUEST_ON_STACK occurrences are updated in this series anyway, perhaps we should add skcipher_[ende]crypt_onstack() flavors that encapsulate the additional check? Only question is how to enforce at compile time that those are used instead of the ordinary ones when using a stack allocated request. Would you mind using some macro foo here involving it?","On 7 September 2018 at 05:42, Herbert Xu <herbert@gondor.apana.org.au> wrote:  OK, so given that all SKCIPHER_REQUEST_ON_STACK occurrences are updated in this series anyway, perhaps we should add skcipher_[en|de]crypt_onstack() flavors that encapsulate the additional check? Only question is how to enforce at compile time that those are used instead of the ordinary ones when using a stack allocated request. Would you mind using some macro foo here involving __builtin_types_compatible_p() ?",technical,Ard Biesheuvel,ard.biesheuvel@linaro.org,1,0,384,0.71875,0.5833333333333334,0,0.0,1.0,0.0,0.0
604,390394,391307,"I'll continue to investigate alternatives, but I wanted to point out that the struct change actually fills an existing padding byte (so no change in memory usage) and marking this as an unlikely() test means it wouldn't even be measurable due to the branch predictor (so no change in speed). encrypt/decrypt entry is a tiny tiny fraction of the actual work done during encryption/decryption, etc.","On Thu, Sep 6, 2018 at 8:42 PM, Herbert Xu <herbert@gondor.apana.org.au> wrote:  I'll continue to investigate alternatives, but I wanted to point out that the struct change actually fills an existing padding byte (so no change in memory usage) and marking this as an unlikely() test means it wouldn't even be measurable due to the branch predictor (so no change in speed). encrypt/decrypt entry is a tiny tiny fraction of the actual work done during encryption/decryption, etc.  -Kees  --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,1,396,0.8020833333333334,0.6666666666666666,0,0.0,1.0,0.0,0.5
605,390394,393691,Something like a completely new type which in reality is just a wrapper around skcipher: These functions would just be trivial inline functions around their crypto_skcipher counterparts.,"On Fri, Sep 07, 2018 at 08:56:23AM +0200, Ard Biesheuvel wrote:  Something like a completely new type which in reality is just a wrapper around skcipher:  	struct crypto_sync_skcipher { 		struct crypto_skcipher base, 	} tfm,  	tfm = crypto_alloc_sync_skcipher(...),  	crypto_sync_skcipher_encrypt(...) 	crypto_sync_skcipher_decrypt(...)  These functions would just be trivial inline functions around their crypto_skcipher counterparts.  Cheers, --  Email: Herbert Xu <herbert@gondor.apana.org.au> Home Page: http://gondor.apana.org.au/~herbert/ PGP Key: http://gondor.apana.org.au/~herbert/pubkey.txt",technical,Herbert Xu,herbert@gondor.apana.org.au,1,0,186,0.3020833333333333,0.75,0,0.6666666666666666,0.3333333333333333,0.5,0.0
606,390394,393692,The point is the ON_STACK request stuff is purely for backwards compatibility and we don't want it to proliferate and pollute the core API.,"On Fri, Sep 07, 2018 at 09:02:45AM -0700, Kees Cook wrote:  The point is the ON_STACK request stuff is purely for backwards compatibility and we don't want it to proliferate and pollute the core API.  Cheers, --  Email: Herbert Xu <herbert@gondor.apana.org.au> Home Page: http://gondor.apana.org.au/~herbert/ PGP Key: http://gondor.apana.org.au/~herbert/pubkey.txt",technical,Herbert Xu,herbert@gondor.apana.org.au,1,0,139,0.2708333333333333,0.8333333333333334,0,0.6666666666666666,0.3333333333333333,0.0,0.3333333333333333
607,390394,396713,"This means new wrappers for the other helpers too, yes? For example ,For the above, we'd also need","On Mon, Sep 10, 2018 at 10:52 PM, Herbert Xu <herbert@gondor.apana.org.au> wrote:  This means new wrappers for the other helpers too, yes? For example:          SKCIPHER_REQUEST_ON_STACK(nreq, ctx->null),          skcipher_request_set_tfm(nreq, ctx->null),         skcipher_request_set_callback(nreq, req->base.flags, NULL, NULL),         skcipher_request_set_crypt(nreq, req->src, req->dst, nbytes, NULL),          return crypto_skcipher_encrypt(nreq),  For the above, we'd also need:  sync_skcipher_request_set_tfm() sync_skcipher_request_set_callback() sync_skcipher_request_set_crypt()  -Kees  --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,1,98,0.23958333333333334,0.9166666666666666,0,1.0,0.0,0.3333333333333333,0.0
608,390394,396772,"Wait, I think I misunderstood you. Did you mean a new top-level thing(tfm?) not a new request type? That would mean at least replacing skcipher_request_set_tfm() with a wrapper (since the tfm argument is different), but _not_encrypt/decrypt like you mention. I could perform a type test in this. API misuse would be caught at build-time (via SKCIPHER_REQUEST_ON_STACK type checking) and any request size problems would be caught at allocation time. Does this sound like what you had in mind?","On Thu, Sep 13, 2018 at 9:46 AM, Kees Cook <keescook@chromium.org> wrote:  Wait, I think I misunderstood you. Did you mean a new top-level thing (tfm?) not a new request type?  That would mean at least replacing skcipher_request_set_tfm() with a wrapper (since the tfm argument is different), but _not_ encrypt/decrypt like you mention. I could perform a type test in SKCIPHER_REQUEST_ON_STACK().  Globally: - add struct crypto_sync_skcipher wrapper - add crypto_alloc_sync_skcipher() to check non-ASYNC and request size of actual tfm - add skcipher_request_set_sync_tfm() to attach the wrapped tfm to the request - SKCIPHER_REQUEST_ON_STACK() would verify the tfm was a struct crypto_sync_skcipher.  Two changes per user: - change allocation to use new crypto_alloc_sync_skcipher() which does the runtime checking - change skcipher_request_set_tfm() to skcipher_request_set_sync_tfm()  This means struct skcipher_request is unchanged, along with _set_callback, _set_crypt, _zero, and en/decrypt.  API misuse would be caught at build-time (via SKCIPHER_REQUEST_ON_STACK type checking) and any request size problems would be caught at allocation time.  Does this sound like what you had in mind?  -Kees  --  Kees Cook Pixel Security",technical,Kees Cook,keescook@chromium.org,1,1,491,1.0,1.0,1,1.0,0.0,0.0,0.0
609,391895,392481,"Hi, Please document both of these kernel parameters in Documentation/admin-guide/kernel-parameters.txt.","On 9/7/18 2:40 PM, Jan H. Schönherr wrote:   Hi, Please document both of these kernel parameters in Documentation/admin-guide/kernel-parameters.txt.  thanks, --  ~Randy",technical,Randy Dunlap,rdunlap@infradead.org,0,0,103,0.016483516483516484,0.543859649122807,0,0.022988505747126436,0.9770114942528736,0.022988505747126436,0.011494252873563218
610,391895,395536,"This setup *should* work. It should be possible to set CPU. Scheduled independent of the CPU. Scheduled values of parent and child task groups. Any intermediate regular task group (i.e. CPU. Scheduled==0) will still contribute the group fairness aspects. That said, I see a hang, too. It seems to happen, when there is a cpu. scheduled!=0 group that is not a direct child of the root task group. You seem to have ""/sys/fs/cgroup/cpu/machine"" as an intermediate group.(The case ==0 within !=0 within the root task group works for me.) I'm going to dive into the code. If you're willing, you can try to get rid of the intermediate ""machine"" cgroup in your setup for the moment. This might tell us, whether we're looking at the same issue.","On 09/12/2018 02:24 AM, Nishanth Aravamudan wrote:  This setup *should* work. It should be possible to set cpu.scheduled independent of the cpu.scheduled values of parent and child task groups. Any intermediate regular task group (i.e. cpu.scheduled==0) will still contribute the group fairness aspects.  That said, I see a hang, too. It seems to happen, when there is a cpu.scheduled!=0 group that is not a direct child of the root task group. You seem to have /sys/fs/cgroup/cpu/machine"" as an intermediate group. (The case ==0 within !=0 within the root task group works for me.)  I'm going to dive into the code.  [...]  If you're willing, you can try to get rid of the intermediate ""machine"" cgroup in your setup for the moment. This might tell us, whether we're looking at the same issue.  Thanks, Jan""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,736,0.22527472527472528,0.5614035087719298,0,0.04597701149425287,0.9425287356321839,0.0,0.0
611,391895,395658,"Ah I see, that makes sense, thank you. Yep I will do this now. Note that if I just try to set machine's CPU. Scheduled to 1, with no other changes (not even changing any child cgroup's CPU. Scheduled yet), I get the following trace I'll reboot and move some cgroups around :)","On 12.09.2018 [21:34:14 +0200], Jan H. Schnherr wrote:  Ah I see, that makes sense, thank you.   Yep I will do this now. Note that if I just try to set machine's cpu.scheduled to 1, with no other changes (not even changing any child cgroup's cpu.scheduled yet), I get the following trace:  [16052.164259] ------------[ cut here ]------------ [16052.168973] rq->clock_update_flags < RQCF_ACT_SKIP [16052.168991] WARNING: CPU: 59 PID: 59533 at kernel/sched/sched.h:1303 assert_clock_updated.isra.82.part.83+0x15/0x18 [16052.184424] Modules linked in: act_police cls_basic ebtable_filter ebtables ip6table_filter iptable_filter nbd ip6table_raw ip6_tables xt_CT iptable_raw ip_tables s [16052.255653]  xxhash raid10 raid0 multipath linear raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq ses libcrc32c raid1 enclosure scsi [16052.276029] CPU: 59 PID: 59533 Comm: bash Tainted: G           O      4.19.0-rc2-amazon-cosched+ #1 [16052.291142] Hardware name: Dell Inc. PowerEdge R640/0W23H8, BIOS 1.4.9 06/29/2018 [16052.298728] RIP: 0010:assert_clock_updated.isra.82.part.83+0x15/0x18 [16052.305166] Code: 0f 85 75 ff ff ff 48 83 c4 08 5b 5d 41 5c 41 5d 41 5e 41 5f c3 48 c7 c7 28 30 eb 94 31 c0 c6 05 47 18 27 01 01 e8 f4 df fb ff <0f> 0b c3 48 8b 970 [16052.324050] RSP: 0018:ffff9cada610bca8 EFLAGS: 00010096 [16052.329361] RAX: 0000000000000026 RBX: ffff8f06d65bae00 RCX: 0000000000000006 [16052.336580] RDX: 0000000000000000 RSI: 0000000000000096 RDI: ffff8f1edf756620 [16052.343799] RBP: ffff8f06e0462e00 R08: 000000000000079b R09: ffff9cada610bc48 [16052.351018] R10: 0000000000000000 R11: 0000000000000000 R12: ffff8f06e0462e80 [16052.358237] R13: 0000000000000001 R14: ffff8f06e0462e00 R15: 0000000000000001 [16052.365458] FS:  00007ff07ab02740(0000) GS:ffff8f1edf740000(0000) knlGS:0000000000000000 [16052.373647] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [16052.379480] CR2: 00007ff07ab139d8 CR3: 0000002ca2aea002 CR4: 00000000007626e0 [16052.386698] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [16052.393917] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 [16052.401137] PKRU: 55555554 [16052.403927] Call Trace: [16052.406460]  update_curr+0x19f/0x1c0 [16052.410116]  dequeue_entity+0x21/0x8c0 [16052.413950]  ? terminate_walk+0x55/0xb0 [16052.417871]  dequeue_entity_fair+0x46/0x1c0 [16052.422136]  sdrq_update_root+0x35d/0x480 [16052.426227]  cosched_set_scheduled+0x80/0x1c0 [16052.430675]  cpu_scheduled_write_u64+0x26/0x30 [16052.435209]  cgroup_file_write+0xe3/0x140 [16052.439305]  kernfs_fop_write+0x110/0x190 [16052.443397]  __vfs_write+0x26/0x170 [16052.446974]  ? __audit_syscall_entry+0x101/0x130 [16052.451674]  ? _cond_resched+0x15/0x30 [16052.455509]  ? __sb_start_write+0x41/0x80 [16052.459600]  vfs_write+0xad/0x1a0 [16052.462997]  ksys_write+0x42/0x90 [16052.466397]  do_syscall_64+0x55/0x110 [16052.470152]  entry_SYSCALL_64_after_hwframe+0x44/0xa9 [16052.475286] RIP: 0033:0x7ff07a1e93c0 [16052.478943] Code: 73 01 c3 48 8b 0d c8 2a 2d 00 f7 d8 64 89 01 48 83 c8 ff c3 66 0f 1f 44 00 00 83 3d bd 8c 2d 00 00 75 10 b8 01 00 00 00 0f 05 <48> 3d 01 f0 ff ff4 [16052.497827] RSP: 002b:00007ffc73e335b8 EFLAGS: 00000246 ORIG_RAX: 0000000000000001 [16052.505498] RAX: ffffffffffffffda RBX: 0000000000000002 RCX: 00007ff07a1e93c0 [16052.512715] RDX: 0000000000000002 RSI: 00000000023a0408 RDI: 0000000000000001 [16052.519936] RBP: 00000000023a0408 R08: 000000000000000a R09: 00007ff07ab02740 [16052.527156] R10: 00007ff07a4bb6a0 R11: 0000000000000246 R12: 00007ff07a4bd400 [16052.534374] R13: 0000000000000002 R14: 0000000000000001 R15: 0000000000000000 [16052.541593] ---[ end trace b20c73e6c2bec22c ]---  I'll reboot and move some cgroups around :)  Thanks, Nish",technical,Nishanth Aravamudan,naravamudan@digitalocean.com,0,0,275,0.09203296703296704,0.5701754385964912,0,0.05747126436781609,0.9425287356321839,0.0,0.0
612,391895,395803,"Yep, this does fix the soft lockups for me, thanks! However, if I do a which should co-schedule all the cgroups for emulator and vcpu threads, I see the same warning I mentioned in my other e-mail:","On 13.09.2018 [01:18:14 +0200], Jan H. Schnherr wrote:  Yep, this does fix the soft lockups for me, thanks! However, if I do a:  # find /sys/fs/cgroup/cpu/machine -mindepth 2 -maxdepth 2 -name cpu.scheduled -exec /bin/sh -c echo 1 > {} "" \,  which should co-schedule all the cgroups for emulator and vcpu threads, I see the same warning I mentioned in my other e-mail:  [10469.832822] ------------[ cut here ]------------ [10469.837555] rq->clock_update_flags < RQCF_ACT_SKIP [10469.837574] WARNING: CPU: 89 PID: 49630 at kernel/sched/sched.h:1303 assert_clock_updated.isra.82.part.83+0x15/0x18 [10469.853042] Modules linked in: act_police cls_basic ebtable_filter ebtables ip6table_filter iptable_filter nbd ip6table_raw ip6_tables xt_CT iptable_raw ip_tables s [10469.924590]  xxhash raid10 raid0 multipath linear raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq ses libcrc32c raid1 enclosure scsi [10469.945010] CPU: 89 PID: 49630 Comm: sh Tainted: G           O      4.19.0-rc2-amazon-cosched+ #2 [10469.960061] Hardware name: Dell Inc. PowerEdge R640/0W23H8, BIOS 1.4.9 06/29/2018 [10469.967657] RIP: 0010:assert_clock_updated.isra.82.part.83+0x15/0x18 [10469.974126] Code: 0f 85 75 ff ff ff 48 83 c4 08 5b 5d 41 5c 41 5d 41 5e 41 5f c3 48 c7 c7 28 30 eb 8d 31 c0 c6 05 67 18 27 01 01 e8 14 e0 fb ff <0f> 0b c3 48 8b 970 [10469.993018] RSP: 0018:ffffabc0b534fca8 EFLAGS: 00010096 [10469.998341] RAX: 0000000000000026 RBX: ffff9d74d12ede00 RCX: 0000000000000006 [10470.005559] RDX: 0000000000000000 RSI: 0000000000000096 RDI: ffff9d74dfb16620 [10470.012780] RBP: ffff9d74df562e00 R08: 0000000000000796 R09: ffffabc0b534fc48 [10470.020005] R10: 0000000000000000 R11: 0000000000000000 R12: ffff9d74d2849800 [10470.027226] R13: 0000000000000001 R14: ffff9d74df562e00 R15: 0000000000000001 [10470.034445] FS:  00007fea86812740(0000) GS:ffff9d74dfb00000(0000) knlGS:0000000000000000 [10470.042678] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [10470.048511] CR2: 00005620f00314d8 CR3: 0000002cc55ea004 CR4: 00000000007626e0 [10470.055739] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [10470.062965] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 [10470.070186] PKRU: 55555554 [10470.072976] Call Trace: [10470.075508]  update_curr+0x19f/0x1c0 [10470.079211]  dequeue_entity+0x21/0x8c0 [10470.083056]  dequeue_entity_fair+0x46/0x1c0 [10470.087321]  sdrq_update_root+0x35d/0x480 [10470.091420]  cosched_set_scheduled+0x80/0x1c0 [10470.095892]  cpu_scheduled_write_u64+0x26/0x30 [10470.100427]  cgroup_file_write+0xe3/0x140 [10470.104523]  kernfs_fop_write+0x110/0x190 [10470.108624]  __vfs_write+0x26/0x170 [10470.112236]  ? __audit_syscall_entry+0x101/0x130 [10470.116943]  ? _cond_resched+0x15/0x30 [10470.120781]  ? __sb_start_write+0x41/0x80 [10470.124871]  vfs_write+0xad/0x1a0 [10470.128268]  ksys_write+0x42/0x90 [10470.131668]  do_syscall_64+0x55/0x110 [10470.135421]  entry_SYSCALL_64_after_hwframe+0x44/0xa9 [10470.140558] RIP: 0033:0x7fea863253c0 [10470.144213] Code: 73 01 c3 48 8b 0d c8 2a 2d 00 f7 d8 64 89 01 48 83 c8 ff c3 66 0f 1f 44 00 00 83 3d bd 8c 2d 00 00 75 10 b8 01 00 00 00 0f 05 <48> 3d 01 f0 ff ff4 [10470.163114] RSP: 002b:00007ffe7cb22d18 EFLAGS: 00000246 ORIG_RAX: 0000000000000001 [10470.170783] RAX: ffffffffffffffda RBX: 00005620f002f4d0 RCX: 00007fea863253c0 [10470.178002] RDX: 0000000000000002 RSI: 00005620f002f4d0 RDI: 0000000000000001 [10470.185222] RBP: 0000000000000002 R08: 0000000000000001 R09: 000000000000006b [10470.192486] R10: 0000000000000008 R11: 0000000000000246 R12: 0000000000000001 [10470.199705] R13: 0000000000000002 R14: 7fffffffffffffff R15: 0000000000000002 [10470.206923] ---[ end trace fbf46e2c721c7acb ]---  Thanks, Nish""",technical,Nishanth Aravamudan,naravamudan@digitalocean.com,0,0,197,0.059065934065934064,0.5877192982456141,0,0.05747126436781609,0.9425287356321839,0.0,0.0
613,391895,396034,This goes away with the change below (which fixes patch 58/60).,"On 09/13/2018 01:15 AM, Nishanth Aravamudan wrote: [snip]  This goes away with the change below (which fixes patch 58/60).  Thanks, Jan  diff --git a/kernel/sched/cosched.c b/kernel/sched/cosched.c index a1f0d3a7b02a..a98ea11ba172 100644 --- a/kernel/sched/cosched.c +++ b/kernel/sched/cosched.c @@ -588,6 +588,7 @@ static void sdrq_update_root(struct sdrq *sdrq)          /* Get proper locks */         rq_lock_irqsave(rq, &rf), +       update_rq_clock(rq),          sdrq->is_root = is_root,         if (is_root)",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,63,0.019230769230769232,0.5964912280701754,0,0.05747126436781609,0.9425287356321839,0.0,0.0
614,391895,396792,"Yep, I can confirm this one as well, is now fixed.","On 13.09.2018 [13:31:36 +0200], Jan H. Schnherr wrote:  Yep, I can confirm this one as well, is now fixed.  -Nish",technical,Nishanth Aravamudan,naravamudan@digitalocean.com,0,0,50,0.019230769230769232,0.6052631578947368,0,0.05747126436781609,0.9310344827586207,0.0,0.0
615,391895,398943,"I guess, it would be possible to flatten the task group hierarchy, that is usually created when nesting cgroups. That is, enqueue task group SEs always within the root task group. That should take away much of the (runtime-)overhead, no? The calculation of shares would need to be a different kind of complex than it is now. But that might be manageable. CFS bandwidth control would also need to change significantly as we would now have to dequeue/enqueue nested cgroups below a throttled/unthrottled hierarchy. Unless *those* task groups don't participate in this flattening.(And probably lots of other stuff, I didn't think about right now.)","On 09/14/2018 06:25 PM, Jan H. Schönherr wrote:  [...]   I guess, it would be possible to flatten the task group hierarchy, that is usually created when nesting cgroups. That is, enqueue task group SEs always within the root task group.  That should take away much of the (runtime-)overhead, no?  The calculation of shares would need to be a different kind of complex than it is now. But that might be manageable.  CFS bandwidth control would also need to change significantly as we would now have to dequeue/enqueue nested cgroups below a throttled/unthrottled hierarchy. Unless *those* task groups don't participate in this flattening.  (And probably lots of other stuff, I didn't think about right now.)  Regards Jan",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,644,0.17445054945054944,0.6403508771929824,0,0.08045977011494253,0.9195402298850575,0.0,0.022988505747126436
616,391895,399869,"That sounds like it will wreck the runnable_weight accounting. Although, if, as you write below, we do away with the hierarchical run queues, that isn't in fact needed anymore I think. Still, even without runnable_weight, I suspect we need the 'runnable' state, even for the other accounting. Can't do that, tasks might have individual constraints that are tighter than the CPU set. Also, changing affinities isn't really a hot path, so who cares. I have yet to go over your earlier email, but no. The scheduler is very much per-cpu. And as I mentioned earlier, CFS as is doesn't work right if you share the runqueue between multiple CPUs (and 'fixing' that is non trivial).Yes, Rik was going to look at trying this. Put all the tasks in the rootrq and adjust the vtime calculations. Facebook is seeing significant overhead from cpu-cgroup and has to disable it because of that on at least part of their setup IIUC. That is the hope, indeed. We'll still need to create the hierarchy for accounting purposes, but it can be a smaller/simpler data structure. So the weight computation would be the normalized product of the parent setc.. and since PELT only updates the values on ~1ms scale, we can keep a cache of the product -- that is, we don't have to recompute that product and walk the hierarchy all the time either. Right, so the whole bandwidth thing becomes a pain, the simplest solution is to detect the throttle at task-pick time, dequeue and try again. But that is indeed quite horrible. I'm not quite sure how this will play out. Anyway, if we pull off this flattening feat, then you can no longer use the hierarchy for this co-scheduling stuff. Now, let me go read your earlier email and reply to that (in parts).","On Sat, Sep 15, 2018 at 10:48:20AM +0200, Jan H. Schnherr wrote:  That sounds like it will wreck the runnable_weight accounting. Although, if, as you write below, we do away with the hierarchical runqueues, that isn't in fact needed anymore I think.  Still, even without runnable_weight, I suspect we need the 'runnable' state, even for the other accounting.   Can't do that, tasks might have individual constraints that are tighter than the cpuset. Also, changing affinities isn't really a hot path, so who cares.   I have yet to go over your earlier email, but no. The scheduler is very much per-cpu. And as I mentioned earlier, CFS as is doesn't work right if you share the runqueue between multiple CPUs (and 'fixing' that is non trivial).   Yes, Rik was going to look at trying this. Put all the tasks in the root rq and adjust the vtime calculations. Facebook is seeing significant overhead from cpu-cgroup and has to disable it because of that on at least part of their setup IIUC.   That is the hope, indeed. We'll still need to create the hierarchy for accounting purposes, but it can be a smaller/simpler data structure.  So the weight computation would be the normalized product of the parents etc.. and since PELT only updates the values on ~1ms scale, we can keep a cache of the product -- that is, we don't have to recompute that product and walk the hierarchy all the time either.   Right, so the whole bandwidth thing becomes a pain, the simplest solution is to detect the throttle at task-pick time, dequeue and try again. But that is indeed quite horrible.  I'm not quite sure how this will play out.  Anyway, if we pull off this flattening feat, then you can no longer use the hierarchy for this co-scheduling stuff.  Now, let me go read your earlier email and reply to that (in parts).",technical,Peter Zijlstra,peterz@infradead.org,1,0,1724,0.49313186813186816,0.6491228070175439,0,0.10344827586206896,0.896551724137931,0.022988505747126436,0.0
617,391895,399944,"You did mention this work first to me in the context of L1TF, so I might have jumped to conclusions here. Also, I have, of course, been looking at (SMT) co-scheduling, specifically in the context of L1TF, myself. I came up with a vastly different approach. Tim - where are we on getting some of that posted? Note, that even though I wrote much of that code, I don't particularly like it either :-)","On Fri, Sep 14, 2018 at 06:25:44PM +0200, Jan H. Schnherr wrote:    You did mention this work first to me in the context of L1TF, so I might have jumped to conclusions here.  Also, I have, of course, been looking at (SMT) co-scheduling, specifically in the context of L1TF, myself. I came up with a vastly different approach. Tim - where are we on getting some of that posted?  Note, that even though I wrote much of that code, I don't particularly like it either :-)",technical,Peter Zijlstra,peterz@infradead.org,1,0,397,0.125,0.6578947368421053,0,0.10344827586206896,0.896551724137931,0.0,0.0
618,391895,399979,"Specifically for L1TF I hooked into/extended KVM's preempt_notifier registration interface, which tells us which tasks are VCPUs and to which VM they belong. But if we want to actually expose this to userspace, we can either do aprctl() or extend struct sched_attr.Well, you mentioned it as an alternative to paravirt spinlocks -- I'm saying that co-scheduling cannot do that, you need full featured gang-scheduling for that.","On Fri, Sep 14, 2018 at 06:25:44PM +0200, Jan H. Schnherr wrote:   Specifically for L1TF I hooked into/extended KVM's preempt_notifier registration interface, which tells us which tasks are VCPUs and to which VM they belong.  But if we want to actually expose this to userspace, we can either do a prctl() or extend struct sched_attr.   Well, you mentioned it as an alternative to paravirt spinlocks -- I'm saying that co-scheduling cannot do that, you need full featured gang-scheduling for that.",technical,Peter Zijlstra,peterz@infradead.org,1,0,425,0.10576923076923077,0.6666666666666666,0,0.10344827586206896,0.896551724137931,0.0,0.0
619,391895,400059,"The thing is, if you drop the full width gang scheduling, you instantly require the paravirt spinlock / tlb-invalidate stuff again. Of course, the constraints of L1TF itself requires the explicit scheduling of idle time under a bunch of conditions. I did not read your [7] in much detail (also very bad quality scan that:-/, but I don't get how they leap from 'thrashing' to co-scheduling. Their initial problem, where A generates data that B needs and the 3scenarios: 1) A has to wait for B 2) B has to wait for A 3) the data gets buffered Seems fairly straight forward and is indeed quite common, needing co-scheduling for that, I'm not convinced. We have of course added all sorts of adaptive wait loops in the kernel to deal with just that issue. With co-scheduling you 'ensure' B is running when A is, but that doesn't mean you can actually make more progress, you could just be burning a lot of CPU cycles (which could've been spend doing other work).I'm also not convinced co-scheduling makes _any_ sense outside SMT --does one of the many papers you cite make a good case for !SMT co-scheduling? It just doesn't make sense to co-schedule the LLC domain, that's 16+ cores on recent chips.","On Fri, Sep 14, 2018 at 06:25:44PM +0200, Jan H. Schnherr wrote:   The thing is, if you drop the full width gang scheduling, you instantly require the paravirt spinlock / tlb-invalidate stuff again.  Of course, the constraints of L1TF itself requires the explicit scheduling of idle time under a bunch of conditions.  I did not read your [7] in much detail (also very bad quality scan that :-/, but I don't get how they leap from 'thrashing' to co-scheduling. Their initial problem, where A generates data that B needs and the 3 scenarios:   1) A has to wait for B  2) B has to wait for A  3) the data gets buffered  Seems fairly straight forward and is indeed quite common, needing co-scheduling for that, I'm not convinced.  We have of course added all sorts of adaptive wait loops in the kernel to deal with just that issue.  With co-scheduling you 'ensure' B is running when A is, but that doesn't mean you can actually make more progress, you could just be burning a lot of CPu cycles (which could've been spend doing other work).  I'm also not convinced co-scheduling makes _any_ sense outside SMT -- does one of the many papers you cite make a good case for !SMT co-scheduling? It just doesn't make sense to co-schedule the LLC domain, that's 16+ cores on recent chips.",technical,Peter Zijlstra,peterz@infradead.org,1,0,1195,0.3447802197802198,0.6754385964912281,0,0.10344827586206896,0.8850574712643678,0.0,0.0
620,391895,400941,"I don't get the affinity part. If I create two cgroups by giving them only cpu shares (no CPU set) and set their CPU. Scheduled=1, will this ensure co-scheduling of each group on core level for all cores in the system?","On 09/07/2018 02:39 PM, Jan H. Schönherr wrote: I don't get the affinity part. If I create two cgroups by giving them only cpu shares (no cpuset) and set their cpu.scheduled=1, will this ensure co-scheduling of each group on core level for all cores in the system?  Thanks, Subhra",technical,Subhra Mazumdar,subhra.mazumdar@oracle.com,0,0,218,0.06593406593406594,0.6842105263157895,0,0.11494252873563218,0.8850574712643678,0.0,0.0
621,391895,401373,"Short answer: Yes. But ignoring the affinity part will very likely result in              a poor experience with this patch set. I was referring to the CPU affinity of a task, that you can set via sched_setaffinity() from within a program or via taskset from the command line. For each task/thread within a cgroup, you should set the affinity to exactly one CPU. Otherwise -- as the load balancing part is still missing --you might end up with all tasks running on one CPU or some other unfortunate load distribution. Coscheduling itself does not care about the load, so each group will be(co-)scheduled at core level, no matter where the tasks ended up. Regards Jan PS: Below is an example to illustrate the resulting schedules a bit better, and what might happen, if you don't bind the to-be-scheduled tasks to individual CPUs. For example, consider a dual-core system with SMT (i.e. 4 CPUs in total),two task groups A and B, and tasks within them a0, a1, ..  and b0, b1, ..respectively .Let the system topology look like this:        System          (level 2)      /        \  Core 0      Core 1    (level 1)  /    \      /    \CPU0  CPU1  CPU2  CPU3  (level 0)If you set CPU. Scheduled=1 for A and B, each core will be scheduled independently, if there are tasks of A or B on the core. Assuming there are runnable tasks in A and B and some other tasks on a core, you will see a schedule like:  A -> B -> other tasks -> A -> B -> other tasks -> ...(or some permutation there of) happen synchronously across both CPUs of a core -- with no guarantees which tasks within A/within B/within the other tasks will execute simultaneously -- and with no guarantee what will execute on the other two CPUs simultaneously. (The distribution of CPU time between A, B, and other tasks follows the usual CFS weight proportional distribution, just at core level.) If neither CPU of a core has any runnable tasks of a certain group, it won't be part of the schedule (e.g., A -> other -> A -> other).With CPU. Scheduled=2, you lift this schedule to system-level and you would see it happen across all four CPUs synchronously. With CPU. Scheduled=0, you get this schedule at CPU-level as we're all used to with no synchronization between CPUs. (It gets a tad more interesting, when you start mixing groups with CPU. Scheduled=1 and =2.)Here are some schedules, that you might see, with A and B scheduled at core level (and that can be enforced this way (along the horizontal dimension) by setting the affinity of tasks, without setting the affinity, it could be any of them):Tasks equally distributed within A and B. You will never see an A-task sharing a core with a B-task at any point in time (except for the 2 microseconds or so, that the collective context switch takes).","On 09/18/2018 02:33 AM, Subhra Mazumdar wrote:  Short answer: Yes. But ignoring the affinity part will very likely result in               a poor experience with this patch set.   I was referring to the CPU affinity of a task, that you can set via sched_setaffinity() from within a program or via taskset from the command line. For each task/thread within a cgroup, you should set the affinity to exactly one CPU. Otherwise -- as the load balancing part is still missing -- you might end up with all tasks running on one CPU or some other unfortunate load distribution.  Coscheduling itself does not care about the load, so each group will be (co-)scheduled at core level, no matter where the tasks ended up.  Regards Jan  PS: Below is an example to illustrate the resulting schedules a bit better, and what might happen, if you don't bind the to-be-coscheduled tasks to individual CPUs.    For example, consider a dual-core system with SMT (i.e. 4 CPUs in total), two task groups A and B, and tasks within them a0, a1, ..  and b0, b1, .. respectively.  Let the system topology look like this:          System          (level 2)       /        \   Core 0      Core 1    (level 1)   /    \      /    \ CPU0  CPU1  CPU2  CPU3  (level 0)   If you set cpu.scheduled=1 for A and B, each core will be coscheduled independently, if there are tasks of A or B on the core. Assuming there are runnable tasks in A and B and some other tasks on a core, you will see a schedule like:    A -> B -> other tasks -> A -> B -> other tasks -> ...  (or some permutation thereof) happen synchronously across both CPUs of a core -- with no guarantees which tasks within A/within B/ within the other tasks will execute simultaneously -- and with no guarantee what will execute on the other two CPUs simultaneously. (The distribution of CPU time between A, B, and other tasks follows the usual CFS weight proportional distribution, just at core level.) If neither CPU of a core has any runnable tasks of a certain group, it won't be part of the schedule (e.g., A -> other -> A -> other).  With cpu.scheduled=2, you lift this schedule to system-level and you would see it happen across all four CPUs synchronously. With cpu.scheduled=0, you get this schedule at CPU-level as we're all used to with no synchronization between CPUs. (It gets a tad more interesting, when you start mixing groups with cpu.scheduled=1 and =2.)   Here are some schedules, that you might see, with A and B coscheduled at core level (and that can be enforced this way (along the horizontal dimension) by setting the affinity of tasks, without setting the affinity, it could be any of them):  Tasks equally distributed within A and B:  t   CPU0  CPU1  CPU2  CPU3 0    a0    a1    b2    b3 1    a0    a1   other other 2    b0    b1   other other 3    b0    b1    a2    a3 4   other other  a2    a3 5   other other  b2    b3  All tasks within A and B on one CPU:  t   CPU0  CPU1  CPU2  CPU3 0    a0    --   other other 1    a1    --   other other 2    b0    --   other other 3    b1    --   other other 4   other other other other 5    a2    --   other other 6    a3    --   other other 7    b2    --   other other 8    b3    --   other other  Tasks within a group equally distributed across one core:  t   CPU0  CPU1  CPU2  CPU3 0    a0    a2    b1    b3 1    a0    a3   other other 2    a1    a3   other other 3    a1    a2    b0    b3 4   other other  b0    b2 5   other other  b1    b2  You will never see an A-task sharing a core with a B-task at any point in time (except for the 2 microseconds or so, that the collective context switch takes).",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,2760,0.8035714285714286,0.6929824561403509,0,0.11494252873563218,0.8850574712643678,0.0,0.0
622,391895,401463,"AFAIK, changing the affinity of a cpu set overwrites the individual affinities of tasks within them. Thus, it shouldn't be an issue. This kind of code path gets a little hotter, when a scheduled set gets load-balanced from one core to another. Apart from that, I also think, that normal user-space applications should never have to concern themselves with actual affinities. More often than not, they only want to express a relation to some other task (or sometimes resource), like ""run on the same NUMA node"", ""run on the same core"", so that application design assumptions are fulfilled. That's an interface, that I'd like to see as a cgroup controller at some point. It would also benefit from the ability to move/balance whole run queues.(It might also be a way to just bulk-balance a bunch of tasks in the current code, by exchanging two CFS run queues. But that has probably some additional issues.)No sharing. Just not allocating run queues that won't be used anyway. Assume you express this ""always run on the same core"" or have other reasons to always restrict tasks in a task group to just one core/node/whatever. On an SMT system, you would typically need at most two run queues for a core, the memory foot-print of a task group would no longer increase linearly with system size. It would be possible to (space-)efficiently express nested parallelism use cases without having to resort to managing affinities manually (which restrict the scheduler more than necessary).(And it would be okay for an adjustment of the maximum number of run queues to fail with an -ENOMEM in dire situations, as this adjustment would be an explicit(user-)action.)Yeah. I might be a bit biased towards keeping or at least not fully throwing away the nesting of CFS run queues. ,)However, the only efficient way that I can currently think of, is a hybrid model between the ""full nesting"" that is currently there, and the ""no nesting"" you were describing above. It would flatten all task groups that do not actively contribute some function, which would be all task groups purely for accounting purposes and those for*unthrottled* CFS hierarchies (and those for coscheduling that contain exactly one SE in a runqueue). The nesting would still be kept for *throttled* hierarchies (and the coscheduling stuff). (And if you wouldn't have mentioned a way to get rid of nesting completely, I would have kept a single level of nesting for accounting purposes as well.) This would allow us to lazily dequeue SEs that have run out of bandwidth when we encounter them, and already enqueue them in the nested task group (whose SE is not enqueued at the moment). That way, it's still a O(1) operation to re-enable all tasks, once runtime is available again. And O(1) to throttle a repeat offender.","On 09/17/2018 11:48 AM, Peter Zijlstra wrote:  AFAIK, changing the affinity of a cpuset overwrites the individual affinities of tasks within them. Thus, it shouldn't be an issue.   This kind of code path gets a little hotter, when a coscheduled set gets load-balanced from one core to another.  Apart from that, I also think, that normal user-space applications should never have to concern themselves with actual affinities. More often than not, they only want to express a relation to some other task (or sometimes resource), like run on the same NUMA node"", ""run on the same core"", so that application design assumptions are fulfilled. That's an interface, that I'd like to see as a cgroup controller at some point. It would also benefit from the ability to move/balance whole runqueues.  (It might also be a way to just bulk-balance a bunch of tasks in the current code, by exchanging two CFS runqueues. But that has probably some additional issues.)    No sharing. Just not allocating runqueues that won't be used anyway. Assume you express this ""always run on the same core"" or have other reasons to always restrict tasks in a task group to just one core/node/whatever. On an SMT system, you would typically need at most two runqueues for a core, the memory foot-print of a task group would no longer increase linearly with system size.  It would be possible to (space-)efficiently express nested parallelism use cases without having to resort to managing affinities manually (which restrict the scheduler more than necessary).  (And it would be okay for an adjustment of the maximum number of runqueues to fail with an -ENOMEM in dire situations, as this adjustment would be an explicit (user-)action.)    Yeah. I might be a bit biased towards keeping or at least not fully throwing away the nesting of CFS runqueues. ,)  However, the only efficient way that I can currently think of, is a hybrid model between the ""full nesting"" that is currently there, and the ""no nesting"" you were describing above.  It would flatten all task groups that do not actively contribute some function, which would be all task groups purely for accounting purposes and those for *unthrottled* CFS hierarchies (and those for coscheduling that contain exactly one SE in a runqueue). The nesting would still be kept for *throttled* hierarchies (and the coscheduling stuff). (And if you wouldn't have mentioned a way to get rid of nesting completely, I would have kept a single level of nesting for accounting purposes as well.)  This would allow us to lazily dequeue SEs that have run out of bandwidth when we encounter them, and already enqueue them in the nested task group (whose SE is not enqueued at the moment). That way, it's still a O(1) operation to re-enable all tasks, once runtime is available again. And O(1) to throttle a repeat offender.  Regards Jan""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,2774,0.7815934065934066,0.7017543859649122,0,0.11494252873563218,0.8850574712643678,0.0,0.0
623,391895,401480,"No, it only shrinks the set. Also nothing stops you calling sched_setaffinity() once you're inside the CPU set. The only constraint is that your mask is a subset of the CPU set mask.","On Tue, Sep 18, 2018 at 03:22:13PM +0200, Jan H. Schnherr wrote:  No, it only shrinks the set. Also nothing stops you calling sched_setaffinity() once you're inside the cpuset. The only contraint is that your mask is a subset of the cpuset mask.",technical,Peter Zijlstra,peterz@infradead.org,1,0,182,0.054945054945054944,0.7105263157894737,0,0.11494252873563218,0.8735632183908046,0.0,0.0
624,391895,401488,You forget that SMT4 and SMT8 are fairly common outside of x86.,"On Tue, Sep 18, 2018 at 03:22:13PM +0200, Jan H. Schnherr wrote:  You forget that SMT4 and SMT8 are fairly common outside of x86.",technical,Peter Zijlstra,peterz@infradead.org,1,0,63,0.017857142857142856,0.7192982456140351,0,0.11494252873563218,0.8735632183908046,0.0,0.0
625,391895,401520,"I meant setting the affinity of the CPU set *after* setting an individual affinity. Like this: pid 4745's current affinity list: 0,1#The individual affinity of  is lost, despite it being a subset.","On 09/18/2018 03:38 PM, Peter Zijlstra wrote:  I meant setting the affinity of the cpuset *after* setting an individual affinity.  Like this:  # mkdir /sys/fs/cgroup/cpuset/test # cat /sys/fs/cgroup/cpuset/cpuset.mems > /sys/fs/cgroup/cpuset/test/cpuset.mems # cat /sys/fs/cgroup/cpuset/cpuset.cpus > /sys/fs/cgroup/cpuset/test/cpuset.cpus # echo $$ > /sys/fs/cgroup/cpuset/test/tasks # taskset -c -p 0 $$ pid 4745's current affinity list: 0-3 pid 4745's new affinity list: 0 # echo 0-1"" > /sys/fs/cgroup/cpuset/test/cpuset.cpus # taskset -c -p $$ pid 4745's current affinity list: 0,1 #  The individual affinity of $$ is lost, despite it being a subset.""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,196,0.057692307692307696,0.7280701754385965,0,0.11494252873563218,0.8735632183908046,0.0,0.0
626,391895,401573,"I do not have a strong bias either way. However, I would like the overhead of the cpu controller to be so low that we can actually use it :)Task priorities in a flat runqueue are relatively straightforward, with vruntime scaling just like done for nice levels, but I have to admit that throttled groups provide a challenge. Dequeueing throttled tasks is pretty straightforward, but requeuing them afterwards when they are no longer throttled could present a real challenge in some situations. I suspect most systems will have a number of runnable tasks no larger than the number of CPUs most of the time. That makes ""re-enable all the tasks"" often equivalent to ""re-enable one task"". Can we handle the re-enabling (or waking up!) of one task almost as fast as we can without the cpu controller?","On Tue, 2018-09-18 at 15:22 +0200, Jan H. Schönherr wrote:  I do not have a strong bias either way. However, I  would like the overhead of the cpu controller to be so low that we can actually use it :)  Task priorities in a flat runqueue are relatively straightforward, with vruntime scaling just like done for nice levels, but I have to admit that throttled groups provide a challenge.  Dequeueing throttled tasks is pretty straightforward, but requeueing them afterwards when they are no longer throttled could present a real challenge in some situations.   I suspect most systems will have a number of runnable tasks no larger than the number of CPUs most of the time.  That makes re-enable all the tasks"" often equivalent to ""re-enable one task"".  Can we handle the re-enabling (or waking up!) of one task almost as fast as we can without the cpu controller?  --  All Rights Reversed. """,technical,Rik van Riel,riel@surriel.com,0,0,794,0.21428571428571427,0.7368421052631579,0,0.11494252873563218,0.8735632183908046,0.0,0.0
627,391895,401578,"What are the other use cases, and what kind of performance numbers do you have to show examples of workloads where coscheduling provides a performance benefit?","On Fri, 2018-09-14 at 18:25 +0200, Jan H. Schönherr wrote:  What are the other use cases, and what kind of performance numbers do you have to show examples of workloads where coscheduling provides a performance benefit?  --  All Rights Reversed.",technical,Rik van Riel,riel@surriel.com,0,0,159,0.038461538461538464,0.7456140350877193,0,0.11494252873563218,0.8735632183908046,0.0,0.0
628,391895,402366,"Do you know/have an idea how the flat approach would further skew the approximations currently done in?With the nested hierarchy the (shared) task group SE is updated whenever something changes. With the flat approach, you'd only be able to update a task SE, when you have to touch the task anyway. Just from thinking briefly about it, it feels like values would be out of date for longer periods of time. We could start by transparently special-casing the ""just one SE in a runqueue"" case, where that single SE is enqueued directly into the next parent, and everything falls back to nesting, the moment a second SE pops up. That might allow reaping the benefits for many cases without hurting other cases. It's also more a gradual conversion of code.","On 09/18/2018 04:35 PM, Rik van Riel wrote: [...]  Do you know/have an idea how the flat approach would further skew the approximations currently done in calc_group_shares()/calc_group_runnable()?  With the nested hierarchy the (shared) task group SE is updated whenever something changes. With the flat approach, you'd only be able to update a task SE, when you have to touch the task anyway. Just from thinking briefly about it, it feels like values would be out of date for longer periods of time.    We could start by transparently special-casing the just one SE in a runqueue"" case, where that single SE is enqueued directly into the next parent, and everything falls back to nesting, the moment a second SE pops up.  That might allow reaping the benefits for many cases without hurting other cases. It's also more a gradual conversion of code.  Regards Jan""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,751,0.20604395604395603,0.7543859649122807,0,0.12643678160919541,0.8735632183908046,0.0,0.0
629,391895,403024,"Ok got it. Can we have a more generic interface, like specifying a set of task ids to be co-scheduled with a particular level rather than tying this with cgroups? KVMs may not always run with cgroups and there might be other use cases where we might want co-scheduling that doesn't relate to cgroups.","On 09/18/2018 04:44 AM, Jan H. Schönherr wrote: Ok got it. Can we have a more generic interface, like specifying a set of task ids to be co-scheduled with a particular level rather than tying this with cgroups? KVMs may not always run with cgroups and there might be other use cases where we might want co-scheduling that doesn't relate to cgroups.",technical,Subhra Mazumdar,subhra.mazumdar@oracle.com,0,0,300,0.08104395604395605,0.7631578947368421,0,0.13793103448275862,0.8620689655172413,0.0,0.04597701149425287
630,391895,408976,"For further use cases (still an incomplete list) let me redirect you to the unabridged Section B of the original e-mail:   If you want me to, I can go into more detail and make the list from that e-mail more complete. Note, that many coscheduling use cases are not primarily about performance. Sure, there are the resource contention use cases, which are barely about anything else. See, e.g., [1] for a survey with further pointers to the potential performance gains. Realizing those use cases would require either a user space component driving this, or another kernel component performing a function similar to the current auto-grouping with some more complexity depending on the desired level of sophistication. This extra component is out of my scope. But I see a coscheduler like this as an enabler for practical applications of these kind of use cases. If you use coscheduling as part of a solution that closes a side-channel, performance is a secondary aspect, and hopefully we don't lose much of it. Then, there's the large fraction of use cases, where coscheduling is primarily about design flexibility, because it enables different (old and new) application designs, which usually cannot be executed in an efficient manner without coscheduling.  For these use cases performance is important, but there is also a trade-off against development costs of alternative solutions to consider. These are also the use cases where we can do measurements today, i.e., without some yet-to-be-written extra component. For example, with coscheduling it is possible to use active waiting instead of passive waiting/spin-blocking on non-dedicated systems, because lock holder pre-emption is not an issue anymore. It also allows using applications that were developed for dedicated scenarios in non-dedicated settings without loss in performance -- like an (unmodified) operating system within a VM, or HPC code. Another example is cache optimization of parallel algorithms, where you don't have to resort to cache-oblivious algorithms for efficiency, but where you can stay with manually tuned or auto-tuned algorithms, even on non-dedicated systems. (You're even able to do the tuning itself on a system that has other load.)Now, you asked about performance numbers, that *I* have. If a workload has issues with lock-holder pre-emption, I've seen up to 5x to20x improvement with coscheduling. (This includes parallel programs [2] and VMs with unmodified guests without PLE [3].) That is of course highly dependent on the workload. I currently don't have any numbers comparing coscheduling to other solutions used to reduce/avoid lock holder pre-emption, that don't mix in any other aspect like resource contention. These would have to be micro-benchmarked. If you're happy to compare across some more moving variables, then more or less blind coscheduling of parallel applications with some automatic workload-driven (but application-agnostic) width adjustment of scheduled sets yielded an overall performance benefit between roughly 10% to 20% compared to approaches with passive waiting [2]. It was roughly on par with pure space-partitioning approaches (slight minus on performance, slight plus on flexibility/fairness).I never went much into the resource contention use cases myself. Though, I did use coscheduling to extend the concept of ""nice"" to sockets by putting all niced programs into a scheduled task group with appropriately reduced shares.  This way, niced programs don't just get any and all idle CPU capacity -- taking away parts of the energy budget of more important tasks all the time -- which leads to important tasks running at turbo frequencies more often. Depending on the parallelism of niced workload and the parallelism of normal workload, this translates to a performance improvement of the normal workload that corresponds roughly to the increase in frequency (for CPU-bound tasks) [4]. Depending on the processor, that can be anything from just a few percent to about a factor of 2.","On 09/18/2018 04:40 PM, Rik van Riel wrote:  For further use cases (still an incomplete list) let me redirect you to the unabridged Section B of the original e-mail:    https://lkml.org/lkml/2018/9/7/1521  If you want me to, I can go into more detail and make the list from that e-mail more complete.   Note, that many coscheduling use cases are not primarily about performance.  Sure, there are the resource contention use cases, which are barely about anything else. See, e.g., [1] for a survey with further pointers to the potential performance gains. Realizing those use cases would require either a user space component driving this, or another kernel component performing a function similar to the current auto-grouping with some more complexity depending on the desired level of sophistication. This extra component is out of my scope. But I see a coscheduler like this as an enabler for practical applications of these kind of use cases.  If you use coscheduling as part of a solution that closes a side-channel, performance is a secondary aspect, and hopefully we don't lose much of it.  Then, there's the large fraction of use cases, where coscheduling is primarily about design flexibility, because it enables different (old and new) application designs, which usually cannot be executed in an efficient manner without coscheduling.  For these use cases performance is important, but there is also a trade-off against development costs of alternative solutions to consider. These are also the use cases where we can do measurements today, i.e., without some yet-to-be-written extra component. For example, with coscheduling it is possible to use active waiting instead of passive waiting/spin-blocking on non-dedicated systems, because lock holder preemption is not an issue anymore. It also allows using applications that were developed for dedicated scenarios in non-dedicated settings without loss in performance -- like an (unmodified) operating system within a VM, or HPC code. Another example is cache optimization of parallel algorithms, where you don't have to resort to cache-oblivious algorithms for efficiency, but where you can stay with manually tuned or auto-tuned algorithms, even on non-dedicated systems. (You're even able to do the tuning itself on a system that has other load.)   Now, you asked about performance numbers, that *I* have.  If a workload has issues with lock-holder preemption, I've seen up to 5x to 20x improvement with coscheduling. (This includes parallel programs [2] and VMs with unmodified guests without PLE [3].) That is of course highly dependent on the workload. I currently don't have any numbers comparing coscheduling to other solutions used to reduce/avoid lock holder preemption, that don't mix in any other aspect like resource contention. These would have to be micro-benchmarked.  If you're happy to compare across some more moving variables, then more or less blind coscheduling of parallel applications with some automatic workload-driven (but application-agnostic) width adjustment of coscheduled sets yielded an overall performance benefit between roughly 10% to 20% compared to approaches with passive waiting [2]. It was roughly on par with pure space-partitioning approaches (slight minus on performance, slight plus on flexibility/fairness).  I never went much into the resource contention use cases myself. Though, I did use coscheduling to extend the concept of nice"" to sockets by putting all niced programs into a coscheduled task group with appropriately reduced shares.  This way, niced programs don't just get any and all idle CPU capacity -- taking away parts of the energy budget of more important tasks all the time -- which leads to important tasks running at turbo frequencies more often. Depending on the parallelism of niced workload and the parallelism of normal workload, this translates to a performance improvement of the normal workload that corresponds roughly to the increase in frequency (for CPU-bound tasks) [4]. Depending on the processor, that can be anything from just a few percent to about a factor of 2.  Regards Jan    References:  [1] S. Zhuravlev, J. C. Saez, S. Blagodurov, A. Fedorova, and M. Prieto,     “Survey of scheduling techniques for addressing shared resources in     multicore processors,” ACM Computing Surveys, vol. 45, no. 1, pp.     4:1–4:28, Dec. 2012.  [2] J. H. Schönherr, B. Juurlink, and J. Richling, “TACO: A scheduling     scheme for parallel applications on multicore architectures,”     Scientific Programming, vol. 22, no. 3, pp. 223–237, 2014.  [3] J. H. Schönherr, B. Lutz, and J. Richling, “Non-intrusive coscheduling     for general purpose operating systems,” in Proceedings of the     International Conference on Multicore Software Engineering,     Performance, and Tools (MSEPT ’12), ser. Lecture Notes in Computer     Science, vol. 7303. Berlin/Heidelberg, Germany: Springer, May 2012,     pp. 66–77.  [4] J. H. Schönherr, J. Richling, M. Werner, and G. Mühl, “A scheduling     approach for efficient utilization of hardware-driven frequency     scaling,” in Workshop Proceedings of the 23rd International Conference     on Architecture of Computing Systems (ARCS 2010 Workshops), M. Beigl     and F. J. Cazorla-Almeida, Eds. Berlin, Germany: VDE Verlag, Feb.     2010, pp. 367–376.""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,4008,1.0,0.7719298245614035,0,0.1839080459770115,0.8045977011494253,0.04597701149425287,0.0
631,391895,408991,"Currently: no. At this point the implementation is tightly coupled to the cpu cgroup controller. This *might* change, if the task group optimizations mentioned in other parts of this e-mail thread are done, as I think, that it would decouple the various mechanisms. That said, what if you were able to disable the ""group-based fairness"" aspect of the cpu cgroup controller? Then you would be able to control just the coscheduling aspects on their own. Would that satisfy the use case you have in mind?","On 09/19/2018 11:53 PM, Subhra Mazumdar wrote:  Currently: no.  At this point the implementation is tightly coupled to the cpu cgroup controller. This *might* change, if the task group optimizations mentioned in other parts of this e-mail thread are done, as I think, that it would decouple the various mechanisms.  That said, what if you were able to disable the group-based fairness"" aspect of the cpu cgroup controller? Then you would be able to control just the coscheduling aspects on their own. Would that satisfy the use case you have in mind?  Regards Jan""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,501,0.13736263736263737,0.7807017543859649,0,0.1839080459770115,0.8045977011494253,0.0,0.0
632,391895,409136,"Sounds like a co-scheduling system would need the following elements:1) Identify groups of runnable tasks to run together.2) Identify hardware that needs to be co-scheduled   (for L1TF reasons, POWER7/8 restrictions, etc).3) Pack task groups into the system in a way that   allows maximum utilization by co-scheduled tasks.4) Leave some CPU time for regular time sliced tasks.5) In some cases, leave some CPU time idle on purpose. Step 1 would have to be revaluated periodically, as tasks (e.g. VCPUs) wake up and go to sleep. I suspect this may be much better done as its own scheduler class, instead of shoehorned into CFS.I like the idea of having some co-scheduling functionality in Linux, but I absolutely abhor the idea of making CFS even more complicated than it already is. The current code is already incredibly hard to debug or improve. Are you getting much out of CFS with your current code? It appears that your patches are fighting CFS as much as they are leveraging it, but admittedly I only looked at them briefly.","On Mon, 2018-09-24 at 17:23 +0200, Jan H. Schönherr wrote:  Sounds like a co-scheduling system would need the following elements: 1) Identify groups of runnable tasks to run together. 2) Identify hardware that needs to be co-scheduled    (for L1TF reasons, POWER7/8 restrictions, etc). 3) Pack task groups into the system in a way that    allows maximum utilization by co-scheduled tasks. 4) Leave some CPU time for regular time sliced tasks. 5) In some cases, leave some CPU time idle on purpose.  Step 1 would have to be reevaluated periodically, as tasks (eg. VCPUs) wake up and go to sleep.  I suspect this may be much better done as its own scheduler class, instead of shoehorned into CFS.  I like the idea of having some co-scheduling functionality in Linux, but I absolutely abhor the idea of making CFS even more complicated than it already is.  The current code is already incredibly hard to debug or improve.  Are you getting much out of CFS with your current code?  It appears that your patches are fighting CFS as much as they are leveraging it, but admittedly I only looked at them briefly.  --  All Rights Reversed.",technical,Rik van Riel,riel@surriel.com,0,0,1029,0.2706043956043956,0.7894736842105263,0,0.1839080459770115,0.8045977011494253,0.0,0.011494252873563218
633,391895,410884,"Can't say much about tlb-invalidate, but yes to the spinlock stuff: if there isn't any additional information available, all runnable tasks/vCPUs have to be scheduled to avoid lock holder preemption. With additional information about tasks potentially holding locks o potentially spinning on a lock, it would be possible to coschedule smaller subsets -- no idea if that would be any more efficient though. That is true for some of the resource contention use cases, too. Though, they are much more relaxed wrt. their requirements on the simultaneousness of the context switch. In my personal interpretation, that analogy refers to the case where the waiting time for a lock is shorter than the time for a context switch --but where the context switch was done anyway, ""thrashing"" the CPU. Anyway. I only brought it up, because everyone has a different understanding of what ""coscheduling"" or ""gang scheduling"" actually means. The memorable quotes are from Ousterhout:  ""A task force is scheduled if all of its runnable processes are exe-   cutting simultaneously on different processors. Each of the processes   in that task force is also said to be scheduled.""(where a ""task force"" is a group of closely cooperating tasks), and from Feitelson and Rudolph:  ""[Gang scheduling is defined] as the scheduling of a group of threads   to run on a set of processors at the same time, on a one-to-one   basis.""(with the additional assumption of time slices, collective preemption, and that threads don't relinquish the CPU during their time slice).That makes gang scheduling much more specific, while coscheduling just refers to the fact that some things are executed simultaneously. I don't think, that coscheduling should be applied blindly. Just like the adaptive wait loops you mentioned: in the beginning there was active waiting, it wasn't that great, so passive waiting was invented, turns out, the overhead is too high in some cases, so let's spin adaptively for a moment. We went from uncoordinated scheduling to system-wide coordinated scheduling(which turned out to be not very efficient for many cases). And now we are in the phase to find the right adaptiveness. There is work on enabling coscheduling only on-demand (when a parallel application profits from it)or to be more fuzzy about it (giving the scheduler more freedom), there is work to go away from system-wide coordination to (dynamically) smaller isles (where I see my own work as well). And ""recently"" we also have the resource contention and security use cases leaving their impression on the topic as well. There's the resource contention stuff, much of which targets the last level cache or memory controller bandwidth. So, that is making a case for coscheduling larger parts than SMT. However, I didn't find anything in a short search that would already cover some of the more recent processors with 16+ cores. There's the auto-tuning of parallel algorithms to a certain system architecture. That would also profit from LLC coscheduling (and slightly larger time slices) to run multiple of those in parallel. Again, no idea for recent processors. There's work to coschedule whole clusters, which goes beyond the scope of a single system, but also predates recent systems. (Search for, e.g., ""implicit coscheduling"").So, 16+ cores is unknown territory, AFAIK. But not every recent system has 16+ cores, or will have 16+ cores in the near future.","On 09/17/2018 03:37 PM, Peter Zijlstra wrote:  Can't say much about tlb-invalidate, but yes to the spinlock stuff: if there isn't any additional information available, all runnable tasks/vCPUs have to be coscheduled to avoid lock holder preemption.  With additional information about tasks potentially holding locks or potentially spinning on a lock, it would be possible to coschedule smaller subsets -- no idea if that would be any more efficient though.    That is true for some of the resource contention use cases, too. Though, they are much more relaxed wrt. their requirements on the simultaneousness of the context switch.    In my personal interpretation, that analogy refers to the case where the waiting time for a lock is shorter than the time for a context switch -- but where the context switch was done anyway, thrashing"" the CPU.   Anyway. I only brought it up, because everyone has a different understanding of what ""coscheduling"" or ""gang scheduling"" actually means. The memorable quotes are from Ousterhout:    ""A task force is coscheduled if all of its runnable processes are exe-    cuting simultaneously on different processors. Each of the processes    in that task force is also said to be coscheduled.""  (where a ""task force"" is a group of closely cooperating tasks), and from Feitelson and Rudolph:    ""[Gang scheduling is defined] as the scheduling of a group of threads    to run on a set of processors at the same time, on a one-to-one    basis.""  (with the additional assumption of time slices, collective preemption, and that threads don't relinquish the CPU during their time slice).  That makes gang scheduling much more specific, while coscheduling just refers to the fact that some things are executed simultaneously.    I don't think, that coscheduling should be applied blindly.  Just like the adaptive wait loops you mentioned: in the beginning there was active waiting, it wasn't that great, so passive waiting was invented, turns out, the overhead is too high in some cases, so let's spin adaptively for a moment.  We went from uncoordinated scheduling to system-wide coordinated scheduling (which turned out to be not very efficient for many cases). And now we are in the phase to find the right adaptiveness. There is work on enabling coscheduling only on-demand (when a parallel application profits from it) or to be more fuzzy about it (giving the scheduler more freedom), there is work to go away from system-wide coordination to (dynamically) smaller isles (where I see my own work as well). And ""recently"" we also have the resource contention and security use cases leaving their impression on the topic as well.    There's the resource contention stuff, much of which targets the last level cache or memory controller bandwidth. So, that is making a case for coscheduling larger parts than SMT. However, I didn't find anything in a short search that would already cover some of the more recent processors with 16+ cores.  There's the auto-tuning of parallel algorithms to a certain system architecture. That would also profit from LLC coscheduling (and slightly larger time slices) to run multiple of those in parallel. Again, no idea for recent processors.  There's work to coschedule whole clusters, which goes beyond the scope of a single system, but also predates recent systems. (Search for, e.g., ""implicit coscheduling"").  So, 16+ cores is unknown territory, AFAIK. But not every recent system has 16+ cores, or will have 16+ cores in the near future.  Regards Jan""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,3416,0.9120879120879121,0.7982456140350878,0,0.20689655172413793,0.7931034482758621,0.011494252873563218,0.0
634,391895,410904,"Both, Peter and Subhra, seem to prefer an interface different than cgroups to specify what to coschedule. Can you provide some extra motivation for me, why you feel that way?(ignoring the current scalability issues with the cpu group controller)After all, cgroups where designed to create arbitrary groups of tasks and to attach functionality to those groups. If we were to introduce a different interface to control that, we'd need to introduce a whole new group concept, so that you make tasks part of some group while at the same time preventing unauthorized tasks from joining a group. I currently don't see any wins, just a loss in flexibility.","On 09/17/2018 02:25 PM, Peter Zijlstra wrote:  Both, Peter and Subhra, seem to prefer an interface different than cgroups to specify what to coschedule.  Can you provide some extra motivation for me, why you feel that way? (ignoring the current scalability issues with the cpu group controller)   After all, cgroups where designed to create arbitrary groups of tasks and to attach functionality to those groups.  If we were to introduce a different interface to control that, we'd need to introduce a whole new group concept, so that you make tasks part of some group while at the same time preventing unauthorized tasks from joining a group.   I currently don't see any wins, just a loss in flexibility.  Regards Jan",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,649,0.17307692307692307,0.8070175438596491,0,0.20689655172413793,0.7931034482758621,0.0,0.0
635,391895,411472,"I found another issue today, while attempting to test (with 61/60applied) separate coscheduling cgroups for vcpus and emulator threads[the default configuration with libvirt]","On 13.09.2018 [21:19:38 +0200], Jan H. Schnherr wrote:  I found another issue today, while attempting to test (with 61/60 applied) separate coscheduling cgroups for vcpus and emulator threads [the default configuration with libvirt].  /sys/fs/cgroup/cpu# cat cpu.scheduled  1 /sys/fs/cgroup/cpu# cd machine/ /sys/fs/cgroup/cpu/machine# cat cpu.scheduled 0 /sys/fs/cgroup/cpu/machine# cd VM-1.libvirt-qemu/ /sys/fs/cgroup/cpu/machine/VM-1.libvirt-qemu# cat cpu.scheduled 0 /sys/fs/cgroup/cpu/machine/VM-1.libvirt-qemu# cd vcpu0/ /sys/fs/cgroup/cpu/machine/VM-1.libvirt-qemu/vcpu0# cat cpu.scheduled 0 /sys/fs/cgroup/cpu/machine/VM-1.libvirt-qemu/vcpu0# echo 1 > cpu.scheduled /sys/fs/cgroup/cpu/machine/VM-1.libvirt-qemu/vcpu0# cd ../emulator/ /sys/fs/cgroup/cpu/machine/VM-1.libvirt-qemu/emulator# echo 1 > cpu.scheduled /sys/fs/cgroup/cpu/machine/VM-1.libvirt-qemu/emulator# <crash>  Serial console output (I apologize that some lines got truncated)  [ 1060.840120] BUG: unable to handle kernel NULL pointer dere0 [ 1060.848782] PGD 0 P4D 0  [ 1060.852068] Oops: 0000 [#1] SMP PTI [ 1060.856207] CPU: 44 PID: 0 Comm: swapper/44 Tainted: G           OE     4.19b [ 1060.867029] Hardware name: Dell Inc. PowerEdge R640/0W23H8, BIOS 1.2.11 10/17 [ 1060.874872] RIP: 0010:set_next_entity+0x15/0x1d0 [ 1060.879770] Code: c8 48 8b 7d d0 eb 96 0f 1f 40 00 66 2e 0f 1f 84 00 00 00 00 [ 1060.899165] RSP: 0018:ffffaa2b98c0fd78 EFLAGS: 00010046 [ 1060.904720] RAX: 0000000000000000 RBX: ffff996940ba2d80 RCX: 0000000000000000 [ 1060.912199] RDX: 0000000000000008 RSI: 0000000000000000 RDI: ffff996940ba2e00 [ 1060.919678] RBP: ffffaa2b98c0fda0 R08: 0000000000000000 R09: 0000000000000000 [ 1060.927174] R10: 0000000000000000 R11: 0000000000000001 R12: ffff996940ba2e00 [ 1060.934655] R13: 0000000000000000 R14: ffff996940ba2e00 R15: 0000000000000000 [ 1060.942134] FS:  0000000000000000(0000) GS:ffff996940b80000(0000) knlGS:00000 [ 1060.950572] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 1060.956673] CR2: 0000000000000040 CR3: 00000064af40a006 CR4: 00000000007626e0 [ 1060.964172] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [ 1060.971677] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 [ 1060.979191] PKRU: 55555554 [ 1060.982282] Call Trace: [ 1060.985126]  pick_next_task_fair+0x8a7/0xa20 [ 1060.989794]  __schedule+0x13a/0x8e0 [ 1060.993691]  ? update_ts_time_stats+0x59/0x80 [ 1060.998439]  schedule_idle+0x2c/0x40 [ 1061.002410]  do_idle+0x169/0x280 [ 1061.006032]  cpu_startup_entry+0x73/0x80 [ 1061.010348]  start_secondary+0x1ab/0x200 [ 1061.014673]  secondary_startup_64+0xa4/0xb0 [ 1061.019265] Modules linked in: act_police cls_basic ebtable_filter ebtables i [ 1061.093145]  mac_hid coretemp lp parport btrfs zstd_compress raid456 async_ri [ 1061.126494] CR2: 0000000000000040 [ 1061.130467] ---[ end trace 3462ef57e3394c4f ]--- [ 1061.147237] RIP: 0010:set_next_entity+0x15/0x1d0 [ 1061.152510] Code: c8 48 8b 7d d0 eb 96 0f 1f 40 00 66 2e 0f 1f 84 00 00 00 00 [ 1061.172573] RSP: 0018:ffffaa2b98c0fd78 EFLAGS: 00010046 [ 1061.178482] RAX: 0000000000000000 RBX: ffff996940ba2d80 RCX: 0000000000000000 [ 1061.186309] RDX: 0000000000000008 RSI: 0000000000000000 RDI: ffff996940ba2e00 [ 1061.194109] RBP: ffffaa2b98c0fda0 R08: 0000000000000000 R09: 0000000000000000 [ 1061.201908] R10: 0000000000000000 R11: 0000000000000001 R12: ffff996940ba2e00 [ 1061.209698] R13: 0000000000000000 R14: ffff996940ba2e00 R15: 0000000000000000 [ 1061.217490] FS:  0000000000000000(0000) GS:ffff996940b80000(0000) knlGS:00000 [ 1061.226236] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 1061.232622] CR2: 0000000000000040 CR3: 00000064af40a006 CR4: 00000000007626e0 [ 1061.240405] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [ 1061.248168] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 [ 1061.255909] PKRU: 55555554 [ 1061.259221] Kernel panic - not syncing: Attempted to kill the idle task! [ 1062.345087] Shutting down cpus with NMI [ 1062.351037] Kernel Offset: 0x33400000 from 0xffffffff81000000 (relocation ra) [ 1062.374645] ---[ end Kernel panic - not syncing: Attempted to kill the idle - [ 1062.383218] WARNING: CPU: 44 PID: 0 at /build/linux-4.19-0rc3.ag.4/kernel/sc0 [ 1062.394380] Modules linked in: act_police cls_basic ebtable_filter ebtables i [ 1062.469725]  mac_hid coretemp lp parport btrfs zstd_compress raid456 async_ri [ 1062.503656] CPU: 44 PID: 0 Comm: swapper/44 Tainted: G      D    OE     4.19b [ 1062.514972] Hardware name: Dell Inc. PowerEdge R640/0W23H8, BIOS 1.2.11 10/17 [ 1062.523357] RIP: 0010:set_task_cpu+0x193/0x1a0 [ 1062.528624] Code: 00 00 04 e9 36 ff ff ff 0f 0b e9 be fe ff ff f7 43 60 fd f5 [ 1062.549066] RSP: 0018:ffff996940b83dc8 EFLAGS: 00010046 [ 1062.555134] RAX: 0000000000000200 RBX: ffff99c90f2a9e00 RCX: 0000000000000080 [ 1062.563096] RDX: ffff99c90f2aa101 RSI: 000000000000000f RDI: ffff99c90f2a9e00 [ 1062.571053] RBP: ffff996940b83de8 R08: 000000000000000f R09: 000000000000002c [ 1062.578990] R10: 0000000000000001 R11: 0000000000000009 R12: ffff99c90f2aa934 [ 1062.586911] R13: 000000000000000f R14: 000000000000000f R15: 0000000000022d80 [ 1062.594826] FS:  0000000000000000(0000) GS:ffff996940b80000(0000) knlGS:00000 [ 1062.603681] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 1062.610182] CR2: 0000000000000040 CR3: 00000064af40a006 CR4: 00000000007626e0 [ 1062.618061] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [ 1062.625919] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 [ 1062.633762] PKRU: 55555554 [ 1062.637186] Call Trace: [ 1062.640350]  <IRQ> [ 1062.643066]  try_to_wake_up+0x159/0x4b0 [ 1062.647588]  default_wake_function+0x12/0x20 [ 1062.652539]  autoremove_wake_function+0x12/0x40 [ 1062.657744]  __wake_up_common+0x8c/0x130 [ 1062.662340]  __wake_up_common_lock+0x80/0xc0 [ 1062.667277]  __wake_up+0x13/0x20 [ 1062.671170]  wake_up_klogd_work_func+0x40/0x60 [ 1062.676275]  irq_work_run_list+0x55/0x80 [ 1062.680860]  irq_work_run+0x2c/0x40 [ 1062.684992]  flush_smp_call_function_queue+0xc0/0x100 [ 1062.690687]  generic_smp_call_function_single_interrupt+0x13/0x30 [ 1062.697430]  smp_call_function_single_interrupt+0x3e/0xe0 [ 1062.703485]  call_function_single_interrupt+0xf/0x20 [ 1062.709100]  </IRQ> [ 1062.711851] RIP: 0010:panic+0x1fe/0x244 [ 1062.716329] Code: eb a6 83 3d 17 bc af 01 00 74 05 e8 b0 72 02 00 48 c7 c6 2f [ 1062.736366] RSP: 0018:ffffaa2b98c0fe60 EFLAGS: 00000286 ORIG_RAX: ffffffffff4 [ 1062.744571] RAX: 000000000000004a RBX: ffff99693243bc00 RCX: 0000000000000006 [ 1062.752328] RDX: 0000000000000000 RSI: 0000000000000096 RDI: ffff996940b96420 [ 1062.760077] RBP: ffffaa2b98c0fed8 R08: 000000000000002c R09: 0000000000aaaaaa [ 1062.767814] R10: 0000000000000040 R11: 0000000000000001 R12: 0000000000000000 [ 1062.775536] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000046 [ 1062.783236]  do_exit+0x886/0xb20 [ 1062.787023]  ? cpu_startup_entry+0x73/0x80 [ 1062.791659]  rewind_stack_do_exit+0x17/0x20 [ 1062.796364] ---[ end trace 3462ef57e3394c50 ]--- [ 1062.801485] ------------[ cut here ]------------ [ 1062.806599] sched: Unexpected reschedule of offline CPU#15! [ 1062.812655] WARNING: CPU: 44 PID: 0 at /build/linux-4.19-0rc3.ag.4/arch/x86/0 [ 1062.825264] Modules linked in: act_police cls_basic ebtable_filter ebtables i [ 1062.899387]  mac_hid coretemp lp parport btrfs zstd_compress raid456 async_ri [ 1062.932747] CPU: 44 PID: 0 Comm: swapper/44 Tainted: G      D W  OE     4.19b [ 1062.943874] Hardware name: Dell Inc. PowerEdge R640/0W23H8, BIOS 1.2.11 10/17 [ 1062.952057] RIP: 0010:native_smp_send_reschedule+0x3f/0x50 [ 1062.958164] Code: c0 84 c0 74 17 48 8b 05 ff d9 36 01 be fd 00 00 00 48 8b 40 [ 1062.978210] RSP: 0018:ffff996940b83de8 EFLAGS: 00010086 [ 1062.984093] RAX: 0000000000000000 RBX: ffff99c90f2a9e00 RCX: 0000000000000006 [ 1062.991894] RDX: 0000000000000007 RSI: 0000000000000086 RDI: ffff996940b96420 [ 1062.999695] RBP: ffff996940b83de8 R08: 000000000000002c R09: 0000000000aaaaaa [ 1063.007501] R10: ffff996940b83dc8 R11: 0000000000000001 R12: ffff99c90f2aa934 [ 1063.015303] R13: 0000000000000004 R14: 0000000000000046 R15: 0000000000022d80 [ 1063.023110] FS:  0000000000000000(0000) GS:ffff996940b80000(0000) knlGS:00000 [ 1063.031881] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 1063.038312] CR2: 0000000000000040 CR3: 00000064af40a006 CR4: 00000000007626e0 [ 1063.046138] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [ 1063.053973] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 [ 1063.061796] PKRU: 55555554 [ 1063.065193] Call Trace: [ 1063.068323]  <IRQ> [ 1063.071021]  try_to_wake_up+0x3e3/0x4b0 [ 1063.075534]  default_wake_function+0x12/0x20 [ 1063.080485]  autoremove_wake_function+0x12/0x40 [ 1063.085682]  __wake_up_common+0x8c/0x130 [ 1063.090259]  __wake_up_common_lock+0x80/0xc0 [ 1063.095172]  __wake_up+0x13/0x20 [ 1063.099029]  wake_up_klogd_work_func+0x40/0x60 [ 1063.104100]  irq_work_run_list+0x55/0x80 [ 1063.108649]  irq_work_run+0x2c/0x40 [ 1063.112767]  flush_smp_call_function_queue+0xc0/0x100 [ 1063.118451]  generic_smp_call_function_single_interrupt+0x13/0x30 [ 1063.125174]  smp_call_function_single_interrupt+0x3e/0xe0 [ 1063.131209]  call_function_single_interrupt+0xf/0x20 [ 1063.136807]  </IRQ> [ 1063.139535] RIP: 0010:panic+0x1fe/0x244 [ 1063.144009] Code: eb a6 83 3d 17 bc af 01 00 74 05 e8 b0 72 02 00 48 c7 c6 2f [ 1063.164062] RSP: 0018:ffffaa2b98c0fe60 EFLAGS: 00000286 ORIG_RAX: ffffffffff4 [ 1063.172269] RAX: 000000000000004a RBX: ffff99693243bc00 RCX: 0000000000000006 [ 1063.180034] RDX: 0000000000000000 RSI: 0000000000000096 RDI: ffff996940b96420 [ 1063.187781] RBP: ffffaa2b98c0fed8 R08: 000000000000002c R09: 0000000000aaaaaa [ 1063.195519] R10: 0000000000000040 R11: 0000000000000001 R12: 0000000000000000 [ 1063.203243] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000046 [ 1063.210950]  do_exit+0x886/0xb20 [ 1063.214736]  ? cpu_startup_entry+0x73/0x80 [ 1063.219371]  rewind_stack_do_exit+0x17/0x20 [ 1063.224076] ---[ end trace 3462ef57e3394c51 ]---",technical,Nishanth Aravamudan,naravamudan@digitalocean.com,0,0,174,0.03983516483516483,0.8157894736842105,0,0.20689655172413793,0.7816091954022989,0.0,0.0
636,391895,412904,Yes that will suffice the use case. We wish to experiment at some point with co-scheduling of certain workers threads in DB parallel query and see if there is any benefit,"On 09/24/2018 08:43 AM, Jan H. Schönherr wrote: Yes that will suffice the use case. We wish to experiment at some point with co-scheduling of certain workers threads in DB parallel query and see if there is any benefit  Thanks, Subhra",technical,Subhra Mazumdar,subhra.mazumdar@oracle.com,0,0,170,0.04395604395604396,0.8333333333333334,0,0.21839080459770116,0.7701149425287356,0.0,0.0
637,391895,412920,"I think cgroups will the get the job done for any use case. But we have, e.g. affinity control via both sched_setaffinity and cgroup CPU sets. It will be good to have an alternative way to specify co-scheduling too for those who don't want to use cgroup for some reason. It can be added later on though, only how one will override the other will need to be sorted out.","On 09/26/2018 02:58 AM, Jan H. Schönherr wrote: I think cgroups will the get the job done for any use case. But we have, e.g. affinity control via both sched_setaffinity and cgroup cpusets. It will be good to have an alternative way to specify co-scheduling too for those who don't want to use cgroup for some reason. It can be added later on though, only how one will override the other will need to be sorted out.",technical,Subhra Mazumdar,subhra.mazumdar@oracle.com,0,0,368,0.10714285714285714,0.8421052631578947,0,0.21839080459770116,0.7701149425287356,0.0,0.034482758620689655
638,391895,415280,"I got it reproduced. I will send a fix, when I have one.","On 09/26/2018 11:05 PM, Nishanth Aravamudan wrote:  I got it reproduced.  I will send a fix, when I have one.  Regards Jan",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,56,0.02197802197802198,0.8508771929824561,0,0.26436781609195403,0.735632183908046,0.034482758620689655,0.034482758620689655
639,391895,419678,"In case nobody else brought it up yet, you're going to need a handshake to strengthen protection against L1TF attacks. Otherwise, there's still a small window where an attack can occur during the reschedule. Perhaps one could then cause this to happen artificially by repeatedly have a VM do some kind of pause/mwait type operation that might do a reschedule.","On 9/7/18 5:39 PM, Jan H. Schönherr wrote:   In case nobody else brought it up yet, you're going to need a handshake to strengthen protection against L1TF attacks. Otherwise, there's still a small window where an attack can occur during the reschedule. Perhaps one could then cause this to happen artificially by repeatedly have a VM do some kind of pause/mwait type operation that might do a reschedule.  Jon.  --  Computer Architect | Sent with my Fedora powered laptop",technical,Jon Masters,jcm@redhat.com,0,0,359,0.09203296703296704,0.8596491228070176,0,0.2988505747126437,0.6896551724137931,0.034482758620689655,0.13793103448275862
640,391895,431875,Have you considered using CPU set to specify the set of CPUs inside which you want to coschedule task groups in? Perhaps that would be more flexible and intuitive to control than this CPU. Scheduled value. Unless you require this feature to act always symmetrical through the branches of a given domain tree? Thanks.,"On Fri, Sep 07, 2018 at 11:39:47PM +0200, Jan H. Schnherr wrote:  Have you considered using cpuset to specify the set of CPUs inside which you want to coschedule task groups in? Perhaps that would be more flexible and intuitive to control than this cpu.scheduled value.  Unless you require this feature to act always symmetrical through the branches of a given domain tree?  Thanks.",technical,Frederic Weisbecker,frederic@kernel.org,1,0,316,0.08104395604395605,0.868421052631579,0,0.4482758620689655,0.5517241379310345,0.13793103448275862,0.011494252873563218
641,391895,434910,Do you know how much is the delay? i.e. what is overlap time when a thread of new group starts executing on one HT while there is still thread of another group running on the other HT?,"Hi Jan,  On 9/7/18 2:39 PM, Jan H. Schönherr wrote: Do you know how much is the delay? i.e what is overlap time when a thread of new group starts executing on one HT while there is still thread of another group running on the other HT?  Thanks, Subhra",technical,Subhra Mazumdar,subhra.mazumdar@oracle.com,0,0,184,0.054945054945054944,0.8771929824561403,0,0.47126436781609193,0.5287356321839081,0.011494252873563218,0.0
642,391895,435224,"Yes, I did consider CPU sets. Though, there are two dimensions to it: a) at what fraction of the system tasks shall be scheduled, and b) where these tasks shall execute within the system.cpusets would be the obvious answer to the ""where"". However, in the current form they are too inflexible with too much overhead. Suppose, you want to coschedule two tasks on SMT siblings of a core. You would be able to restrict the tasks to a specific core with a CPU set. But then, it is bound to that core, and the load balancer cannot move the group of two tasks to a different core. Now, it would be possible to ""invent"" relocatable CPU sets to address that issue (""I want affinity restricted to a core, I don't care which""), but then, the current way how CPU set affinity is enforced doesn't scale for making use of it from within the balancer. (The upcoming load balancing portion of the coscheduler currently uses a file similar to CPU. Scheduled to restrict affinity to a load-balancer-controlled subset of the system.)Using CPU sets as the mean to describe which parts of the system are to be scheduled *may* be possible. But if so, it's a long way out. The current implementation uses scheduling domains for this, because (a) most coscheduling use cases require an alignment to the topology, and (b) it integrates really nicely with the load balancer. AFAIK, there is already some interaction between CPU sets and scheduling domains. But it is supposed to be rather static and as soon as you have overlapping CPU sets, you end up with the default scheduling domains. If we were able to make the scheduling domains more dynamic than they are today, we might be able to couple that to CPU sets (or some similar interface to *define* scheduling domains).","On 17/10/2018 04.09, Frederic Weisbecker wrote: [...]  Yes, I did consider cpusets. Though, there are two dimensions to it: a) at what fraction of the system tasks shall be coscheduled, and b) where these tasks shall execute within the system.  cpusets would be the obvious answer to the where"". However, in the current form they are too inflexible with too much overhead. Suppose, you want to coschedule two tasks on SMT siblings of a core. You would be able to restrict the tasks to a specific core with a cpuset. But then, it is bound to that core, and the load balancer cannot move the group of two tasks to a different core.  Now, it would be possible to ""invent"" relocatable cpusets to address that issue (""I want affinity restricted to a core, I don't care which""), but then, the current way how cpuset affinity is enforced doesn't scale for making use of it from within the balancer. (The upcoming load balancing portion of the coscheduler currently uses a file similar to cpu.scheduled to restrict affinity to a load-balancer-controlled subset of the system.)   Using cpusets as the mean to describe which parts of the system are to be coscheduled *may* be possible. But if so, it's a long way out. The current implementation uses scheduling domains for this, because (a) most coscheduling use cases require an alignment to the topology, and (b) it integrates really nicely with the load balancer.  AFAIK, there is already some interaction between cpusets and scheduling domains. But it is supposed to be rather static and as soon as you have overlapping cpusets, you end up with the default scheduling domains. If we were able to make the scheduling domains more dynamic than they are today, we might be able to couple that to cpusets (or some similar interface to *define* scheduling domains).  Regards Jan""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,1748,0.5041208791208791,0.8859649122807017,0,0.47126436781609193,0.5287356321839081,0.0,0.0
643,391895,435407,"Oh ok, I understand now. Affinity and node-scope mutual exclusion are entirely decoupled, I see. So what is the need for cosched_split_domains? What kind of corner case won't fit into scheduler domains? Can you perhaps spare that part in this patchset to simplify it somehow? If it happens to be necessary, it can still be added iteratively. Thanks.","On Fri, Oct 19, 2018 at 01:40:03PM +0200, Jan H. Schnherr wrote:  Oh ok, I understand now. Affinity and node-scope mutual exclusion are entirely decoupled, I see.   So what is the need for cosched_split_domains? What kind of corner case won't fit into scheduler domains? Can you perhaps spare that part in this patchset to simplify it somehow? If it happens to be necessary, it can still be added iteratively.  Thanks.",technical,Frederic Weisbecker,frederic@kernel.org,1,0,349,0.09478021978021978,0.8947368421052632,0,0.47126436781609193,0.5172413793103449,0.0,0.0
644,391895,435419,"Oh boy, so the coscheduler is going to get its own load balancer? At that point, why bother integrating the coscheduler into CFS, instead of making it its own scheduling class? CFS is already complicated enough that it borders on unmaintainable. I would really prefer to have the coscheduler code separate from CFS, unless there is a really compelling reason to do otherwise.","On Fri, 2018-10-19 at 13:40 +0200, Jan H. Schönherr wrote:  Oh boy, so the coscheduler is going to get its own load balancer?  At that point, why bother integrating the coscheduler into CFS, instead of making it its own scheduling class?  CFS is already complicated enough that it borders on unmaintainable. I would really prefer to have the coscheduler code separate from CFS, unless there is a really compelling reason to do otherwise.  --  All Rights Reversed.",technical,Rik van Riel,riel@surriel.com,0,0,375,0.09752747252747253,0.9035087719298246,0,0.47126436781609193,0.5172413793103449,0.0,0.0
645,391895,435432,"I guess he wants to reuse as much as possible from the CFS features and code present or to come (nice, fairness, load balancing, power aware, NUMA aware, etc...). OTOH you're right, the thing has specific enough requirements to consider a new sched policy. And really I would love to see all that code separate from CFS, for the reasons you just outlined. So I cross my fingers on what Jan is going to answer on a new policy.","On Fri, Oct 19, 2018 at 11:16:49AM -0400, Rik van Riel wrote:  I guess he wants to reuse as much as possible from the CFS features and code present or to come (nice, fairness, load balancing, power aware, NUMA aware, etc...).  OTOH you're right, the thing has specific enough requirements to consider a new sched policy. And really I would love to see all that code separate from CFS, for the reasons you just outlined. So I cross my fingers on what Jan is going to answer on a new policy.",technical,Frederic Weisbecker,frederic@kernel.org,1,0,425,0.12912087912087913,0.9122807017543859,0,0.47126436781609193,0.5172413793103449,0.0,0.0
646,391895,435445,"I wonder if things like nice levels, fairness, and balancing could be broken out into code that could be reused from both CFS and a new co-scheduler scheduling class. A bunch of the cgroup code is already broken out, but maybe some more could be broken out and shared, too? Some bits of functionality come to mind:- track groups of tasks that should be co-scheduled  (eg all the VCPUs of a virtual machine)- track the subsets of those groups that are runnable  (e.g. the currently runnable VCPUs of a virtual machine)- figure out time slots and CPU assignments to efficiently  use CPU time for the co-scheduled tasks  (while leaving some configurable(?) amount of CPU time  available for other tasks)- configuring some lower-level code on each affected CPU  to ""run task A in slot X"", etc This really does not seem like something that could be shoehorned into CFS without making it unmaintainable. Furthermore, it also seems like the thing that you could never really get into a highly efficient state as long as it is weighed down by the rest of CFS.","On Fri, 2018-10-19 at 17:33 +0200, Frederic Weisbecker wrote:  I wonder if things like nice levels, fairness, and balancing could be broken out into code that could be reused from both CFS and a new co-scheduler scheduling class.  A bunch of the cgroup code is already broken out, but maybe some more could be broken out and shared, too?   Some bits of functionality come to mind: - track groups of tasks that should be co-scheduled   (eg all the VCPUs of a virtual machine) - track the subsets of those groups that are runnable   (eg. the currently runnable VCPUs of a virtual machine) - figure out time slots and CPU assignments to efficiently   use CPU time for the co-scheduled tasks   (while leaving some configurable(?) amount of CPU time    available for other tasks) - configuring some lower-level code on each affected CPU   to run task A in slot X"", etc  This really does not seem like something that could be shoehorned into CFS without making it unmaintainable.  Furthermore, it also seems like the thing that you could never really get into a highly efficient state as long as it is weighed down by the rest of CFS.  --  All Rights Reversed. """,technical,Rik van Riel,riel@surriel.com,0,0,1051,0.28708791208791207,0.9210526315789473,0,0.47126436781609193,0.5172413793103449,0.0,0.0
647,391895,435593,"Not ""its own"". The load balancer already aggregates statistics about sched-groups. With the coscheduler as posted, there is now a runqueue per scheduling group. The current ""ad-hoc"" gathering of data per scheduling group is then basically replaced with looking up that data at the corresponding runqueue, where it is kept up-to-date automatically. Exactly. I want a user to be able to ""switch on"" coscheduling for those parts of the workload that profit from it, without affecting the behavior we are all used to. For both: scheduling behavior for tasks that are not scheduled, as well as scheduling behavior for tasks *within* the group of scheduled tasks. Maybe. The primary issue that I have with a new scheduling class, is that they are strictly priority ordered. If there is a runnable task in a higher class, it is executed, no matter the situation in lower classes. ""Coscheduling"" would have to be higher in the class hierarchy than CFS. And then, all kinds of issues appear from starvation of CFS threads and other unfairness, to the necessity of (re-)defining a set of pre-emption rules, nice and other things that are given with CFS.cgroups runqueues CFS run queues and associated rules for pre-emption/time slices/etc. There is no ""slot"" concept, as it does not fit my idea of interactive usage. (As in ""slot X will execute from time T to T+1.) It is purely event-driven right now (eg, ""group X just became runnable, it is considered more important than the currently running group Y, all CPUs (in the affected part of the system) switch to group X"", or ""group X ran long enough, next group"").While some planning ahead seems possible (as demonstrated by the Tableau scheduler that Peter already pointed me at), I currently cannot imagine such an approach working for general purpose workloads. The absence of true pre-emption being my primary concern. I still have this idealistic notion, that there is no ""weighing down"". I see it more as profiting from all the hard work that went into CFS,avoiding the same mistakes, being backwards compatible, etc. If I were to do this ""outside of CFS"", I'd overhaul the scheduling class concept as it exists today. Instead, I'd probably attempt to schedule instantiations of scheduling classes. In its easiest setup, nothing would change: one CFS instance, one RT instance, one DL instance, strictly ordered by priority (on each CPU). The coscheduler as it is posted (and task groups in general), are effectively some form of multiple CFS instances being governed by a CFS instance. This approach would allow, for example, multiple CFS instances that are scheduled with explicit priorities, or some tasks that are scheduled with a custom scheduling class, while the whole group of tasks competes for time with other tasks via CFS rules. I'd still keep the feature of ""coscheduling"" orthogonal to everything else, though. Essentially, I'd just give the user/admin the possibility to choose the set of rules that shall be applied to entities in a runqueue. Your idea of further modularization seems to go in a similar direction, or at least is not incompatible with that. If it helps keeping things maintainable, I'm all for it. For example, some of the (upcoming) load balancing changes are just generalizations, so that the functions don't operate on *the* set of CFS run queues, but just *a* set of CFS runqueues. Similarly in the already posted code, where task picking now starts at*some* top CFS runqueue, instead of *the* top CFS runqueue.","On 19/10/2018 17.45, Rik van Riel wrote:  Not its own"". The load balancer already aggregates statistics about sched-groups. With the coscheduler as posted, there is now a runqueue per scheduling group. The current ""ad-hoc"" gathering of data per scheduling group is then basically replaced with looking up that data at the corresponding runqueue, where it is kept up-to-date automatically.    Exactly. I want a user to be able to ""switch on"" coscheduling for those parts of the workload that profit from it, without affecting the behavior we are all used to. For both: scheduling behavior for tasks that are not coscheduled, as well as scheduling behavior for tasks *within* the group of coscheduled tasks.    Maybe.    The primary issue that I have with a new scheduling class, is that they are strictly priority ordered. If there is a runnable task in a higher class, it is executed, no matter the situation in lower classes. ""Coscheduling"" would have to be higher in the class hierarchy than CFS. And then, all kinds of issues appear from starvation of CFS threads and other unfairness, to the necessity of (re-)defining a set of preemption rules, nice and other things that are given with CFS.    cgroups   runqueues   CFS runqueues and associated rules for preemption/time slices/etc.   There is no ""slot"" concept, as it does not fit my idea of interactive usage. (As in ""slot X will execute from time T to T+1.) It is purely event-driven right now (eg, ""group X just became runnable, it is considered more important than the currently running group Y, all CPUs (in the affected part of the system) switch to group X"", or ""group X ran long enough, next group"").  While some planning ahead seems possible (as demonstrated by the Tableau scheduler that Peter already pointed me at), I currently cannot imagine such an approach working for general purpose workloads. The absence of true preemption being my primary concern.    I still have this idealistic notion, that there is no ""weighing down"". I see it more as profiting from all the hard work that went into CFS, avoiding the same mistakes, being backwards compatible, etc.    If I were to do this ""outside of CFS"", I'd overhaul the scheduling class concept as it exists today. Instead, I'd probably attempt to schedule instantiations of scheduling classes. In its easiest setup, nothing would change: one CFS instance, one RT instance, one DL instance, strictly ordered by priority (on each CPU). The coscheduler as it is posted (and task groups in general), are effectively some form of multiple CFS instances being governed by a CFS instance.  This approach would allow, for example, multiple CFS instances that are scheduled with explicit priorities, or some tasks that are scheduled with a custom scheduling class, while the whole group of tasks competes for time with other tasks via CFS rules.  I'd still keep the feature of ""coscheduling"" orthogonal to everything else, though. Essentially, I'd just give the user/admin the possibility to choose the set of rules that shall be applied to entities in a runqueue.    Your idea of further modularization seems to go in a similar direction, or at least is not incompatible with that. If it helps keeping things maintainable, I'm all for it. For example, some of the (upcoming) load balancing changes are just generalizations, so that the functions don't operate on *the* set of CFS runqueues, but just *a* set of CFS runqueues. Similarly in the already posted code, where task picking now starts at *some* top CFS runqueue, instead of *the* top CFS runqueue.  Regards Jan""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,3494,0.9739010989010989,0.9298245614035088,0,0.47126436781609193,0.5172413793103449,0.0,0.08045977011494253
648,391895,439336,The leader doesn't kick the other cpus _immediately_ to switch to a different cosched group. So threads from previous cosched group will keep running in other HTs till their sched_slice is over (in worst case). This can still keep the window of L1TF vulnerability open?,The leader doesn't kick the other cpus _immediately_ to switch to a different cosched group. So threads from previous cosched group will keep running in other HTs till their sched_slice is over (in worst case). This can still keep the window of L1TF vulnerability open?,technical,Subhra Mazumdar,subhra.mazumdar@oracle.com,0,0,269,0.07005494505494506,0.9385964912280702,0,0.5632183908045977,0.4367816091954023,0.08045977011494253,0.0
649,391895,439353,"It does. (Or at least, it should, in case you found evidence that it does not.)Specifically, the logic to not pre-empt the currently running task before some minimum time has passed, is without effect for a collective context switch. No. Per the above, the window due to the collective context switch should not be as long as ""the remaining time slice"" but more towards the IPI delay. During this window, tasks of different coscheduling groups may execute simultaneously. In addition (as mentioned in the quoted text above), there more cases where a task of a scheduled group on one SMT sibling may execute simultaneously with some other code not from the same scheduled group: tasks in scheduling classes higher than CFS, and interrupts -- as both of them operate outside the scope of the coscheduler.","On 27/10/2018 01.05, Subhra Mazumdar wrote:  It does. (Or at least, it should, in case you found evidence that it does not.)  Specifically, the logic to not preempt the currently running task before some minimum time has passed, is without effect for a collective context switch.   No. Per the above, the window due to the collective context switch should not be as long as the remaining time slice"" but more towards the IPI delay. During this window, tasks of different coscheduling groups may execute simultaneously.  In addition (as mentioned in the quoted text above), there more cases where a task of a coscheduled group on one SMT sibling may execute simultaneously with some other code not from the same coscheduled group: tasks in scheduling classes higher than CFS, and interrupts -- as both of them operate outside the scope of the coscheduler.  Regards Jan""",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,802,0.21703296703296704,0.956140350877193,0,0.5632183908045977,0.4367816091954023,0.0,0.022988505747126436
650,391895,440566,"Can you point to where the leader is sending the IPI to other siblings? I did some experiment and delay seems to be sub microsec. I ran 2 threads that are just looping in one cosched group and affinitized to the 2 HTs of a core. And another thread in a different cosched group starts running affinitized to the first HT of the same core. I time stamped just before context_switch() in __schedule() for the threads switching from one to another and one to idle. Following is what I get on cpu 1 and 45 that are siblings, cpu 1 is where the other thread preempts. Not sure why the first switch on cpu to idle happened. But then onwards the difference in timestamps is less than a microsec. This is just a crude way to get a sense of the delay, may not be exact.","On 10/26/18 4:44 PM, Jan H. Schönherr wrote: Can you point to where the leader is sending the IPI to other siblings?  I did some experiment and delay seems to be sub microsec. I ran 2 threads that are just looping in one cosched group and affinitized to the 2 HTs of a core. And another thread in a different cosched group starts running affinitized to the first HT of the same core. I time stamped just before context_switch() in __schedule() for the threads switching from one to another and one to idle. Following is what I get on cpu 1 and 45 that are siblings, cpu 1 is where the other thread preempts:  [  403.216625] cpu:45 sub1->idle:403216624579 [  403.238623] cpu:1 sub1->sub2:403238621585 [  403.238624] cpu:45 sub1->idle:403238621787 [  403.260619] cpu:1 sub1->sub2:403260619182 [  403.260620] cpu:45 sub1->idle:403260619413 [  403.282617] cpu:1 sub1->sub2:403282617157 [  403.282618] cpu:45 sub1->idle:403282617317 ..  Not sure why the first switch on cpu to idle happened. But then onwards the difference in timestamps is less than a microsec. This is just a crude way to get a sense of the delay, may not be exact.  Thanks, Subhra",technical,Subhra Mazumdar,subhra.mazumdar@oracle.com,0,0,759,0.22115384615384615,0.9649122807017544,0,0.5977011494252874,0.40229885057471265,0.022988505747126436,0.034482758620689655
651,391895,445407,"Did your approach get posted to LKML? I never saw it I don't think, and I don't see it on lore. Could it be posted as an RFC, even if not suitable for upstreaming yet, just for comparison? Thanks!-Nish","On 17.09.2018 [13:33:15 +0200], Peter Zijlstra wrote:  Did your approach get posted to LKML? I never saw it I don't think, and I don't see it on lore. Could it be posted as an RFC, even if not suitable for upstreaming yet, just for comparison?  Thanks! -Nish",technical,Nishanth Aravamudan,naravamudan@digitalocean.com,0,0,201,0.0673076923076923,0.9736842105263158,0,0.6436781609195402,0.3563218390804598,0.034482758620689655,0.22988505747126436
652,391895,467930,"I kind of agree with Jan here that this is just going to add yet another task group mechanism, very similar to the existing one, with run queues inside and all. Can you imagine kernel/sched/fair.c now dealing with both groups implementations? What happens when cgroup task groups and cosched sched groups don't match wrt. Their tasks, their priorities, etc...I understand cgroup task group has become infamous. But it may be a better idea in the long run to fix it.","On Thu, Sep 27, 2018 at 11:36:34AM -0700, Subhra Mazumdar wrote:  I kind of agree with Jan here that this is just going to add yet another task group mechanism, very similar to the existing one, with runqueues inside and all.  Can you imagine kernel/sched/fair.c now dealing with both groups implementations? What happens when cgroup task groups and cosched sched groups don't match wrt. their tasks, their priorities, etc...  I understand cgroup task group has become infamous. But it may be a better idea in the long run to fix it.",technical,Frederic Weisbecker,frederic@kernel.org,1,0,465,0.12637362637362637,0.9824561403508771,0,0.8735632183908046,0.11494252873563218,0.22988505747126436,0.0
653,391895,467946,"One detail here, is that hierarchical task group a strong requirement for cosched or could you live with it flattened in the end?","On Tue, Sep 18, 2018 at 03:22:13PM +0200, Jan H. Schnherr wrote:  One detail here, is that hierarchical task group a strong requirement for cosched or could you live with it flattened in the end?",technical,Frederic Weisbecker,frederic@kernel.org,1,0,129,0.034340659340659344,0.9912280701754386,0,0.8735632183908046,0.11494252873563218,0.0,0.11494252873563218
654,391895,479735,"Currently, it is a strong requirement. As mentioned at the bottom of  it should be possible to pull the hierarchical aspect out of CFS and implement it one level higher. But that would be a major re-design of everything. I use the hierarchical aspect to a) keep scheduled groups in separate sets of runqeues,so that it is easy to select/balance tasks within a particular group, and b) to implement per-core, per-node, per-system run queues that represent larger fractions of the system, which then fan out into per-CPU run queues (eventually).","On 23/11/2018 17.51, Frederic Weisbecker wrote:  Currently, it is a strong requirement.  As mentioned at the bottom of https://lkml.org/lkml/2018/10/19/859 it should be possible to pull the hierarchical aspect out of CFS and implement it one level higher. But that would be a major re-design of everything.  I use the hierarchical aspect to a) keep coscheduled groups in separate sets of runqeues, so that it is easy to select/balance tasks within a particular group, and b) to implement per-core, per-node, per-system runqueues that represent larger fractions of the system, which then fan out into per-CPU runqueues (eventually).  Regards Jan",technical,Jan H. Schönherr,jschoenh@amazon.de,0,1,543,0.14423076923076922,1.0,1,1.0,0.0,0.11494252873563218,0.0
655,392362,392411,"Thanks for your patch.  Unfortunately, this entire function is scheduled for deletion, so I won't be applying it. If you're interested in the radix tree, I'd recommend looking at its replacement, the XArray.  The current version is at but I'll be pushing another version out in the next few days.","On Sun, Sep 09, 2018 at 09:21:00PM +0800, Wang Long wrote:  Thanks for your patch.  Unfortunately, this entire function is scheduled for deletion, so I won't be applying it.  If you're interested in the radix tree, I'd recommend looking at its replacement, the XArray.  The current version is at http://git.infradead.org/users/willy/linux-dax.git/shortlog/refs/heads/xarray but I'll be pushing another version out in the next few days.",technical,Matthew Wilcox,willy@infradead.org,1,0,296,1.0,1.0,1,0.0,0.0,0.0,0.0
656,392938,393561,Why? What bug does this fix?,"On Mon, Sep 10, 2018 at 05:09:52AM -0700, swkhack wrote:  Why? What bug does this fix?  -Dave. --  Dave Chinner david@fromorbit.com",technical,Dave Chinner,david@fromorbit.com,0,0,28,1.0,1.0,1,0.0,0.0,0.0,0.0
657,394729,395028,"NACK for any bindings that are linux specific. The suspend feature is so platform dependent that I see no need for generic Linux bindings for the same. We have power domains and idle states. If you have platforms that doesn't support some of the states, just disable them in the DT.What makes any of the above linux specific. So once again NACK.","On 12/09/18 05:09, Keerthy wrote:  NACK for any bindings that are linux specific. The suspend feature is so platform dependent that I see no need for generic Linux bindings for the same.  We have power domains and idle states. If you have platforms that doesn't support some of the states, just disable them in the DT.   What makes any of the above linux specific. So once again NACK.   --  Regards, Sudeep",technical,Sudeep Holla,sudeep.holla@arm.com,1,0,345,1.0,0.2857142857142857,0,0.0,0.0,0.0,0.0
658,394729,395034,suspend to mem and suspend to disk are pretty generic states and i agree implementation is platform dependent so why not have properties that convey if they are supported? Is the disagreement over making the properties being linux specific?,"On Wednesday 12 September 2018 04:32 PM, Sudeep Holla wrote:  suspend to mem and suspend to disk are pretty generic states and i agree implementation is platform dependent so why not have properties that convey if they are supported?  Is the disagreement over making the properties being linux specific?",technical,Keerthy,j-keerthy@ti.com,1,1,240,0.5942028985507246,0.42857142857142855,0,0.0,0.0,0.0,0.0
659,394729,395037,"We already have power domains and idle states for that. If you need to restrict few states on some platform for whatever reasons, just disable those states. I don't see the need to add any more bindings for the same. Yes.","On 12/09/18 12:19, Keerthy wrote:  We already have power domains and idle states for that. If you need to restrict few states on some platform for whatever reasons, just disable those states. I don't see the need to add any more bindings for the same.   Yes.  --  Regards, Sudeep",technical,Sudeep Holla,sudeep.holla@arm.com,1,0,221,0.6811594202898551,0.5714285714285714,0,0.0,0.0,0.0,0.0
660,394729,395124,"Oh do you mean the ""domain-idle-states"" property as mentioned in the Documentation? Yeah that should do and the DOMAIN_PWR_DN and DOMAIN_RET can be SoC specific and then the board can select which ones to use depending on how things are wired for GPIOs, memory, PMIC and so on. Hmm I don't see any users for this binding though?","* Sudeep Holla <sudeep.holla@arm.com> [180912 11:41]:  Oh do you mean the domain-idle-states"" property as mentioned in the Documentation/devicetree/bindings/power/power_domain.txt?  Yeah that should do and the DOMAIN_PWR_DN and DOMAIN_RET can be SoC specific and then the board can select which ones to use depending on how things are wired for GPIOs, memory, PMIC and so on.  Hmm I don't see any users for this binding though?  Regards,  Tony""",technical,Tony Lindgren,tony@atomide.com,1,0,328,0.9565217391304348,0.7142857142857143,0,0.0,0.0,0.0,0.0
661,394729,395142,"Yes, exactly that. All the idle-states are platform specific. DOMAIN_RET and DOMAIN_PWR_DN are just examples used in the bindings. It was added specifically to deal with such SoC idles states or hierarchical CPU power domains states, no users in upstream yet. But IMO it fits what subject is trying to address.","On 12/09/18 14:32, Tony Lindgren wrote:  Yes, exactly that.   All the idle-states are platform specific. DOMAIN_RET and DOMAIN_PWR_DN are just examples used in the bindings.   It was added specifically to deal with such SoC idles states or hierarchical CPU power domains states, no users in upstream yet. But IMO it fits what $subject is trying to address.  --  Regards, Sudeep",technical,Sudeep Holla,sudeep.holla@arm.com,1,0,310,0.8405797101449275,0.8571428571428571,0,0.0,0.0,0.0,0.0
662,394729,395166,OK OK great thanks for confirming that.,"* Sudeep Holla <sudeep.holla@arm.com> [180912 13:47]:  OK   OK great thanks for confirming that.  Regards,  Tony",technical,Tony Lindgren,tony@atomide.com,1,0,39,0.11594202898550725,1.0,1,0.0,0.0,0.0,0.0
663,397223,401940,"......The copypasta above and below is not my favorite, but I suppose it's either this or wrap it all up in a macro that you stamp down 4 times. I'm not sure if that's really any cleaner, so I guess this is fine.CONFIGFS_ITEM_NAME_LEN is only 20. Is there anything preventing the device name passed in here from being longer than that? You'd have a nasty overrun on your hands if not. Maybe snprintf here?","On Fri, Sep 14, 2018 at 2:43 AM Sayali Lokhande <sayalil@codeaurora.org> wrote: ... ...  The copypasta above and below is not my favorite, but I suppose it's either this or wrap it all up in a macro that you stamp down 4 times. I'm not sure if that's really any cleaner, so I guess this is fine.   CONFIGFS_ITEM_NAME_LEN is only 20. Is there anything preventing the device name passed in here from being longer than that? You'd have a nasty overrun on your hands if not. Maybe snprintf here?",technical,Evan Green,evgreen@chromium.org,0,0,405,1.0,0.6666666666666666,0,0.5714285714285714,0.2857142857142857,0.5714285714285714,0.2857142857142857
664,397223,404823,Agree. I need to use snprintf here. Will update.,"On 9/19/2018 2:26 AM, Evan Green wrote: Agree. I need to use snprintf here. Will update.",technical,Sayali Lokhande,sayalil@codeaurora.org,0,1,48,0.13953488372093023,1.0,1,1.0,0.0,0.2857142857142857,0.0
665,397603,426062,Quoting So then why use devm_clk_get()? Please replace both so that devm_clk_put() doesn't need to be used..,Quoting Gregory CLEMENT (2018-09-14 08:34:21)  So then why use devm_clk_get()? Please replace both so tha devm_clk_put() doesn't need to be used..,technical,Stephen Boyd,sboyd@kernel.org,1,0,108,1.0,0.6666666666666666,0,1.0,0.0,1.0,0.0
666,397603,426075,Indeed between the successful devm_clk_get and the devm_clk_put we don't exit the function in error so I can use clk_get and clk_put.,"Hi Stephen,    On mer., oct. 10 2018, Stephen Boyd <sboyd@kernel.org> wrote:   Indeed between the successful devm_clk_get and the devm_clk_put we don't exit the function in error so I can use clk_get and clk_put.  Gregory   --  Gregory Clement, Bootlin Embedded Linux and Kernel engineering http://bootlin.com",technical,Gregory CLEMENT,gregory.clement@bootlin.com,1,1,133,1.0,1.0,1,1.0,0.0,0.0,0.0
667,398596,398630,"This kind of coding style fix has very little value for a subsystem which is essentially frozen from changes, and to which the less changes that happen to it the better in order to avoid potential regressions. Therefore I am not applying this patch, sorry.","From: Romain Aviolat <r.aviolat@gmail.com> Date: Fri, 14 Sep 2018 22:25:40 +0200   This kind of coding style fix has very little value for a subsystem which is essentially frozen from changes, and to which the less changes that happen to it the better in order to avoid potential regressions.  Therefore I am not applying this patch, sorry.",technical,David Miller,davem@davemloft.net,1,0,256,1.0,1.0,1,0.0,0.0,0.0,0.0
668,399771,401073,"for reviewing the patch. rtnl_needed is not a shared variable, it is part of bonding structure, that is one per bonding driver instance. There can't be two parallel instances of bond_miimon_inspect for a single   bonding driver instance at any given point of time. and only bond_miimon_inspect updates it. That ™s why I think there is no need of any synchronization here. Thank you for cautioning us on bool usage. even a u8 can meet our requirement.  we will change it.  but, if time permits can you share more on ""particularly dangerous here, at least on some arches"".","Thanks Eric for reviewing the patch. rtnl_needed is not a shared variable, it is part of bonding structure, that is one per bonding driver instance. There can't be two parallel instances of bond_miimon_inspect for a single  bonding driver instance at any given point of time. and only bond_miimon_inspect updates it. That’s why I think there is no need of any synchronization here.    Thank you for cautioning us on bool usage. even a u8 can meet our requirement.  we will change it.  but, if time permits can you share more on particularly dangerous here, at least on some arches"".  F""",technical,Manish Kumar Singh,mk.singh@oracle.com,0,0,570,1.0,0.5,0,0.0,0.9714285714285714,0.0,0.0
669,399771,406074,"Thank you, we are making the changes and will repost the patch after testing it.","   Thankyou Eric, we are making the changes and will repost the patch after testing it. -Manish",technical,Manish Kumar Singh,mk.singh@oracle.com,0,0,80,0.1504424778761062,0.8333333333333334,0,0.17142857142857143,0.8,0.14285714285714285,0.8
670,399771,436246,"Please review the updated patch, I have changed the rtnl_needed to an atomic variable, to avoid any race condition: When link status change needs to be committed and rtnl lock couldn't be taken, avoid redisplay of same link status change message.","Please review the updated patch, I have changed the rtnl_needed to an atomic variable, to avoid any race condition:  From: Manish Kumar Singh <mk.singh@oracle.com>  When link status change needs to be committed and rtnl lock couldn't be taken, avoid redisplay of same link status change message.  Signed-off-by: Manish Kumar Singh <mk.singh@oracle.com> ---  drivers/net/bonding/bond_main.c | 6 ++++--  include/net/bonding.h           | 1 +  2 files changed, 5 insertions(+), 2 deletions(-)  diff --git a/drivers/net/bonding/bond_main.c b/drivers/net/bonding/bond_main.c index 217b790d22ed..fac5350bf19c 100644 --- a/drivers/net/bonding/bond_main.c +++ b/drivers/net/bonding/bond_main.c @@ -2087,7 +2087,7 @@ static int bond_miimon_inspect(struct bonding *bond)  			bond_propose_link_state(slave, BOND_LINK_FAIL),  			commit++,  			slave->delay = bond->params.downdelay, -			if (slave->delay) { +			if (slave->delay && !atomic_read(&bond->rtnl_needed)) {  				netdev_info(bond->dev, link status down for %sinterface %s, disabling it in %d ms\n"",  					    (BOND_MODE(bond) ==  					     BOND_MODE_ACTIVEBACKUP) ? @@ -2127,7 +2127,7 @@ static int bond_miimon_inspect(struct bonding *bond)  			commit++,  			slave->delay = bond->params.updelay,   -			if (slave->delay) { +			if (slave->delay && !atomic_read(&bond->rtnl_needed)) {  				netdev_info(bond->dev, ""link status up for interface %s, enabling it in %d ms\n"",  					    slave->dev->name,  					    ignore_updelay ? 0 : @@ -2301,9 +2301,11 @@ static void bond_mii_monitor(struct work_struct *work)  		if (!rtnl_trylock()) {  			delay = 1,  			should_notify_peers = false, +			atomic_set(&bond->rtnl_needed, 1),  			goto re_arm,  		}   +		atomic_set(&bond->rtnl_needed, 0),  		bond_for_each_slave(bond, slave, iter) {  			bond_commit_link_state(slave, BOND_SLAVE_NOTIFY_LATER),  		} diff --git a/include/net/bonding.h b/include/net/bonding.h index 808f1d167349..ffc1219f7a07 100644 --- a/include/net/bonding.h +++ b/include/net/bonding.h @@ -234,6 +234,7 @@ struct bonding {  	struct	 dentry *debug_dir,  #endif /* CONFIG_DEBUG_FS */  	struct rtnl_link_stats64 bond_stats, +	atomic_t rtnl_needed,  },    #define bond_slave_get_rcu(dev) \ -- 2.14.1    Thanks, Manish """,technical,Manish Kumar Singh,mk.singh@oracle.com,0,0,246,0.415929203539823,1.0,1,1.0,0.0,0.8,0.0
671,402510,413033,What happens to pmus that got added later? The rest looks good. Can you post a non RFC version?, What happens to pmus that got added later?  The rest looks good.  Can you post a non RFC version?  -Andi,technical,Andi Kleen,ak@linux.intel.com,0,0,95,0.03514376996805112,0.18421052631578946,0,0.5333333333333333,0.4,0.5333333333333333,0.0
672,402510,413483,There is a hunk a bit lower in the patch where in perf_pmu_register the initial setting is assigned from the global sysctl. Sure!,"On 27/09/2018 21:15, Andi Kleen wrote:  There is a hunk a bit lower in the patch where in perf_pmu_register the  initial setting is assigned from the global sysctl.   Sure!  Regards,  Tvrtko",technical,Tvrtko Ursulin,tvrtko.ursulin@linux.intel.com,1,0,129,0.039936102236421724,0.21052631578947367,0,0.5333333333333333,0.4,0.0,0.0
673,402510,413776,Which paranoia level would be used for the i915.perf_event_paranoid setting in such a case? Perhaps also CC  on the next version.,"On Fri, Sep 28, 2018 at 3:22 PM Tvrtko Ursulin <tvrtko.ursulin@linux.intel.com> wrote:  Which paranoia level would be used for the i915.perf_event_paranoid setting in such a case?  Perhaps also CC kernel-hardening@lists.openwall.com on the next version.",technical,Jann Horn,jannh@google.com,0,0,129,0.036741214057507986,0.34210526315789475,0,0.6,0.4,0.0,0.0
674,402510,413803,"thanks a lot for involving the folks! If you ask me then, IMHO, unprivileged access to CBOX pmu looks unsafe and is now governed by traditional *core* perf_event_paranoid setting. But *core* paranoid >= 1 (per-process mode) prevents simultaneous perf record sampling and perf stat -I reading from IMC, UPI, PCIe and other uncore counters. This kind of monitoring could make process performance observability thru Perf subsystem more flexible and better tailored for cloud and cluster environments. However it requires fine-tuning control capabilities in order to still keep system as secure as possible. Could i915, IMC, UPI, PCIe pmus be safe enough to be governed by a separate perf_event_paranoid settings?","Hello,  On 28.09.2018 17:02, Thomas Gleixner wrote:  Thomas, thanks a lot for involving the folks!   If you ask me then, IMHO, unprivileged access to CBOX pmu looks unsafe  and is now governed by traditional *core* perf_event_paranoid setting.  But *core* paranoid >= 1 (per-process mode) prevents simultaneous perf  record sampling and perf stat -I reading from IMC, UPI, PCIe and other  uncore counters.  This kind of monitoring could make process performance observability  thru Perf subsystem more flexible and better tailored for cloud and  cluster environments. However it requires fine-tuning control  capabilities in order to still keep system as secure as possible.  Could i915, IMC, UPI, PCIe pmus be safe enough to be governed by  a separate perf_event_paranoid settings?  Thanks! Alexey",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,0,709,0.20607028753993611,0.39473684210526316,0,0.6,0.4,0.0,0.0
675,402510,413860,"There's also been prior discussion on these feature in other contexts(e.g. android exploits resulting from out-of-tree drivers). It would be nice to see those considered. IIRC The conclusion from prior discussions (e.g. [1]) was that we wanted finer granularity of control such that we could limit PMU access to specific users -- e.g. disallow arbitrary android apps from poking *any*PMU, while allowing some more trusted apps/users to uses *some* specific PMUs. e.g. we could add /sys/bus/event_source/devices/{PMU}/device, protect this via the usual fs ACLs, and pass the fd to perf_event_open() somehow. A valid fd would act as a capability, taking precedence over perf_event_paranoid.","On Fri, Sep 28, 2018 at 12:26:52PM +0200, Thomas Gleixner wrote:  There's also been prior discussion on these feature in other contexts (e.g. android expoits resulting from out-of-tree drivers). It would be nice to see those considered.  IIRC The conclusion from prior discussions (e.g. [1]) was that we wanted finer granularity of control such that we could limit PMU access to specific users -- e.g. disallow arbitrary android apps from poking *any* PMU, while allowing some more trusted apps/users to uses *some* specific PMUs.  e.g. we could add /sys/bus/event_source/devices/${PMU}/device, protect this via the usual fs ACLs, and pass the fd to perf_event_open() somehow. A valid fd would act as a capability, taking precedence over perf_event_paranoid.  Thanks, Mark.  [1] https://patchwork.kernel.org/patch/9249919/",technical,Mark Rutland,mark.rutland@arm.com,1,0,688,0.21405750798722045,0.42105263157894735,0,0.6,0.4,0.0,0.0
676,402510,413893,That sounds like an orthogonal feature. I don't think the original patchkit would need to be hold up for this. It would be something in addition.BTW can't you already do that with the syscall filter? I assume the Android sandboxes already use that. Just forbid perf_event_openfor the apps., That sounds like an orthogonal feature. I don't think the original patchkit would need to be hold up for this. It would be something in addition.  BTW can't you already do that with the syscall filter? I assume the Android sandboxes already use that. Just forbid perf_event_open for the apps.  -Andi,technical,Andi Kleen,ak@linux.intel.com,0,0,289,0.0878594249201278,0.4473684210526316,0,0.6,0.3333333333333333,0.0,0.0
677,402510,413913,"I have to say that I disagree -- these controls will have to interact somehow, and the fewer of them we have, the less complexity we'll have to deal with longer-term. Note that this was about providing access to *some* PMUs in some cases.IIUC, if that can be done today via a syscall filter, the same is true of per-pmu paranoid settings.","On Fri, Sep 28, 2018 at 10:23:40AM -0700, Andi Kleen wrote:  I have to say that I disagree -- these controls will have to interact somehow, and the fewer of them we have, the less complexity we'll have to deal with longer-term.   Note that this was about providing access to *some* PMUs in some cases.  IIUC, if that can be done today via a syscall filter, the same is true of per-pmu paranoid settings.  Thanks, Mark.",technical,Mark Rutland,mark.rutland@arm.com,1,0,338,0.1134185303514377,0.47368421052631576,0,0.6,0.3333333333333333,0.0,0.0
678,402510,413936,"Just to make it clear. I'm not against separate settings at all. But I'm against adding knobs for every PMU the kernel supports wholesale without analysis and documentation just because we can and somebody wants it. Right now we have a single knob, which is poorly documented and that should be fixed first. But some googling gives you the information that allowing unprivileged access is a security risk. So the security focussed sysadmin will deny access to the PMUs no matter what. Now we add more knobs without documentation what the exposure and risk of each PMU is. The proposed patch set does this wholesale for every PMU supported by the kernel. So the GPU user will ask the sysadmin to allow him access. How can he make an informed decision? If he grants it then the next user comes around and wants it for something else arguing that the other got it for the GPU already. How can he make an informed decision about that one? We provide the knobs, so it's also our responsibility towards our users to give them the information about their usage and scope. And every single PMU knob has a different scope. The documentation of the gazillion of knobs in /proc and /sysfs is not really brilliant, but we should really not continue this bad practice especially not when these knobs have potentially security relevant implications. Yes, I know, writing documentation is work, but it's valuable and is appreciated by our users. To make this doable and not blocked by requiring every PMU to be analysed and documented at once, I suggested to make this opt-in. Do analysis for a given PMU, decide whether it should be exposed at all. If so, document it proper and flip the bit. That way this can be done gradually as the need arises and we can exclude the riskier ones completely. I don't think this is an unreasonable request as it does not require thei915 folks to look at PMUs they are not familiar with and does not get blocked by waiting on every PMU wizard on the planet to have time. Start with something like Documentation/admin-guide/perf-security.rst or whatever name fits better and add a proper documentation for the existing knob. With the infrastructure for fine grained access control add the general explanation for fine grained access control. With each PMU which opts in for the knob, add a section with guidance about scope and risk for this particular one.","Alexey,  On Fri, 28 Sep 2018, Alexey Budankov wrote:  Just to make it clear. I'm not against separate settings at all. But I'm against adding knobs for every PMU the kernel supports wholesale without analysis and documentation just because we can and somebody wants it.  Right now we have a single knob, which is poorly documented and that should be fixed first. But some googling gives you the information that allowing unprivilegded access is a security risk. So the security focussed sysadmin will deny access to the PMUs no matter what.  Now we add more knobs without documentation what the exposure and risk of each PMU is. The proposed patch set does this wholesale for every PMU supported by the kernel. So the GPU user will ask the sysadmin to allow him access. How can he make an informed decision? If he grants it then the next user comes around and wants it for something else arguing that the other got it for the GPU already. How can he make an informed decision about that one?  We provide the knobs, so it's also our responsibility towards our users to give them the information about their usage and scope. And every single PMU knob has a different scope.  The documentation of the gazillion of knobs in /proc and /sysfs is not really brilliant, but we should really not continue this bad practice especially not when these knobs have potentially security relevant implcations. Yes, I know, writing documentation is work, but it's valuable and is appreciated by our users.  To make this doable and not blocked by requiring every PMU to be analyzed and documented at once, I suggested to make this opt-in. Do analysis for a given PMU, decide whether it should be exposed at all. If so, document it proper and flip the bit. That way this can be done gradually as the need arises and we can exclude the riskier ones completely.  I don't think this is an unreasonable request as it does not require the i915 folks to look at PMUs they are not familiar with and does not get blocked by waiting on every PMU wizard on the planet to have time.  Start with something like Documentation/admin-guide/perf-security.rst or whatever name fits better and add a proper documentation for the existing knob. With the infrastructure for fine grained access control add the general explanation for fine grained access control. With each PMU which opt's in for the knob, add a section with guidance about scope and risk for this particular one.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,2377,0.7220447284345048,0.5,0,0.6,0.3333333333333333,0.0,0.0
679,402510,414003,"You're proposing to completely redesign perf_event_open. This new file descriptor argument doesn't exist today so it would need to create a new system call with more arguments(and BTW it would be more than the normal 6 argument limit we have, so actually it couldn't even be a standard sycall) Obviously we would need to keep the old system call around for compatibility, so you would need to worry about this interaction in any case! So tying it together doesn't make any sense, because the problem has to be solved separately anyways. The difference is that the Android sandboxes likely already doing this and have all the infrastructure, and it's just another rule. Requiring syscall filters just to use the PMU on xn system that otherwise doesn't need them would be very odd.","On Fri, Sep 28, 2018 at 06:40:17PM +0100, Mark Rutland wrote:  You're proposing to completely redesign perf_event_open.  This new file descriptor argument doesn't exist today so it would need to create a new system call with more arguments  (and BTW it would be more than the normal 6 argument limit we have, so actually it couldn't even be a standard sycall)  Obviously we would need to keep the old system call around for compability, so you would need to worry about this interaction in any case!  So tying it together doesn't make any sense, because the problem has to be solved separately anyways.   The difference is that the Android sandboxes likely already doing this and have all the infrastructure, and it's just another rule.  Requiring syscall filters just to use the PMU on xn system that otherwise doesn't need them would be very odd.  -Andi",technical,Andi Kleen,ak@linux.intel.com,0,0,779,0.23961661341853036,0.5526315789473685,0,0.6,0.3333333333333333,0.0,0.0
680,402510,414004,"And I think it would be a very good redesign. :) I love things that use file descriptors to represent capabilities. Is that true? The first argument is a pointer to a struct that contains its own size, so it can be expanded without an ABI break. I don't see any reason why you couldn't cram more stuff in there.","On Fri, Sep 28, 2018 at 10:49 PM Andi Kleen <ak@linux.intel.com> wrote:  And I think it would be a very good redesign. :) I love things that use file descriptors to represent capabilities.   Is that true? The first argument is a pointer to a struct that contains its own size, so it can be expanded without an ABI break. I don't see any reason why you couldn't cram more stuff in there.",technical,Jann Horn,jannh@google.com,0,0,311,0.11022364217252396,0.5789473684210527,0,0.6,0.3333333333333333,0.0,0.0
681,402510,414007,"You're right we could put the fd into the perf_event, but the following is still true:-"," You're right we could put the fd into the perf_event, but the following is still true:    -Andi",technical,Andi Kleen,ak@linux.intel.com,0,0,87,0.03194888178913738,0.6052631578947368,0,0.6,0.3333333333333333,0.0,0.0
682,402510,414021,"<blasphemy>Is that true? IIRC if you want to use the perf tools after a kernel update, you have to install a new version of perf anyway, no? I think after I run a kernel update, when I run ""perf top"", it just refuses to start and tells me to go install a newer version. Would the users of perf_event_open() that want to monitor this graphics stuff normally \keep working after a kernel version bump? I realize that the kernel is very much against breaking userspace interfaces, but if userspace has already decided to break itself after every update, we might as well take advantage of that...</blasphemy>","On Fri, Sep 28, 2018 at 10:59 PM Andi Kleen <ak@linux.intel.com> wrote:  <blasphemy> Is that true? IIRC if you want to use the perf tools after a kernel update, you have to install a new version of perf anyway, no? I think after I run a kernel update, when I run perf top"", it just refuses to start and tells me to go install a newer version. Would the users of perf_event_open() that want to monitor this graphics stuff normally keep working after a kernel version bump? I realize that the kernel is very much against breaking userspace interfaces, but if userspace has already decided to break itself after every update, we might as well take advantage of that... </blasphemy> """,technical,Jann Horn,jannh@google.com,0,0,605,0.20287539936102236,0.631578947368421,0,0.6,0.3333333333333333,0.0,0.0
683,402510,414024,"Not at all. perf is fully ABI compatible. Yes Ubuntu/Debian make you do it, but there is no reason for it other than their ignorance. Other sane distributions don't. Usually the first step when I'm forced to use one of those machine is to remove the useless wrapper and call the perf binary directly.","On Fri, Sep 28, 2018 at 11:22:37PM +0200, Jann Horn wrote:  Not at all. perf is fully ABI compatible.  Yes Ubuntu/Debian make you do it, but there is no reason for it other than their ignorance. Other sane distributions don't.  Usually the first step when I'm forced to use one of those machine is to remove the useless wrapper and call the perf binary directly.  -Andi",technical,Andi Kleen,ak@linux.intel.com,0,0,300,0.09904153354632587,0.6578947368421053,0,0.6,0.3333333333333333,0.0,0.0
684,402510,414076,"Ah, I guess the answer is ""0"", since you want to see data about what other users are doing. Does the i915 PMU expose sampling events, counting events, or both? The thing about sampling events is that they AFAIK always let the user pick arbitrary data to collect - like register contents, or userspace stack memory -, and independent of the performance counter being monitored, this kind of access should not be permitted to other contexts. (But it might be that I misunderstand how perf works - I'm not super familiar with its API.)","On Fri, Sep 28, 2018 at 5:12 PM Jann Horn <jannh@google.com> wrote:  Ah, I guess the answer is 0"", since you want to see data about what other users are doing.  Does the i915 PMU expose sampling events, counting events, or both? The thing about sampling events is that they AFAIK always let the user pick arbitrary data to collect - like register contents, or userspace stack memory -, and independent of the performance counter being monitored, this kind of access should not be permitted to other contexts. (But it might be that I misunderstand how perf works - I'm not super familiar with its API.)""",technical,Jann Horn,jannh@google.com,0,0,532,0.1757188498402556,0.6842105263157895,0,0.6,0.3333333333333333,0.0,0.0
685,402510,414271,"And why so? You can keep the original functionality around with the existing restrictions without breaking any existing user space. That existing functionality does not require new knobs. It stays as is. So if you want to use the enhanced version with per PMU permissions based on file descriptors you need a new version of perf. That's nothing new, if the kernel adds new features to any syscall, then you need new tools, new libraries etc. The only guarantee the kernel makes is not to break existing user space, but there is no guarantee that you can utilize new features with existing userspace.","On Fri, 28 Sep 2018, Andi Kleen wrote:  And why so? You can keep the original functionality around with the existing restrictions without breaking any existing user space. That existing functionality does not require new knobs. It stays as is.  So if you want to use the enhanced version with per PMU permissions based on file descriptors you need a new version of perf. That's nothing new, if the kernel adds new features to any syscall, then you need new tools, new libraries etc. The only guarantee the kernel makes is not to break existing user space, but there is no guarantee that you can utilize new features with existing userspace.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,599,0.18370607028753994,0.7368421052631579,0,0.6,0.3333333333333333,0.0,0.06666666666666667
686,402510,415183,"Hello, Sounds like a plan. Thanks!","Hello,  On 28.09.2018 21:20, Thomas Gleixner wrote: <SNIP>  Sounds like a plan. Thanks!  BR, Alexey",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,0,34,0.01437699680511182,0.7631578947368421,0,0.7333333333333333,0.2,0.06666666666666667,0.0
687,402510,415184,"Hello, There are usages in production where perf_event_open() syscall accompanied with read(), mmap() etc. is embedded into application on per-thread basis and is used for self monitoring and dynamic execution tuning. There are also other Perf tools around that, for example, are statically linked and then used as on Linux as on Android. Backward compatibility does matter in these cases.","Hello Jann,  <SNIP>  There are usages in production where perf_event_open() syscall  accompanied with read(), mmap() etc. is embedded into application  on per-thread basis and is used for self monitoring and dynamic  execution tuning.  There are also other Perf tools around that, for example, are  statically linked and then used as on Linux as on Android.  Backward compatibility does matter in these cases.  Thanks, Alexey",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,0,389,0.1182108626198083,0.7894736842105263,0,0.7333333333333333,0.2,0.0,0.0
688,402510,415185,"Hell, Currently *core* paranoid >= 1 (per-process mode) prevents simultaneous sampling on CPU events (perf record) and reading of uncore HW counters (perf stat -I), because uncore counters count system wide and that is allowed only when *core* paranoid <= 0.Uncore counts collected simultaneously with CPU event samples can be correlated using timestamps taken from some common system clock e.g.CLOCK_MONOTONIC_RAW. Could it be secure enough to still allow reading of system wide uncore HW counters when sampling of CPU events is limited to specific processes by *core* paranoid >= 1?","Hello Jann and Kees,  On 29.09.2018 1:02, Jann Horn wrote: <SNIP>  Currently *core* paranoid >= 1 (per-process mode) prevents simultaneous  sampling on CPU events (perf record) and reading of uncore HW counters  (perf stat -I), because uncore counters count system wide and that is  allowed only when *core* paranoid <= 0.  Uncore counts collected simultaneously with CPU event samples can be  correlated using timestamps taken from some common system clock e.g.  CLOCK_MONOTONIC_RAW.  Could it be secure enough to still allow reading of system wide uncore  HW counters when sampling of CPU events is limited to specific processes  by *core* paranoid >= 1?  Thanks, Alexey",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,0,584,0.17412140575079874,0.8157894736842105,0,0.7333333333333333,0.2,0.0,0.0
689,402510,415677,"Well, it's nothing fundamentally new, that new features require changes to applications, libraries etc. It's nice if it can be avoided of course. From a design POV, Jann's idea to have a per PMU special file which you need to open for getting access is way better than the extra knobs. It allows to use all existing security mechanisms to be used. Peter and I discussed that and we came up with the idea that the file descriptor is not even required, i.e. you could make it backward compatible.perf_event_open() knows which PMU is associated with the event the caller tries to open. So perf_event_open() can try to access/open the special per PMU file on behalf of the caller. That should get the same security treatment like a regular open() from user space. If that succeeds, access is granted. The magic file could still be writeable for root to give general restrictions aside of the file based ones similar to what you are proposing. The analysis and documentation requirements still remain of course.","Alexey,  On Mon, 1 Oct 2018, Alexey Budankov wrote:  Well, it's nothing fundamentally new, that new features require changes to applications, libraries etc. It's nice if it can be avoided of course.  From a design POV, Jann's idea to have a per PMU special file which you need to open for getting access is way better than the extra knobs. It allows to use all existing security mechanisms to be used.  Peter and I discussed that and we came up with the idea that the file descriptor is not even required, i.e. you could make it backward compatible.  perf_event_open() knows which PMU is associated with the event the caller tries to open. So perf_event_open() can try to access/open the special per PMU file on behalf of the caller. That should get the same security treatment like a regular open() from user space. If that succeeds, access is granted.  The magic file could still be writeable for root to give general restrictions aside of the file based ones similar to what you are proposing.  The analysis and documentation requirements still remain of course.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,1006,0.31629392971246006,0.8421052631578947,0,0.8,0.2,0.0,0.0
690,402510,415679,"(That was Mark's idea, not mine, I just agree with his idea a lot.)","On Mon, Oct 1, 2018 at 6:12 PM Thomas Gleixner <tglx@linutronix.de> wrote:  (That was Mark's idea, not mine, I just agree with his idea a lot.)",technical,Jann Horn,jannh@google.com,0,0,67,0.03194888178913738,0.868421052631579,0,0.8,0.2,0.0,0.0
691,402510,415872,"Hello,<SNIP>Let me wrap up all the requirements and ideas that have been captured so far.1. A file [1] is added so that it can belong to a group of users allowed to use {PMU},   something like this Modifications of file content are allowed to those who can   modify setting.2. Semantics and content of the introduced paranoid file is   similar to this   The perf_event_paranoid file can be set to restrict access   to the performance counters.   2   allow only user-space measurements (default since Linux 4.6).   1   allow both kernel and user measurements (default before Linux 4.6).   0   allow access to CPU-specific data but not raw trace point samples.  -1  no restrictions.   The existence of the perf_event_paranoid file is the official method   for determining if a kernel supports perf_event_open().3. Every time an event for {PMU} is created over perf_event_open():   a) the calling thread's euid is checked to belong to {PMU}_users group      and if it does then the event's fd is allocated,   b) then traditional checks against perf_event_pranoid content are applied,   c) if the file doesn't exist the access is governed by global setting, 4. Documentation/admin-guide/perf-security.rst file is introduced that:   a) contains general explanation for fine grained access control,   b) contains a section with guidance about scope and risk for each PMU      which is enabled for fine grained access control,   c) file is extended when more PMUs are enabled for fine grain control,Security analysis for uncore IMC, QPI/UPI, PC Ie PMUs is still required to be enabled for fine grain control.","Hello,  On 01.10.2018 19:11, Thomas Gleixner wrote:  <SNIP>   Let me wrap up all the requirements and ideas that have been captured so far.  1. A file [1] is added so that it can belong to a group of users allowed to use ${PMU},     something like this:  ls -alh /sys/bus/event_source/devices/${PMU}/caps/ total 0 drwxr-xr-x 2 root root            0 Oct  1 20:36 . drwxr-xr-x 6 root root            0 Oct  1 20:36 .. -r--r--r-- 1 root root         4.0K Oct  1 20:36 branches -r--r--r-- 1 root root         4.0K Oct  1 20:36 max_precise -r--r--r-- 1 root root         4.0K Oct  1 20:36 pmu_name -rw-r--r--   root ${PMU}_users                   paranoid        <===     Modifications of file content are allowed to those who can     modify /proc/sys/kernel/perf_event_paranoid setting.  2. Semantics and content of the introduced paranoid file is     similar to /proc/sys/kernel/perf_even_paranoid [2]:     The perf_event_paranoid file can be set to restrict access    to the performance counters.     2   allow only user-space measurements (default since Linux 4.6).    1   allow both kernel and user measurements (default before Linux 4.6).    0   allow access to CPU-specific data but not raw trace‐point samples.   -1  no restrictions.     The existence of the perf_event_paranoid file is the official method     for determining if a kernel supports perf_event_open().  3. Every time an event for ${PMU} is created over perf_event_open():    a) the calling thread's euid is checked to belong to ${PMU}_users group        and if it does then the event's fd is allocated,    b) then traditional checks against perf_event_pranoid content are applied,    c) if the file doesn't exist the access is governed by global setting        at /proc/sys/kernel/perf_even_paranoid,  4. Documentation/admin-guide/perf-security.rst file is introduced that:    a) contains general explanation for fine grained access control,    b) contains a section with guidance about scope and risk for each PMU       which is enabled for fine grained access control,    c) file is extended when more PMUs are enabled for fine grain control,   Security analysis for uncore IMC, QPI/UPI, PCIe PMUs is still required  to be enabled for fine grain control.  Thanks, Alexey  [1] https://patchwork.kernel.org/patch/9249919/#19714087 [2] http://man7.org/linux/man-pages/man2/perf_event_open.2.html",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,0,1601,0.48083067092651754,0.8947368421052632,0,0.8,0.13333333333333333,0.0,0.0
692,402510,416961,"Right, though I personally prefer something like 'access control' as filename, but that's bike shed painting realm. Not only the user group, it really should do the full security checks which are done on open().Hmm, not sure about that because that might be conflicting. Correct.     0) Better documentation of this.","Alexey,  On Mon, 1 Oct 2018, Alexey Budankov wrote:  Right, though I personaly prefer something like 'access_control' as file name, but that's bike shed painting realm.   Not only the user group, it really should do the full security checks which are done on open().   Hmm, not sure about that because that might be conflicting.   Correct.        0) Better documentation of /proc/sys/kernel/perf_even_paranoid   Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,316,0.10223642172523961,0.9210526315789473,0,0.8,0.13333333333333333,0.0,0.0
693,402510,417240,"Hello, I expect it is already implemented by some internal kernel API so that it could be reused. Well, possible contradictions could be converged to some reasonable point during technical review stage. Current perf_event_paranoid semantics is still required for PMUs that are governed by global setting at. Exactly. perf_event_open man7 [1] requires update as well, however this is not a part of kernel source tree so these docs changes are to be mailed TO:","Hello,  On 02.10.2018 9:40, Thomas Gleixner wrote:  <SNIP>   I expect it is already implemented by some internal kernel API so that  it could be reused.   Well, possible contradictions could be converged to some reasonable point  during technical review stage.  Current perf_event_paranoid semantics is still required for PMUs  that are governed by global setting at /proc/sys/kernel/perf_event_paranoid.  <SNIP>   Exactly. perf_event_open man7 [1] requires update as well, however  this is not a part of kernel source tree so these docs changes are  to be mailed TO: mtk.manpages@gmail.com and CC: linux-api@vger.kernel.org.  Thanks, Alexey  [1] http://man7.org/linux/man-pages/man2/perf_event_open.2.html",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,0,458,0.134185303514377,0.9473684210526315,0,0.8,0.13333333333333333,0.0,0.06666666666666667
694,402510,418932,"You'll also have to make sure that this thing in kernel/events/core.c doesn't have any bad effect:    /*    * Special case software events and allow them to be part of    * any hardware group.    */As in, make sure that you can't smuggle in arbitrary software events by attaching them to a whitelisted hardware event. And you can't whitelist anything that permits using sampling events with arbitrary sample_type.","On Mon, Oct 1, 2018 at 10:53 PM Alexey Budankov <alexey.budankov@linux.intel.com> wrote:  You'll also have to make sure that this thing in kernel/events/core.c doesn't have any bad effect:      /*     * Special case software events and allow them to be part of     * any hardware group.     */  As in, make sure that you can't smuggle in arbitrary software events by attaching them to a whitelisted hardware event.   And you can't whitelist anything that permits using sampling events with arbitrary sample_type.",technical,Jann Horn,jannh@google.com,0,0,413,0.12300319488817892,0.9736842105263158,0,0.9333333333333333,0.06666666666666667,0.06666666666666667,0.06666666666666667
695,402510,419917,"Hi,<SNIP>Yes, makes sense. Please see and comment below.<SNIP>It appears that there is a dependency on the significance of data that PMUs captures for later analysis. Currently there are following options for data being captured (please correct or extend if something is missing from the list below):1) Monitored process details:   - system information on a process as a container (of threads, memory data and     IDs (e.g. open fds) from process specific namespaces and etc.),   - system information on threads as containers (of execution context details),2) Execution context details:   - memory addresses,   - memory data,   - calculation results,   - calculation state in HW,3) Monitored process and execution context telemetry data, used for building   various performance metrics and can come from:   - user mode code and OS kernel,   - various parts of HW e.g. core, uncore, peripheral and etc. Group 2) is the potential leakage source of sensitive process data so if a PMU, at some mode, samples execution context details then the PMU, working in that mode, is the subject for *access* and *scope* control. On the other hand if captured data contain only the monitored process details and/or associated execution telemetry, there is probably no sensitive data leakage thru that captured data. For example, if cpu PMU samples PC addresses overtime, e.g. for providing hotspots-by-function profile, then this requires to be controlled as from access as from scope perspective, because PC addresses is execution context details that can contain sensitive data. However, if cpu PMU does counting of some metric value, or if software PMU reads value of thread active time from the OS, possibly overtime, for later building some rating profile, or reading of some HW counter value without attribution to any execution context details, that is probably not that risky as in the case of PC address sampling. Uncore PMUs e.g. memory controller (IMC), interconnect (QPI/UPI) and peripheral (PCIe) currently only read counters values that are captured system wide by HW, and provide no attribution to any specific execution context details, thus, sensitive process data. Based on that, A) paranoid knob is required for a PMU if it can capture data from group 2)B) paranoid knob limits scope of capturing sensitive data:   -3 - *scope* is defined by some high level setting   -2 - disabled - no allowed *scope*   -1 - no restrictions - max *scope*    0 - system wide    1 - process user and kernel space    2 - process user space onlyC) paranoid knob has to be checked every time the PMU is going to start   capturing sensitive data to avoid capturing beyond the allowed scope. PMU *access* semantics is derived from fs ACLs and could look like this:r - read PMU architectural and configuration details, read PMU *access* settingsw - modify PMU *access* settingsx - modify PMU configuration and collect data So levels of *access* to PMU could look like this:root=rwx, {PMU}_users=r-x, other=r--.Possible examples of *scope* control settings could look like this:1) system wide user+kernel mode CPU sampling with context switches   and uncore counting: Please share more thought so that it eventually could go into Documentation/admin-guide/perf-security.rst.","Hi,  On 03.10.2018 20:01, Jann Horn wrote: <SNIP>  Yes, makes sense. Please see and comment below.  <SNIP>  It appears that there is a dependency on the significance of data that PMUs captures  for later analysis. Currently there are following options for data being captured  (please correct or extend if something is missing from the list below):  1) Monitored process details:    - system information on a process as a container (of threads, memory data and       IDs (e.g. open fds) from process specific namespaces and etc.),    - system information on threads as containers (of execution context details), 2) Execution context details:    - memory addresses,    - memory data,    - calculation results,    - calculation state in HW, 3) Monitored process and execution context telemetry data, used for building     various performance metrics and can come from:    - user mode code and OS kernel,    - various parts of HW e.g. core, uncore, peripheral and etc.  Group 2) is the potential leakage source of sensitive process data so if a PMU,  at some mode, samples execution context details then the PMU, working in that mode,  is the subject for *access* and *scope* control.  On the other hand if captured data contain only the monitored process details  and/or associated execution telemetry, there is probably no sensitive data leakage  thru that captured data.  For example, if cpu PMU samples PC addresses overtime, e.g. for providing  hotspots-by-function profile, then this requires to be controlled as from access as  from scope perspective, because PC addresses is execution context details that  can contain sensitive data.  However, if cpu PMU does counting of some metric value, or if software PMU reads  value of thread active time from the OS, possibly overtime, for later building some  rating profile, or reading of some HW counter value without attribution to any  execution context details, that is probably not that risky as in the case of  PC address sampling.  Uncore PMUs e.g. memory controller (IMC), interconnect (QPI/UPI) and peripheral (PCIe)  currently only read counters values that are captured system wide by HW, and provide  no attribution to any specific execution context details, thus, sensitive process data.  Based on that,  A) paranoid knob is required for a PMU if it can capture data from group 2) B) paranoid knob limits scope of capturing sensitive data:    -3 - *scope* is defined by some high level setting    -2 - disabled - no allowed *scope*    -1 - no restrictions - max *scope*     0 - system wide     1 - process user and kernel space     2 - process user space only C) paranoid knob has to be checked every time the PMU is going to start     capturing sensitive data to avoid capturing beyond the allowed scope.  PMU *access* semantics is derived from fs ACLs and could look like this:  r - read PMU architectural and configuration details, read PMU *access* settings w - modify PMU *access* settings x - modify PMU configuration and collect data  So levels of *access* to PMU could look like this:  root=rwx, ${PMU}_users=r-x, other=r--.  Possible examples of *scope* control settings could look like this:  1) system wide user+kernel mode CPU sampling with context switches     and uncore counting:  	/proc/sys/kernel/perf_event_paranoid (-2, 2): 0 	SW.paranoid  (-3, 2):(root=rwx, SW_users=r-x,other=r--): -3 	CPU.paranoid (-3, 2):(root=rwx,CPU_users=r-x,other=r--): -3 	IMC.paranoid (-3,-1):(root=rwx,IMC_users=r-x,other=r--): -3 	UPI.paranoid (-3,-1):(root=rwx,UPI_users=r-x,other=r--): -3 	PCI.paranoid (-3,-1):(root=rwx,PCI_users=r-x,other=r--): -3  2) per-process CPU sampling with context switches and uncore counting:  	/proc/sys/kernel/perf_event_paranoid (-2, 2): 1|2 	SW.paranoid  (-3, 2):(root=rwx, SW_users=r-x,other=r--): -3 	CPU.paranoid (-3, 2):(root=rwx,CPU_users=r-x,other=r--): -3 	IMC.paranoid (-3,-1):(root=rwx,IMC_users=r-x,other=r--): -1 	UPI.paranoid (-3,-1):(root=rwx,UPI_users=r-x,other=r--): -1 	PCI.paranoid (-3,-1):(root=rwx,PCI_users=r-x,other=r--): -1  3) per-process user mode CPU sampling allowed to specific ${PMU}_groups only:  	/proc/sys/kernel/perf_event_paranoid (-2, 2): -2 	SW.paranoid  (-3, 2):(root=rwx, SW_users=r-x,other=r--):  2 	CPU.paranoid (-3, 2):(root=rwx,CPU_users=r-x,other=r--):  2 	IMC.paranoid (-3,-1):(root=rwx,IMC_users=r-x,other=r--): -3 	UPI.paranoid (-3,-1):(root=rwx,UPI_users=r-x,other=r--): -3 	PCI.paranoid (-3,-1):(root=rwx,PCI_users=r-x,other=r--): -3  4) uncore HW counters monitoring, possibly overtime:  	/proc/sys/kernel/perf_event_paranoid (-2, 2): -2 	SW.paranoid  (-3, 2):(root=rwx, SW_users=r-x,other=r--): -3 	CPU.paranoid (-3, 2):(root=rwx,CPU_users=r-x,other=r--): -3 	IMC.paranoid (-3,-1):(root=rwx,IMC_users=r-x,other=r--): -1 	UPI.paranoid (-3,-1):(root=rwx,UPI_users=r-x,other=r--): -1 	PCI.paranoid (-3,-1):(root=rwx,PCI_users=r-x,other=r--): -1  Please share more thought so that it eventually could go into  Documentation/admin-guide/perf-security.rst.  Thanks, Alexey",technical,Alexey Budankov,alexey.budankov@linux.intel.com,0,0,3254,1.0,1.0,1,1.0,0.0,0.06666666666666667,0.0
696,410867,414672,I missed this and do the wrong thing. I'm really sorry for this.,"On 2018/9/30 2:38, David Miller wrote:  I missed this and do the wrong thing.   I'm really sorry for this.",technical,YueHaibing,yuehaibing@huawei.com,0,1,64,1.0,1.0,1,1.0,0.0,0.0,0.0
697,418140,421868,This patch doesn't apply to cryptodev because the bug has already been fixed by another patch.,"On Tue, Oct 02, 2018 at 11:00:03PM +0200, Arnd Bergmann wrote:  This patch doesn't apply to cryptodev because the bug has already been fixed by another patch.  Thanks, --  Email: Herbert Xu <herbert@gondor.apana.org.au> Home Page: http://gondor.apana.org.au/~herbert/ PGP Key: http://gondor.apana.org.au/~herbert/pubkey.txt",technical,Herbert Xu,herbert@gondor.apana.org.au,1,0,94,1.0,1.0,1,1.0,0.0,1.0,0.0
698,418207,453431,"While I agree that it is polling anyway, this change can add significant burden when debugging and trace is enabled for cpu_idle, if idle state 0is used often. For example: Phoronix dbench test, 96 clients: 900 second trace:Kernel 4.20-rc1:idle state 0 entry exits: 686,724 Does trace being enabled effect the system under test: Yes. Kernel 4.20-rc1 with this patch reverted: idle state 0 entry exits: 66,185Does trace being enabled effect the system under test: No, or minimal....","On 2018.10.02 14:51 Rafael J. Wysocki wrote:   While I agree that it is polling anyway, this change can add significant burden when debugging and trace is enabled for cpu_idle, if idle state 0 is used often.  For example: Phoronix dbench test, 96 clients: 900 second trace:  Kernel 4.20-rc1: idle state 0 entry exits: 686,724 Does trace being enabled effect the system under test: Yes.  Kernel 4.20-rc1 with this patch reverted: idle state 0 entry exits: 66,185 Does trace being enabled effect the system under test: No, or minimal.  ... Doug",technical,Doug Smythies,dsmythies@telus.net,0,0,481,1.0,1.0,1,1.0,0.0,1.0,0.0
699,418534,419202,"Thank you for your patch If my understanding was correct, the chance to use BUSIFx is when TDM split mode. And this patch selects it on runtime (= hw_param) ?But, I think we can/should select it on probe timing from DT connection. Am I misunderstanding ? I'm not sure how to select, but adding new ssiuX0 - ssiuX7 is realistic idea (parse sound card is not realistic...) ?If so, your rxu/txu DMA can be more simple ?","Hi Jiada  Thank you for your patch   If my understanding was correct, the chance to use BUSIFx is when TDM split mode. And this patch selects it on runtime (= hw_param) ? But, I think we can/should select it on probe timing from DT connection. Am I misunderstanding ?  I'm not sure how to select, but adding new ssiuX0 - ssiuX7 is realistic idea (parse sound card is not realistic...) ? If so, your rxu/txu DMA can be more simple ?",technical,Kuninori Morimoto,kuninori.morimoto.gx@renesas.com,0,0,416,0.38396624472573837,0.13333333333333333,0,0.0,0.8333333333333334,0.0,0.0
700,418534,419214,"Yes, only when SSI works in Split/Ex-Split mode, BUSIFx other than 0 is necessary Because, in order to automatically determine BUSIF number, information like SSI mode (non-Split/Split/Ex-Split), runtime channel, are required(in our internal implementation, SSI mode is selected by kctrl)because of this, in this patch, BUSIF is selected on runtime with the above reasoning, BUSIF is selected on runtime. What do you think?","Hi Morimoto-san   On 2018/10/04 10:41, Kuninori Morimoto wrote: Yes, only when SSI works in Split/Ex-Split mode, BUSIFx other than 0 is  necessary Because, in order to automatically determine BUSIF number, information like SSI mode (non-Split/Split/Ex-Split), runtime channel,  are required (in our internal implementation, SSI mode is selected by kctrl) because of this, in this patch, BUSIF is selected on runtime  with the above reasoning, BUSIF is selected on runtime. what do you think?  Thanks, Jiada",technical,Jiada Wang,jiada_wang@mentor.com,0,0,422,0.34177215189873417,0.26666666666666666,0,0.0,0.8333333333333334,0.0,0.0
701,418534,419241,"Thank you for your feedback(snip)I have no objection that you are customizing your kernel locally. But, upstreaming kernel based on it is not acceptable for me. I'm not sure detail of your local implementation, but I don't think we need to select SSI mode by kctrl. If my understanding was correct, it can also be selected automatically somehow. Or, am I misunderstanding ?I could understand what you want to do, and yes, I can agree that we want/need to have it on upstream. Thank you very much to indicating it to me. But we need to consider more how to implement it. Especially, it is related to DT bindings. As you already know, if it is implemented on upstream kernel, we need to keep compatibility in the future, and it is very difficult. So, my opinions for BUSIFn support are - SSI mode should be selected automatically - BUSIFn connection should be selected on DT   (I think we don't want random sound output position ?)   - To select it, we need to have new ""ssiu"" DT settings,     or parse sound card. Maybe adding ssiu is realistic."," Hi Jiada  Thank you for your feedback  (snip)  I have no objection that you are customizing your kernel locally. But, upstreaming kernel based on it is not acceptable for me. I'm not sure detail of your local implementation, but I don't think we need to select SSI mode by kctrl. If my understanding was correct, it can also be selected automatically somehow. Or, am I misunderstanding ?  I could understand what you want to do, and yes, I can agree that we want/need to have it on upstream. Thank you very much to indicating it to me. But we need to consider more how to implement it. Especially, it is related to DT bindings. As you already know, if it is implemented on upstream kernel, we need to keep compatibility in the future, and it is very difficult.  So, my opinions for BUSIFn support are 	- SSI mode should be selected automatically 	- BUSIFn connection should be selected on DT 	  (I think we don't want random sound output position ?) 	  - To select it, we need to have new ssiu"" DT seetings, 	    or parse sound card. Maybe adding ssiu is realistic.   Best regards --- Kuninori Morimoto""",technical,Kuninori Morimoto,kuninori.morimoto.gx@renesas.com,0,0,1044,0.9367088607594937,0.3333333333333333,0,0.0,0.8333333333333334,0.0,0.0
702,418534,419299,"Thanks for your comments SSI can work in following modes1. Basic Mode: (channel 1, 2, 4, 6, 8, 16)2. TDM Extended Mode: (channel 6, 8)3. TDM Split Mode: (channel 1, 2)4. TDM Ex-Split mode: (Channel 2, 4, 6, 8, 10) for example user asks dai-link0 to playback 2ch audio stream, driver can't determine which mode to work, as it can be Basic mode ,Split mode or Ex-Split mode. Yes, I agree with you, upstream need to consider lots of things can you give me your idea, how to automatically determine working mode, when user plays 2 channel stream on playback dai-link since which BUSIFx is used during audio data transfer, is not consideration of user, I think your previous suggestion, (automatically select BUSIFx) makes more sense","Hi Morimoto-san   Thanks for your comments   On 2018/10/04 12:43, Kuninori Morimoto wrote: SSI can work in following modes 1. Basic Mode: (channel 1, 2, 4, 6, 8, 16) 2. TDM Extended Mode: (channel 6, 8) 3. TDM Split Mode: (channel 1, 2) 4. TDM Ex-Split mode: (Channel 2, 4, 6, 8, 10)  for example user asks dai-link0 to playback 2ch audio stream, driver can't determine which mode to work, as it can be Basic mode,  Split mode or Ex-Split mode.  Yes, I agree with you, upstream need to consider lots of things can you give me your idea, how to automatically determine working mode, when user plays 2 channel stream on playback dai-link since which BUSIFx is used during audio data transfer, is not  consideration of user, I think your previous suggestion, (automatically select BUSIFx) makes  more sense  Thanks, Jiada",technical,Jiada Wang,jiada_wang@mentor.com,0,0,728,0.7172995780590717,0.4666666666666667,0,0.0,0.8333333333333334,0.0,0.6666666666666666
703,418534,423435,"If my understanding was correct, we can do like this If DT indicated sound card has dai-link x N, tdm-slots = <M>, If (N, M) = (1, 2) : Basic mode If (N, M) = (1, >2): TDM mode If (N, M) = (2, 4) : TDM Split mode If (N, M) = (2, >4): TDM Ex-Split mode If (N, M) = (>2, 8): TDM Split mode ...Maybe some combination was wrong, but we can do something like this ?Why do we need to use Basic mode if HW has TDM Split mode connection? If user playbacks 2ch audio in such situation, we can use TDM Split mode (= only 2ch has sound, other channel has no sound ?) user might start to playback for other channels. I'm not sure how it works...I'm not yet sure detail, but in your idea, does it mean,BUSIFx connection might be exchanged runtime ?I think BUSIFx connection shouldn't exchanged runtime IMO. Otherwise, sound position can't be fixed, and user can't control sound, I think...","Hi Jiada  (snip)  If my understanding was correct, we can do like this If DT indicated sound card has dai-link x N, tdm-slots = <M>, 	If (N, M) = (1, 2) : Basic mode 	If (N, M) = (1, >2): TDM mode 	If (N, M) = (2, 4) : TDM Split mode 	If (N, M) = (2, >4): TDM Ex-Split mode 	If (N, M) = (>2, 8): TDM Split mode 	...  Maybe some combination was wrong, but we can do something like this ?   Why do we need to use Basic mode if HW has TDM Split mode connection? If user playbacks 2ch audio in such situation, we can use TDM Split mode (= only 2ch has sound, other channel has no sound ?)  user might start to playback for other channels. I'm not sure how it works...   I'm not yet sure detail, but in your idea, does it mean, BUSIFx connection might be exchanged runtime ? I think BUSIFx connection shouldn't exchanged runtime IMO. Otherwise, sound position can't be fixed, and user can't control sound, I think...  Best regards --- Kuninori Morimoto",technical,Kuninori Morimoto,kuninori.morimoto.gx@renesas.com,0,0,876,1.0,0.5333333333333333,0,0.8333333333333334,0.0,0.6666666666666666,0.0
704,418534,423626,"Thanks for your comment The idea to consider tdm_slot when determine SSI mode makes sense to me, by checking runtime channel and tdm_slots combination, I think SSI mode can be automatically selected like following: it shouldn't be changed during runtime, my idea is BUSIFx can be automatically selected when corresponding dai-link is not active The reason I added rsnd_ssi_select_busif(io, chan) in rsnd_hw_params()in patch ASoC: rsnd: add busif property to dai stream of v2 patch-set,is because runtime channel is necessary information to determine which BUSIFx to select,(which is mentioned in above) and at this stage (rsnd_hw_params()), all other control settings(register setting, dma address calculation etc) haven't been done, so corresponding dai-link can be considered to be not active at this timing but maybe you have better suggestion when to automatically select BUSIFx What is your opinion?","Hi Morimoto-san  Thanks for your comment  On 2018/10/09 9:44, Kuninori Morimoto wrote: The idea to consider tdm_slot when determine SSI mode makes sense to me, by checking runtime channel and tdm_slots combination, I think SSI mode can be automatically selected like following:  1ch:  (tdm_slots < 4) Basic mode, (tdm_slots >= 4) TDM Split mode 2ch: (2 <= tdm_slots < 8) Basic mode, (tdm_slots >= 8) TDM Ex-Split mode 4ch: (4 <= tdm_slots < 8) Basic mode, (tdm_slots >= 8) TDM Ex-Split mode 6ch: (6 <= tdm_slots < 8) Basic mode, (tdm_slots == 8) TDM Extended  mode, (8 < tdm_slots) TDM Ex-Split mode 8ch: (6 <= tdm_slots < 8) TDM Extended mode, (8 <= tdm_slots < 16) Basic  mode, (tdm_slots == 16) TDM Ex-Split 10ch: TDM Ex-Split mode 16ch: Basic Mode  no BUSIFx shouldn't be changed during runtime, my idea is BUSIFx can be  automatically selected when corresponding dai-link is not active  The reason I added rsnd_ssi_select_busif(io, chan) in rsnd_hw_params()  in patch ASoC: rsnd: add busif property to dai stream of v2 patch-set, is because runtime channel is necessary information to determine which  BUSIFx to select, (which is mentioned in above) and at this stage (rsnd_hw_params()), all other control settings  (register setting, dma address calculation etc) haven't been done, so corresponding dai-link can be considered to be not  active at this timing but maybe you have better suggestion when to automatically select BUSIFx  What is your opinion?  Thanks, Jiada",technical,Jiada Wang,jiada_wang@mentor.com,0,0,904,0.7046413502109705,0.6666666666666666,0,0.8333333333333334,0.0,0.0,0.0
705,418534,423654,"Thanks for your feedback Sorry, but I couldn't understand what this table means ? For example, what does ""1ch"" mean ?It looks like ""1ch playback by TDM""...My image is like this.","Hi Jiada  Thanks for your feedback   Sorry, but I couldn't understand what this table means ? For example, what does 1ch"" mean ? It looks like ""1ch playback by TDM""...   My image is like this.  	sound { 		compatible = ""simple-scu-audio-card or new card"", 		... 		simple-audio-card,convert-channels = <8>, 		... 		busif0: simple-audio-card,cpu@0 { sound-dai = <&rcar_sound 0>, }, 		busif1: simple-audio-card,cpu@1 { sound-dai = <&rcar_sound 1>, }, 		busif2: simple-audio-card,cpu@2 { sound-dai = <&rcar_sound 2>, }, 		busif3: simple-audio-card,cpu@3 { sound-dai = <&rcar_sound 3>, }, 		        simple-audio-card,codec { sound-dai = <&xxx>,          }, 	},  	rcar_sound { 		dai0 { playback = <&ssiu0 ssi0>, } 		dai1 { playback = <&ssiu1 ssi0>, } 		dai2 { playback = <&ssiu2 ssi0>, } 		dai3 { playback = <&ssiu3 ssi0>, } 	},  Best regards --- Kuninori Morimoto""",technical,Kuninori Morimoto,kuninori.morimoto.gx@renesas.com,0,0,177,0.17721518987341772,0.7333333333333333,0,0.8333333333333334,0.0,0.0,0.0
706,418534,423786,"Thanks for your feedback sorry for vague explanation, by ""1ch"" I mean runtime 1 channel playback for example: if user plays 1 channel stream, by checking tdm_slots value, if it is < 4, then it can't be working in Split mode, so driver will automatically set SSI to work in Basic mode, otherwise, SSI will work in TDM Split mode. After re-think about BUSIFx selection, I agree with you, it shouldn't be random, and select it via device-tree, as you have already demonstrated is a good idea Based on our discussion so far, I think we both agree on the following points1. Driver select SSI mode automatically (by checking tdm_slots, runtime channel, etc)2. Driver parse BUSIFx for each dai-link from device-tree I will review my v2 patch-set, drop changes violate to above two points","Hi Morimoto-san  Thanks for your feedback  On 2018/10/09 16:47, Kuninori Morimoto wrote: sorry for vague explanation, by 1ch"" I mean runtime 1 channel playback for example: if user plays 1 channel stream, by checking tdm_slots value, if it is < 4, then it can't be working in Split mode, so driver will automatically set SSI to work in Basic mode, otherwise, SSI will work in TDM Split mode.  After re-think about BUSIFx selection, I agree with you, it shouldn't be random, and select it via device-tree, as you have  already demonstrated is a good idea  Based on our discussion so far, I think we both agree on the following  points 1. Driver select SSI mode automatically (by checking tdm_slots, runtime  channel, etc) 2. Driver parse BUSIFx for each dai-link from device-tree  I will review my v2 patch-set, drop changes violate to above two points   Thanks, Jiada""",technical,Jiada Wang,jiada_wang@mentor.com,0,0,780,0.6751054852320675,0.8666666666666667,0,1.0,0.0,0.0,0.0
707,418534,424536,"Hmm... ??Maybe, we are misunderstanding each other...In my understanding, if platform can use TDM 8ch Split mode, user interface will be for example ser can playback like this sound will be converted to 2ch by alsalib, and daiX will receive converted 2ch sound. SSI always playbacks it as part of TDM 8ch Split mode. In this platform, it can handle stereo sound only on each daiX, and always works as TDM Split mode, never works as Basic Mode / TDM Ex-Split mode. If DAI was dai0, dai1 only, it will be TDM Ex-Split mode. It depends on tdm-slots, I think. If tdm-slots was  something like this, is my understanding. Thank you for understanding my idea. Nice to know","Hi Jiada   Hmm... ?? Maybe, we are misunderstanding each other...  In my understanding, if platform can use TDM 8ch Split mode, user interface will be for example dai0, dai1, dai2, dai3 (= for TDM 8ch). Here, each daiX can handle stereo sound only. Then, user can playback like this  	aplay -D plughw:0,0 xxx.wav (= will be 1ch, 2ch) 	aplay -D plughw:0,1 xxx.wav (= will be 3ch, 4ch) 	aplay -D plughw:0,2 xxx.wav (= will be 5ch, 6ch) 	aplay -D plughw:0,3 xxx.wav (= will be 7ch, 8ch) 	 1ch sound will be converted to 2ch by alsalib, and daiX will receive converted 2ch sound. SSI always playbacks it as part of TDM 8ch Split mode. In this platform, it can handle stereo sound only on each daiX, and always works as TDM Split mode, never works as Basic Mode / TDM Ex-Split mode.  If DAI was dai0, dai1 only, it will be TDM Ex-Split mode. It depends on tdm-slots, I think.  if tdm-slots was 6ch... 	aplay -D plughw:0,0 xxx.wav (= will be 1ch, 2ch, 3ch, 4ch) 	aplay -D plughw:0,1 xxx.wav (= will be 5ch, 6ch)  if tdm-slots was 8ch... 	aplay -D plughw:0,0 xxx.wav (= will be 1ch, 2ch, 3ch, 4ch, 5ch, 6ch) 	aplay -D plughw:0,1 xxx.wav (= will be 7ch, 8ch)  something like this, is my understanding.    Thank you for understanding my idea. Nice to know   Best regards --- Kuninori Morimoto",technical,Kuninori Morimoto,kuninori.morimoto.gx@renesas.com,0,0,665,0.5949367088607594,0.9333333333333333,1,1.0,0.0,0.0,0.0
708,419207,419324,"I believe this hasn't addressed my questions in  Namely ""It is the more general idea that I am not really sure about. First of all. Does it make _any_ sense to randomize 4MB blocks by default? Why cannot we simply have it disabled? Then and more concerning question is, does it even make sense to have this randomization applied to higher orders than 0? Attacker might fragment the memory and keep recycling the lowest order and get the predictable behavior that we have right now.","On Wed 03-10-18 19:15:18, Dan Williams wrote:  I believe this hasn't addressed my questions in http://lkml.kernel.org/r/20181002143015.GX18290@dhcp22.suse.cz. Namely  It is the more general idea that I am not really sure about. First of all. Does it make _any_ sense to randomize 4MB blocks by default? Why cannot we simply have it disabled? Then and more concerning question is, does it even make sense to have this randomization applied to higher orders than 0? Attacker might fragment the memory and keep recycling the lowest order and get the predictable behavior that we have right now. ""  --  Michal Hocko SUSE Labs""",technical,Michal Hocko,mhocko@kernel.org,1,0,481,0.18095238095238095,0.2777777777777778,0,0.0,1.0,0.0,0.0
709,419207,419333,This is the biggest portion of the series and I am wondering why do we need it at all. Why it isn't sufficient to rely on the patch 3 here? Pages freed from the bootmem allocator go via the same path so they might be shuffled at that time. Or is there any problem with that? Not enough entropy at the time when this is called or the final result is s not randomized enough (some numbers would be helpful).,"On Wed 03-10-18 19:15:24, Dan Williams wrote:  This is the biggest portion of the series and I am wondering why do we need it at all. Why it isn't sufficient to rely on the patch 3 here? Pages freed from the bootmem allocator go via the same path so they might be shuffled at that time. Or is there any problem with that? Not enough entropy at the time when this is called or the final result is not randomized enough (some numbers would be helpful). --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,405,0.1676190476190476,0.3333333333333333,0,0.0,1.0,0.0,0.0
710,419207,419896,"I'm not aware of any CVE that this would directly preclude, but that said the entropy injected at 4MB boundaries raises the bar on heap attacks. Environments that want more can adjust that with the boot parameter. Given the potential benefits I think it would only make sense to default disable it if there was a significant runtime impact, from what I have seen there isn't. Certainly I expect there are attacks that can operate within a 4MB window, as I expect there are attacks that could operate within a 4K window that would need sub-page randomization to deter. In fact I believe that is the motivation for CONFIG_SLAB_FREELIST_RANDOM.Combining that with page allocator randomization makes the kernel less predictable.Is that enough justification for this patch on its own? It's debatable. Combine that though with the wider availability of platforms with memory-side-cache and I think it's a reasonable default behavior for the kernel to deploy.","Hi Michal,  On Thu, Oct 4, 2018 at 12:53 AM Michal Hocko <mhocko@kernel.org> wrote:  I'm not aware of any CVE that this would directly preclude, but that said the entropy injected at 4MB boundaries raises the bar on heap attacks. Environments that want more can adjust that with the boot parameter. Given the potential benefits I think it would only make sense to default disable it if there was a significant runtime impact, from what I have seen there isn't.   Certainly I expect there are attacks that can operate within a 4MB window, as I expect there are attacks that could operate within a 4K window that would need sub-page randomization to deter. In fact I believe that is the motivation for CONFIG_SLAB_FREELIST_RANDOM. Combining that with page allocator randomization makes the kernel less predictable.  Is that enough justification for this patch on its own? It's debatable. Combine that though with the wider availability of platforms with memory-side-cache and I think it's a reasonable default behavior for the kernel to deploy.",technical,Dan Williams,dan.j.williams@intel.com,1,1,952,0.32,0.3888888888888889,0,0.0,0.9285714285714286,0.0,0.0
711,419207,419916,"In fact we started with only patch3 and it had no measurable impact on the cache conflict rate. So the reason front-back randomization is not enough is due to the in-order initial freeing of pages. At the start of that process putting page1 in front or behind page0 still keeps them close together, page2 is still near page1 and has a high chance of being adjacent. As more pages are added ordering diversity improves, but there is still high page locality for the low address pages and this leads to no significant impact to the cache conflict rate. Patch3 is enough to keep the entropy sustained over time, but it's not enough initially.","On Thu, Oct 4, 2018 at 12:48 AM Michal Hocko <mhocko@kernel.org> wrote:  In fact we started with only patch3 and it had no measurable impact on the cache conflict rate.   So the reason front-back randomization is not enough is due to the in-order initial freeing of pages. At the start of that process putting page1 in front or behind page0 still keeps them close together, page2 is still near page1 and has a high chance of being adjacent. As more pages are added ordering diversity improves, but there is still high page locality for the low address pages and this leads to no significant impact to the cache conflict rate. Patch3 is enough to keep the entropy sustained over time, but it's not enough initially.",technical,Dan Williams,dan.j.williams@intel.com,1,1,639,0.23238095238095238,0.4444444444444444,0,0.0,0.9285714285714286,0.0,0.14285714285714285
712,419207,421458,Does the above address your concerns? v4.20 is perhaps the last upstream kernel release in advance of wider hardware availability.,"On Thu, Oct 4, 2018 at 9:44 AM Dan Williams <dan.j.williams@intel.com> wrote:  Hi Michal,  Does the above address your concerns? v4.20 is perhaps the last upstream kernel release in advance of wider hardware availability.",technical,Dan Williams,dan.j.williams@intel.com,1,1,130,0.0419047619047619,0.5,0,0.14285714285714285,0.7857142857142857,0.14285714285714285,0.14285714285714285
713,419207,423810,That should be in the changelog IMHO.,"On Thu 04-10-18 09:51:37, Dan Williams wrote:  That should be in the changelog IMHO.  --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,37,0.015238095238095238,0.5555555555555556,0,0.35714285714285715,0.6428571428571429,0.14285714285714285,0.0
714,419207,423883,"I am sorry but this hasn't explained anything (at least to me). I can still see a way to bypass this randomization by fragmenting the memory. With that possibility in place this doesn't really provide the promised additional security. So either I am missing something or the per-order threshold is simply a wrong interface to a broken security misfeature. I do not think so from what I have heard so far.OK, this sounds a bit more interesting. I am going to speculate because memory-side-cache is way too generic of a term for me to imagine anything specific. Many years back while at a university I was playing with page coloring as a method to reach a more stable performance results due to reduced cache conflicts. It was not always a performance gain but it definitely allowed for more stable run-to-run comparable results. I can imagine that a randomization might lead to a similar effect although I am not sure how much and it would be more interesting to hear about that effect. If this is really the case then I would assume on/off knob to control the randomization without something as specific as order.","On Thu 04-10-18 09:44:35, Dan Williams wrote:  I am sorry but this hasn't explained anything (at least to me). I can still see a way to bypass this randomization by fragmenting the memory. With that possibility in place this doesn't really provide the promissed additional security. So either I am missing something or the per-order threshold is simply a wrong interface to a broken security misfeature.   I do not think so from what I have heard so far.   OK, this sounds a bit more interesting. I am going to speculate because memory-side-cache is way too generic of a term for me to imagine anything specific. Many years back while at a university I was playing with page coloring as a method to reach a more stable performance results due to reduced cache conflicts. It was not always a performance gain but it definitely allowed for more stable run-to-run comparable results. I can imagine that a randomization might lead to a similar effect although I am not sure how much and it would be more interesting to hear about that effect. If this is really the case then I would assume on/off knob to control the randomization without something as specific as order. --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,1113,0.3980952380952381,0.6111111111111112,0,0.35714285714285715,0.6428571428571429,0.0,0.0
715,419207,424350,"I think a similar argument can be made against CONFIG_SLAB_FREELIST_RANDOM the randomization benefits can be defeated with more effort, and more effort is the entire point. I'm missing what bar you are judging the criteria for these patches, my bar is increased protection against allocation ordering attacks as seconded by Kees, and the memory side caching effects. That said I don't have a known CVE in my mind that would be mitigated by 4MB page shuffling. No need to imagine, a memory side cache shipped on a previous product as Robert linked in his comments. Cache coloring is effective up until your workload no longer fits in that color. Randomization helps to attenuate the cache conflict rate when that happens. For workloads that may fit in the cache, and/or environments that need more explicit cache control we have the recent changes to numa_emulation [1] to arrange for cache sized numa nodes. Are we only debating the enabling knob at this point? I'm not opposed to changing that, but I do think we want to keep the rest of the infrastructure to allow for shuffling on a variable page size boundary in case there is enhanced security benefits at smaller buddy-page sizes.","On Tue, Oct 9, 2018 at 4:28 AM Michal Hocko <mhocko@kernel.org> wrote:  I think a similar argument can be made against CONFIG_SLAB_FREELIST_RANDOM the randomization benefits can be defeated with more effort, and more effort is the entire point.   I'm missing what bar you are judging the criteria for these patches, my bar is increased protection against allocation ordering attacks as seconded by Kees, and the memory side caching effects. That said I don't have a known CVE in my mind that would be mitigated by 4MB page shuffling.   No need to imagine, a memory side cache shipped on a previous product as Robert linked in his comments.   Cache coloring is effective up until your workload no longer fits in that color. Randomization helps to attenuate the cache conflict rate when that happens. For workloads that may fit in the cache, and/or environments that need more explicit cache control we have the recent changes to numa_emulation [1] to arrange for cache sized numa nodes.   Are we only debating the enabling knob at this point? I'm not opposed to changing that, but I do think we want to keep the rest of the infrastructure to allow for shuffling on a variable page size boundary in case there is enhanced security benefits at smaller buddy-page sizes.  [1]: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=cc9aec03e58f",technical,Dan Williams,dan.j.williams@intel.com,1,1,1186,0.42095238095238097,0.6666666666666666,0,0.35714285714285715,0.5714285714285714,0.0,0.0
716,419207,424352,"Fair enough, I'll fold that in when I rebase on top of -next.","On Tue, Oct 9, 2018 at 4:16 AM Michal Hocko <mhocko@kernel.org> wrote: [..]  Fair enough, I'll fold that in when I rebase on top of -next.",technical,Dan Williams,dan.j.williams@intel.com,1,1,61,0.030476190476190476,0.7222222222222222,0,0.35714285714285715,0.5714285714285714,0.0,0.0
717,419207,424792,"If there is relatively simple way to achieve that (which I dunno about the slab free list randomization because I am not familiar with the implementation) then the feature is indeed questionable. I would understand an argument about feasibility if bypassing was extremely hard but fragmenting the memory is relatively a simple task. As said above, if it is quite easy to bypass the randomization then calling and advertizing this as a security feature is a dubious. Not enough to outright nak it of course but also not something I would put my stamp on. And arguments would be much more solid if they were backed by some numbers (not only for the security aspect but also the side caching effects).Could you make this a part of the changelog? I would really appreciate  to see justification based on actual numbers rather than quite hand wavy ""it helps"". Yes, that was my observation back then more or less. But even when you do not fit into the cache a color aware strategy (I was playing with bin hoping as well) produced a more deterministic/stable results. But that is just a side note as it doesn't directly relate to your change. I can imagine that. Do we have any numbers to actually back that claim though? Could you point me to some more documentation. My google-fu is failing me and ""5.2.27.5 Memory Side Cache Information Structure"" doesn't point to anything official (except for your patch referencing it).I am still trying to understand the benefit of this change. If the caching effects are actually the most important part and there is a reasonable cut in allocation order to keep the randomization effective during the runtime then I would like to understand the thinking behind that. In other words does the randomization at smaller orders than biggest order still visible in actual benchmarks? If not then on/off knob should be sufficient with potential auto tuning based on actual HW rather than to expect poor admin to google for RANDOM_ORDER to use on a specific HW and all the potential cargo cult that will grow around it. As I've said before, I am not convinced about the security argument but even if I am wrong here then I am still quite sure that you do not want to expose the security aspect as ""chose an order to randomize from ""because admins will have no real way to know what is the RANDOM_ORDER to set. So even then it should be on/off thing. You are going to pay some of the performance because you would lose some page allocator optimizations (e.g. pcp lists) but that is unavoidable AFAICS. With all that being said, I think the overall idea makes sense but you should try much harder to explain _why_ we need it and back your justification by actual _data_ before I would consider my ack.","On Tue 09-10-18 10:34:55, Dan Williams wrote:  If there is relatively simple way to achieve that (which I dunno about the slab free list randomization because I am not familiar with the implementation) then the feature is indeed questionable. I would understand an argument about feasibility if bypassing was extremely hard but fragmenting the memory is relatively a simple task.   As said above, if it is quite easy to bypass the randomization then calling and advertizing this as a security feature is a dubious. Not enough to ouright nak it of course but also not something I would put my stamp on. And arguments would be much more solid if they were backed by some numbers (not only for the security aspect but also the side caching effects).   Could you make this a part of the changelog? I would really appreciate to see justification based on actual numbers rather than quite hand wavy it helps"".   Yes, that was my observation back then more or less. But even when you do not fit into the cache a color aware strategy (I was playing with bin hoping as well) produced a more deterministic/stable results. But that is just a side note as it doesn't directly relate to your change.   I can imagine that. Do we have any numbers to actually back that claim though?   Could you point me to some more documentation. My google-fu is failing me and ""5.2.27.5 Memory Side Cache Information Structure"" doesn't point to anything official (except for your patch referencing it).   I am still trying to understand the benefit of this change. If the caching effects are actually the most important part and there is a reasonable cut in allocation order to keep the randomization effective during the runtime then I would like to understand the thinking behind that. In other words does the randomization at smaller orders than biggest order still visible in actual benchmarks? If not then on/off knob should be sufficient with potential auto tuning based on actual HW rather than to expect poor admin to google for $RANDOM_ORDER to use on a specific HW and all the potential cargo cult that will grow around it.  As I've said before, I am not convinced about the security argument but even if I am wrong here then I am still quite sure that you do not want to expose the security aspect as ""chose an order to randomize from"" because admins will have no real way to know what is the $RANDOM_ORDER to set. So even then it should be on/off thing. You are going to pay some of the performance because you would lose some page allocator optimizations (e.g. pcp lists) but that is unavoidable AFAICS.  With all that being said, I think the overal idea makes sense but you should try much harder to explain _why_ we need it and back your justification by actual _data_ before I would consider my ack.   --  Michal Hocko SUSE Labs""",technical,Michal Hocko,mhocko@kernel.org,1,0,2725,1.0,0.7777777777777778,0,0.42857142857142855,0.5714285714285714,0.0,0.0
718,419207,426337,"In fact you don't even need to fragment since you'll have 4MB contiguous targets by default, but that's not the point. We'll now have more entropy in the allocation order to compliment the entropy introduced at the per-SLAB level with CONFIG_SLAB_FREELIST_RANDOM....and now that I've made that argument I think I've come around to your point about the shuffle_page_order parameter. The only entity that might have a better clue about ""safer"" shuffle orders than MAX_ORDER is the distribution provider. I'll cut a v4 to move all of this under a configuration symbol and make the shuffle order a compile time setting. I put in the changelog that these patches reduced the cache conflict rate by 2.5X on a Java benchmark. I specifically did not put KNL data directly into the changelog because that is not a general purpose server platform. Note, you can also think about this just on pure architecture terms.I.e. that for a direct mapped cache anywhere in a system you can have a near zero cache conflict rate on a first run of a workload and high conflict rate on a second run based on how lucky you are with memory allocation placement relative to the first run. Randomization keeps you out of such performance troughs and provides more reliable average performance.  With the numa emulation patch I referenced an administrator could constrain a workload to run in a cache-sized subset of the available memory if they really know what they are doing and need firmer guarantees. The risk if Linux does not have this capability is unstable hacks like zone sort and rebooting, as referenced in that KNL article, which are not suitable for a general purpose kernel / platform. Yes, 2.5X cache conflict rate reduction, in the change log. So, I've come around to your viewpoint on this. Especially when we have CONFIG_SLAB_FREELIST_RANDOM the security benefit of smaller than MAX_ORDER shuffling is hard to justify and likely does not need kernel parameter based control. I don't have a known CVE, I only have the ack of people more knowledgeable about security than myself like Kees to say in effect, ""yes, this complicates attacks"". If you won't take Kees' word for it, I'm not sure what other justification I can present on the security aspect.2.5X cache conflict reduction on a Java benchmark workload that the exceeds the cache size by multiple factors is the data I can provide today. Post launch it becomes easier to share more precise data, but that's post 4.20. The hope of course is to have this capability available in an upstream released kernel in advance of wider hardware availability.","On Wed, Oct 10, 2018 at 1:48 AM Michal Hocko <mhocko@kernel.org> wrote:  In fact you don't even need to fragment since you'll have 4MB contiguous targets by default, but that's not the point. We'll now have more entropy in the allocation order to compliment the entropy introduced at the per-SLAB level with CONFIG_SLAB_FREELIST_RANDOM.  ...and now that I've made that argument I think I've come around to your point about the shuffle_page_order parameter. The only entity that might have a better clue about safer"" shuffle orders than MAX_ORDER is the distribution provider. I'll cut a v4 to move all of this under a configuration symbol and make the shuffle order a compile time setting.   I put in the changelog that these patches reduced the cache conflict rate by 2.5X on a Java benchmark. I specifically did not put KNL data directly into the changelog because that is not a general purpose server platform.  Note, you can also think about this just on pure architecture terms. I.e. that for a direct mapped cache anywhere in a system you can have a near zero cache conflict rate on a first run of a workload and high conflict rate on a second run based on how lucky you are with memory allocation placement relative to the first run. Randomization keeps you out of such performance troughs and provides more reliable average performance.  With the numa emulation patch I referenced an administrator could constrain a workload to run in a cache-sized subset of the available memory if they really know what they are doing and need firmer guarantees.  The risk if Linux does not have this capability is unstable hacks like zonesort and rebooting, as referenced in that KNL article, which are not suitable for a general purpose kernel / platform.   Yes, 2.5X cache conflict rate reduction, in the change log.   http://www.uefi.org/sites/default/files/resources/ACPI%206_2_A_Sept29.pdf   So, I've come around to your viewpoint on this. Especially when we have CONFIG_SLAB_FREELIST_RANDOM the security benefit of smaller than MAX_ORDER shuffling is hard to justify and likely does not need kernel parameter based control.   I don't have a known CVE, I only have the ack of people more knowledgeable about security than myself like Kees to say in effect, ""yes, this complicates attacks"". If you won't take Kees' word for it, I'm not sure what other justification I can present on the security aspect.  2.5X cache conflict reduction on a Java benchmark workload that the exceeds the cache size by multiple factors is the data I can provide today. Post launch it becomes easier to share more precise data, but that's post 4.20. The hope of course is to have this capability available in an upstream released kernel in advance of wider hardware availability.""",technical,Dan Williams,dan.j.williams@intel.com,1,1,2594,0.9257142857142857,0.8333333333333334,0,0.42857142857142855,0.5,0.0,0.0
719,419207,426703,"And how is somebody providing a kernel for large variety of workloads supposed to know?[...]I am not disagreeing here. That reliable average might be worse than what you get with the non-randomized case. And that might be a fair deal for some workloads. You are, however, providing a functionality which is enabled by default without any actual numbers (well except for_a_java_ workload that seems to benefit) so you should really do your homework stop hand waving and give us some numbers and/or convincing arguments please. Then mention how and what you can achieve by that in the changelog. We could have lived without those for quite some time so this doesn't seem to be anything super urgent to push through without a proper justification. Which is a single benchmark result which is not even described in detail to be able to reproduce that measurement. I am sorry for nagging here but I would expect something less obscure. How does this behave for usual workloads that we test cache sensitive workloads. I myself am not a benchmark person but I am pretty sure there are people who can help  you to find proper ones to run and evaluate. Thanks! In general (nothing against Kees here of course), I prefer a stronger justification than ""somebody said it will make attacks harder"". At least my concern about fragmented memory which is not really hard to achieve at all should be reasonably clarified. I am fully aware there is no absolute measure here but making something harder under ideal conditions doesn't really help for common attack strategies which can prepare the system into an actual state to exploit allocation predictability. I amno expert here but if an attacker can deduce the allocation pattern then fragmenting the memory is one easy step to overcome what people would consider a security measure. So color me unconvinced for now. I will not comment on timing but in general, any performance related changes should come with numbers for a wider variety of workloads. In any case, I believe the change itself is not controversial as long it is opt-in (potentially autotuned based on specific HW) with a reasonable API. And no I do not consider RANDOM_ORDER a good interface.","On Wed 10-10-18 17:13:14, Dan Williams wrote: [...]  And how is somebody providing a kernel for large variety of workloads supposed to know?  [...]   I am not disagreeing here. That reliable average might be worse than what you get with the non-randomized case. And that might be a fair deal for some workloads. You are, however, providing a functionality which is enabled by default without any actual numbers (well except for _a_java_ workload that seems to benefit) so you should really do your homework stop handwaving and give us some numbers and/or convincing arguments please.   Then mention how and what you can achieve by that in the changelog.   We could have lived without those for quite some time so this doesn't seem to be anything super urgent to push through without a proper justification.   Which is a single benchmark result which is not even described in detail to be able to reproduce that measurement. I am sorry for nagging here but I would expect something less obscure. How does this behave for usual workloads that we test cache sensitive workloads. I myself am not a benchmark person but I am pretty sure there are people who can help you to find proper ones to run and evaluate.   Thanks!  [...]   In general (nothing against Kees here of course), I prefer a stronger justification than somebody said it will make attacks harder"". At least my concern about fragmented memory which is not really hard to achieve at all should be reasonably clarified. I am fully aware there is no absolute measure here but making something harder under ideal conditions doesn't really help for common attack strategies which can prepare the system into an actual state to exploit allocation predictability. I am no expert here but if an attacker can deduce the allocation pattern then fragmenting the memory is one easy step to overcome what people would consider a security measure.  So color me unconvinced for now.   I will not comment on timing but in general, any performance related changes should come with numbers for a wider variety of workloads.  In any case, I believe the change itself is not controversial as long it is opt-in (potentially autotuned based on specific HW) with a reasonable API. And no I do not consider $RANDOM_ORDER a good interface. --  Michal Hocko SUSE Labs""",technical,Michal Hocko,mhocko@kernel.org,1,0,2195,0.780952380952381,0.8888888888888888,0,0.5,0.5,0.0,0.0
720,419207,427307,"True, this would be a much easier discussion with a wider / deeper data set. The latest version of the patches no longer enable it by default. I'm giving you the data I can give with respect to pre-production hardware.The numa_emulation aspect is orthogonal to the randomization implementation. It does not belong in the randomization changelog. We lived without them previously because memory-side-caches were limited to niche hardware, now this is moving into general purpose server platforms and the urgency / impact goes up accordingly. No need to apologize. I wouldn't pick benchmarks that are cpu-cache sensitive since those are small number of MBs in size, a memory-side cache is on the order of 10s of GBs.Another way to attack heap randomization without fragmentation is to just perform heap spraying and hope that lands the data the attacker needs in the right place. I still think that allocation entropy > 0 is positive benefit, but I don't know how to determine the curve of security benefit relative to shuffle order. That's fair. Do you mean disable shuffling on systems that don't have a memory-side-cache unless / until we can devise a security benefit curve relative to shuffle-order? The former I can do, the latter, I'm  at a loss. I think the current v4 proposal of compile-time setting is reasonable once we have consensus / guidance on the default shuffle-order.","On Thu, Oct 11, 2018 at 4:56 AM Michal Hocko <mhocko@kernel.org> wrote:  True, this would be a much easier discussion with a wider / deeper data set.   The latest version of the patches no longer enable it by default. I'm giving you the data I can give with respect to pre-production hardware.   The numa_emulation aspect is orthogonal to the randomization implementation. It does not belong in the randomization changelog.   We lived without them previously because memory-side-caches were limited to niche hardware, now this is moving into general purpose server platforms and the urgency / impact goes up accordingly.   No need to apologize.   I wouldn't pick benchmarks that are cpu-cache sensitive since those are small number of MBs in size, a memory-side cache is on the order of 10s of GBs.   Another way to attack heap randomization without fragmentation is to just perform heap spraying and hope that lands the data the attacker needs in the right place. I still think that allocation entropy > 0 is positive benefit, but I don't know how to determine the curve of security benefit relative to shuffle order.   That's fair.   Do you mean disable shuffling on systems that don't have a memory-side-cache unless / until we can devise a security benefit curve relative to shuffle-order? The former I can do, the latter, I'm at a loss.   I think the current v4 proposal of compile-time setting is reasonable once we have consensus / guidance on the default shuffle-order.",technical,Dan Williams,dan.j.williams@intel.com,1,1,1385,0.4819047619047619,0.9444444444444444,0,0.5,0.42857142857142855,0.0,0.42857142857142855
721,419207,433489,"Yes, enable when the HW requires that for whatever reason and make add a global knob to enable it for those that might find it useful for security reasons with a clear cost/benefit description. Not ""this is the security thingy enable and feel safe(r)""","On Thu 11-10-18 11:03:07, Dan Williams wrote: [...]  Yes, enable when the HW requires that for whatever reason and make add a global knob to enable it for those that might find it useful for security reasons with a clear cost/benefit description. Not this is tha security thingy enable and feel safe(r)"" --  Michal Hocko SUSE Labs""",technical,Michal Hocko,mhocko@kernel.org,1,0,251,0.09714285714285714,1.0,1,1.0,0.0,0.42857142857142855,0.0
722,419845,440490,"Hello, Can anyone please confirm this bug and apply the patch? Thanks!","Hello,  Can anyone please confirm this bug and apply the patch? Thanks!  Wenwen  On Thu, Oct 4, 2018 at 11:00 AM Wenwen Wang <wang6495@umn.edu> wrote:",technical,Wenwen Wang,wang6495@umn.edu,0,1,70,1.0,1.0,1,1.0,0.0,1.0,0.0
723,421365,421381,"I assume that this co-processor only deals with the routing itself, and doesn't need to be talked to during interrupt processing, right? I don't really see the point of making this user-selectable. If you're compiling support for a given platform, this platform configuration fragment should itself select the necessary dependencies for the system to work as expected. Here, you are leaving the choice to the user, with a 50% chance of getting a system that doesn't boot...nit: s/(HWIRQ)/(hwirq)/g Oh great. So this is reinventing the GICv3 ITS, only for SPIs. :-(Now, this structure seems completely useless, see below. Maybe it would make sense to have a macro that hides this:              *hwirq = FWSPEC_TO_HWIRQ(fwspec),This looks horrible. Why doesn't your firmware interface have a helper functions that hides this? Something like: ,and you could even add some error checking. And put this where it belongs (in the helper function).I don't think this structure serves any purpose. src_id and src_index are just a decomposition of hwirq. dst_irq is the GIC interrupt, which is stored... by the GIC driver. Also, it is worth realising that you're allocating per-interrupt data, but none of the per-interrupt callbacks are using it. In my book, that's a sure sign that this structure is pointless. Am I missing anything here? Same remarks about the horrible interface. Please address this. But it also worth realising that this code will never be called with nr_irqs!=1 (that's only for things like PCIMulti-MSI).Do you expect other drivers to require similar resource request? If so, It might be worth getting the firmware interface to do that work. Specially the ""give me my SCI"" part.","Hi Lokesh,  On Sat, 06 Oct 2018 08:28:12 +0100, Lokesh Vutla <lokeshvutla@ti.com> wrote:  I assume that this co-processor only deals with the routing itself, and doesn't need to be talked to during interrupt processing, right?   I don't really see the point of making this user-selectable. If you're compiling support for a given platform, this platform configuration fragment should itself select the necessary dependencies for the system to work as expected. Here, you are leaving the choice to the user, with a 50% chance of getting a system that doesn't boot...   nit: s/(HWIRQ)/(hwirq)/g   Oh great. So this is reinventing the GICv3 ITS, only for SPIs. :-(  Now, this structure seems completely useless, see below.   Maybe it would make sense to have a macro that hides this:        	       *hwirq = FWSPEC_TO_HWIRQ(fwspec),   This looks horrible. Why doesn't your firmware interface have a helper functions that hides this? Something like:  	ti_sci_free_direct_irq(intr, src_id, src_index, dst_irq),  and you could even add some error checking.   And put this where it belongs (in the helper function).   I don't think this structure serves any purpose. src_id and src_index are just a decomposition of hwirq. dst_irq is the GIC interrupt, which is stored... by the GIC driver. Also, it is worth realising that you're allocating per-interrupt data, but none of the per-interrupt callbacks are using it. In my book, that's a sure sign that this structure is pointless.  Am I missing anything here?   Same remarks about the horrible interface.   Please address this. But it also worth realising that this code will never be called with nr_irqs!=1 (that's only for things like PCI Multi-MSI).   Do you expect other drivers to require similar resource request? If so, It might be worth getting the firmware interface to do that work. Specially the give me my SCI"" part.   Thanks,  	M.  --  Jazz is not dead, it just smell funny.""",technical,Marc Zyngier,marc.zyngier@arm.com,1,0,1692,1.0,0.2857142857142857,0,0.0,1.0,0.0,0.0
724,421365,421383,"I would drop the GIC here, and replace it by ""parent interrupt controller"", as nothing here is GIC specific. Are all trigger types supported? Why that constraint? From what I can see, the two are fairly independent, and the constraint looks more of a Linux driver issue than a DT constraint.","On Sat, 06 Oct 2018 08:28:11 +0100, Lokesh Vutla <lokeshvutla@ti.com> wrote:  I would drop the GIC here, and replace it by parent interrupt controller"", as nothing here is GIC specific.   Are all trigger types supported?   Why that constraint? From what I can see, the two are fairly independent, and the constraint looks more of a Linux driver issue than a DT constraint.   Thanks,  	M.  --  Jazz is not dead, it just smell funny.""",technical,Marc Zyngier,marc.zyngier@arm.com,1,0,291,0.17579250720461095,0.35714285714285715,0,0.0,1.0,0.0,0.0
725,421365,421449,"Rob, DT maintainers, I'd like a feedback from DT maintainers on this 'range' topic.TISCI Firmware [1] currently seems to define a type corresponding to a device ID[2]. in AM6 device, for example, this is different, however have a 1 to 1 correspondence. However, there is expectation that type will end up as device ID in a future SoC.While this is subject to much debate internally, I'd like some feedback if this is OK from Device tree representation - it is true that Firmware does look at it as type, however in some future SoC, it could be that the values themselves may correspond one to one with a device id -> The original wish was that types might be something reusable across SoCs,but that is turning out to be more of a theoretical wish than any thing practical.","On 12:58-20181006, Lokesh Vutla wrote:  Rob, DT maintainers,  I'd like a feedback from DT maintainers on this 'range' topic.  TISCI Firmware [1] currently seems to define a type corresponding to a device ID[2]. in AM6 device, for example, this is different, however have a 1 to 1 correspondence. However, there is expectation that type will end up as device ID in a future SoC.  While this is subject to much debate internally, I'd like some feedback if this is OK from Device tree representation - it is true that Firmware does look at it as type, however in some future SoC, it could be that the values themselves may correspond one to one with a device id -> The original wish was that types might be something reusable across SoCs, but that is turning out to be more of a theoretical wish than any thing practical.  [1]http://software-dl.ti.com/tisci/esd/latest/5_soc_doc/am6x/resasg_types.html [2]http://software-dl.ti.com/tisci/esd/latest/5_soc_doc/am6x/devices.html --  Regards, Nishanth Menon",technical,Nishanth Menon,nm@ti.com,1,0,772,0.46397694524495675,0.42857142857142855,0,0.0,0.9090909090909091,0.0,0.09090909090909091
726,421365,422058,"Nope, only level interrupts are supported. Will fix it in v2.Driver when calling irq_domain_alloc_irqs_parent(), the fwspec node that gets passed assumes that parent is gic. parameters are filled in with such assumption. Do you suggest anything to make it more generic? Thanks a lot for the review. Also, I need a suggestion regarding one more interrupt controller(Interrupt Aggregator) on the same SoC controlled by TISCI_PROTOCOL.The Interrupt Aggregator (INTA) provides a centralized machine which handles the termination of system events to that they can be coherently processed by the host(s) in the system. Integration looks something similar  .Configuration of the Int map registers that maps global events to vint is done by a system controller (like the Device Memory and Security Controller on K3AM654 SoC). Driver should request the system controller to get the range of global events and vints assigned to the requesting host. Management of these requested resources should be handled by driver and requests system controller to map specific global event to vint, bit pair. There can be cases such that IRQ routes can involve both INTR and INTA like below: IP ---> INTA ---> INTR ----> GIC.In these cases TISCI involves only one message with parameters (source id, source offset, inta_id, dst id) for configuring IRQ route till the destination. Coprocessor will detect there is INTR in the IRQ path and configure that as well. Right now I kind of differentiated this scenario in INTA driver by passing a flag(TI_SCI_EVENT) to INTR driver. If such flag comes, INTR driver should avoid calling ti_sci api for configuring. Do you think this is the right direction or do you suggest a better solution. If I am not clear in the above description, I can post an RFC for INTA driver for continuing this discussion.","Hi Marc,  On Saturday 06 October 2018 03:32 PM, Marc Zyngier wrote:  Nope, only level interrupts are supported. Will fix it in v2.   Driver when calling irq_domain_alloc_irqs_parent(), the fwspec node that gets  passed assumes that parent is gic. parameters are filled in with such  assumption. Do you suggest anything to make it more generic?   Thanks a lot for the review. Also, I need a suggestion regarding one more  interrupt controller(Interrupt Aggregator) on the same SoC controlled by  TISCI_PROTOCOL.  The Interrupt Aggregator (INTA) provides a centralized machine which handles the termination of system events to that they can be coherently processed by the host(s) in the system. Integration looks  something similar https://pastebin.ubuntu.com/p/T32vbrwsch/ .  Configuration of the Intmap registers that maps global events to vint is done by a system controller (like the Device Memory and Security Controller on K3 AM654 SoC). Driver should request the system controller to get the range of global events and vints assigned to the requesting host. Management of these requested resources should be handled by driver and requests system controller to map specific global event to vint, bit pair.  There can be cases such that IRQ routes can involve both INTR and INTA like below: 	IP ---> INTA ---> INTR ----> GIC.  In these cases TISCI involves only one message with parametes(source id, source  offset, inta_id, dst id) for configuring IRQ route till the destination. Co  processor will detect there is INTR in the IRQ path and configure that as well.  Right now I kind of differentiated this scenario in INTA driver by passing a  flag(TI_SCI_EVENT) to INTR driver. If such flag comes, INTR driver should avoid  calling ti_sci api for configuring. Do you think this is the right direction or  do you suggest a better solution.  If I am not clear in the above description, I can post an RFC for INTA driver  for continuing this discussion.  Thanks and regards, Lokesh",technical,Lokesh Vutla,lokeshvutla@ti.com,0,1,1819,0.9884726224783862,0.5,0,0.18181818181818182,0.8181818181818182,0.09090909090909091,0.0
727,421365,422060,"Yes, that's right. There are 2 reasons why I made it tristate:- Not all interrupts go through this irqchip(At least in the AM6 SoC using this). Most of the legacy peripherals still are directly connected to GIC- TI_SCI_PROTOCOL is defined as tristate. If you still feel I should not make it user-selectable, I can drop it.okay.okay.All existing TISCI users follow the same convention, so I did not bother adding any such wrapper. Will update TISCI with these wrappers and see what firmware maintainer says.hmm..you are right, these 3 fields can be dropped completely. Will fix it in v2.I tried to consolidate sci resource part under devm_ti_sci_get_of_resource() api but dst-id is something that is used by irqchip driver. So couldn't consolidate it and had to get it from dt in the driver probe.","Hi Marc,  On 10/6/2018 3:25 PM, Marc Zyngier wrote:  Yes, that's right.   There are 2 reasons why I made it tristate: - Not all interrupts go through this irqchip(At least in the AM6 SoC using this). Most of the legacy peripherals still are directly connected to GIC - TI_SCI_PROTOCOL is defined as tristate.  If you still feel I should not make it user-selectable, I can drop it.   okay.   okay.   All existing TISCI users follow the same convention, so I did not bother adding  any such wrapper. Will update TISCI with these wrappers and see what firmware  maintainer says.   hmm..you are right, these 3 fields can be dropped completely.   will fix it in v2.   I tried to consolidate sci resource part under devm_ti_sci_get_of_resource() api  but dst-id is something that is used by irqchip driver. So couldn't consolidate  it and had to get it from dt in the driver probe.  Thanks and regards, Lokesh",technical,Lokesh Vutla,lokeshvutla@ti.com,0,1,796,0.4438040345821326,0.5714285714285714,0,0.18181818181818182,0.8181818181818182,0.0,0.0
728,421365,422278,"But as you said, these are ""legacy"" interrupts, and most of the interesting stuff is routed through the system controller. We also try not to have core interrupt controllers as modules. As for having the firmware interface as a module, I wonder what the use-case is. I really wonder what the added value is for the user. Frankly, exposing all kind of data structures to the world is a pretty poor form of abstraction, which is what the firmware is supposed to provide. I'd strongly suggest that include/linux/soc/ti/ti_sci_protocol.h gets cleaned up, and that the whole ti_sci_ops disappears from the that file. Nobody outside of the firmware *implementation* needs to know about its, and it would be much better served by a set of helpers. Finally, please make the TISCI interrupt management part of this series, so that I can review it as part of the code that uses it.","Hi Lokesh,  On 08/10/18 10:48, Lokesh Vutla wrote:  But as you said, these are legacy"" interrupts, and most of the  interesting stuff is routed through the system controller. We also try  not to have core interrupt controllers as modules. As for having the  firmware interface as a module, I wonder what the use-case is.   I really wonder what the added value is for the user.   Frankly, exposing all kind of data structures to the world is a pretty  poor form of abstraction, which is what the firmware is supposed to  provide.  I'd strongly suggest that include/linux/soc/ti/ti_sci_protocol.h gets  cleaned up, and that the whole ti_sci_ops disappears from the that file.  Nobody outside of the firmware *implementation* needs to know about its,  and it would be much better served by a set of helpers.  Finally, please make the TISCI interrupt management part of this series,  so that I can review it as part of the code that uses it.  Thanks,  	M. --  Jazz is not dead. It just smells funny...""",technical,Marc Zyngier,marc.zyngier@arm.com,1,0,871,0.49279538904899134,0.6428571428571429,0,0.18181818181818182,0.8181818181818182,0.0,0.0
729,421365,422340,"As I said, that's a Linux driver issue, not a DT specification at all. It is not worth it trying to generalize it in the driver implementation, but the DT spec it self should be generic enough. I'm sorry, but I really have no idea what the global events and the vints are. Maybe you should describe what this is all about, and maybe provide a pointer to some documentation...Frankly, it mostly indicates that the firmware does too much, and should be more flexible. That'd be preferable, IMO. Please provide definitions for all the above jargon, as well as pointers to publicly available documentation, if any.","On 08/10/18 10:46, Lokesh Vutla wrote:  As I said, that's a Linux driver issue, not a DT specification at all.  It is not worth it trying to generalize it in the driver implementation,  but the DT spec it self should be generic enough.   I'm sorry, but I really have no idea what the global events and the  vints are. Maybe you should describe what this is all about, and maybe  provide a pointer to some documentation...   Frankly, it mostly indicates that the firmware does too much, and should  be more flexible.   That'd be preferable, IMO. Please provide definitions for all the above  jargon, as well as pointers to publicly available documentation, if any.  Thanks,  	M. --  Jazz is not dead. It just smells funny...",technical,Marc Zyngier,marc.zyngier@arm.com,1,0,610,0.3659942363112392,0.7142857142857143,0,0.18181818181818182,0.7272727272727273,0.0,0.0
730,421365,422429,"okay, will not make it use configurable in v2.Sure, my next version will include TISCI interrupt management as well. Thanks and regards, Lokesh","On Monday 08 October 2018 06:30 PM, Marc Zyngier wrote:  okay, will not make it use configurable in v2.   Sure, my next version will include TISCI interrupt management as well.  Thanks and regards, Lokesh",technical,Lokesh Vutla,lokeshvutla@ti.com,0,1,143,0.07780979827089338,0.7857142857142857,0,0.18181818181818182,0.7272727272727273,0.0,0.0
731,421365,422526,"okay, will fix it in next version. Sorry I should have done that earlier. TRM is available here[1], Section 9.3 talks about Interrupt Router, Section 10.2.7 talks about Interrupt aggregator. Documentation for TISCI IRQ management is available here Sure, will try to post the consolidated series asap. Thanks a lot for the help.","On Monday 08 October 2018 07:25 PM, Marc Zyngier wrote:  okay, will fix it in next version.   Sorry I should have done that earlier. TRM is available here[1], Section 9.3  talks about Interrupt Router, Section 10.2.7 talks about Interrupt aggregator.  Documentation for TISCI IRQ management is available here[2].   [1] http://www.ti.com/lit/ug/spruid7a/spruid7a.pdf [2] http://downloads.ti.com/tisci/esd/latest/2_tisci_msgs/rm/rm_irq.html   Sure, will try to post the consolidated series asap. Thanks a lot for the help.  Regards, Lokesh",technical,Lokesh Vutla,lokeshvutla@ti.com,0,1,327,0.1873198847262248,0.8571428571428571,0,0.18181818181818182,0.7272727272727273,0.0,0.5454545454545454
732,421365,429538,"DT maintainers, Any help on this topic?","Hi Rob, DT maintainers,  On Saturday 06 October 2018 09:59 PM, Nishanth Menon wrote:  Any help on this topic?  Thanks and regards, Lokesh",technical,Lokesh Vutla,lokeshvutla@ti.com,0,1,39,0.025936599423631124,0.9285714285714286,0,0.8181818181818182,0.18181818181818182,0.5454545454545454,0.18181818181818182
733,421365,432395,"I'm not sure I follow all the terminology here of type, subtype, ""dsthost irq"", etc. It looks to me like you should be using interrupt-map property. Rob","On Sat, Oct 06, 2018 at 11:29:29AM -0500, Nishanth Menon wrote:  I'm not sure I follow all the terminology here of type, subtype, dst  host irq"", etc. It looks to me like you should be using interrupt-map property.  Rob """,technical,Rob Herring,robh@kernel.org,1,0,152,0.10086455331412104,1.0,1,1.0,0.0,0.18181818181818182,0.0
734,422694,428584,"I agree with you in principle Peter and have tweaked the patch description to make it clearer that we are doing this to make GCC static analysis more helpful (suppressing a false warning is a worthwhile if you are dealing with lots of them).However, nice though it is to have elegant comment structure I think we should still have this patch in place.  This effort to 'fix' these warnings has already identified a few places where it was wrong so I'm keen to see it applied by default even if it isn't perfect.","On Mon, 8 Oct 2018 20:42:41 +0000 Peter Rosin <peda@axentia.se> wrote:  I agree with you in principle Peter and have tweaked the patch description to make it clearer that we are doing this to make GCC static analysis more helpful (suppressing a false warning is a worthwhile if you are dealing with lots of them).  However, nice though it is to have elegant comment structure I think we should still have this patch in place.  This effort to 'fix' these warnings has already identified a few places where it was wrong so I'm keen to see it applied by default even if it isn't perfect.  Jonathan",technical,Jonathan Cameron,jic23@kernel.org,1,0,510,1.0,0.36363636363636365,0,0.5,0.375,0.5,0.0
735,422694,428602,"Thanks. Below are some examples of cases in which the fall-through warning turned out to be an actual bug So, yeah. This effort is worth it.","On 10/13/18 2:38 PM, Jonathan Cameron wrote:  Thanks, Jonathan. Below are some examples of cases in which the fall-through warning turned out to be an actual bug:  commit c24bfa8f21b59283580043dada19a6e943b6e426 commit ad0eaee6195db1db1749dd46b9e6f4466793d178 commit 9ba8376ce1e2cbf4ce44f7e4bee1d0648e10d594 commit dc586a60a11d0260308db1bebe788ad8973e2729 commit a8e9b186f153a44690ad0363a56716e7077ad28c commit 4e57562b4846e42cd1c2e556f0ece18c1154e116 commit 7c92e5fbf4dac0dd4dd41a0383adc54f16f403e2 commit c5b974bee9d2ceae4c441ae5a01e498c2674e100 commit 2c930e3d0aed1505e86e0928d323df5027817740 commit 882518debc8487147d618d5f26f4bb0bea1cc05b commit f745e9cc7e40c4570ab5e8d5ef32bfaa6e8ced46 commit 5dc874252faa818426480a7c00fa05738fe05402 commit 4a00aa057759d713e1296ecbc614fa560d569977 commit 6d3f06a0042ebd59a5e9d4ba6e8a85596901e140 commit 827d240a232d27cc12e9657d012f2e5ba953e98a commit a28b259b43914b04746184cec318c67bded7234c commit 9e7b319e1d1e6cba41ae96f791789a7806b29584 commit d393be3ed0bebb30a4666d7f5ed4486cd6b31716 commit 680682d4d537565e2c358483e1feeca30a8cf3d4 commit 06af9b0f4949b85b20107e6d75f5eba15111d220  So, yeah. This effort is worth it.  Thanks -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,140,0.29411764705882354,0.45454545454545453,0,0.5,0.375,0.0,0.0
736,422694,429195,Done the first of the above…,"On Sat, 13 Oct 2018 15:14:34 +0000 Peter Rosin <peda@axentia.se> wrote:   Done the first of the above...  Thanks,  Jonathan",technical,Jonathan Cameron,jic23@kernel.org,1,0,28,0.058823529411764705,0.7272727272727273,0,0.625,0.25,0.125,0.125
737,422694,432016,"Indeed. I meant to respond earlier, but then forgot... Thank you!","On 2018-10-16 13:01, Gustavo A. R. Silva wrote:    Indeed. I meant to respond earlier, but then forgot... Thank you!    Cheers,  Peter",technical,Peter Rosin,peda@axentia.se,1,0,65,0.14705882352941177,0.9090909090909091,1,1.0,0.0,0.0,0.0
738,424089,424727,"x86_64 is our case, I should have documented it more clearly. Right, as you said, this is easy to fix. I found recent discussion about why x86-64 is using generic string function here:","On Wed, Oct 10, 2018 at 1:13 AM Andrew Morton <akpm@linux-foundation.org> wrote: x86_64 is our case, I should have documented it more clearly. Right, as you said, this is easy to fix. I found recent discussion about why x86-64 is using generic string function here: +cc John, x86 https://lkml.org/lkml/2018/10/3/818   --  Jack Wang Linux Kernel Developer  ProfitBricks GmbH Greifswalder Str. 207 D - 10405 Berlin  Tel:       +49 30 577 008  042 Fax:      +49 30 577 008 299 Email:    jinpu.wang@profitbricks.com URL:      https://www.profitbricks.de  Sitz der Gesellschaft: Berlin Registergericht: Amtsgericht Charlottenburg, HRB 125506 B Geschäftsführer: Achim Weiss, Matthias Steinberg, Christoph Steffens",technical,Jinpu Wang,jinpu.wang@profitbricks.com,0,0,184,1.0,1.0,1,0.0,0.0,0.0,0.0
739,431048,431876,"Repeating my comment on version 1:My understanding of the concern behind this change is that we should be able to use an email address for the current development practices, such as Reported-by, Suggested-by, etc tags when the email address was provided in what is a public space for the project.  The public space is visible to anyone in the world who desires to access  do not understand how ""ordinarily collected by the project"" is equivalent to ""an email address that was provided in a public space for the project"". Ordinarily collected could include activities that can be expected to be private and not visible to any arbitrary person in the world. My issue is with the word choice.  I agree with the underlying concept.","On 10/16/18 07:58, James Bottomley wrote:  Repeating my comment on version 1:  My understanding of the concern behind this change is that we should be able to use an email address for the current development practices, such as Reported-by, Suggested-by, etc tags when the email address was provided in what is a public space for the project.  The public space is visible to anyone in the world who desires to access it.  I do not understand how ordinarily collected by the project"" is equivalent to ""an email address that was provided in a public space for the project"". Ordinarily collected could include activities that can be expected to be private and not visible to any arbitrary person in the world.  My issue is with the word choice.  I agree with the underlying concept.  -Frank""",technical,Frank Rowand,frowand.list@gmail.com,1,0,727,0.3942857142857143,0.2631578947368421,0,0.0,0.75,0.0,0.0
740,431048,431883,"I don't think it is ... or should be.  This section is specifically enumerating unacceptable behaviours.  The carve out ""email address not ordinarily collected by the project"" means that adding someone's email address in a tag isn't immediately sanctionable in the code of conduct as unacceptable behaviour if a question about whether you asked explicit permission arises.  Equally, a carve out from unacceptable behaviours doesn't make the action always acceptable, so it's not a licence to publish someone's email address regardless of context. It's not a blanket permission, it's an exclusion from being considered unacceptable behaviour.  I would be interested to know what information we ordinarily collect in the course of building linux that should be considered private because I might have missed something about the implications here.","On Tue, 2018-10-16 at 19:10 -0700, Frank Rowand wrote:  I don't think it is ... or should be.  This section is specifically enumerating unacceptable behaviours.  The carve out email address not ordinarily collected by the project"" means that adding someone's email address in a tag isn't immediately sanctionable in the code of conduct as unacceptable behaviour if a question about whether you asked explicit permission arises.  Equally, a carve out from unacceptable behaviours doesn't make the action always acceptable, so it's not a licence to publish someone's email address regardless of context.   It's not a blanket permission, it's an exclusion from being considered unacceptable behaviour.  I would be interested to know what information we ordinarily collect in the course of building linux that should be considered private because I might have missed something about the implications here.  James """,technical,James Bottomley,James.Bottomley@HansenPartnership.com,1,1,844,0.42,0.3157894736842105,0,0.0,0.75,0.0,0.0
741,431048,432535,Acked-by : Shuah Khan,"On 10/16/2018 09:00 AM, James Bottomley wrote: Acked-by : Shuah Khan <shuah@kernel.org>  thanks, -- Shuah",technical,Shuah Khan,shuah@kernel.org,1,0,21,0.011428571428571429,0.3684210526315789,0,0.25,0.75,0.0,0.0
742,431048,432763,"does that include bugzilla.kernel.org, or should we think of those email addresses (of bug submitters) as private?  They look public to me.","On 10/17/18 11:49 AM, Frank Rowand wrote:  does that include bugzilla.kernel.org, or should we think of those email addresses (of bug submitters) as private?  They look public to me.    --  ~Randy",technical,Randy Dunlap,rdunlap@infradead.org,0,0,139,0.07714285714285714,0.47368421052631576,0,0.25,0.5,0.0,0.0
743,431048,432762,"No, that's not my desired goal.   The section is not about giving permission it's about making sure listed unacceptable behaviours don't overlap what we normally do.  The goal is to exclude email the project ordinarily collects from immediate sanction under the unacceptable behaviours clause.  I deliberately didn't add anything about permission because that's up to the project to define in its more standard contribution documents. I agree, but, as I said, my goal wasn't to provide explicit permission(because the list is too long and too dependent on the way the project operates) it was to carve out an exclusion from sanction for stuff the kernel normally does.  The carve out doesn't translate into explicit permission because the project can define other standards for the way email addresses are added to the tags. I think the crux of the disagreement is that you think the carve out equates to a permission which is not specific enough and I think it doesn't equate to a permission at all, which is why there's no need to make it more explicit.  Is that a fair characterisation?","On Wed, 2018-10-17 at 11:49 -0700, Frank Rowand wrote: [...]  No, that's not my desired goal.   The section is not about giving permission it's about making sure listed unacceptable behaviours don't overlap what we normally do.  The goal is to exclude email the project ordinarily collects from immediate sanction under the unacceptable behaviours clause.  I deliberately didn't add anything about permission because that's up to the project to define in its more standard contribution documents.   I agree, but, as I said, my goal wasn't to provide explicit permission (because the list is too long and too dependent on the way the project operates) it was to carve out an exclusion from sanction for stuff the kernel normally does.  The carve out doesn't translate into explicit permission because the project can define other standards for the way email addresses are added to the tags.   I think the crux of the disagreement is that you think the carve out equates to a permission which is not specific enough and I think it doesn't equate to a permission at all, which is why there's no need to make it more explicit.  Is that a fair characterisation?  James",technical,James Bottomley,James.Bottomley@HansenPartnership.com,1,1,1089,0.5914285714285714,0.5263157894736842,0,0.25,0.5,0.0,0.0
744,431048,432777,"Hello, What about properly formatted patches (with From and SoB) sent to them maintainer, without copying any mailing lists? To me, a patch sent to a maintainer is obviously sent for inclusion in the kernel.","Hello,  On 17/10/2018 11:49:06-0700, Frank Rowand wrote:  What about properly formatted patches (with From and SoB) sent to the maintainer, without copying any mailing lists? To me, a patch sent to a maintainer is obviously sent for inclusion in the kernel.   --  Alexandre Belloni, Bootlin Embedded Linux and Kernel engineering https://bootlin.com",technical,Alexandre Belloni,alexandre.belloni@bootlin.com,1,0,207,0.12,0.5789473684210527,0,0.25,0.5,0.0,0.0
745,431048,432795,"OK.  I am fine with the goal of wording that excludes certain things from unacceptable behavior instead providing permissions for certain things.  I think me phrasing as permission instead of carve out is creating a lot of the miscommunication. Please re-read my comments, but in every place where I state things in a way of providing permissions, re-state it in your mind as the same sentence _except_ phrased as excluding from unacceptable behavior.  (I started to do that explicitly, but it looked like I was just going to create a whole lot of distracting text.)Nope.  That is a big place where I was not transferring my thoughts to clear communication.  I agree that what I wrote should have been written in terms of carve out instead of permission. Nope.  My concern is ""which email addresses"".","On 10/17/18 12:08, James Bottomley wrote:  OK.  I am fine with the goal of wording that excludes certain things from unacceptable behavior instead providing permissions for certain things.  I think me phrasing as permission instead of carve out is creating a lot of the miscommunication.  Please re-read my comments, but in every place where I state things in a way of providing permissions, re-state it in your mind as the same sentence _except_ phrased as excluding from unacceptable behavior.  (I started to do that explicitly, but it looked like I was just going to create a whole lot of distracting text.)    Nope.  That is a big place where I was not transferring my thoughts to clear communication.  I agree that what I wrote should have been written in terms of carve out instead of permission.    Nope.  My concern is which email addresses"".  -Frank  """,technical,Frank Rowand,frowand.list@gmail.com,1,0,800,0.4342857142857143,0.631578947368421,0,0.25,0.5,0.0,0.0
746,431048,433530,"[...]The idea here was because it's a carve out that doesn't give permission and because the permission is ruled by the project contribution documents, the carve out should be broad enough to cover anything they might say hence ""email addresses not ordinarily collected by the project"" are still included as unacceptable behaviour. Perhaps if you propose the wording you'd like to see it would help because there still looks to be some subtlety I'm not getting.","On Wed, 2018-10-17 at 12:53 -0700, Frank Rowand wrote: [...]  The idea here was because it's a carve out that doesn't give permission and because the permission is ruled by the project contribution documents, the carve out should be broad enough to cover anything they might say hence email addresses not ordinarily collected by the project"" are still included as unacceptable behaviour.  Perhaps if you propose the wording you'd like to see it would help because there still looks to be some subtlety I'm not getting.  James""",technical,James Bottomley,James.Bottomley@HansenPartnership.com,1,1,461,0.25142857142857145,0.6842105263157895,0,0.25,0.5,0.0,0.0
747,431048,434712,"This ends up reading like so:----Examples of unacceptable behavior by participants include:...* Publishing others ™ private information, such as a physical or electronic address that has been provided in a public space for the project, without explicit permission.----I think that in context, you want a 'not' in there.  That is: unacceptable behavior includes publishing others' private information... that has *not*been provided in a public space.  So, I think the suggested text needs some fixing, IMHO.I looked at this issue upstream, and decided to leave the wording in the CoC itself alone - favoring instead to add a clarifying addition to the upstream CoC FAQ, about some email addresses not being private information. The reason I took that approach, rather than try to change the wording inside the CoC, is that the current wording seems to me to be sufficient. The thing that is unacceptable is publishing private information.  The ""such as..."" clause is intended to convey examples of the types of thing that might usually be considered private information.  But it is not exhaustive, nor is it necessarily correct, depending on the circumstances.  In particular, email addresses are sometimes private information and sometimes not. In the context of kernel development, many email addresses are not private. I am sympathetic to the argument that we use emails as public information so much in kernel development processes, that it makes sense to omit this or qualify it more. My own views are that:1) if we change this line at all, we should simply omit the ""such as..."" part of the phrase, and leave it at:* Publishing others ™ private information without explicit permission but also2) I'm OK with leaving the phrase as is and handling the concerns in an clarifying document. Just my 2 cents. --","This ends up reading like so:    ----  Examples of unacceptable behavior by participants include:  ...  * Publishing others’ private information, such as a physical or electronic  address that has been provided in a public space for the project, without  explicit permission.  ----    I think that in context, you want a 'not' in there.  That is: unacceptable  behavior includes publishing others' private information... that has *not*  been provided in a public space.  So, I think the suggested text needs  some fixing, IMHO.    I looked at this issue upstream, and decided to leave the wording in  the CoC itself alone - favoring instead to add a clarifying addition  to the upstream CoC FAQ, about some email addresses not being  private information.    The reason I took that approach, rather than try to change the wording  inside the CoC, is that the current wording seems to me to be sufficient.  The thing that is unacceptable is publishing private information.  The such as...""  clause is intended to convey examples of the types of thing that might  usually be considered private information.  But it is not exhaustive, nor  is it necessarily correct, depending on the circumstances.  In particular,  email addresses are sometimes private information and sometimes not.  In the context of kernel development, many email addresses are not private.    I am sympathetic to the argument that we use emails as public information  so much in kernel development processes, that it makes sense to omit this or  qualify it more.    My own views are that:  1) if we change this line at all, we should simply omit the ""such as..."" part of  the phrase, and leave it at:    * Publishing others’ private information without explicit permission    but also  2) I'm OK with leaving the phrase as is and handling the concerns  in an clarifying document.    Just my 2 cents.   -- Tim        """,technical,Unkown Name,Tim.Bird@sony.com,0,0,1810,1.0,0.7894736842105263,0,0.5,0.25,0.0,0.0
748,431048,434717,"You beat me to this one.  However, there is another issue that I did touch on but perhaps not in this sub thread: For those of us who live in the US, our addresses (that's physical and sometimes email) are actually provided in a public space because they're available in the public property records.  That's actually why I chose ""not ordinarily collected by the project"" as opposed to ""not previously provided in the public space"" or an equivalent because doxxing in the US is mostly finding this information from public sources and broadcasting  think that's the sense of the people who acked this, yes.  Personally I'm happy with a separate clarification in another document, but I can also see the argument that we do need our single CoC to be consistent with our operational method, which is why I proposed the patch. This looks OK to me too ... the problem with the original is that the additional qualification overlaps our normal project method of operation, this solves the issue as well.","On Thu, 2018-10-18 at 19:49 +0000, Tim.Bird@sony.com wrote:  You beat me to this one.  However, there is another issue that I did touch on but perhaps not in this subthread: For those of us who live in the US, our addresses (that's physical and sometimes email) are actually provided in a public space because they're available in the public property records.  That's actually why I chose not ordinarily collected by the project"" as opposed to ""not previously provided in the public space"" or an equivalent because doxxing in the US is mostly finding this information from public sources and broadcasting it.   I think that's the sense of the people who acked this, yes.  Personally I'm happy with a separate clarification in another document, but I can also see the argument that we do need our single CoC to be consistent with our operational method, which is why I proposed the patch.   This looks OK to me too ... the problem with the original is that the additional qualification overlaps our normal project method of operation, this solves the issue as well.  James  """,technical,James Bottomley,James.Bottomley@HansenPartnership.com,1,1,996,0.5571428571428572,0.8947368421052632,0,0.5,0.25,0.0,0.0
749,431048,434864,"Yes, thank you. That clarification helps a _lot_ in understanding what you have said previously in this thread.  Thanks.  :-) Looks good to me.","On 10/18/18 12:57, James Bottomley wrote:   Yes, thank you.    That clarification helps a _lot_ in understanding what you have said previously in this thread.  Thanks.  :-)    Looks good to me.  -Frank",technical,Frank Rowand,frowand.list@gmail.com,1,0,143,0.08857142857142856,0.9473684210526315,0,0.5,0.25,0.0,0.25
750,431048,435932,"James, and our other friends, More than one ambiguity. This whole file needs to go. Who decides what is trolling, and what is a technique for raising awareness or sparking discussion on an issue? Why should this last bit remain?  Any literate person with access to a dictionary should know how ambiguous the word professional is.  As an amateur contributor to the FOSS ecosystem I am more than a bit offended by the decision to use such divisive, politically charged, and financially discriminatory language in a project of such massive technical importance.  This entire file should be expunged from the repository and replaced by well defined minimalistic guidelines for maintaining order on the mailing lists, rather than a set of ambiguous codes that force maintainers to take politically motivated actions against contributors for undefined reasons. Using words like professional is a distressing red flag because it doesn't add any clarification on the issue (what was the issue again?), it only raises more questions.  I can't think of any reason that word would be needed unless you're trying to push out unpaid contributors.  Why should someone's employment status be held against them when contributing ideas or code to a technical project that has benefited greatly from amateur contributions? I fear for the kernels future now that irrational politics are beginning to creep.","James, and our other friends,   On Tue, Oct 16, 2018 at 2:59 PM James Bottomley <James.Bottomley@hansenpartnership.com> wrote:  More than one ambiguity. This whole file needs to go.   Who decides what is trolling, and what is a technique for raising awareness or sparking discussion on an issue?    Why should this last bit remain?  Any literate person with access to a dictionary should know how ambiguous the word professional is.  As an amateur contributor to the FOSS ecosystem I am more than a bit offended by the decision to use such divisive, politically charged, and financially discriminatory language in a project of such massive technical importance.  This entire file should be expunged from the repository and replaced by well defined minimalistic guidelines for maintaining order on the mailing lists, rather than a set of ambiguous codes that force maintainers to take politically motivated actions against contributors for undefined reasons.  Using words like professional is a distressing red flag because it doesn't add any clarification on the issue (what was the issue again?), it only raises more questions.  I can't think of any reason that word would be needed unless you're trying to push out unpaid contributors.  Why should someones employment status be held against them when contributing ideas or code to a technical project that has benefited greatly from amateur contributions?  I fear for the kernels future now that irrational politics are beginning to creep.",technical,Michael Tirado,mtirado418@gmail.com,0,0,1387,0.7028571428571428,1.0,1,1.0,0.0,0.25,0.0
751,433343,433344,"No any reason I know of.  It must be just an old convention. Yes, please.","On Thu, 18 Oct 2018 12:10:28 +0200, Philipp K wrote:  No any reason I know of.  It must be just an old convention.   Yes, please.   thanks,  Takashi",technical,Takashi Iwai,tiwai@suse.de,1,0,73,1.0,1.0,1,0.0,0.0,0.0,0.0
752,434675,434727,These kind of issues are usually fixed by fixing the network driver's shutdown routine to ensure that MSI interrupts are cleared there.,"On 10/18/2018 2:37 PM, Guilherme G. Piccoli wrote:  These kind of issues are usually fixed by fixing the network driver's shutdown routine to ensure that MSI interrupts are cleared there.",technical,Sinan Kaya,okaya@kernel.org,1,0,135,0.057692307692307696,0.4444444444444444,0,0.0,1.0,0.0,0.0
753,434675,434733,"I'm not sure shutdown handlers for drivers are called in panic exec (I remember of an old experiment I did, loading a kernel with ""kexec -p"" didn't trigger the handlers).But this case is even worse, because the NICs were in PCI passthrough mode, using vfio. So, they were completely unaware of what happened in the host kernel. Also, this is spec compliant - system reset events should guarantee the bits are cleared (although kexec is not exactly a system reset, it's similar)","On 18/10/2018 17:08, Sinan Kaya wrote:   Sinan, I'm not sure shutdown handlers for drivers are called in panic kexec (I remember of an old experiment I did, loading a kernel with kexec -p"" didn't trigger the handlers).  But this case is even worse, because the NICs were in PCI passthrough mode, using vfio. So, they were completely unaware of what happened in the host kernel.  Also, this is spec compliant - system reset events should guarantee the bits are cleared (although kexec is not exactly a system reset, it's similar)  Cheers,   Guilherme""",technical,Guilherme G. Piccoli,gpiccoli@canonical.com,0,1,477,0.2403846153846154,0.5555555555555556,0,0.0,1.0,0.0,0.0
754,434675,434751,"AFAIK, all shutdown (not remove) routines are called before launching the next kernel even in crash scenario. It is not safe to start the new kernel while hardware is doing a DMA to the system memory and triggering interrupts. Shutdown routine in PCI core used to disable MSI/MSI-x on behalf of all endpoints but it was later decided that this is the responsibility of the endpoint driver.","On 10/18/2018 4:13 PM, Guilherme G. Piccoli wrote:  AFAIK, all shutdown (not remove) routines are called before launching the next kernel even in crash scenario. It is not safe to start the new kernel while hardware is doing a DMA to the system memory and triggering interrupts.  Shutdown routine in PCI core used to disable MSI/MSI-x on behalf of all endpoints but it was later decided that this is the responsibility of the endpoint driver.  commit fda78d7a0ead144f4b2cdb582dcba47911f4952c Author: Prarit Bhargava <prarit@redhat.com> Date:   Thu Jan 26 14:07:47 2017 -0500       PCI/MSI: Stop disabling MSI/MSI-X in pci_device_shutdown()       The pci_bus_type .shutdown method, pci_device_shutdown(), is called from      device_shutdown() in the kernel restart and shutdown paths.       Previously, pci_device_shutdown() called pci_msi_shutdown() and      pci_msix_shutdown().  This disables MSI and MSI-X, which causes the device      to fall back to raising interrupts via INTx.  But the driver is still bound      to the device, it doesn't know about this change, and it likely doesn't      have an INTx handler, so these INTx interrupts cause nobody cared""      warnings like this:         irq 16: nobody cared (try booting with the ""irqpoll"" option)        CPU: 0 PID: 0 Comm: swapper/0 Not tainted 4.8.2-1.el7_UNSUPPORTED.x86_64 #1        Hardware name: Hewlett-Packard HP Z820 Workstation/158B, BIOS J63 v03.90 06/        ...       The MSI disabling code was added by d52877c7b1af (""pci/irq: let      pci_device_shutdown to call pci_msi_shutdown v2"") because a driver left MSI      enabled and kdump failed because the kexeced kernel wasn't prepared to      receive the MSI interrupts.      Subsequent commits 1851617cd2da (""PCI/MSI: Disable MSI at enumeration even      if kernel doesn't support MSI"") and  e80e7edc55ba (""PCI/MSI: Initialize MSI      capability for all architectures"") changed the kexeced kernel to disable      all MSIs itself so it no longer depends on the crashed kernel to clean up      after itself.       Stop disabling MSI/MSI-X in pci_device_shutdown().  This resolves the      ""nobody cared"" unhandled IRQ issue above.  It also allows PCI serial      devices, which may rely on the MSI interrupts, to continue outputting      messages during reboot/shutdown.       [bhelgaas: changelog, drop pci_msi_shutdown() and pci_msix_shutdown() calls      altogether]      Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=187351      Signed-off-by: Prarit Bhargava <prarit@redhat.com>      Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>      CC: Alex Williamson <alex.williamson@redhat.com>      CC: David Arcari <darcari@redhat.com>      CC: Myron Stowe <mstowe@redhat.com>      CC: Lukas Wunner <lukas@wunner.de>      CC: Keith Busch <keith.busch@intel.com>      CC: Mika Westerberg <mika.westerberg@linux.intel.com>   """,technical,Sinan Kaya,okaya@kernel.org,1,0,389,0.17548076923076922,0.6666666666666666,0,0.0,1.0,0.0,0.0
755,434675,434805,"I don't want to expand the early quirk infrastructure unless there is absolutely no other way to solve this.  The early quirk stuff isx86-specific, and it's not obvious that this problem is x86-only.This patch scans buses 0-255, but still only in domain 0, so it won't help with even more complicated systems that use other domains. I'm not an IRQ expert, but it seems wrong to me that we are enabling this interrupt before we're ready for it.  The MSI should target anIOAPIC.  Can't that IOAPIC entry be masked until later?  I guess the kdump kernel doesn't know what MSI address the device might be using. Could the IRQ core be more tolerant of this somehow, e.g., if it notices incoming interrupts with no handler, could it disable the IOAPIC entry and fall back to polling periodically until a handler is added?","On Thu, Oct 18, 2018 at 03:37:19PM -0300, Guilherme G. Piccoli wrote:  I don't want to expand the early quirk infrastructure unless there is absolutely no other way to solve this.  The early quirk stuff is x86-specific, and it's not obvious that this problem is x86-only.  This patch scans buses 0-255, but still only in domain 0, so it won't help with even more complicated systems that use other domains.  I'm not an IRQ expert, but it seems wrong to me that we are enabling this interrupt before we're ready for it.  The MSI should target an IOAPIC.  Can't that IOAPIC entry be masked until later?  I guess the kdump kernel doesn't know what MSI address the device might be using.  Could the IRQ core be more tolerant of this somehow, e.g., if it notices incoming interrupts with no handler, could it disable the IOAPIC entry and fall back to polling periodically until a handler is added?",technical,Bjorn Helgaas,helgaas@kernel.org,1,0,815,0.3918269230769231,0.7777777777777778,0,0.0,0.75,0.0,0.75
756,434675,436717,"I agree with you, it's definitely not safe to start a new kernel within-flight DMA transactions, but in the crash scenario I think the rationale was that running kernel is broken so it's even more unreliable to try gracefully shutdown the devices than hope-for-the-best and start the kdump kernel right away heheh Fact is that the shutdown handlers are not called in the crash scenario. They come from device_shutdown(), the code paths are as follow. To validate this, one can load a kernel with ""initcall_debug"" parameter, and performs a kexec - if the shutdown handlers are called, there's adev_info() call that shows a message per device. This may be a good idea, using the pci layer to disable MSIs in the quiesce path of the broken kernel. I'll follow-up this discussion in Bjorn's reply.","On 18/10/2018 17:30, Sinan Kaya wrote:  Hi Sinan,  I agree with you, it's definitely not safe to start a new kernel with in-flight DMA transactions, but in the crash scenario I think the rationale was that running kernel is broken so it's even more unreliable to try gracefully shutdown the devices than hope-for-the-best and start the kdump kernel right away heheh  Fact is that the shutdown handlers are not called in the crash scenario. They come from device_shutdown(), the code paths are as follow:  Regular kexec flow:  syscall_reboot()   kernel_kexec()     kernel_restart_prepare() 	  device_shutdown() 	machine_kexec() 	 Although if CONFIG_KEXEC_JUMP is set, it doesn't call device_shutdown() either.   Crash kexec flow:   __crash_kexec()       machine_kexec()  There are some entry points to __crash_kexec(), like panic() or die() in x86, for example. To validate this, one can load a kernel with initcall_debug"" parameter, and performs a kexec - if the shutdown handlers are called, there's a dev_info() call that shows a message per device.    This may be a good idea, using the pci layer to disable MSIs in the quiesce path of the broken kernel. I'll follow-up this discussion in Bjorn's reply.  Thanks,   Guilherme""",technical,Guilherme G. Piccoli,gpiccoli@canonical.com,0,1,793,0.37740384615384615,0.8888888888888888,0,1.0,0.0,0.75,0.0
757,434675,436738,"thanks for your quick reply. I understand your point, but I think this is inherently an architecture problem. No matter what solution we decide for, it'll need to be applied in early boot time, like before the PCI layer gets initialized.So, I think a first step would be to split the solution ""timing"" in 2 possibilities: a) We could try to disable MSIs or whatever approach we take in the quiesce path of crash_kexec(), before the bootstrap of the kdump kernel. The pro is we could use PCI handlers to do it generically. The con is it'd touch that delicate shutdown path, from a broken kernel, and this is unreliable. Also, I've noticed changes in those crash paths usually gain huge amount of criticism by community, seems nobody wants to change a bit of this code, if not utterly necessary.b) Continue using an early boot approach. IMO, this would be per-arch by nature. Currently, powerpc for example does not suffer this issue due to their arch code performing a FW-aided PCI fundamental reset in the devices[0].On the other hand, x86 has no generic fundamental reset infrastructure to my knowledge (we tried some alternatives, like a Bridge reset[1] that didn't work, or zeroing the the command register, which worked), but if we go with the IOAPIC way of handling this (which we tried a bit and failed too), it'll be even more arch-dependent, since IOAPIC is x86 concept. After discussing here internally, an alternative way for this MSI approach work without requiring the change in the early PCI infrastructure is to check if we're in kdump kernel and perform manually the full scan in that case, instead of changing the generic case as proposed here. This would still be x86-only, but again, it's difficult if not impossible to fix all archs using the same code here. Finally, about multi-domain PCI topologies, I've never saw it on x86, I wasn't aware that such things existed in x86 - but if required we can quickly extend the logic to contemplate it too. Thanks again, looking , adapted to work in early boot time.","On 18/10/2018 19:15, Bjorn Helgaas wrote:   Hi Bjorn, thanks for your quick reply. I understand your point, but I think this is inherently an architecture problem. No matter what solution we decide for, it'll need to be applied in early boot time, like before the PCI layer gets initialized.  So, I think a first step would be to split the solution timing"" in 2 possibilities:  a) We could try to disable MSIs or whatever approach we take in the quiesce path of crash_kexec(), before the bootstrap of the kdump kernel. The pro is we could use PCI handlers to do it generically. The con is it'd touch that delicate shutdown path, from a broken kernel, and this is unreliable. Also, I've noticed changes in those crash paths usually gain huge amount of criticism by community, seems nobody wants to change a bit of this code, if not utterly necessary.  b) Continue using an early boot approach. IMO, this would be per-arch by nature. Currently, powerpc for example does not suffer this issue due to their arch code performing a FW-aided PCI fundamental reset in the devices[0]. On the other hand, x86 has no generic fundamental reset infrastructure to my knowledge (we tried some alternatives, like a Bridge reset[1] that didn't work, or zeroing the the command register, which worked), but if we go with the IOAPIC way of handling this (which we tried a bit and failed too), it'll be even more arch-dependent, since IOAPIC is x86 concept.   After discussing here internally, an alternative way for this MSI approach work without requiring the change in the early PCI infrastructure is to check if we're in kdump kernel and perform manually the full scan in that case, instead of changing the generic case as proposed here. This would still be x86-only, but again, it's difficult if not impossible to fix all archs using the same code here.  Finally, about multi-domain PCI topologies, I've never saw it on x86, I wasn't aware that such things existed in x86 - but if required we can quickly extend the logic to contemplate it too.  Thanks again, looking forward for you suggestions. Cheers,   Guilherme  [0] https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/arch/powerpc/platforms/powernv/pci-ioda.c#n3992  [1] Based in https://patchwork.kernel.org/patch/2562841, adapted to work in early boot time.""",technical,Guilherme G. Piccoli,gpiccoli@canonical.com,0,1,2027,1.0,1.0,1,1.0,0.0,0.0,0.0
758,436735,440119,"Thank you for the review.  Basically we use these prints to get a notification when a system is having thermal issues.  It's easy to look in dmesg and see the prints and know that something temperature related is going on. However, I agree that the current solution is a bit hacky, and in looking at it a bit further we don't even cover all the paths that we need to.  The processor_set_cur_state()  function in drivers/acpi/processor_thermal.c, for example, is used on the x86_64systems I'm testing with and wasn't augmented with prints. I'm going to take a step back and try and find another solution.  The info you added to sysfs looks very promising, thank you for pointing it out.","On Wed, Oct 24, 2018 at 1:22 AM Viresh Kumar <viresh.kumar@linaro.org> wrote:  Thank you for the review.  Basically we use these prints to get a notification when a system is having thermal issues.  It's easy to look in dmesg and see the prints and know that something temperature related is going on.  However, I agree that the current solution is a bit hacky, and in looking at it a bit further we don't even cover all the paths that we need to.  The processor_set_cur_state()  function in drivers/acpi/processor_thermal.c, for example, is used on the x86_64 systems I'm testing with and wasn't augmented with prints.  I'm going to take a step back and try and find another solution.  The info you added to sysfs looks very promising, thank you for pointing it out.",technical,Ross Zwisler,zwisler@google.com,0,1,685,1.0,1.0,1,1.0,0.0,1.0,0.0
759,438299,438492,"Thank you for the patch. Nonetheless, I've just applied similar Liviu's patch [0], since it arrived one week ago already. I'll send it upstream with LED fixes for 4-20-rc2.","Hi Dan,  Thank you for the patch. Nonetheless, I've just applied similar Liviu's patch [0], since it arrived one week ago already.  I'll send it upstream with LED fixes for 4-20-rc2.  [0] https://lkml.org/lkml/2018/10/18/154  Best regards, Jacek Anaszewski  On 10/25/2018 05:06 PM, Dan Sneddon wrote:",technical,Jacek Anaszewski,jacek.anaszewski@gmail.com,1,0,172,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
760,438299,438545,"If people are hitting it this often, maybe it is time to push it early? Linus should not have problem taking two pull requests in a merge window.","On Thu 2018-10-25 21:22:34, Jacek Anaszewski wrote:  If people are hitting it this often, maybe it is time to push it early? Linus should not have problem taking two pull requests in a merge window.  Best regards, 								Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,145,0.7948717948717948,1.0,1,0.0,0.0,0.0,0.0
761,442776,442803,"Thank you for your patch! It would be much better if you can send it using traditional tools,i.e. `git send-email ...`.","On Wed, Oct 31, 2018 at 5:57 PM <ayman.bagabas@gmail.com> wrote:  Thank you for your patch!  It would be much better if you can send it using traditional tools, i.e. `git send-email ...`.  --  With Best Regards, Andy Shevchenko",technical,Andy Shevchenko,andy.shevchenko@gmail.com,1,0,119,1.0,1.0,1,0.0,0.0,0.0,0.0
762,443957,444326,"Hi, It's up to Rob of course, but IMO it seems a nicer way forward to include both the SoC-specific string and the ""version"" string in all cases.  I'd write this for the full text: - compatible.  NOTE that some old device tree files may be floating around that only   have the string ""qcom,sdhci-msm-v4"" without the SoC compatible string   but doing that should be considered a deprecated practice.","Hi,  On Thu, Nov 1, 2018 at 5:07 AM Veerabhadrarao Badiganti <vbadigan@codeaurora.org> wrote:  It's up to Rob of course, but IMO it seems a nicer way forward to include both the SoC-specific string and the version"" string in all cases.  I'd write this for the full text:   - compatible: Should contain a SoC-specific string and a IP version string:       version strings:            ""qcom,sdhci-msm-v4"" for sdcc versions less than 5.0            ""qcom,sdhci-msm-v5"" for sdcc version 5.0       full compatible strings with SoC and version:            ""qcom,apq8084"", ""qcom,sdhci-msm-v4""            ""qcom,msm8974"", ""qcom,sdhci-msm-v4""            ""qcom,msm8916"", ""qcom,sdhci-msm-v4""            ""qcom,msm8992"", ""qcom,sdhci-msm-v4""            ""qcom,msm8996"", ""qcom,sdhci-msm-v4""            ""qcom,sdm845-sdhci"", ""qcom,sdhci-msm-v5""     NOTE that some old device tree files may be floating around that only    have the string ""qcom,sdhci-msm-v4"" without the SoC compatible string    but doing that should be considered a deprecated practice.  -Doug""",technical,Doug Anderson,dianders@google.com,0,0,398,1.0,0.4,0,0.0,0.8,0.0,0.8
763,443957,447243,Fine by me if you update all the dts files. I assume you meant to append '-sdhci' here?,"On Thu, Nov 01, 2018 at 01:29:54PM -0700, Doug Anderson wrote:  Fine by me if you update all the dts files.   I assume you meant to append '-sdhci' here?",technical,Rob Herring,robh@kernel.org,1,0,87,0.25609756097560976,0.6,0,0.8,0.0,0.8,0.0
764,443957,447279,What I'd like to see: Remove control-bank-cfg. Add required child propertyset of outputs this child controls.,"Hi,  On Mon, Nov 5, 2018 at 12:37 PM Rob Herring <robh@kernel.org> wrote:  Done and done.  https://lkml.kernel.org/r/20181105210921.253707-1-dianders@chromium.org https://lkml.kernel.org/r/20181105210921.253707-2-dianders@chromium.org    Oops, yes!             qcom,apq8084-sdhci"", ""qcom,sdhci-msm-v4""            ""qcom,msm8974-sdhci"", ""qcom,sdhci-msm-v4""            ""qcom,msm8916-sdhci"", ""qcom,sdhci-msm-v4""            ""qcom,msm8992-sdhci"", ""qcom,sdhci-msm-v4""            ""qcom,msm8996-sdhci"", ""qcom,sdhci-msm-v4""  -Doug""",technical,Doug Anderson,dianders@google.com,0,0,109,0.24390243902439024,0.8,0,0.8,0.0,0.0,0.0
765,443957,448296,Thank you. Will update the documentation.,"On 11/6/2018 2:41 AM, Doug Anderson wrote:  Thank you. Will update the documentation.",technical,Veerabhadrarao Badiganti,vbadigan@codeaurora.org,0,1,41,0.0975609756097561,1.0,1,1.0,0.0,0.0,0.0
766,444348,444874,"Assuming this is for arm64, I'm somewhat surprised that memset() could be that much faster than clear_page(), since they should effectively amount to the same thing (a DC ZVA loop). What hardware is this on? Profiling to try and see exactly where the extra time goes would be interesting too. Or just mask it out in __iommu_dma_alloc_pages()?What if the pages came from highmem? I know that doesn't happen on arm64 today, but the point of this code *is* to be generic, and other users will arrive eventually.","On 01/11/2018 21:35, Nicolin Chen wrote:  Assuming this is for arm64, I'm somewhat surprised that memset() could  be that much faster than clear_page(), since they should effectively  amount to the same thing (a DC ZVA loop). What hardware is this on?  Profiling to try and see exactly where the extra time goes would be  interesting too.   Or just mask it out in __iommu_dma_alloc_pages()?   What if the pages came from highmem? I know that doesn't happen on arm64  today, but the point of this code *is* to be generic, and other users  will arrive eventually.  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,508,0.5759162303664922,0.2,0,0.0,0.8571428571428571,0.0,0.0
767,444348,445475,"I am running with tegra 186-p2771-0000.dtb so it's arm64 yes. I re-ran the test to get some accuracy within the function and got. Note: new memset takes about 164 usec, resulting in 400 usec diff      for the entire iommu_dma_alloc() function call. It looks like this might be more than the diff between clear_page and memset, and might be related to mapping and cache. Any idea? Yea, the change here would be neater then. Hmm, so it probably should use sg_miter_start/stop() too? Looking at the flush routine doing in PAGE_SIZE for each iteration, would be possible to map and memset contiguous pages together? Actually the flush routine might be also optimized if we can map contiguous pages.","On Fri, Nov 02, 2018 at 04:54:07PM +0000, Robin Murphy wrote:  I am running with tegra186-p2771-0000.dtb so it's arm64 yes.   I re-ran the test to get some accuracy within the function and got: 1) pages = __iommu_dma_alloc_pages(count, alloc_sizes >> PAGE_SHIFT, gfp),    // reduced from 422 usec to 56 usec == 366 usec less 2) if (!(prot & IOMMU_CACHE)) {...}	//flush routine    // reduced from 439 usec to 236 usec == 203 usec less Note: new memset takes about 164 usec, resulting in 400 usec diff       for the entire iommu_dma_alloc() function call.  It looks like this might be more than the diff between clear_page and memset, and might be related to mapping and cache. Any idea?   Yea, the change here would be neater then.   Hmm, so it probably should use sg_miter_start/stop() too? Looking at the flush routine doing in PAGE_SIZE for each iteration, would be possible to map and memset contiguous pages together? Actually the flush routine might be also optimized if we can map contiguous pages.  Thank you Nicolin",technical,Nicolin Chen,nicoleotsuka@gmail.com,1,1,694,0.7120418848167539,0.3,0,0.14285714285714285,0.8571428571428571,0.0,0.14285714285714285
768,444348,446096,And in what case does dma_alloc_* performance even matter?,"On Thu, Nov 01, 2018 at 02:35:00PM -0700, Nicolin Chen wrote:  And in what case does dma_alloc_* performance even matter?",technical,Christoph Hellwig,hch@infradead.org,1,0,58,0.05759162303664921,0.4,0,0.2857142857142857,0.5714285714285714,0.14285714285714285,0.0
769,444348,446904,"FYI, I have patches I plan to submit soon that gets rid of the struct scatter list use in this code to simplify it:","On Fri, Nov 02, 2018 at 04:36:13PM -0700, Nicolin Chen wrote:  FYI, I have patches I plan to submit soon that gets rid of the struct scatterlist use in this code to simplify it:  http://git.infradead.org/users/hch/misc.git/commitdiff/84e837fc3248b513f73adde49e04e7c58f605113",technical,Christoph Hellwig,hch@infradead.org,1,0,115,0.13612565445026178,0.5,0,0.42857142857142855,0.42857142857142855,0.0,0.0
770,444348,448033,"...and I have some significant objections to that simplification which I plan to respond with ,)(namely that it defaults the whole higher-order page allocation business which will have varying degrees of performance impact on certain cases)","On 05/11/2018 14:58, Christoph Hellwig wrote:  ...and I have some significant objections to that simplification which I  plan to respond with ,)  (namely that it defaults the whole higher-order page allocation business  which will have varying degrees of performance impact on certain cases)  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,240,0.21465968586387435,0.6,0,0.5714285714285714,0.2857142857142857,0.0,0.0
771,444348,448330,"Hmm, I guess it might not be so much clear_page() itself as all the gubbins involved in getting there from prep_new_page(). I could perhaps make some vague guesses about how the A57 cores might get tickled by the different code patterns, but the Denver cores are well beyond my ability to reason about. Out of even further curiosity, how does the quick hack below compare? I suppose the ideal point at which to do it would be after the remapping when we have the entire buffer contiguous in vmalloc space and can make best use of prefetchers etc. - DMA_ATTR_NO_KERNEL_MAPPING is a bit of a spanner in the works, but we could probably accommodate a special case for that. As Christoph points out, this isn't really the place to be looking for performance anyway (unless it's pathologically bad as per the DMA_ATTR_ALLOC_SINGLE_PAGES fun), but if we're looking at pulling the remapping out of the arch code, maybe we could aim to rework the zeroing completely as part of that.","On 02/11/2018 23:36, Nicolin Chen wrote:  Hmm, I guess it might not be so much clear_page() itself as all the  gubbins involved in getting there from prep_new_page(). I could perhaps  make some vague guesses about how the A57 cores might get tickled by the  different code patterns, but the Denver cores are well beyond my ability  to reason about. Out of even further curiosity, how does the quick hack  below compare?   I suppose the ideal point at which to do it would be after the remapping  when we have the entire buffer contiguous in vmalloc space and can make  best use of prefetchers etc. - DMA_ATTR_NO_KERNEL_MAPPING is a bit of a  spanner in the works, but we could probably accommodate a special case  for that. As Christoph points out, this isn't really the place to be  looking for performance anyway (unless it's pathologically bad as per  the DMA_ATTR_ALLOC_SINGLE_PAGES fun), but if we're looking at pulling  the remapping out of the arch code, maybe we could aim to rework the  zeroing completely as part of that.  Robin.  ----->8----- diff --git a/drivers/iommu/dma-iommu.c b/drivers/iommu/dma-iommu.c index d1b04753b204..7d28db3bf4bf 100644 --- a/drivers/iommu/dma-iommu.c +++ b/drivers/iommu/dma-iommu.c @@ -569,7 +569,7 @@ struct page **iommu_dma_alloc(struct device *dev,  size_t size, gfp_t gfp,   		alloc_sizes = min_size,    	count = PAGE_ALIGN(size) >> PAGE_SHIFT, -	pages = __iommu_dma_alloc_pages(count, alloc_sizes >> PAGE_SHIFT, gfp), +	pages = __iommu_dma_alloc_pages(count, alloc_sizes >> PAGE_SHIFT, gfp  & ~__GFP_ZERO),   	if (!pages)   		return NULL,  @@ -581,15 +581,18 @@ struct page **iommu_dma_alloc(struct device *dev,  size_t size, gfp_t gfp,   	if (sg_alloc_table_from_pages(&sgt, pages, count, 0, size, GFP_KERNEL))   		goto out_free_iova,  -	if (!(prot & IOMMU_CACHE)) { +	{   		struct sg_mapping_iter miter,   		/*   		 * The CPU-centric flushing implied by SG_MITER_TO_SG isn't   		 * sufficient here, so skip it by using the wrong"" direction.   		 */   		sg_miter_start(&miter, sgt.sgl, sgt.orig_nents, SG_MITER_FROM_SG), -		while (sg_miter_next(&miter)) +		while (sg_miter_next(&miter)) { +			clear_page(miter.addr), +			if (!(prot & IOMMU_CACHE))   			flush_page(dev, miter.addr, page_to_phys(miter.page)), +		}   		sg_miter_stop(&miter),   	}""",technical,Robin Murphy,robin.murphy@arm.com,1,0,974,1.0,0.7,0,0.5714285714285714,0.2857142857142857,0.0,0.0
772,444348,448571,"Honestly, this was amplified by running a local iommu benchmark test. Practically dma_alloc/free() should not be that stressful, but we cannot say the performance doesn't matter at all, right? Though many device drivers pre-allocate memory for DMA usage, it could matter where a driver dynamically allocates and releases. And actually I have a related question for you: I saw that the dma_direct_alloc() cancels the __GFP_ZERO flag and does manual memset() after allocation. Might that be possibly related to a performance concern? Though I don't see any performance keyword for that part of code, especially seems that memset() was there from the beginning.","Hi Christoph,  On Sun, Nov 04, 2018 at 07:50:01AM -0800, Christoph Hellwig wrote:  Honestly, this was amplified by running a local iommu benchmark test. Practically dma_alloc/free() should not be that stressful, but we cannot say the performance doesn't matter at all, right? Though many device drivers pre-allocte memory for DMA usage, it could matter where a driver dynamically allocates and releases.  And actually I have a related question for you: I saw that the dma_direct_alloc() cancels the __GFP_ZERO flag and does manual memset() after allocation. Might that be possibly related to a performance concern? Though I don't see any performance keyword for that part of code, especially seems that memset() was there from the beginning.  Thanks Nicolin",technical,Nicolin Chen,nicoleotsuka@gmail.com,1,1,658,0.6544502617801047,0.8,0,0.7142857142857143,0.2857142857142857,0.0,0.0
773,444348,448586,"I tried out that change. And the results are as followings: a. Routine (1) reduced from 422 usec to 55 usecb. Routine (2) increased from 441 usec to 833 usecc. Overall, it seems to remain the same: 900+ usecI would understand the point. So probably it'd be more plausible to have the change if it reflects on some practical benchmark. I might need to re-run some tests with heavier use cases. That'd be nice. I believe it'd be good to have.","Hi Robin,  On Tue, Nov 06, 2018 at 06:27:39PM +0000, Robin Murphy wrote:  I tried out that change. And the results are as followings: a. Routine (1) reduced from 422 usec to 55 usec b. Routine (2) increased from 441 usec to 833 usec c. Overall, it seems to remain the same: 900+ usec   I would understand the point. So probably it'd be more plausible to have the change if it reflects on some practical benchmark. I might need to re-run some tests with heavier use cases.   That'd be nice. I believe it'd be good to have.  Thanks Nicolin",technical,Nicolin Chen,nicoleotsuka@gmail.com,1,1,440,0.5235602094240838,0.9,0,0.7142857142857143,0.2857142857142857,0.0,0.2857142857142857
774,444348,452311,"Well, please place your objection there.  The behavior does match what every other iommu-based dma ops implementation outside of arm/arm64 does, so there is some precedent for it to say the least.  But if the only current users objects I'll surely find a way to accommodate it, but a good rationale including numbers would be useful to document it.","On Tue, Nov 06, 2018 at 02:39:26PM +0000, Robin Murphy wrote:  Well, please place your objection there.  The behavior does match what every other iommu-based dma ops implementation ouside of arm/arm64 does, so there is some precedent for it to say the least.  But if the only current users objects I'll surely find a way to accomodate it, but a good rationale including numbers would be useful to document it.",technical,Christoph Hellwig,hch@infradead.org,1,0,348,0.34554973821989526,1.0,1,1.0,0.0,0.2857142857142857,0.0
775,444822,447202,It's better to not have a mixture of nodes at a level with and without unit-addresses. So I'd move all the i2c nodes under an 'i2c-mux' node.,"On Fri, Nov 02, 2018 at 03:47:20PM +0000, Kieran Bingham wrote:   It's better to not have a mixture of nodes at a level with and without  unit-addresses. So I'd move all the i2c nodes under an 'i2c-mux' node.  Rob",technical,Rob Herring,robh@kernel.org,1,0,141,0.0443213296398892,0.23076923076923078,0,0.0625,0.9375,0.0625,0.0
776,444822,449145,"<Top posting for new topic>I'm replying here rather than spam the IRC channel with a big paste. It's also a useful description to the probe sequence, so I've kept it with the driver posting. I hope the following helps illustrate the sequences which are involved So yes sensors are only communicated with once the link is brought up as much as possible. Because the sensors are i2c devices on the i2c_mux - they are not probed until their adapters are created and added. At this stage the i2c-mux core framework will iterate all the devices described by the DT for that adapter. As each one is probed - the i2c_mux framework will call max9286_i2c_mux_select() and enable only the single link. This allows us to configure each camera independently (which is essential because they are all configured to the same i2caddress by default at power on) Hope this helps, and feel free to ask if you have any more questions.","Hi Luca  <Top posting for new topic>   I'm replying here rather than spam the IRC channel with a big paste. It's also a useful description to the probe sequence, so I've kept it with the driver posting.  I hope the following helps illustrate the sequences which are involved:  max9286_probe()  - max9286_i2c_mux_close() # Disable all links  - max9286_configure_i2c # Configure early communication settings  - max9286_init():    - regulator_enable() # Power up all cameras    - max9286_setup() # Most link setup is done here.    ... Set up v4l2/async/media-controller endpoints    - max9286_i2c_mux_init() # Start configuring cameras:      - i2c_mux_alloc() # Create our mux device      - for_each_source(dev, source)            i2c_mux_add_adapter() # This is where sensors get probed.  So yes sensors are only communicated with once the link is brought up as much as possible.  Because the sensors are i2c devices on the i2c_mux - they are not probed until their adapters are created and added.  At this stage the i2c-mux core framework will iterate all the devices described by the DT for that adapter.  As each one is probed - the i2c_mux framework will call max9286_i2c_mux_select() and enable only the single link.  This allows us to configure each camera independently  (which is essential because they are all configured to the same i2c address by default at power on)   Hope this helps, and feel free to ask if you have any more questions.  -- Regards  Kieran   On 02/11/2018 15:47, Kieran Bingham wrote:",technical,Kieran Bingham,kieran.bingham+renesas@ideasonboard.com,1,0,914,0.24653739612188366,0.3076923076923077,0,0.08333333333333333,0.8958333333333334,0.020833333333333332,0.0
777,444822,449312,"thanks for the clarification. One additional note below. For the records, an additional bit of explanation I got from Kieran via IRC.The fact that link is already up when the sensors are probed is due to the fact that the power regulator has a delay of *8 seconds*. This is intended, because there's an MCU on the camera modules that talks on the I2C bus during that time, and thus the drivers need to wait after it's done. This delay happens before max9286_setup() is called.","Hi Kieran,  thanks for the clarification. One additional note below.  On 07/11/18 16:06, Kieran Bingham wrote:  For the records, an additional bit of explanation I got from Kieran via IRC.  The fact that link is already up when the sensors are probed is due to the fact that the power regulator has a delay of *8 seconds*. This is intended, because there's an MCU on the camera modules that talks on the I2C bus during that time, and thus the drivers need to wait after it's done.  This delay happens before max9286_setup() is called.  --  Luca",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,476,0.1371191135734072,0.34615384615384615,0,0.10416666666666667,0.8958333333333334,0.0,0.0
778,444822,449965,"Sorry to jump up, but I feel this should be clarified. The 8sec delay is due to the fact an integrated MCU on the remote camera module programs the local sensor and the serializer integrated in the module in to some default configuration state. At power up, we just want to let it finish, with all reverse channels closed(camera module -> SoC direction) not to have the MCU transmitted messages repeated to the local side (our remote serializer does repeat messages not directed to it on it's remote side, as our local deserializer does).The ""link up"" thing is fairly more complicated for GMSL than just having a binary ""on"" or ""off"" mode. This technology defines two different ""channels"", a 'configuration-channel' for transmitting control messages on the serial link (i2c messages for the deserializer/serializer pair this patches support) and a 'video-channel' for transmission of high-speed data, such as, no surprise, video and images :)GMSL also defines two ""link modes"": a clock-less ""configuration link ""and an high-speed ""video link"". The ""configuration link"" is available a few msec after power up (roughly), while the ""video link"" needs a pixel clock to be supplied to the serializer for it to enter this mode and be able to lock the status between itself and the deserializer. Then it can begin serializing video data. The 'control channel' is available both when the link is in 'configuration' and 'video' mode, while the 'video' channel is available only when the link is in 'video' mode (or, to put it more simply: you can send i2c configuration messages while the link is serializing video).Our implementation uses the link in 'configuration mode' during the remote side programming phase, at 'max9286_i2c_mux_init()' time, with the 'max9286_i2c_mux_select()' function enabling selectively the 'configuration link' of each single remote end. It probes the remote device by instantiating a new i2c_adapter connected to the mux, one for each remote end, and performs the device configuration by initially using its default power up i2c address (it is safe to do so, all other links are closed), then changes the remote devices address to an unique one(as our devices allows us to do so, otherwise you should use the deserializer address translation feature to mask and translate the remote addresses).Now all remote devices have an unique i2c address, and we can operate with all 'configuration links' open with no risk of i2c addresses collisions. At this point when we want to start the video stream, we send a control message to the remote device, which enables the pixel clock output from the image sensor, and activate the 'video channel' on the remote serializer. The local deserializer makes sure all 'video links 'are locked (see 'max9286_check_video_links()') and at this point we can begin serializing/deserializing video data. As you can see, the initial delay only plays a role in avoiding collision before we properly configure the channels and the i2c addresses. The link setup phase is instead an integral part of the system configuration, and there are no un-necessary delays used to work around it setup procedure. Does this help clarifying the system start-up procedure?","Hi Luca, Kieran,     sorry to jump up, but I feel this should be clarified.  On Wed, Nov 07, 2018 at 06:24:18PM +0100, Luca Ceresoli wrote:  The 8sec delay is due to the fact an integrated MCU on the remote camera module programs the local sensor and the serializer intgrated in the module in to some default configuration state. At power up, we just want to let it finish, with all reverse channels closed (camera module -> SoC direction) not to have the MCU transmitted messages repeated to the local side (our remote serializer does repeat messages not directed to it on it's remote side, as our local deserialier does).  The link up"" thing is fairly more complicated for GMSL than just having a binary ""on"" or ""off"" mode. This technology defines two different ""channels"", a 'configuration-channel' for transmitting control messages on the serial link (i2c messages for the deserializer/serializer pair this patches support) and a 'video-channel' for transmission of high-speed data, such as, no surprise, video and images :)  GMSL also defines two ""link modes"": a clock-less ""configuration link"" and an high-speed ""video link"". The ""configuration link"" is available a few msec after power up (roughly), while the ""video link"" needs a pixel clock to be supplied to the serializer for it to enter this mode and be able to lock the status between itself and the deserializer. Then it can begin serializing video data.  The 'control channel' is available both when the link is in 'configuration' and 'video' mode, while the 'video' channel is available only when the link is in 'video' mode (or, to put it more simply: you can send i2c configuration messages while the link is serializing video).  Our implementation uses the link in 'configuration mode' during the remote side programming phase, at 'max9286_i2c_mux_init()' time, with the 'max9286_i2c_mux_select()' function enabling selectively the 'configuration link' of each single remote end. It probes the remote device by instantiating a new i2c_adapter connected to the mux, one for each remote end, and performs the device configuration by initially using its default power up i2c address (it is safe to do so, all other links are closed), then changes the remote devices address to an unique one (as our devices allows us to do so, otherwise you should use the deserializer address translation feature to mask and translate the remote addresses).  Now all remote devices have an unique i2c address, and we can operate with all 'configuration links' open with no risk of i2c addresses collisions.  At this point when we want to start the video stream, we send a control message to the remote device, which enables the pixel clock output from the image sensor, and activate the 'video channel' on the remote serializer. The local deserializer makes sure all 'video links' are locked (see 'max9286_check_video_links()') and at this point we can begin serializing/deserializing video data.  As you can see, the initial delay only plays a role in avoiding collision before we properly configure the channels and the i2c addresses. The link setup phase is instead an integral part of the system configuration, and there are no un-necessary delays used to work around it setup procedure.  Does this help clarifying the system startup procedure?  Thanks    j """,technical,jacopo mondi,jacopo@jmondi.org,1,0,3202,0.8545706371191135,0.38461538461538464,0,0.10416666666666667,0.875,0.0,0.0
779,444822,450046,"Yes, that's very informative, thank you very much. Given the complexity of the driver and the non-obviousness of some workarounds to ""unfortunate hardware design choices"", I think [some of]this explanation should be committed together with the driver, in order to make it more understandable to other people. Even more since you've already taken time to write it.","Hi Jacopo,  On 08/11/18 11:11, jacopo mondi wrote:  Yes, that's very informative, thank you very much.  Given the complexity of the driver and the non-obviousness of some workarounds to unfortunate hardware design choices"", I think [some of] this explanation should be committed together with the driver, in order to make it more understandable to other people. Even more since you've already taken time to write it.  Thanks, --  Luca""",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,363,0.09833795013850416,0.4230769230769231,0,0.10416666666666667,0.875,0.0,0.10416666666666667
780,444822,457521,"All, sorry for joining this late... See below my considerations. I find this kind of address mapping is the weak point in this patchset. The ser-desert chipset splits the world in ""local"" and ""remote"" side. The camera node belongs to the remote side, but the 0x51 and 0x61 addresses belong to the local side. Think about supporting N different main boards and M remote boards. 0x51 might be available on some main boards but not all. IMO under the camera@51 (even the i2c@0) node there should be only remote hardware description. To support the N*M possible combinations, there should be: * a DT for the main board mentioning only addresses for the   local i2c bus, down to the i2c@0 with address-cells, size-cells and   reg properties * a DT overlay for each remote board, mentioning the remote i2c   chips with their physical addresses, but no local addresses The only way I could devise to be generic is to bind each physical remote address to a local address at runtime. Also, to be implemented reliably, an address translation feature is required on the local (de)ser chip. So the question is: can the  chip do i2c address translation?","Hi Kieran, All,  sorry for joining this late... See below my considerations.  On 02/11/18 16:47, Kieran Bingham wrote:  I find this kind of address mapping is the weak point in this patchset.  The ser-deser chipset splits the world in local"" and ""remote"" side. The camera node belongs to the remote side, but the 0x51 and 0x61 addresses belong to the local side. Think about supporting N different main boards and M remote boards. 0x51 might be available on some main boards but not all. IMO under the camera@51 (even the i2c@0) node there should be only remote hardware description. To support the N*M possible combinations, there should be:   * a DT for the main board mentioning only addresses for the    local i2c bus, down to the i2c@0 with address-cells, size-cells and    reg properties  * a DT overlay for each remote board, mentioning the remote i2c    chips with their physical addresses, but no local addresses  The only way I could devise to be generic is to bind each physical remote address to a local address at runtime.  Also, to be implemented reliably, an address translation feature is required on the local (de)ser chip.  So the question is: can the max9286 chip do i2c address translation?  Thanks, --  Luca""",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,1140,0.32409972299168976,0.46153846153846156,0,0.22916666666666666,0.7708333333333334,0.10416666666666667,0.0
781,444822,457530,"All, below a few minor questions, and a big one at the bottom.[...]5 pads, 4 formats. Why does the source node have no fmt? , even though using device tree I think this won't matter in the current kernel code. However I think ""max9286->sd.flags =..."" is more correct here, and it's also what most other drivers do. According to the docs MEDIA_ENT_F_VID_IF_BRIDGE appears more fitting. I can't manage to like this initialization sequence, sorry. If at all possible, each max9286 should initialize itself independently from each other, like any normal driver. First, it requires that each chip on the remote side can configure its own slave address. Not all chips do. Second, using a static i2c address map does not scale well and limits hot plugging, as I discussed in my reply to patch 1/4. The problem should be solvable cleanly if the MAX9286 supports address translation like the TI chips.","Hi Kieran, All,  below a few minor questions, and a big one at the bottom.  On 02/11/18 16:47, Kieran Bingham wrote:  [...]   5 pads, 4 formats. Why does the source node have no fmt?                            ^  This way you're clearing the V4L2_SUBDEV_FL_IS_I2C set by v4l2_i2c_subdev_init(), even though using devicetree I think this won't matter in the current kernel code. However I think max9286->sd.flags |= ..."" is more correct here, and it's also what most other drivers do.   According to the docs MEDIA_ENT_F_VID_IF_BRIDGE appears more fitting.   I can't manage to like this initialization sequence, sorry. If at all possible, each max9286 should initialize itself independently from each other, like any normal driver.  First, it requires that each chip on the remote side can configure its own slave address. Not all chips do.  Second, using a static i2c address map does not scale well and limits hotplugging, as I discussed in my reply to patch 1/4. The problem should be solvable cleanly if the MAX9286 supports address translation like the TI chips.  Thanks, --  Luca""",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,892,0.2548476454293629,0.5,0,0.22916666666666666,0.7708333333333334,0.0,0.0
782,444822,457544,"I'd say you're on time - not late, Thanks for joining :)Well, in our use case - in fact the camera has a set of fixed addresses for each camera - and these are the addresses we are requesting the camera to be updated to. Once the camera is communicated with - the first step is to reprogram the device to respond to the addresses specified here. Of course - well in fact all of our I2C addresses across our two max9286 instances, and 8 camera devices share the same bus 'address space'. It's crucial to provide this address on a per board level, which is why it is specified in the DT.I wonder if perhaps it was a mistake to include the camera description in this part of the example, as it's not related to the max9286 specifically. Rob has already suggested moving these to a lower 'i2c-node' level which I like the sound of, and might make this separation more clear. Yes, The max9286 (desert) can do i2c address translation - but so too can the max9271 (serialiser)We do our address translation on the camera (serialiser) side. The cameras *all* boot with the same i2c address (and thus all conflict) - We disable all links - We enable /one/ link - We initialise and reprogram the address of that camera to the address   specified in the camera node. - Then we move to the next camera. The reality is we 'just need' a spare address on the I2C bus - but as yet - there is no mechanism in I2C core to request a spare address. Thus it is the responsibility of the DT node to ensure there is no conflict. For an example, here is our DT overlay file for our max9286 expansion board:","Hi Luca,  On 13/11/2018 14:42, Luca Ceresoli wrote:  I'd say you're on time - not late,  Thanks for joining :)   Well, in our use case - in fact the camera has a set of fixed addresses (0x30,0x40,0x50) for each camera - and these are the addresses we are requesting the camera to be updated to. Once the camera is communicated with - the first step is to reprogram the device to respond to the addresses specified here.    Of course - well in fact all of our I2C addresses across our two max9286 instances, and 8 camera devices share the same bus 'address space'.  It's crucial to provide this address on a per board level, which is why it is specified in the DT.  I wonder if perhaps it was a mistake to include the camera description in this part of the example, as it's not related to the max9286 specifically.  Rob has already suggested moving these to a lower 'i2c-node' level which I like the sound of, and might make this separation more clear.    Yes, The max9286 (deser) can do i2c address translation - but so too can the max9271 (serialiser)  We do our address translation on the camera (serialiser) side.  The cameras *all* boot with the same i2c address (and thus all conflict)  - We disable all links  - We enable /one/ link  - We initialise and reprogram the address of that camera to the address    specified in the camera node. - Then we move to the next camera.  The reality is we 'just need' a spare address on the I2C bus - but as yet - there is no mechanism in I2C core to request a spare address.  Thus it is the responsibility of the DT node to ensure there is no conflict.  For an example, here is our DT overlay file for our max9286 expansion board:  https://git.kernel.org/pub/scm/linux/kernel/git/kbingham/rcar.git/commit/?h=gmsl/v5&id=6f2ec549e128b3ca36e9cae59256723cc39df2b1    --  Regards -- Kieran",technical,Kieran Bingham,kieran.bingham@ideasonboard.com,1,1,1581,0.46537396121883656,0.5384615384615384,0,0.22916666666666666,0.75,0.0,0.0
783,444822,457593,"Thank you for your review, The source pad is a CSI2 link - so a 'frame format' would be inappropriate. A quick glance looks like you're right. That looks like a good catch! I've updated locally ready for v5.Yes, I agree. We recently updated the adv748x to this too. Also updated locally to add to v5.Yes, I think we're in agreement here, but unfortunately this section is a workaround for the fact that our devices share a common address space. We (currently) *must* disable both devices before we start the initialisation process for either on our platform currently...That said - I think this section needs to be removed from the upstream part at least for now. I think we should probably carry this 'workaround' separately. This part is the core issue that I talked about in my presentation at ALS-Japan [0] don't think we can treat GMSL as hot-pluggable currently ... But as we discussed - I see that we should think about this for FPD-Link Also as a further aside here, we use ""device_is_bound"" which is not exported, and means that this driver won't compile successfully as a module currently (thanks to the kbuild test robot for highlighting that)","Hi Luca,  Thank you for your review,  On 13/11/2018 14:49, Luca Ceresoli wrote:  The source pad is a CSI2 link - so a 'frame format' would be inappropriate.    A quick glance looks like you're right. That looks like a good catch!  I've updated locally ready for v5.   Yes, I agree. We recently updated the adv748x to this too.  Also updated locally to add to v5.    Yes, I think we're in agreement here, but unfortunately this section is a workaround for the fact that our devices share a common address space.  We (currently) *must* disable both devices before we start the initialisation process for either on our platform currently...  That said - I think this section needs to be removed from the upstream part at least for now. I think we should probably carry this 'workaround' separately.  This part is the core issue that I talked about in my presentation at ALS-Japan [0]   [0] https://sched.co/EaXa   I don't think we can treat GMSL as hot-pluggable currently ... But as we discussed - I see that we should think about this for FPD-Link  Also as a further aside here, we use device_is_bound"" which is not exported, and means that this driver won't compile successfully as a module currently (thanks to the kbuild test robot for highlighting that)    --  Regards -- Kieran""",technical,Kieran Bingham,kieran.bingham@ideasonboard.com,1,1,1154,0.32271468144044324,0.5769230769230769,0,0.22916666666666666,0.75,0.0,0.0
784,444822,457869,"Yes, the way it works is clear. Interesting point. In my case I'm thinking DT overlays, they help me a lot in finding a proper generalization. With some generalization, camera modules [the same would happen with display modules] are similar to beaglebone capes or rpi hats: 1. there can be different camera modules being designed over time 2. there can be different base boards being designed over time 3. there is a standard inter connection between them (mechanical,    electrical, communication bus) 4. camera modules and base boards are designed and sold independently    (thanks to point 3)Overlays are a natural choice in this case. Even bootloader-time overlays will suffice for my reasoning, let's remove the hot plug mess from this discussion. Now, in this patch you are modeling the remote camera as if it were a""normal"" I2C device, except: a) it has 2 slave addresses (no problem with this) b) the 2 slave addresses in DT are not the physical ones With this model it seems natural to write ""camera@51/reg = <0x51 0x61>""in the camera DT overlay. Except 0x51 and 0x61 do not exist on the camera module, those numbers come from the base board, since you know those two addresses are not used on the bus where gmsl-deserializer@2cis. But it works. Then one year later a random SBC vendor starts selling a new base board that has on the same i2c bus a GMSL desert and a random i2c chip, unrelated to cameras, at address 0x51. Bang, the camera sensor does not work anymore, but there is no hardware reason for it not to work. Well, easy to fix, find an address that is unused on all known base boards and replace, say, 0x51->0x71 in the camera overlay. (OK, I violated the ""DT as a stable ABI"" principle) But then other boards appear and, taking this to an extreme, you can get to a situation where every i2c address is used on at least one board. How do you fix that? Maybe this scenario is a bit too apocalyptic, and maybe too much for current automotive uses, but I think it illustrates how the current model is not generic enough. Since there is no existing code in the kernel yet, I think we should strive to do better in order to minimize future problems. My approach is instead to clearly split the local and remote domain. The latter is what could be moved to an overlay. For example: The core difference is that I split the camera@51/reg property in two: * sensor@50/reg: the remote side (camera overlay),   carries the physical i2c address (note both sensors are at 0x50) * serializer@3d/i2c-alias-pool: the local side (base board),   lists a pool of addresses that are not used on the i2c bus See how there is no mixing between local and remote. The pool will differ from one base board to another. To implement this, I developed an ""i2c address translator"" that maps physical remote addresses to local addresses from the pool at runtime. It still needs some work, but address translation it is working. Good!By ""address translation"" I mean the i2c address is changed by some device in the middle between the i2c master and the slave. In this sense you are not doing address translation, you are rather modifying the chip addresses. Then transactions happen with the new (0x51/0x61) address, which does not get modified during subsequent transactions. Not a reliable one, definitely, since there could be i2c devices unknown to the software. This is why I had to introduce the alias pool: the DT writer is required to know which addresses are available and list them in DT","Hi Kieran,  On 14/11/18 00:12, Kieran Bingham wrote:  Yes, the way it works is clear.   Interesting point. In my case I'm thinking DT overlays, they help me a lot in finding a proper generalization. With some generalization, camera modules [the same would happen with display modules] are similar to beaglebone capes or rpi hats:   1. there can be different camera modules being designed over time  2. there can be different base boards being designed over time  3. there is a standard interconnection between them (mechanical,     electrical, communication bus)  4. camera modules and base boards are designed and sold independently     (thanks to point 3)  Overlays are a natural choice in this case. Even bootloader-time overlays will suffice for my reasoning, let's remove the hotplug mess from this discussion.  Now, in this patch you are modeling the remote camera as if it were a normal"" I2C device, except:   a) it has 2 slave addresses (no problem with this)  b) the 2 slave addresses in DT are not the physical ones  With this model it seems natural to write ""camera@51/reg = <0x51 0x61>"" in the camera DT overlay. Except 0x51 and 0x61 do not exist on the camera module, those numbers come from the base board, since you know those two addresses are not used on the bus where gmsl-deserializer@2c is. But it works.  Then one year later a random SBC vendor starts selling a new base board that has on the same i2c bus a GMSL deser and a random i2c chip, unrelated to cameras, at address 0x51. Bang, the camera sensor does not work anymore, but there is no hardware reason for it not to work. Well, easy to fix, find an address that is unused on all known base boards and replace, say, 0x51->0x71 in the camera overlay. (OK, I violated the ""DT as a stable ABI"" principle)  But then other boards appear and, taking this to an extreme, you can get to a situation where every i2c address is used on at least one board. How do you fix that?  Maybe this scenario is a bit too apocalyptic, and maybe too much for current automotive uses, but I think it illustrates how the current model is not generic enough. Since there is no existing code in the kernel yet, I think we should strive to do better in order to minimize future problems.   My approach is instead to clearly split the local and remote domain. The latter is what could be moved to an overlay. For example:  &i2c0 {     serializer@3d {         reg = <0x3d>,         ...          /* Guaranteed not physically present on i2c0 */         i2c-alias-pool = /bits/ 16 <0x20 0x21 0x22 0x23 0x24 0x25>,          i2c-mux {             #address-cells = <1>,             #size-cells = <0>,  	    i2c@0 {                 reg = <0>,                 #address-cells = <1>,                 #size-cells = <0>,                  // ------8<------ this could be moved to an overlay                 sensor@50 {                     reg = <0x50>,                     ...                     endpoint {...},                 },                 eeprom@51 {                     reg = <0x51>,                     ...                 },                 // ------8<------             },  	    i2c@1 {                 reg = <1>,                 #address-cells = <1>,                 #size-cells = <0>,                  // ------8<------                 sensor@50 {                     reg = <0x50>,                     ...                     endpoint {...},                 },                 eeprom@51 {                     reg = <0x51>,                     ...                 },                 // ------8<------             },         },     }, },  The core difference is that I split the camera@51/reg property in two:   * sensor@50/reg: the remote side (camera overlay),    carries the physical i2c address (note both sensors are at 0x50)  * serializer@3d/i2c-alias-pool: the local side (base board),    lists a pool of addresses that are not used on the i2c bus  See how there is no mixing between local and remote. The pool will differ from one base board to another.  To implement this, I developed an ""i2c address translator"" that maps physical remote addresses to local addresses from the pool at runtime. It still needs some work, but address translation it is working.   Good!   By ""address translation"" I mean the i2c address is changed by some device in the middle between the i2c master and the slave. In this sense you are not doing address translation, you are rather modifying the chip addresses. Then transactions happen with the new (0x51/0x61) address, which does not get modified during subsequent transactions.   Not a reliable one, definitely, since there could be i2c devices unknown to the software. This is why I had to introduce the alias pool: the DT writer is required to know which addresses are available and list them in DT.  --  Luca""",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,3488,1.0,0.6153846153846154,0,0.22916666666666666,0.75,0.0,0.0
785,444822,457868,"Ok, thanks for the clarification. The model I proposed in my review to patch 1/4 (split remote physical address from local address pool) allows to avoid this workaround. Oh, interesting, I hadn't noticed that you gave this talk -- at the same conference as Vladimir's talk! No video recording apparently, but are slides available at least? I've been mixing hot plug and DT overlays and that generated confusion, sorry. My point exists even with no hot plug, see the reply to patch 1/4.","Hi Kieran,  On 14/11/18 01:46, Kieran Bingham wrote:  Ok, thanks for the clarification.   The model I proposed in my review to patch 1/4 (split remote physical address from local address pool) allows to avoid this workaround.   Oh, interesting, I hadn't noticed that you gave this talk -- at the same conference as Vladimir's talk! No video recording apparently, but are slides available at least?   I've been mixing hotplug and DT overlays and that generated confusion, sorry. My point exists even with no hotplug, see the reply to patch 1/4.  --  Luca",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,485,0.13850415512465375,0.6538461538461539,0,0.22916666666666666,0.75,0.0,0.10416666666666667
786,444822,464006,"I'm happy to see this will be well maintained. :-)A part of this driver looks like a driver for an OV camera sensor. Would there be something that prevents separating the camera sensor driver from this one? Hmm. What are you using g_mbus_config() for? Do you need a busy loop? Could you use msleep()?return ov...(), ?You could use devm_kzalloc().=, as in the other patch. You're missing v4l2_ctrl_handler_free() here. As well as here. Could you use probe_new, so you could remove the i2c ID table? Or do you need that for something?-","Hi Kieran,  On Fri, Nov 02, 2018 at 03:47:23PM +0000, Kieran Bingham wrote:  I'm happy to see this will be well maintained. :-)   A part of this driver looks like a driver for an OV camera sensor. Would there be something that prevents separating the camera sensor driver from this one?   Hmm. What are you using g_mbus_config() for?   Do you need a busy loop? Could you use msleep()?   return ov...(), ?   You could use devm_kzalloc().   |=, as in the other patch.   You're missing v4l2_ctrl_handler_free() here.   As well as here.   Could you use probe_new, so you could remove the i2c ID table? Or do you need that for something?   --  Regards,  Sakari Ailus e-mail: sakari.ailus@iki.fi",technical,Sakari Ailus,sakari.ailus@iki.fi,1,0,533,0.1745152354570637,0.7307692307692307,0,0.3541666666666667,0.625,0.0,0.0
787,444822,464230,"No problem. Can the remote (max9271) translate addresses for transactions originating from the local side? This would make it possible to do a proper address translation, although 2 addresses is a quite small amount. BTW all the TI chips I'm looking at can do address translation but, as far as I understand, only when acting as ""slave proxy"", i.e. when attached to the bus master. If the Maxim chips do the same, the ""remote translation"" would be unusable. Sadly, it looks pretty much unavoidable...Thanks. Indeed it would have been! But hey, The FOSDEM CFPs are still open!","Hi Kieran,  On 20/11/18 01:32, Kieran Bingham wrote:  No problem.  [...]  Can the remote (max9271) translate addresses for transactions originating from the local side? This would make it possible to do a proper address translation, although 2 addresses is a quite small amount.  BTW all the TI chips I'm looking at can do address translation but, as far as I understand, only when acting as slave proxy"", i.e. when attached to the bus master. If the Maxim chips do the same, the ""remote translation"" would be unusable.   Sadly, it looks pretty much unavoidable...   Thanks.   Indeed it would have been!  But hey, The FOSDEM CFPs are still open!  Bye, --  Luca""",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,575,0.16897506925207756,0.7692307692307693,0,0.3541666666666667,0.625,0.0,0.0
788,444822,464769,"Yes, that's true for systems with a single max9286 [1]We have a system with 2 de-serializers, and what happens is the following: The system starts with the following configuration: with a single max9286 it would be easy. We operate on one channel at the time, do the reprogramming (or set up the translation, for the TIchip use case) when adding the adapter for the channel, and then we can talk with all remotes, which now have a different address2)      Of course, to do the reprogramming, we need to initially send messages to the default 0x40 address each max9271 boots with. If we don't close all channels but the one we intend to reprogram, all remotes would receive the same message, and thus will be re-programmed to the same address (not nice). [2]Now, if you have two max9286, installed on the same i2c bus, then you need to make sure all channels of the 'others' are closed, before you can reprogram your remotes, otherwise, you would end up reprogramming all the remotes of the 'others' when trying to reprogram yours, as our local de-serializers, bounces everything they receives, not directed to them, to their remote sides. When addr reprogramming is done, we enter the image streaming phase, with all channels open, as now, all remotes, have a different i2c address assigned. Suggestions on how to better handle this are very welcome. The point here is that, to me, this is a gmsl-specific implementation thing. Do you think for your chips, if they do translations, can you easy mask them with the i2c address you want (being that specified in the remote node or selected from an i2c-addr-pool, or something else) without having to care about others remotes to be accidentally programmed to an i2c address they're not intended to be assigned to. Hope this helps clarify your concerns, and I think the actual issue to discuss, at least on bindings, would be the i2c-address assignment method, as this impacts GMSL, as well as other implementation that would use the same binding style as this patches. Thanks   j[1] I still don't get why 'addr translation' >> 'addr reprogramming'. Even the GMSL application development examples uses addr reprogramming, so I guess this is how those chips are supposed to work.[2] If your local side supports address translation, you don't need to talk with the remote side to 'mask' it, so you don't need this workaround.","Hi Luca,  On Tue, Nov 20, 2018 at 11:51:37AM +0100, Luca Ceresoli wrote:  Yes, that's true for systems with a single max9286 [1]  We have a system with 2 de-serializers, and what happens is the following:  The system starts with the following configuration:  1)                     +------- max9271@40                     +------- max9271@40 Soc ----> max9286 --+------- max9271@40                     +------- max9271@40  with a single max9286 it would be easy. We operate on one channel at the time, do the reprogramming (or set up the translation, for the TI chip use case) when adding the adapter for the channel, and then we can talk with all remotes, which now have a different address  2)                     +-------- max9271@50                     +--- / -- max9271@40 Soc ----> max9286 --+--- / -- max9271@40                     +--- / -- max9271@40                       +--- / -- max9271@50                     +-------- max9271@51 Soc ----> max9286 --+--- / -- max9271@40                     +--- / -- max9271@40                      +--- / -- max9271@50                     +--- / -- max9271@51 Soc ----> max9286 --+-------- max9271@52                     +--- / -- max9271@40                      +--- / -- max9271@50                     +--- / -- max9271@51 Soc ----> max9286 --+--- / -- max9271@52                     +-------- max9271@53  Of course, to do the reprogramming, we need to initially send messages to the default 0x40 address each max9271 boots with. If we don't close all channels but the one we intend to reprogram, all remotes would receive the same message, and thus will be re-programmed to the same address (not nice). [2]  Now, if you have two max9286, installed on the same i2c bus, then you need to make sure all channels of the 'others' are closed, before you can reprogram your remotes, otherwise, you would end up reprogramming all the remotes of the 'others' when trying to reprogram yours, as our local de-serializers, bounces everything they receives, not directed to them, to their remote sides.  3)                        +-------- max9271@50                        +--- / -- max9271@40 Soc --+-> max9286@4c --+--- / -- max9271@40       |                +--- / -- max9271@40       |       |-> max9286@6c --+-------- max9271@50  <-- not nice                        +-------- max9271@50                        +-------- max9271@50                        +-------- max9271@50                         +--- / -- max9271@50                        +-------- max9271@51 Soc --+-> max9286@4c --+--- / -- max9271@40       |                +--- / -- max9271@40       |       |-> max9286@6c --+-------- max9271@51 <-- not nice                        +-------- max9271@51                        +-------- max9271@51                        +-------- max9271@51  ....  With the (not nice) 'max9286_is_bound()' we make sure we close all channels on all max9286 first  4)                        +--- / -- max9271@40                        +--- / -- max9271@40 Soc --+-> max9286@4c --+--- / -- max9271@40       |                +--- / -- max9271@40       |       |-> max9286@6c --+--- / -- max9271@40                        +--- / -- max9271@40                        +--- / -- max9271@40                        +--- / -- max9271@40  And then only the last one to probe calls the re-programming phase for all its fellows de-serializers on the bus.  5)                        +-------- max9271@50                        +--- / -- max9271@40 Soc --+-> max9286@4c --+--- / -- max9271@40       |                +--- / -- max9271@40       |       |-> max9286@6c --+--- / -- max9271@40                        +--- / -- max9271@40                        +--- / -- max9271@40                        +--- / -- max9271@40     ....                          +--- / -- max9271@50                        +--- / -- max9271@51 Soc --+-> max9286@4c --+--- / -- max9271@52       |                +--- / -- max9271@53       |       |-> max9286@6c --+-------- max9271@54                        +--- / -- max9271@40                        +--- / -- max9271@40                        +--- / -- max9271@40  When addr reprogramming is done, we enter the image streaming phase, with all channels open, as now, all remotes, have a different i2c address assigned.  Suggestions on how to better handle this are very welcome. The point here is that, to me, this is a gmsl-specific implementation thing.  Do you think for your chips, if they do translations, can you easy mask them with the i2c address you want (being that specified in the remote node or selected from an i2c-addr-pool, or something else) without having to care about others remotes to be accidentally programmed to an i2c address they're not intended to be assigned to.  Hope this helps clarify your concerns, and I think the actual issue to discuss, at least on bindings, would be the i2c-address assignment method, as this impacts GMSL, as well as other implementation that would use the same binding style as this patches.  Thanks    j  [1] I still don't get why 'addr translation' >> 'addr reprogramming'. Even the GMSL application development examples uses addr reprogramming, so I guess this is how those chips are supposed to work.  [2] If your local side supports address translation, you don't need to talk with the remote side to 'mask' it, so you don't need this workaround.",technical,jacopo mondi,jacopo@jmondi.org,1,0,2370,0.6717451523545707,0.8076923076923077,0,0.375,0.625,0.0,0.0
789,444822,465337,"This last sentence is the one point that makes things so hard on the GMSL chips. In my previous email(s) I partially forgot about this, so I was hoping  a better implementation could be possible. Thanks for re-focusing me.It would have been lovely if the hardware designers had at least put ani2c mux between the soc and those chatty deserializers... :-\Yes. The TI chips have a ""passthrough-all"" option to propagate all transactions with an unknown address, but it's mostly meant for debugging. In normal usage the local chip will propagate (with addresses translated) only transactions coming with a known slave address, including its own address(es), the remote (de)ser aliases and the remote chip aliases. All aliases are disabled until programmed. Absolutely.","Hi Jacopo,  On 20/11/18 18:44, jacopo mondi wrote:  This last sentence is the one point that makes things so hard on the GMSL chips. In my previous email(s) I partially forgot about this, so I was hoping  a better implementation could be possible. Thanks for re-focusing me.  It would have been lovely if the hardware designers had at least put an i2c mux between the soc and those chatty deserializers... :-\   Yes. The TI chips have a passthrough-all"" option to propagate all transactions with an unknown address, but it's mostly meant for debugging. In normal usage the local chip will propagate (with addresses translated) only transactions coming with a known slave address, including its own address(es), the remote (de)ser aliases and the remote chip aliases. All aliases are disabled until programmed.   Absolutely.  Bye, --  Luca""",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,764,0.20498614958448755,0.8461538461538461,0,0.375,0.6041666666666666,0.0,0.125
790,444822,472626,"Thank you for your review, Well it means /someone/ should always be able to pick it up :D I didn't know who to put here - so I put all of the current blames ,) We've all put a lot of time and work in to the GMSL bring up and refactoring. If you think it's overkill, I can reduce the names. Same on max9286.I don't think there's anything preventing it - except (a fair bit of) development time. We also have the RDACM21 to support, which uses the max9271 and anOV10640. At that time - this will absolutely have to be split. We shouldn't replicate the max9271 code. I mentioned briefly in the cover letter: But to get more dedicated time to work this - we need to show some progress on GMSL up-streaming, so the max9286 and bindings take priority for now. A little bit catch 22 ... :D I'm sure there will be overlap between GMSL and FPD-Link with the RDACM range [0] of cameras too, which also provide TI-FPD Link serialisers. I currently envisage that we would have an RDACM20 'driver' which would know that it has a max9271 serialiser and an OV10635 sensor, and would handle the links of any subdevices internally. As the RDACM20 is an object itself, I think this makes sense ... unless anyone suggests that each part should be broken down into the DT directly ? (I think that would possibly be a bit too much)[0]Good point here ... I assumed it was passed through up to the VIN - but it's really not applicable here. Or if it is - then it should be describing the GMSL bus link! I'll bet this isn't even getting called and can likely be removed. Checkpatch warns here: WARNING: msleep < 20ms can sleep for up to 20ms, see Documentation/timers/timers-howto.txt#10: FILE: drivers/media/i2c/rdacm20.c:461:+       msleep(10),I think for this context, msleep(10) even with the warning is fine here, but perhaps we can meet that with a usleep_range(10000, 20000), too. Yes, that would be nicer. Will change. Bah - yes :) Good spot. Thanks Ack.I believe probe_new is probably fine.I should really resurrect my i2c-probe-coccinelle patch and get that conversion task done, so we can get to removing and replacing .probe :)( note to self ... starting projects when unemployed becomes difficult to continue when someone else gives you projects to work on all the time ...) Changes described above made and tested on a *single MAX9286* capturing4 cameras simultaneously, now on my rcar.git gmsl/v5 branch ... :D","Hi Sakari,  Thank you for your review,  On 20/11/2018 08:34, Sakari Ailus wrote:  Well it means /someone/ should always be able to pick it up :D  I didn't know who to put here - so I put all of the current blamees ,) We've all put a lot of time and work in to the GMSL bring up and refactoring.  If you think it's overkill, I can reduce the names. Same on max9286.    I don't think there's anything preventing it - except (a fair bit of) development time.  We also have the RDACM21 to support, which uses the max9271 and an OV10640. At that time - this will absolutely have to be split. We shouldn't replicate the max9271 code.  I mentioned briefly in the cover letter:   But to get more dedicated time to work this - we need to show some progress on GMSL up-streaming, so the max9286 and bindings take priority for now.  A little bit catch 22 ... :D  I'm sure there will be overlap between GMSL and FPD-Link with the RDACM range [0] of cameras too, which also provide TI-FPD Link serialisers.  I currently envisage that we would have an RDACM20 'driver' which would know that it has a max9271 serialiser and an OV10635 sensor, and would handle the links of any subdevices internally.  As the RDACM20 is an object itself, I think this makes sense ... unless anyone suggests that each part should be broken down into the DT directly ? (I think that would possibly be a bit too much)   [0] https://www.global-imi.com/sites/default/files/Generic-Minicube-Catalogue-1020151.pdf    Good point here ... I assumed it was passed through up to the VIN - but it's really not applicable here.  Or if it is - then it should be describing the GMSL bus link!  I'll bet this isn't even getting called and can likely be removed.    Checkpatch warns here:  WARNING: msleep < 20ms can sleep for up to 20ms, see Documentation/timers/timers-howto.txt #10: FILE: drivers/media/i2c/rdacm20.c:461: +       msleep(10),  I think for this context, msleep(10) even with the warning is fine here, but perhaps we can meet that with a usleep_range(10000, 20000), too.    Yes, that would be nicer.   Will change.   Bah - yes :)    Good spot. Thanks    Ack.    I believe probe_new is probably fine.  I should really resurrect my i2c-probe-coccinelle patch and get that conversion task done, so we can get to removing and replacing .probe :)  (note to self ... starting projects when unemployed becomes difficult to continue when someone else gives you projects to work on all the time ...)    Changes described above made and tested on a *single MAX9286* capturing 4 cameras simultaneously, now on my rcar.git gmsl/v5 branch ... :D   --  Regards -- Kieran",technical,Kieran Bingham,kieran.bingham@ideasonboard.com,1,1,2401,0.6952908587257618,0.9230769230769231,0,0.5208333333333334,0.4583333333333333,0.0,0.0625
791,444822,476569,Yes. Indeed the pool can be seen as a list of physical addresses that: - are unused by other chips on the local bus - and given to the ser/des to use for the remote devices - will physically appear on the local bus to talk to remote devices Whether they are runtime translated or reprogrammed on the remote devices is not much relevant for the local bus. I don't foresee any problem in moving from a large pool at the serializer to small pools at each port.,"Hi Kieran,  On 27/11/18 23:47, Kieran Bingham wrote:  Yes. Indeed the pool can be seen as a list of physical addresses that:  - are unused by other chips on the local bus  - and given to the ser/des to use for the remote devices  - will physycally appear on the local bus to talk to remote devices  Whether they are runtime translated or reprogrammed on the remode devices is not much relevant for the local bus.   I don't foresee any problem in moving from a large pool at the serializer to small pools at each port.  --  Luca",technical,Luca Ceresoli,luca@lucaceresoli.net,1,0,457,0.12880886426592797,0.9615384615384616,0,0.6041666666666666,0.3958333333333333,0.0625,0.3958333333333333
792,444822,499770,"Apologies for the late reply. Not at all. There are too many drivers that do not receive the attention they'd need. :-I Btw. I think this might be worth a new comment to tell what devices can be found here --- it's not a camera sensor as such really. But I wonder how should it be called. We do have ""Miscellaneous helper chips"" at the end. I'm not sure that'd be better. As-is could be fine, too. Oh, sorry, I missed that. Does the DT currently contain all the necessary information for the drivers to get everything they need, if you separated them? I don't remember all the details, but my understanding is that RDACM20 is much more than just a box that contains the serialiser and the sensors. So very probably it'll need its own driver, too. Powering on the sensors, for instance, seemed hard to make generic. There are two use cases I know for this --- SoC camera and something that changes dynamically. The former is obsolete and the latter is better addressed by the frame descriptors I'd like to see go in for 4.22.usleep_range(), then, but just setting the delay to precisely 10 ms is much better than a 10 ms busy loop. That'd be nice!","Hu Kieran,  Apologies for the late reply.  On Wed, Nov 28, 2018 at 12:51:39PM +0000, Kieran Bingham wrote:  Not at all. There are too many drivers that do not receive the attention they'd need. :-I   Btw. I think this might be worth a new comment to tell what devices can be found here --- it's not a camera sensor as such really. But I wonder how should it be called. We do have Miscellaneous helper chips"" at the end. I'm not sure that'd be better. As-is could be fine, too.   Oh, sorry, I missed that.  Does the DT currently contain all the necessary information for the drivers to get everything they need, if you separated them?   I don't remember all the details, but my understanding is that RDACM20 is much more than just a box that contains the serialiser and the sensors. So very probably it'll need its own driver, too. Powering on the sensors, for instance, seemed hard to make generic.   There are two use cases I know for this --- SoC camera and something that changes dynamically. The former is obsolete and the latter is better addressed by the frame descriptors I'd like to see go in for 4.22.   usleep_range(), then, but just setting the delay to precisely 10 ms is much better than a 10 ms busy loop.   That'd be nice!   --  Regards,  Sakari Ailus""",technical,Sakari Ailus,sakari.ailus@iki.fi,1,0,1146,0.3490304709141274,1.0,1,1.0,0.0,0.3958333333333333,0.0
793,445707,445723,This bit is pretty unsightly. Especially the static in each inline,"On Sat, 2018-11-03 at 17:51 +0000, Pawel Laszczak wrote: []  This bit is pretty unsightly. Especially the static in each inline  [] [] [] []",technical,Joe Perches,joe@perches.com,1,0,66,0.027842227378190254,0.5961538461538461,0,0.0,1.0,0.0,0.25
794,445707,446342,"I understood that you mean line static char str[256],This array will be defined several times. I will remove inline form function definition. It's not necessary. Thank you for comment.","Hi Joe.  I understood that you mean line  static char str[256], This array will be defined several times.   I will remove inline form function definition.  It's not necessary.    Thank you for comment. Cheers Pawel",technical,Pawel Laszczak,pawell@cadence.com,1,1,184,0.09048723897911833,0.6153846153846154,0,0.25,0.75,0.25,0.0
795,445707,447965,Why not depend on USB instead of USB_XHCI_HCD? Need a comma between switch and Host-only. Why depend on Config options to populate resources? The resources should be there regardless. It is a lot simpler that way as it reflects the hardware as-is.dev_dbg() for this and all occurrences below?,"Hi Pawel,  On 03/11/18 19:51, Pawel Laszczak wrote:  s/drivier/driver s/creaties/creates s/in system/in-system   Why not depend on USB instead of USB_XHCI_HCD?   Need a coma between switch and Host-only.   Why depend on Config options to populate resources? The resources should be there regardless. It is a lot simpler that way as it reflects the hardware as-is.   dev_dbg() for this and all occurrences below?   cheers, -roger --  Texas Instruments Finland Oy, Porkkalankatu 22, 00180 Helsinki. Y-tunnus/Business ID: 0615521-4. Kotipaikka/Domicile: Helsinki",technical,Roger Quadros,rogerq@ti.com,1,0,292,0.12761020881670534,0.6538461538461539,0,0.5,0.5,0.25,0.0
796,445707,448013,"Is it better to split this patch into 3 parts?1) host support2) gadget support3) DRD support (along with patch 4)how about naming this to cnds3_get_current_role_driver() ?You are changing the role here. Shouldn't it just start whatever role is already in cdns->role? And you have a cnds3_set_role() function to set role. Where is the balancing pm_runtime_put() for this? All role switching code can come as part of DRD driver. Here you are not checking for Kconfig options before getting resources which is the right thing. However this will be broken if you don't get rid of the Kconfig checks when you populate the resources in patch 1.What exactly does role_start have to do? Can you start the role before requesting irq? Shouldn't the order bepm_runtime_get_sync(),cdns3_remove_roles(),pm_runtime_put_noidle(),pm_runtime_disable(),you didn't call usb_phy_init() anywhere. Why is a call to host_driver_init() required? Why is OTG after END? Does OTG have a role driver as well? If not it must not come here. It is a mode, not a role. Why is cdns3_host_driver_init() required?","On 03/11/18 19:51, Pawel Laszczak wrote:  Is it better to split this patch into 3 parts? 1) host support 2) gadget support 3) DRD support (along with patch 4)   how about naming this to cnds3_get_current_role_driver() ?   You are changing the role here. Shouldn't it just start whatever role is already in cdns->role? And you have a cnds3_set_role() function to set role.   where is the balancing pm_runtime_put() for this?   All role switching code can come as part of DRD driver.   Here you are not checking for Kconfig options before getting resources which is the right thing. However this will be broken if you don't get rid of the Kconfig checks when you populate the resources in patch 1.   What exactly does role_start have to do?  Can you start the role before requesting irq?   Shouldn't the order be  pm_runtime_get_sync(), cdns3_remove_roles(), pm_runtime_put_noidle(), pm_runtime_disable(),   you didn't call usb_phy_init() anywhere.   Why is a call to host_driver_init() required?   Why is OTG after END? Does OTG have a role driver as well? If not it must not come here. It is a mode, not a role.   why is cdns3_host_driver_init() required?   cheers, -roger --  Texas Instruments Finland Oy, Porkkalankatu 22, 00180 Helsinki. Y-tunnus/Business ID: 0615521-4. Kotipaikka/Domicile: Helsinki",technical,Roger Quadros,rogerq@ti.com,1,0,1078,0.5220417633410673,0.6730769230769231,0,0.5,0.5,0.0,0.0
797,445707,448022,"I think we need to make a clear distinction between mode and role. Mode is the controller mode. (Host-only, Device-only, dual-role[otg]) Role is the USB controller state should correspond to enum usb_dr_mode which should be the argument for this function if you want to set mode. Why not just use cdns->dr_mode directly? Do you want to check if it is host role here? e.g. if dr_mode = USB_DR_MODE_OTG. Looks like this function should be on core.c?CDNS3_ROLE_HOST.should this be called cdns3_drd_init()?s/HOST/PERIPHERAL?","On 03/11/18 19:51, Pawel Laszczak wrote:  I think we need to make a clear distinction between mode and role.  Mode is the controller mode. (Host-only, Device-only, dual-role[otg]) Role is the USB controller state (A-Host, B-Device, Idle)  cnds->dr_mode should correspond to enum usb_dr_mode which should be the argument for this function if you want to set mode.   Why not just use cdns->dr_mode directly?  Do you want to check if it is host role here? e.g. if dr_mode = USB_DR_MODE_OTG.   Looks like this function should be on core.c?   CDNS3_ROLE_HOST.  should this be called cdns3_drd_init()?   s/HOST/PERIPHERAL?   cheers, -roger  --  Texas Instruments Finland Oy, Porkkalankatu 22, 00180 Helsinki. Y-tunnus/Business ID: 0615521-4. Kotipaikka/Domicile: Helsinki",technical,Roger Quadros,rogerq@ti.com,1,0,520,0.23897911832946636,0.6923076923076923,0,0.5,0.25,0.0,0.0
798,445707,448039,Why does role driver need hook to irq handler? Can't each driver host or gadget handle it's respective irq on its own? If the same IRQ line is used it could be requested as a shared IRQ.,"On 03/11/18 19:51, Pawel Laszczak wrote:   <snip>   Why does role driver need hook to irq handler?  Can't each driver host or gadget handle it's respective irq on its own? If the same IRQ line is used it could be requested as a shared IRQ.   cheers, -roger  --  Texas Instruments Finland Oy, Porkkalankatu 22, 00180 Helsinki. Y-tunnus/Business ID: 0615521-4. Kotipaikka/Domicile: Helsinki",technical,Roger Quadros,rogerq@ti.com,1,0,186,0.09744779582366589,0.7115384615384616,0,0.5,0.25,0.0,0.0
799,445707,448046,If we start with OTG mode and user says change mode to device will we still switch to host based on ID pin change? If it does then this isn't working correctly. We need to stop processing ID interrupts and keep the role static till the user switches it back to otg.,"On 03/11/18 19:51, Pawel Laszczak wrote:  If we start with OTG mode and user says change mode to device will we still switch to host based on ID pin change?  If it does then this isn't working correctly. We need to stop processing ID interrupts and keep the role static till the user switches it back to otg.   cheers, -roger --  Texas Instruments Finland Oy, Porkkalankatu 22, 00180 Helsinki. Y-tunnus/Business ID: 0615521-4. Kotipaikka/Domicile: Helsinki",technical,Roger Quadros,rogerq@ti.com,1,0,265,0.12993039443155452,0.7307692307692307,0,0.5,0.25,0.0,0.0
800,445707,448833,"I will replace it with this: Depend on USB_SUPPORT && (USB l USB_GADGET) && HAS_DMA You're right. I will remove these Config options from this code. Ok , I replaced it. Thanks for all comments","Hi Roger, 	 I will replace it with this: Depend on USB_SUPPORT && (USB l| USB_GADGET) && HAS_DMA   You're right. I will remove these Config options from this code. Ok , I replaced it.    Thanks for all comments  Regards, Pawel Laszczak",technical,Pawel Laszczak,pawell@cadence.com,1,1,192,0.10208816705336426,0.75,0,0.75,0.25,0.0,0.0
801,445707,449038,"Currently we have:0003-usb-cdns3-Driver-initialization-code.patch - generic initialization common code. I agree that some fragment could be moved to next free patches. It could improve understanding of code.0004-usb-cdns3-Added-DRD-support.patch - it's short file so contains initialization and other related to DRD function.0005-usb-cdns3-Added-Wrapper-to-XCHI-driver.patch - host support - quite short patch0006-usb-cdns3-Initialization-code-for-Device-side - device initialization It's better. I've changed it. The function cdns3_set_role currently do nothing. A little explanation. The main author of this file is Peter Chan. We use the same controller, but we have different platforms. I adopt this file to my platform. I tried to keep the concept of his solution and remove only platform specific codes. I assume this approach allows him easily to re-use this code in the feature. In cdns3_set_role he makes some platform specific code that allow him to switch between Host and device. I do not need it In the  feature this function probably will call some platform specific function. Currently this function is not used so I will remove  will remove this fragment. It will be implemented later in pm runtime functions. Detecting Host and Device mode can be achieved in different ways depending on platform. I assume that core.c file is generic part that allow to connect different way of detecting modes. In my solution I use OTG registers which are part of this controller. But someone can use  extcon to get information about mode e.g,I treat drd.c file rather as platform specific extension for this driver, patch 1 will be changed. Start the Device or Host mode.  After this operation device/host can handle interrupts. A good point. It's works correct, but I will change this. It's look strange. Yes, I found it too. In next series this will be removed. Also, I will add the phy initialization, but I want to use generic phy instead of usb_phy. I'm not sure whether it is proper solution, but we have only generic simple phy driver. No OTG doesn't have role driver. It was the simplest solution. It's used only in debugfs.c  and drd.c files. I'll give it some thought how to change it without big impact to rest codes. It's initialize xhci driver with. Maybe I will move this function to  0005-usb-cdns3-Added-Wrapper-to-XCHI-driver.patch then all code will be in this some patch. Thanks for all comment","Currently we have: 0003-usb-cdns3-Driver-initialization-code.patch - generic initialization common code.  I agree that some fragment could be moved to next free patches. It could improve understanding of code.   0004-usb-cdns3-Added-DRD-support.patch - it's short file so contains initialization and other related to DRD function.  0005-usb-cdns3-Added-Wrapper-to-XCHI-driver.patch - host support - quite short patch  0006-usb-cdns3-Initialization-code-for-Device-side - device initialization    It's better. I've changed it.   The function cdns3_set_role currently do nothing.  A little explanation. The main author of this file is Peter Chan.  We use the same controller, but we have different platforms. I adopt this file to my platform. I tried to keep the concept of his solution and remove only platform specific codes.  I assume this approach allows him easily to re-use this code in the feature.   In cdns3_set_role he makes some platform specific code that allow him to switch between Host and device. I do not need it In the  feature this function probably will call some platform specific function.  Currently this function is not used so I will remove it.  I will remove this fragment. It will be implemented later in pm runtime functions.   Detecting Host and Device mode can be achieved in different ways depending on platform. I assume that core.c file is generic part that allow to connect different way of detecting modes. In my solution I use OTG registers which are part of this controller. But someone can use  extcon to get information about mode e.g: 	host = extcon_get_state(cdns->extcon, EXTCON_USB_HOST), 	device = extcon_get_state(cdns->extcon, EXTCON_USB),  I treat drd.c file rather as platform specific extension for this driver.    Ok, patch 1 will be changed. Start the Device or Host mode.  After this operation device/host can handle interrupts.  A good point.  It's works correct, but I will change this. It's look strange.   Yes, I found it too. In next series this will be removed.  Also, I will add the phy initialization, but I want to use generic phy instead of usb_phy. I'm not sure whether it is proper solution, but we have only generic simple phy driver.     No OTG doesn't have role driver.   It was the simplest solution. It's used only in debugfs.c  and drd.c files. I'll give it some thought how to change it without big impact to rest codes.   It's initialize xhci driver with. Maybe   static const struct xhci_driver_overrides xhci_cdns3_overrides __initconst = { 	.extra_priv_size = sizeof(struct xhci_hcd), 	.reset = xhci_cdns3_setup, }, void __init cdns3_host_driver_init(void) { 	xhci_init_driver(&xhci_cdns3_hc_driver, &xhci_cdns3_overrides), }   I will move this function to  0005-usb-cdns3-Added-Wrapper-to-XCHI-driver.patch then all code will be in this some patch.     Thanks for all comment,  Regards, Pawel Laszczak.",technical,Pawel Laszczak,pawell@cadence.com,1,1,2415,1.0,0.7884615384615384,0,0.75,0.25,0.0,0.0
802,445707,449946,How often are these invoked? For I/O intensive cases dev_dbg() will not be useful as it will affect timing adversely because of which it might prevent the issue from happening when debug is enabled. How about using tracepoints instead?,"Hi,  On 03/11/18 19:51, Pawel Laszczak wrote:  How often are these invoked?  For I/O intensive cases dev_dbg() will not be useful as it will affect timing adversely because of which it might prevent the issue from happening when debug is enabled.  How about using tracepoints instead?  cheers, -roger --  Texas Instruments Finland Oy, Porkkalankatu 22, 00180 Helsinki. Y-tunnus/Business ID: 0615521-4. Kotipaikka/Domicile: Helsinki",technical,Roger Quadros,rogerq@ti.com,1,0,235,0.10208816705336426,0.8269230769230769,0,1.0,0.0,0.0,0.0
803,445707,450053,"I agree with you,  In next set patch distinction between mode and role will be clear. The function will be slightly completed. After corrections: dr_mode holds the hardware configuration. It will be set during initialization and will not be changed. current_dr_mode will hold current mode. This mode can be changed by debugfs. This function will look: If DRD mode is set to HOST then always return true, elsewhere it will be based on ID pin. So, for USB_DR_MODE_OTG driver will waiting for appropriate role but when current_dr_mode == USB_DR_MODE_HOST then we know that only HOST role can be set and driver can return true immediately.> The name will be changed to cdns3_drd_update_mode and will be called during initialization, or when user change mode by means of debugfs.I found it too and was corrected. I changed the name cdns3_drd_init().  I call this cdns3_drd_probe, because I was not sure whether DRD will be part of whole driver or will be separate driver. Thanks for all your comments","Hi Roger,   I agree with you,  In next set patch distinction between mode and role will be clear.   The function will be slightly completed.   After corrections: dr_mode holds the hardware configuration. It will be set during initialization and will not be changed.  current_dr_mode will hold current mode. This mode can be changed by debugfs.   This function will look:  int cdns3_is_host(struct cdns3 *cdns) { 	if (cdns->current_dr_mode == USB_DR_MODE_HOST) 		return 1, 	else if (cdns->current_dr_mode == USB_DR_MODE_OTG) 		if (!cdns3_otg_get_id(cdns)) 			return 1,  	return 0, }  If DRD mode is set to HOST then always return true, elsewhere it will be based on ID pin.  So, for USB_DR_MODE_OTG driver will waiting for appropriate role but when  current_dr_mode == USB_DR_MODE_HOST then we know that only HOST role can be set  and driver can return true immediately.>  The name will be changed to cdns3_drd_update_mode and will be called during initialization,  or when user change mode by means of debugfs.  I found it too and was corrected.   I changed the name cdns3_drd_init().  I call this cdns3_drd_probe, because I was not sure whether DRD will be part of whole driver or will be separate driver.     Thanks for all your comments Cheers, Pawel Laszczak",technical,Pawel Laszczak,pawell@cadence.com,1,1,995,0.42923433874709976,0.8461538461538461,0,1.0,0.0,0.0,0.0
804,445707,450058,"When controller is working as Device then host part of controller is held in reset and vice versa, so driver has restricted access to registers."," When controller is working as Device then host part of controller is held in reset and  vice versa, so driver has restricted access to registers.    Thanks, Cheers, Pawel Laszczak",technical,Pawel Laszczak,pawell@cadence.com,1,1,144,0.06264501160092807,0.8846153846153846,0,1.0,0.0,0.0,0.0
805,445707,450064,Switching role form user space will limited driver only to selected mode. Only for USB_DR_MODE_OTG driver should base on ID pin. That's my intension.," Switching role form user space will limited driver only to selected mode.  Only for USB_DR_MODE_OTG driver should base on ID pin.  That's my intension.    Thanks, Cheers, Pawel",technical,Pawel Laszczak,pawell@cadence.com,1,1,149,0.06496519721577726,0.9230769230769231,0,1.0,0.0,0.0,0.0
806,445707,450069,"Yes, I know and I agree with you. I have plan to replace this with trace points. But I need some time for this. Currently I'm focusing on testing. Probably  I will change It by the end of this month, so it should be in RFC PATCH v3.","Yes, I know and I agree with you.  I have plan to replace this with tracepoints.  But I need some time for this. Currently I'm focusing on testing. Probably  I will change It by the end of this month, so it should be in  RFC PATCH v3.    Thanks, Cheers, Pawell",technical,Pawel Laszczak,pawell@cadence.com,1,1,232,0.12993039443155452,0.9615384615384616,0,1.0,0.0,0.0,0.0
807,445707,450220,User space setting should override ID if it was OTG mode. At least this is how it is for dwc3. That way it is useful for testing role swap.,"On 08/11/18 13:51, Pawel Laszczak wrote:  User space setting should override ID if it was OTG mode. At least this is how it is for dwc3. That way it is useful for testing role swap.  cheers, -roger   --  Texas Instruments Finland Oy, Porkkalankatu 22, 00180 Helsinki. Y-tunnus/Business ID: 0615521-4. Kotipaikka/Domicile: Helsinki",technical,Roger Quadros,rogerq@ti.com,1,0,139,0.07424593967517401,1.0,1,1.0,0.0,0.0,0.0
808,446846,460398,I think you should send it to ASoC maintainer and list for applying. Shawn,"On Mon, Nov 05, 2018 at 02:58:02PM +0100, Clment Pron wrote:  I think you should send it to ASoC maintainer and list for applying.  Shawn",technical,Shawn Guo,shawnguo@kernel.org,1,0,74,0.42857142857142855,0.3333333333333333,0,0.9090909090909091,0.09090909090909091,0.9090909090909091,0.0
809,446846,460606,"It's still quite new for me to submit patch, but if this patch should be sent to ASoC maintainer maybe there is a line missing in the MAINTAINERS file no ? SOUNDM:","Hi Shawn,  On Fri, 16 Nov 2018 at 03:59, Shawn Guo <shawnguo@kernel.org> wrote:  It's still quite new for me to submit patch, but if this patch should be sent to ASoC maintainer maybe there is a line missing in the MAINTAINERS file no ?  SOUND M:      Jaroslav Kysela <perex@perex.cz> M:      Takashi Iwai <tiwai@suse.com> L:      alsa-devel@alsa-project.org (moderated for non-subscribers) W:      http://www.alsa-project.org/ T:      git git://git.kernel.org/pub/scm/linux/kernel/git/tiwai/sound.git T:      git git://git.alsa-project.org/alsa-kernel.git Q:      http://patchwork.kernel.org/project/alsa-devel/list/ S:      Maintained F:      Documentation/sound/ + F:      include/dt-bindings/sound/ F:      include/sound/ F:      include/uapi/sound/ F:      sound/  Regards, Clement",technical,Clément Péron,peron.clem@gmail.com,0,1,163,1.0,0.5,0,0.9090909090909091,0.09090909090909091,0.0,0.0
810,446846,460805,"Mark, comment?","On Fri, Nov 16, 2018 at 11:25:07AM +0100, Clment Pron wrote:  @Mark, comment?  Shawn",technical,Shawn Guo,shawnguo@kernel.org,1,0,14,0.11428571428571428,0.6666666666666666,0,1.0,0.0,0.0,0.0
811,446846,461273,"Adding a file pattern for this seems sensible, yes.","On Fri, Nov 16, 2018 at 10:45:22PM +0800, Shawn Guo wrote:    Adding a file pattern for this seems sensible, yes.",technical,Mark Brown,broonie@kernel.org,1,0,51,0.3142857142857143,0.8333333333333334,0,1.0,0.0,0.0,0.0
812,446846,461367,"OK, I will send a v2 for the IMX6 Audmux bindings with ASoC List and Maintainers.","Hi Mark,  On Sat, 17 Nov 2018 at 04:09, Mark Brown <broonie@kernel.org> wrote: OK,  I will send a v2 for the IMX6 Audmux bindings with ASoC List and Maintainers.  Thanks, Clement",technical,Clément Péron,peron.clem@gmail.com,0,1,81,0.5142857142857142,1.0,1,1.0,0.0,0.0,0.0
813,450129,450132,"This is v2 patch. I'm sorry that I forgot to add the word ""v2"" to the subject.","Hi Peter,  This is v2 patch. I'm sorry that I forgot to add the word v2"" to the subject.  Yours, Muchun Song""",technical,Muchun Song,smuchun@gmail.com,0,1,78,1.0,1.0,1,0.0,0.0,0.0,0.0
814,452344,452491,"AFAICS this will break e.g. x86 which can have both ZONE_DMA andZONE_DMA32, and now you would make kmalloc(__GFP_DMA) return objects from ZONE_DMA32 instead of __ZONE_DMA, which can break something. Also I'm probably missing the point of this all. In patch 3 you use__get_dma32_pages() thus __get_free_pages(__GFP_DMA32), which uses alloc_pages, thus the page allocator directly, and there's no slab caches involved. It makes little sense to involve slab for page table allocations anyway, as those tend to be aligned to a page size (or high-order page size). So what am I missing?","On 11/9/18 9:24 AM, Nicolas Boichat wrote:  AFAICS this will break e.g. x86 which can have both ZONE_DMA and ZONE_DMA32, and now you would make kmalloc(__GFP_DMA) return objects from ZONE_DMA32 instead of __ZONE_DMA, which can break something.  Also I'm probably missing the point of this all. In patch 3 you use __get_dma32_pages() thus __get_free_pages(__GFP_DMA32), which uses alloc_pages, thus the page allocator directly, and there's no slab caches involved. It makes little sense to involve slab for page table allocations anyway, as those tend to be aligned to a page size (or high-order page size). So what am I missing?",technical,Vlastimil Babka,vbabka@suse.cz,1,0,581,0.7651006711409396,0.7142857142857143,0,0.0,0.0,0.0,0.0
815,452344,452548,"Oh, I was not aware that both ZONE_DMA and ZONE_DMA32 can be defined at the same time. I guess the test should be inverted, something like this (can be simplified...) 32_pages fixes level 1 page allocations in the patch 3.This change fixes level 2 page allocations(kmem_cache_zalloc(data->l2_tables, gfp  GFP_DMA)), by transparently remapping GFP_DMA to an underlying ZONE_DMA32.The alternative would be to create a new SLAB_CACHE_DMA32 when CONFIG_ZONE_DMA32 is defined, but then I'm concerned that the callers would need to choose between the 2 (GFP_DMA or GFP_DMA32...), and also need to use some ifdefs (but maybe that's not a valid concern?).Level 2 tables are ARM_V7S_TABLE_SIZE(2) => 1kb, so we'd waste 3kb if we allocated a full page.","On Fri, Nov 9, 2018 at 6:43 PM Vlastimil Babka <vbabka@suse.cz> wrote:  Oh, I was not aware that both ZONE_DMA and ZONE_DMA32 can be defined at the same time. I guess the test should be inverted, something like this (can be simplified...): #ifdef CONFIG_ZONE_DMA #define SLAB_CACHE_DMA_GFP GFP_DMA #elif defined(CONFIG_ZONE_DMA32) #define SLAB_CACHE_DMA_GFP GFP_DMA32 #else #define SLAB_CACHE_DMA_GFP GFP_DMA // ? #endif   __get_dma32_pages fixes level 1 page allocations in the patch 3.  This change fixes level 2 page allocations (kmem_cache_zalloc(data->l2_tables, gfp | GFP_DMA)), by transparently remapping GFP_DMA to an underlying ZONE_DMA32.  The alternative would be to create a new SLAB_CACHE_DMA32 when CONFIG_ZONE_DMA32 is defined, but then I'm concerned that the callers would need to choose between the 2 (GFP_DMA or GFP_DMA32...), and also need to use some ifdefs (but maybe that's not a valid concern?).   Level 2 tables are ARM_V7S_TABLE_SIZE(2) => 1kb, so we'd waste 3kb if we allocated a full page.  Thanks,",technical,Nicolas Boichat,drinkcat@chromium.org,0,1,742,1.0,0.8571428571428571,0,0.0,0.0,0.0,0.0
816,452344,452554,"Oh, I see. Well, I think indeed the most transparent would be to support SLAB_CACHE_DMA32. The callers of kmem_cache_zalloc() would then need not add anything special to gfp, as that's stored internally upon kmem_cache_create(). Of course SLAB_BUG_MASK would no longer have to treat __GFP_DMA32 as unexpected. It would be unexpected when passed to kmalloc() which doesn't have special dma32 caches, but for a cache explicitly created to allocate from ZONE_DMA32, I don't see why not. I'm somewhat surprised that there wouldn't be a need for this earlier, so maybe I'm still missing something.","On 11/9/18 12:57 PM, Nicolas Boichat wrote:  Oh, I see.  Well, I think indeed the most transparent would be to support SLAB_CACHE_DMA32. The callers of kmem_cache_zalloc() would then need not add anything special to gfp, as that's stored internally upon kmem_cache_create(). Of course SLAB_BUG_MASK would no longer have to treat __GFP_DMA32 as unexpected. It would be unexpected when passed to kmalloc() which doesn't have special dma32 caches, but for a cache explicitly created to allocate from ZONE_DMA32, I don't see why not. I'm somewhat surprised that there wouldn't be a need for this earlier, so maybe I'm still missing something.",technical,Vlastimil Babka,vbabka@suse.cz,1,0,592,0.785234899328859,1.0,1,0.0,0.0,0.0,0.0
817,454270,455529,Do you have any numbers for how much difference this change makes with various different workloads?,"On Mon, Nov 12, 2018 at 12:26:10AM +0300, Timofey Titovets wrote:  Do you have any numbers for how much difference this change makes with various different workloads?",technical,Matthew Wilcox,willy@infradead.org,1,0,99,0.07296137339055794,0.6666666666666666,0,0.0,0.0,0.0,0.0
818,454270,456049,"Yep, I got some non KVM numbers, Formulas: Percentage - (pages_sharing - pages_shared)/pages_unshared Memory saved - (pages_sharing - pages_shared)*4/1024 MiB- My working laptop: 5% - ~100 MiB saved ~2GiB used  Many different chrome based apps + KDE- K8s test VM:  40% - ~160 MiB saved ~920MiB used  With some small running docker images- Ceph test VM: 20% - ~60MiB saved ~600MiB used  With ceph mon, osd. Develop cluster servers:- K8s server backend: 72%, ~5800 MiB saved ~35.7 GiB used  (With backend apps: C, java, go & etc server apps)- K8s server processing: 55%, ~2600 MiB saved ~28 GiB used  (90% of load many instance of one CPU intensive application)- Ceph node: 2%, ~190 MiB saved ~11.7 GiB used  (OSD only) So numbers, as always depends on the load. Thanks!- - -P.S.On recent kernels (4.19) i see BUG_ON message, that ksmd scheduled while in critical section/atomic context, not sure how to properly fix that. (If i understood correctly, i can use preempt_disable(), but that looks more like hack, not a fix). Any feedback are welcome.","пн, 12 нояб. 2018 г. в 6:58, Matthew Wilcox <willy@infradead.org>:  Yep, i got some non KVM numbers, Formulas:  Percentage - (pages_sharing - pages_shared)/pages_unshared  Memory saved - (pages_sharing - pages_shared)*4/1024 MiB  - My working laptop: 5% - ~100 MiB saved ~2GiB used   Many different chrome based apps + KDE  - K8s test VM:  40% - ~160 MiB saved ~920MiB used   With some small running docker images  - Ceph test VM: 20% - ~60MiB saved ~600MiB used   With ceph mon, osd.  Develop cluster servers: - K8s server backend: 72%, ~5800 MiB saved ~35.7 GiB used   (With backend apps: C, java, go & etc server apps)  - K8s server processing: 55%, ~2600 MiB saved ~28 GiB used   (90% of load many instance of one CPU intensive application)  - Ceph node: 2%, ~190 MiB saved ~11.7 GiB used   (OSD only)   So numbers, as always depends on the load.  Thanks! - - - P.S. On recent kernels (4.19) i see BUG_ON message, that ksmd scheduled while in critical section/atomic context, not sure how to properly fix that. (If i understood correctly, i can use preempt_disable(), but that looks more like hack, not a fix).  Any feedback are welcome.",technical,Timofey Titovets,timofey.titovets@synesis.ru,0,1,1046,1.0,1.0,1,0.0,0.0,0.0,0.0
819,459182,459240,"Isn't it more straight-forward to use ""rcu-rscs^-1"" other than""rcu-rscsi"" in the definition of ""rcu-fence"", is it? The introduction of ""rcu-rscsi"" makes sense in the first patch, but with this refactoring, I think it's better we just don't use it.","Hi Alan,  On Thu, Nov 15, 2018 at 11:19:58AM -0500, Alan Stern wrote:  Isn't it more straight-forward to use rcu-rscs^-1"" other than ""rcu-rscsi"" in the definition of ""rcu-fence"", is it?  The introduction of ""rcu-rscsi"" makes sense in the first patch, but with this refactoring, I think it's better we just don't use it.  Regards, Boqun  """,technical,Boqun Feng,boqun.feng@gmail.com,1,0,247,0.875,0.6666666666666666,0,0.0,0.0,0.0,0.0
820,459182,459311,"It's a matter of personal preference.  I prefer to store the inverse relation in a separate variable rather than recomputing it multiple times.  (Maybe OCaml is smart enough to recognize when a value has already been computed and avoid computing it again, I don't know.) In the end this probably doesn't make much difference.","On Fri, 16 Nov 2018, Boqun Feng wrote:   It's a matter of personal preference.  I prefer to store the inverse relation in a separate variable rather than recomputing it multiple times.  (Maybe OCaml is smart enough to recognize when a value has  already been computed and avoid computing it again, I don't know.)   In the end this probably doesn't make much difference.  Alan",technical,Alan Stern,stern@rowland.harvard.edu,1,1,325,1.0,1.0,1,0.0,0.0,0.0,0.0
821,467091,467226,Isn't it too early to do this change? Can't we wait until we have a SoC that actually embeds this IP?I'd prefer to have a dw/ subdir where you'd place all dw files. Do we really have to create one module for the core and one per SoC? Can't we have everything in the same .ko?,"On Thu, 22 Nov 2018 17:54:54 +0000 Vitor Soares <vitor.soares@synopsys.com> wrote:   Isn't it too early to do this change? Can't we wait until we have a SoC that actually embeds this IP?   I'd prefer to have a dw/ subdir where you'd place all dw files.   Do we really have to create one module for the core and one per SoC? Can't we have everything in the same .ko?",technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,275,0.1595238095238095,0.11764705882352941,0,0.0,1.0,0.0,0.0
822,467091,467748,I'm trying to turn it more flexible so the other can reuse the code. Sure. I will change to this:../dwc  -core.h -master.c  -platdrv.cso the user doesn't need to write dw-i3c.. several times. The folder name is the same as for other subsystem (e.g. PCI). What do you think? This will help the introduction of new modules. The design in my mind is to have:-core.h-common.c-master.c-slave.c... I'm not sure if make sense to change core.h to common.h. Thanks for your feedback.,"Hi Boris,   On 22/11/18 20:02, Boris Brezillon wrote:   I'm trying to turn it more flexible so the other can reuse the code.     Sure. I will change to this:  ../dwc     |-core.h     |-master.c     |-platdrv.c   so the user doesn't need to write dw-i3c.. several times. The folder  name is the same as for other subsystem (e.g. PCI).  What do you think?    This will help the introduction of new modules. The design in my mind is  to have:  -core.h  -common.c  -master.c  -slave.c  ...  I'm not sure if make sense to change core.h to common.h.    Thaks for your feedback.   Best regards,  Vitor Soares",technical,vitor,vitor.soares@synopsys.com,1,0,474,0.23809523809523808,0.17647058823529413,0,0.0,0.9090909090909091,0.0,0.0
823,467091,467759,"Looking at the separation you've done here, I don't see why you need it. All the resources you request are generic, so why not just adding a new compat in the of_match_table?Just realized the driver is named dw-i3c-master, while the cadence driver is named i3c-master-cdns.c. I'll send a patch to make that consistent and follow the initial naming scheme: i3c-master-<ipname>.c.","On Fri, 23 Nov 2018 12:39:31 +0000 vitor <vitor.soares@synopsys.com> wrote:   Looking at the separation you've done here, I don't see why you need it. All the resources you request are generic, so why not just adding a new compat in the of_match_table?   Just realized the driver is named dw-i3c-master, while the cadence driver is named i3c-master-cdns.c. I'll send a patch to make that consistent and follow the initial naming scheme: i3c-master-<ipname>.c.",technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,378,0.18095238095238095,0.23529411764705882,0,0.0,0.9090909090909091,0.0,0.18181818181818182
824,467091,470207,"I understand your point. I'm just following what it's done in others Synopsys drivers and what I expect is that in the future we will have the same for the I3C. Some of the current generic functions might be override according with SoC requirements (e.g. i2c-designware, pcie-designware). for now what do you prefer? As I shared with you in previous email, the structure that I have in mind is this one:- core.h (or common.h, any though?)- common.c- master.c- slave.cso for me doesn't make sense to have for instance: i3c-master-dw-slave.c But seeing what is already in the kernel I wasn't coherent and it should be named to i3c-designware-master.cor follow this  This topic rise another one related with the master folder. I understand that now the subsystem doesn't have slave support but the name is limited. Isn't better to have something like controller or busses? What do you have in mind for the slave?","Hi Boris,   On 23/11/18 12:50, Boris Brezillon wrote:  I understand your point.   I'm just following what it's done in others Synopsys drivers and what I  expect is that in the future we will have the same for the I3C.  Some of the current generic functions might be override according with  SoC requirements (e.g i2c-designware, pcie-designware).   for now what do you prefer?   As I shared with you in previous email, the structure that I have in  mind is this one:  - core.h (or common.h, any though?)  - common.c  - master.c  - slave.c   so for me doesn't make sense to have for instance: i3c-master-dw-slave.c  But seeing what is already in the kernel I wasn't coherent and it should  be named to i3c-designware-master.c   or   follow this https://lkml.org/lkml/2017/7/12/430   This topic rise another one related with the master folder. I understand  that now the subsystem doesn't have slave support but the name is  limited. Isn't better to have something like controller or busses? What  do you have in mind for the slave?   Best regards,  Vitor Soares",technical,vitor,vitor.soares@synopsys.com,1,0,909,0.4261904761904762,0.29411764705882354,0,0.2727272727272727,0.6363636363636364,0.18181818181818182,0.0
825,467091,470218,"I prefer that we keep the driver as is until we actually need to split things up. If you have several files and they're all placed in a dw/ subdir, then I agree, prefixing everything with i3c-master- is useless, as you'll have to define a custom rule to create the i3c-master-dw.ko object. When there's a single source file, and this source file is directly used to create a .ko, we need this prefix, otherwise we would have dw.ko, and this would basically conflict with any other designware driver that does not have a proper prefix. Actually it's i3c-master-designware.c (or i3c-master-dw.c) if we follow what's been done for the cadence driver. And I agree with Linus on this, except that does not apply to single source file drivers.drivers/i3c/slave/... for slave drivers and drivers/i3c/slave.c for the framework, just like we have drivers/i3c/master/ for master controller drivers and drivers/i3c/master.c.","On Mon, 26 Nov 2018 12:06:24 +0000 vitor <vitor.soares@synopsys.com> wrote:   I prefer that we keep the driver as is until we actually need to split things up.   If you have several files and they're all placed in a dw/ subdir, then I agree, prefixing everything with i3c-master- is useless, as you'll have to define a custom rule to create the i3c-master-dw.ko object.  When there's a single source file, and this source file is directly used to create a .ko, we need this prefix, otherwise we would have dw.ko, and this would basically conflict with any other designware driver that does not have a proper prefix.   Actually it's i3c-master-designware.c (or i3c-master-dw.c) if we follow what's been done for the cadence driver.   And I agree with Linus on this, except that does not apply to single source file drivers.   drivers/i3c/slave/... for slave drivers and drivers/i3c/slave.c for the framework, just like we have drivers/i3c/master/ for master controller drivers and drivers/i3c/master.c.",technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,913,0.4,0.35294117647058826,0,0.2727272727272727,0.6363636363636364,0.0,0.0
826,467091,470593,This is already done and will benefit everyone: - for me is better do it now than the secondary master and slave development.  - for the others it will easy the SoC integration avoiding duplicated work and doing things from scratch. I was referring to what was made in other modules and should be applied here too. I have to disagree here. I don't see any place on the kernel with.../master/ and ../slave/ folders and it is very likely that both rules will have some common code. With this structure the user will have the code spread in /master and/slave folders...I would like you consider to change the folder name and the names rules to something like in i2c. Maybe someone else can give his opinion too.,"On 26/11/18 12:35, Boris Brezillon wrote:  This is already done and will benefit everyone:       - for me is better do it now than the secondary master and slave  development.       - for the others it will easy the SoC integration avoiding  duplicated work and doing things from scratch.    I was referring to what was made in other modules and should be applied  here too.    I have to disagree here. I don't see any place on the kernel with  .../master/ and ../slave/ folders and it is very likely that both rules  will have some common code.  With this structure the user will have the code spread in /master and  /slave folders...   I would like you consider to change the folder name and the names rules  to something like in i2c.   Maybe someone else can give his opinion too.   Best regards,  Vitor Soares",technical,vitor,vitor.soares@synopsys.com,1,0,708,0.3357142857142857,0.4117647058823529,0,0.36363636363636365,0.6363636363636364,0.0,0.0
827,467091,470605,"Sorry, I don't get that one. What would be duplicated? You want to support a new SoC, just add a new entry in the of_match_table and you're done. When you need to add SoC/integration specific stuff, create a struct and attach a different instance per-compatible so that each SoC can have its own configuration(or even init sequence if needed). That's how we do for pretty much all IPs out there, why should design ware ones be different? This is a subsystem decision. I don't mind changing the naming scheme, though I don't see why yours is better than the one I initially proposed. In any case, what's important here is to keep driver names consistent. I see at least one that uses this model: the USB framework (drivers/usb/gadget/ for device controllers and drivers/usb/host/ for host controllers). Given that I3C is closer to USB than I2C I initially decided to keep this separation. Maybe I'm wrong, but I'd like to understand why you think it's not appropriate. Not sure who you call ""user"" here, but yes,  master controller and slave controller drivers would be placed in different dirs. Why? And more importantly, why is this coming up now? You've been reviewing the framework since the beginning, and never complained about the subdirs/files organization so far. I'm okay changing it, but I want to understand why the proposed separation is not good.","On Mon, 26 Nov 2018 18:33:37 +0000 vitor <vitor.soares@synopsys.com> wrote:   Sorry, I don't get that one.   What would be duplicated? You want to support a new SoC, just add a new entry in the of_match_table and you're done. When you need to add SoC/integration specific stuff, create a struct and attach a different instance per-compatible so that each SoC can have its own configuration (or even init sequence if needed). That's how we do for pretty much all IPs out there, why should designware ones be different?   This is a subsystem decision. I don't mind changing the naming scheme, though I don't see why yours is better than the one I initially proposed. In any case, what's important here is to keep driver names consistent.   I see at least one that uses this model: the USB framework (drivers/usb/gadget/ for device controllers and drivers/usb/host/ for host controllers). Given that I3C is closer to USB than I2C I initially decided to keep this separation. Maybe I'm wrong, but I'd like to understand why you think it's not appropriate.   Not sure who you call user"" here, but yes, master controller and slave controller drivers would be placed in different dirs.   Why? And more importantly, why is this coming up now? You've been reviewing the framework since the beginning, and never complained about the subdirs/files organization so far.  I'm okay changing it, but I want to understand why the proposed separation is not good.""",technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,1359,0.6595238095238095,0.47058823529411764,0,0.36363636363636365,0.6363636363636364,0.0,0.0
828,467091,470628,"To be more specific, I'd like a real example that shows why the separation is needed.","On Mon, 26 Nov 2018 19:56:18 +0100 Boris Brezillon <boris.brezillon@bootlin.com> wrote:    To be more specific, I'd like a real example that shows why the separation is needed.",technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,85,0.04523809523809524,0.5294117647058824,0,0.36363636363636365,0.6363636363636364,0.0,0.0
829,467091,470684,"I finally understand what this separation is all about: supporting both PCI and platform devices. I guess I've been distracted by this sentence: ""This patch will allow SOC integrators to add their code specific to DesignWare I3C IP.""which for me meant each SoC would have its own platform_driver.In any case, I think this is a bit premature do this separation, unless you already know about one integrator planning to expose this IP over PCI.","On Mon, 26 Nov 2018 19:28:02 +0000 vitor <vitor.soares@synopsys.com> wrote:   I finally understand what this separation is all about: supporting both PCI and platform devices. I guess I've been distracted by this sentence:   This patch will allow SOC integrators to add their code specific to DesignWare I3C IP. ""  which for me meant each SoC would have its own platform_driver.  In any case, I think this is a bit premature do this separation, unless you already know about one integrator planning to expose this IP over PCI.""",technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,442,0.20238095238095238,0.6470588235294118,0,0.36363636363636365,0.6363636363636364,0.0,0.0
830,467091,470690,"I already share my plan with you. See the structure above. You can say there some features from USB in I3C but cannot compare USBvs I3C since they are in different championships. The aim of I3C is to fill the gaps discovery on I2C over the years but still keeping its simplicity not to go to the complexity of USB. I'm not sure but I think that a controller cannot change between gadget to host in USB in runtime. Even so, this kind of behavior is more likely to have in an I3C bus. Sorry for that and don't take me wrong... maybe I should rise this question early but this only came up now when I started splitting and thinking where to put what is for master for slave, what is common and the thing of putting everything of controller in a folder. Taking the USB as example do you prefer a dwc folder on i3c root? I already tell you my use case and as I said maybe someone can advise :)","On 26/11/18 18:56, Boris Brezillon wrote:   I already share my plan with you. See the structure above.    You can say there some features from USB in I3C but cannot compare USB  vs I3C since they are in different championships.  The aim of I3C is to fill the gaps discovery on I2C over the years but  still keeping its simplicity not to go to the complexity of USB.   I'm not sure but I think that a controller cannot change between gadget  to host in USB in runtime. Even so, this kind of behavior is more likely  to have in an I3C bus.     Sorry for that and don't take me wrong... maybe I should rise this  question early but this only came up now when I started splitting and  thinking where to put what is for master for slave, what is common and  the thing of putting everything of controller in a folder.   Taking the USB as exemple do you prefer a dwc folder on i3c root?     I already tell you my use case and as I said maybe someone can advise :)",technical,vitor,vitor.soares@synopsys.com,1,0,888,0.4523809523809524,0.7058823529411765,0,0.36363636363636365,0.6363636363636364,0.0,0.0
831,467091,470763,"My point is, I don't get the relationship between your patch and secondary-master/slave-mode support.I maintain that functionally, I3C is closer to USB than I2C. That does not mean that it's competing with USB performance-wise, it just means  that the SW stack is likely to resemble the USB one (probably a bit simpler, at least at the beginning). Look at the bus discovery mechanism, the notion of DCR (close to the concept of USB class), or the fact that each dev has a unique manufacturer+PID pair (which resemble the product/vendor ID concept) that allows us to easily bind a dev to a driver without requiring a static description.That's called USB OTG. Okay, to be fair, it's not exactly the same, and the mastership handover in I3C is probably more complex than what we have with USB OTG (I'm not a USB expert, so I might be wrong here). Maybe. So you have such a dual-role controller? I mean, Cadence IP has a dummy GPIO mode in its Master IP when is operating in slave mode (secondary master, or main master after it's released the bus), but I'm not sure this was designed for anything else but testing. What I call a slave controller would be something that lets you reply to SDR/DDR transactions or fill a generic regmap that would be exposed to other masters on the bus. This way we could implement generic slave drivers in Linux (the same way we have gadget drivers). Anything else is likely to be to specific to be exposed as a generic framework. Hm, not sure I like this idea either. So I see 2 options:1/ put all controller drivers (both master and slave ones) in a common directory (drivers/i3c/controllers) as you suggest, and prefix them   correctly  place them in separate directories I'm fine either way. I think I understand your concerns now, but only because you started to mention a few things that were not clearly stated before (at least, I didn't understand it this way), like the fact that your controller (and probably others too) supports dual-role, or the fact that you plan to expose your IP through the PCI bus.","On Mon, 26 Nov 2018 20:11:39 +0000 vitor <vitor.soares@synopsys.com> wrote:   My point is, I don't get the relationship between your patch and secondary-master/slave-mode support.   I maintain that functionally, I3C is closer to USB than I2C. That does not mean that it's competing with USB performance-wise, it just means that the SW stack is likely to resemble the USB one (probably a bit simpler, at least at the beginning).    Look at the bus discovery mechanism, the notion of DCR (close to the concept of USB class), or the fact that each dev has a unique manufacturer+PID pair (which resemble the product/vendor ID concept) that allows us to easily bind a dev to a driver without requiring a static description.   That's called USB OTG. Okay, to be fair, it's not exactly the same, and the mastership handover in I3C is probably more complex than what we have with USB OTG (I'm not a USB expert, so I might be wrong here).   Maybe.   So you have such a dual-role controller? I mean, Cadence IP has a dummy GPIO mode in its Master IP when is operating in slave mode (secondary master, or main master after it's released the bus), but I'm not sure this was designed for anything else but testing.  What I call a slave controller would be something that lets you reply to SDR/DDR transactions or fill a generic regmap that would be exposed to other masters on the bus. This way we could implement generic slave drivers in Linux (the same way we have gadget drivers). Anything else is likely to be to specific to be exposed as a generic framework.   Hm, not sure I like this idea either. So I see 2 options:  1/ put all controller drivers (both master and slave ones) in a common    directory (drivers/i3c/controllers) as you suggest, and prefix them    correctly (i3c-master-<ip>.c, i3c-slave-<ip>.c and i3c-dual-<ip>.c) 2/ place them in separate directories: drivers/i3c/{master,slave,dual}  I'm fine either way.   I think I understand your concerns now, but only because you started to mention a few things that were not clearly stated before (at least, I didn't understand it this way), like the fact that your controller (and probably others too) supports dual-role, or the fact that you plan to expose your IP through the PCI bus.",technical,Boris Brezillon,boris.brezillon@bootlin.com,1,0,2044,1.0,0.7647058823529411,0,0.36363636363636365,0.6363636363636364,0.0,0.0
832,467091,478983,"Sorry for the delayed response. I think this should be discuss in another thread. Do you agree? Yes, that was what I trying to tell you. For me this might be the best option. I would like to avoid having dual role i3c driver in a master folder. I don't disagree, and for those that have more than one file they should be in a folder, right? What prefix do you have in mind for those files inside a folder? No, it's not. But as you can see to slipt the driver in parts this subject has some relevance.","Hi Boris,   Sorry for the delayed response.   On 27/11/18 12:33, Boris Brezillon wrote:   I think this should be discuss in another thread. Do you agree?     Yes, that was what I trying to tell you. For me this might be the best  option.  I would like to avoid having dual role i3c driver in a master folder.     I don't disagree, and for those that have more than one file they should  be in a folder, right?  What prefix do you have in mind for those files inside a folder?     No, it's not. But as you can see to slipt the driver in parts this  subject has some relevance.   Best regards,  Vitor Soares",technical,vitor,vitor.soares@synopsys.com,1,0,500,0.27380952380952384,0.9411764705882353,0,1.0,0.0,0.5454545454545454,0.0
833,467091,470644,Ok no problem. We can delay this for PCI and other rules support.,"On 26/11/18 19:08, Boris Brezillon wrote:  Ok no problem. We can delay this for PCI and other rules support.",technical,vitor,vitor.soares@synopsys.com,1,0,65,0.03571428571428571,0.5882352941176471,0,0.36363636363636365,0.6363636363636364,0.0,0.0
834,468230,471396,It's quite unusual for a backlight device to have a trivial binding. The driver supports fairly extensive parametrization via structlm3530a_platform_data. It is really the case that none of these properties should ever be set via DT?,"On Sat, Nov 24, 2018 at 09:17:02AM -0500, Brian Masney wrote:  It's quite unusual for a backlight device to have a trivial binding.  The driver supports fairly extensive parametrization via struct lm3530a_platform_data. It is really the case that none of these properties should ever be set via DT?   Daniel.",technical,Daniel Thompson,daniel.thompson@linaro.org,1,0,233,0.31746031746031744,0.375,0,0.3333333333333333,0.5,0.3333333333333333,0.0
835,468230,471397,Similar to my reply to the DT bindings: I would have expected there to be code to handle DT properties here.,"On Sat, Nov 24, 2018 at 09:17:03AM -0500, Brian Masney wrote:  Similar to my reply to the DT bindings: I would have expected there to be code to handle DT properties here.   Daniel.",technical,Daniel Thompson,daniel.thompson@linaro.org,1,0,108,0.18253968253968253,0.5,0,0.3333333333333333,0.5,0.0,0.5
836,468230,475713,"I initially assumed that we would let user space configure these values once the system has booted, but you are right that these should be available in device tree. The driver has two different LED banks that can be configured independently. How do you feel about having a single property in device tree populate the initial values for both banks? I propose that we could use the property default-brightness-level for leda_init_brtand ledb_init_brt in struct lm3630a_platform_data. The max-brightness property can populate leda_max_brt and ledb_max_brt. I need to look at other bindings this weekend to see if there are any standard properties that I can use for leda_ctrl/ledb_ctrl, pwm_ctrl,and pwm_period.","On Tue, Nov 27, 2018 at 10:56:42AM +0000, Daniel Thompson wrote:  Hi Daniel,  I initially assumed that we would let user space configure these values once the system has booted, but you are right that these should be available in device tree.   The driver has two different LED banks that can be configured independently. How do you feel about having a single property in device tree populate the initial values for both banks? I propose that we could use the property default-brightness-level for leda_init_brt and ledb_init_brt in struct lm3630a_platform_data. The max-brightness property can populate leda_max_brt and ledb_max_brt.  I need to look at other bindings this weekend to see if there are any standard properties that I can use for leda_ctrl/ledb_ctrl, pwm_ctrl, and pwm_period.  Brian",technical,Brian Masney,masneyb@onstation.org,0,1,708,0.9365079365079365,0.625,0,0.8333333333333334,0.0,0.5,0.0
837,468230,475724,Thanks for your advice. I'll keep that in mind.,"+Dan M  On Fri, Nov 30, 2018 at 7:59 AM Brian Masney <masneyb@onstation.org> wrote:  How does this chip relate to ones Dan has been working on?   That is usually represented with child nodes which makes this anything but trivial. Plus, given that we have bindings for LEDs/backlights, no LED/backlight controller is a trivial device.",technical,Rob Herring,robh+dt@kernel.org,1,0,47,0.09523809523809523,0.75,0,0.8333333333333334,0.0,0.0,0.0
838,468230,475731,I agree and I'm not going to use a trivial binding for v2. See below for some questions that I have from my last email.,"On Fri, Nov 30, 2018 at 08:13:04AM -0600, Rob Herring wrote:  Hi Rob,  I agree and I'm not going to use a trivial binding for v2. See below for some questions that I have from my last email.",technical,Brian Masney,masneyb@onstation.org,0,1,119,0.2222222222222222,0.875,0,1.0,0.0,0.0,0.0
839,468230,475828,"This is a standard 8-bit white LED driver.  It looks like Brian is just adding DT support to load the driver. I would expect that the bindings need to be updated to be able to register one string or another using the led-sources property.  There are a couple of examples in the kernel and a couple of them in patch form. This driver and binding need to be updated to the latest spec, as you pointed out with child nodes. And Jacek has some new proposed bindings for the LED class so we may want to adopt those standards here as well.  This is what I am waiting on for agreement so I can update my patch set.","Rob/Brian  On 11/30/2018 08:13 AM, Rob Herring wrote:  This is a standard 8-bit white LED driver.  It looks like Brian is just adding DT support to load the driver.  I would expect that the bindings need to be updated to be able to register one string or another using the led-sources property.  There are a couple of examples in the kernel and a couple of them in patch form.  This driver and binding need to be updated to the latest spec, as you pointed out with child nodes.  And Jacek has some new proposed bindings for the LED class so we may want to adopt those standards here as well.  This is what I am waiting on for agreement so I can update my patch set.  Dan    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,0,607,1.0,1.0,1,1.0,0.0,0.0,0.0
840,469310,470762,"This member is only written to, but never used. This function is only used once. Maybe drop the function and put the if to the caller. This error message looks wrong, several others, too. return PTR_ERR(clk)?sun8i_pwm is shared for all 8 PWMs, right? So if you assign mux-1 here for the second mux, how does this influence the first PWM?mux-0 might already be enabled, it is then never disabled. This looks wrong. If val is > 1 there shouldn't be a reason to abort? I'd degrade this to dev_dbg. Noting the underlying formula for the calculation and the bit width for the related register fields above would be good. Why ""<< 0""?sun8i_pwm_config writes the registers that are relevant for period and duty_cycle. When do these values take effect? If it's already here, switching the polarity below might introduce a glitch. Please document how the hardware behaves when being disabled. (Does itdrive a 0? Does it drive a 1 when inverted? Or is the output high-z?) This looks strange to me. While syntactically equivalent it is more usual to write this as if (val & PWM_ACT_STA)When the PWM should be enabled, you also set the CLK_GATING bit. Should this better be checked for in .get_state, too?.data doesn't need to be specified. This prevents a match by driver-name, right? Other than that match is only used to assign pwm->data below to NULL. You might want to handle pwm->clk == ERR_PTR(-EPROBE_DEFER) without falling back to mux-1 and without printing an error. This is equivalent to if (ret)because &pwm->chip.npwm is only modified if of_property_read_u32returns 0 and the variable holds a 0 before.dev_dbg? If you do this earlier (typically after the allocation succeeded) you can simplify the last few lines to: ret = pwmchip_add(&pwm->chip), if (ret < 0)  dev_err(&pdev->dev, ...), return ret, This is at least unusual (and maybe broken). Please call pwmchip_removebefore clk_disable_unprepare. I think the space in the alias must be dropped. Giving that the driver doesn't bind by driver-name I suggest to drop this completely.","On Mon, Nov 26, 2018 at 12:23:19AM +0800, Hao Zhang wrote:  This member is only written to, but never used.   This function is only used once. Maybe drop the function and put the if to the caller.   This error message looks wrong, several others, too.   return PTR_ERR(clk)?   sun8i_pwm is shared for all 8 PWMs, right? So if you assign mux-1 here for the second mux, how does this influence the first PWM?  mux-0 might already be enabled, it is then never disabled.    This looks wrong. If val is > 1 there shouldn't be a reason to abort?   I'd degrade this to dev_dbg.   Noting the underlying formula for the calculation and the bitwidth for the related register fields above would be good.   Why << 0""?   sun8i_pwm_config writes the registers that are relevant for period and duty_cycle. When do these values take effect? If it's already here, switching the polarity below might introduce a glitch.    Please document how the hardware behaves when being disabled. (Does it drive a 0? Does it drive a 1 when inverted? Or is the output high-z?)   This looks strange to me. While syntactically equivalent it is more usual to write this as  	if (val & PWM_ACT_STA)    When the PWM should be enabled, you also set the CLK_GATING bit. Should this better be checked for in .get_state, too?   .data doesn't need to be specified.   This prevents a match by driver-name, right? Other than that match is only used to assign pwm->data below to NULL.   You might want to handle pwm->clk == ERR_PTR(-EPROBE_DEFER) without falling back to mux-1 and without printing an error.   This is equivalent to  	if (ret)  because &pwm->chip.npwm is only modified if of_property_read_u32 returns 0 and the variable holds a 0 before.   dev_dbg?   If you do this earlier (typically after the allocation succeeded) you can simplify the last few lines to:  	ret = pwmchip_add(&pwm->chip), 	if (ret < 0) 		dev_err(&pdev->dev, ...),  	return ret,   This is at least unusual (and maybe broken). Please call pwmchip_remove before clk_disable_unprepare.   I think the space in the alias must be dropped. Giving that the driver doesn't bind by driver-name I suggest to drop this completely.   Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |""",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,2034,1.0,0.25,0,0.009433962264150943,0.9905660377358491,0.009433962264150943,0.0
841,469310,471166,"HI! That should be a separate patch.(also, your patch series don't seem to have the threading properly configured, you might want to fix that.) sun8i here (and in the rest of the driver) is too vague. There's already plenty of SoCs part of the sun8i family that are supported by the other driver. sun8i-r40 would be a better fit (and there's no need to mention all the rebranding that allwinner has done with the R40, just use R40).This is pretty much reimplementing the clock framework. I guess you'd be better off just modeling this clock as a clock registered in the framework. It will take care by itself of the combination of muxing and rate, and making sure the parent clocks are properly enabled when needed. Do you really need that field if you leave it NULL?","Hi!  On Mon, Nov 26, 2018 at 12:23:19AM +0800, Hao Zhang wrote:  That should be a separate patch.  (also, your patch series don't seem to have the threading properly configured, you might want to fix that.)   sun8i here (and in the rest of the driver) is too vague. There's already plenty of SoCs part of the sun8i family that are supported by the other driver. sun8i-r40 would be a better fit (and there's no need to mention all the rebranding that allwinner has done with the R40, just use R40).   This is pretty much reimplementing the clock framework. I guess you'd be better off just modeling this clock as a clock registered in the framework. It will take care by itself of the combination of muxing and rate, and making sure the parent clocks are properly enabled when needed.   Do you really need that field if you leave it NULL?  Thanks! Maxime  --  Maxime Ripard, Bootlin Embedded Linux and Kernel engineering https://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,767,0.37587006960556846,0.375,0,0.009433962264150943,0.9811320754716981,0.0,0.0
842,469310,471380,"Hello, Given that the documentation is publically available, I suggest to add a link to it in a comment here. I think this is the case after taking a look into the reference manual. There are two 16 bit fields in the PWM_PERIOD_REG. One specifies the number of clock ticks defining the period (""PWM_ENTIRE_CYCLE"") and the other the duty cycle (""PWM_ACT_CYCLE""). So if you go from duty_cycle=5 + period=10 + POLARITY_NORMAL toduty_cycle=3 + period=10 + POLARITY_INVERTED this might generate: Also there is a PWM_PERIOD_RDY bit field that probably has to be consulted before writing to the PWM_PERIOD_REG register. It's not entirely clear to me if the PWM_ACT_STA bit that is used for inversion is shadowed until the next period, too. That's what I assumed above. If it's not the wave might look as follows: Where * marks the point where the inversion starts to take effect.","Hello,  On Mon, Nov 26, 2018 at 10:31:58PM +0100, Uwe Kleine-Knig wrote:  Given that the documentation is publically available, I suggest to add a link to it in a comment here. (http://linux-sunxi.org/File:Allwinner_R40_User_Manual_V1.0.pdf)   I think this is the case after taking a look into the reference manual.  There are two 16 bit fields in the PWM_PERIOD_REG. One specifies the number of clock ticks defining the period (PWM_ENTIRE_CYCLE"") and the other the duty cycle (""PWM_ACT_CYCLE"").  So if you go from duty_cycle=5 + period=10 + POLARITY_NORMAL to duty_cycle=3 + period=10 + POLARITY_INVERTED this might generate:   ____      __           ______ /    \____/  \_________/      \__/ ^         ^         ^         ^  Also there is a PWM_PERIOD_RDY bit field that probably has to be consulted before writing to the PWM_PERIOD_REG register.  It's not entirely clear to me if the PWM_ACT_STA bit that is used for inversion is shadowed until the next period, too. That's what I assumed above. If it's not the wave might look as follows:   ____      __  _____    ______ /    \____/  \/     \__/      \__/ ^         ^   *     ^         ^  Where * marks the point where the inversion starts to take effect.  Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |""",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,872,0.39443155452436196,0.5,0,0.009433962264150943,0.9811320754716981,0.0,0.04716981132075472
843,469310,477213,"Hello, To clarify my question: after the first pwm is used and enabled (maybe using mux-0) changing sun8i_pwm->clk for the second pwm is broken because then when the firstpwm is disabled the wrong clock is stopped.","Hello,  On Mon, Nov 26, 2018 at 10:31:58PM +0100, Uwe Kleine-Knig wrote:  To clearify my question:  after the first pwm is used and enabled (maybe using mux-0) changing sun8i_pwm->clk for the second pwm is broken because then when the first pwm is disabled the wrong clock is stopped.  Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,214,0.09976798143851508,0.625,0,0.0660377358490566,0.9245283018867925,0.04716981132075472,0.0
844,469310,478104,"Hi, No, I meant for your mails, sorry. Each patch should be sent in reply to your cover letter, and they are all sent as separate mails, which makes it hard to track. I'm not sure how you're sending your patches, but using git send-email this would be using --no-chain-reply-to --thread if I remember well. You don't need to move it anywhere, you can declare a clock in adriver, without being in drivers/clk. We're doing that in the DRM or the RTC drivers for example.","Hi,  On Sat, Dec 01, 2018 at 11:23:40PM +0800, Hao Zhang wrote:  No, I meant for your mails, sorry. Each patch should be sent in reply to your cover letter, and they are all sent as separate mails, which makes it hard to track.  I'm not sure how you're sending your patches, but using git send-email this would be using --no-chain-reply-to --thread if I remember well.   You don't need to move it anywhere, you can declare a clock in a driver, without being in drivers/clk. We're doing that in the DRM or the RTC drivers for example.  Maxime  --  Maxime Ripard, Bootlin Embedded Linux and Kernel engineering https://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,468,0.24129930394431554,0.75,0,0.0660377358490566,0.9245283018867925,0.0,0.16037735849056603
845,469310,499514,"You should never try to get resources at this point. You should have requested them already at ->probe() time. Otherwise, how are you going to handle failures here? So this isn't really how atomic is supposed to work. The whole point of the single callback is to allow the driver to apply the changes in an atomic way, which means that either everything is applied or nothing is applied. That's not what you do here. In the above you can end up with an enabled clock but the settings not being applied. Similarly sun8i_pwm_config() can abort in a number of places, which would leave you with a half-configured PWM channel. Instead, what you should be doing is precompute everything and check that the configuration can be applied before touching any registers or enabling clocks. Once you've validate the new state, you need to write everything and there should be no more risk of failure.","On Mon, Nov 26, 2018 at 12:23:19AM +0800, Hao Zhang wrote:  You should never try to get resources at this point. You should have requested them already at ->probe() time. Otherwise, how are you going to handle failures here?   So this isn't really how atomic is supposed to work. The whole point of the single callback is to allow the driver to apply the changes in an atomic way, which means that either everything is applied or nothing is applied.  That's not what you do here. In the above you can end up with an enabled clock but the settings not being applied. Similarly sun8i_pwm_config() can abort in a number of places, which would leave you with a half- configured PWM channel.  Instead, what you should be doing is precompute everything and check that the configuration can be applied before touching any registers or enabling clocks. Once you've validate the new state, you need to write everything and there should be no more risk of failure.  Thierry",technical,Thierry Reding,thierry.reding@gmail.com,1,0,889,0.41299303944315546,0.875,0,0.2358490566037736,0.7641509433962265,0.16037735849056603,0.7641509433962265
846,469310,575219,"Something wrong here, i will fix it, thanks :) Got it, it is useful for me !","Thierry Reding <thierry.reding@gmail.com> 于2018年12月21日周五 上午1:57写道：  Something wrong here, i will fix it, thanks :)   Got it, it is useful for me !",technical,Hao Zhang,hao5781286@gmail.com,0,1,76,0.048723897911832945,1.0,1,1.0,0.0,0.7641509433962265,0.0
847,469469,470441,"So, now it's enabled to be added via regular ndo. I have similar change in mind, but was going to send it after mcast/ucast, and - enabling same vlans patch...2 things stopped me to add this: 1) Moving it to be enabled via regular call is Ok, but in dual mac mode it causes overlaps, at least while vid deletion. So decided to wait till same vlans series is applied. 2) Wanted implement somehow similar handling for single port boards in one patch, not only for dual mac mode. This part was not clear and not verified completely...So, if it's needed  now, maybe better at this moment only remove untag field? and remove vlan0 later, once other vlan changes applied.","On Sun, Nov 25, 2018 at 05:46:26PM -0600, Grygorii Strashko wrote: So, now it's enabled to be added via regular ndo. I have similar change in mind, but was going to send it after mcast/ucast, and - enabling same vlans patch...  2 things stopped me to add this:  1) Moving it to be enabled via regular call is Ok, but in dual mac mode it causes overlaps, at least while vid deletion. So decided to wait till same vlans series is applied.  2) Wanted implement somehow similar handling for single port boards in one patch, not only for dual mac mode. This part was not clear and not verified completely...  So, if it's needed now, maybe better at this moment only remove untag field? and remove vlan0 later, once other vlan changes applied.  Say:  cpsw_ale_add_vlan(cpsw->ale, cpsw->data.default_vlan, 		  ALE_ALL_PORTS, 0, ALE_ALL_PORTS, 0),  instead of: cpsw_ale_add_vlan(cpsw->ale, cpsw->data.default_vlan, 		  ALE_ALL_PORTS, ALE_ALL_PORTS, 0, 0),   --  Regards, Ivan Khoronzhuk",technical,Ivan Khoronzhuk,ivan.khoronzhuk@linaro.org,0,0,665,0.676056338028169,0.2,0,0.0,0.75,0.0,0.0
848,469469,470606,"TI driver documentation mentions this restriction ""While adding VLAN id to the eth interfaces, same VLAN id should not be added in both interfaces which will lead to VLAN forwarding and act as switch"" This patch affects only dual_mac mode and in this mode adding vid0 by default is definitely make no sense in any case.","On 11/26/18 10:26 AM, Ivan Khoronzhuk wrote:  TI driver documentation mentions this restriction While adding VLAN id to the eth interfaces,  same VLAN id should not be added in both interfaces which will lead to VLAN forwarding and act as switch""   This patch affects only dual_mac mode and in this mode adding vid0 by default is definitely make no sense in any case.   [1] http://processors.wiki.ti.com/index.php/Linux_Core_CPSW_User%27s_Guide#Dual_Standalone_EMAC_mode  --  regards, -grygorii""",technical,Grygorii Strashko,grygorii.strashko@ti.com,1,1,319,0.28169014084507044,0.3,0,0.0,0.75,0.0,0.0
849,469469,470686,"It's not accurate now. This sw bug ""acting like a switch"" was fixed indirectly in LKML . And at least for upstream version, not TISDK, desc should be updated, but better do this when it fixed completely and merged with TISDK. I know about this ""written"" restriction (for tiSDK, and it's not TRM after all ...), it can be avoided and it's partly avoided now ...Also, for notice, while you add any of the vlans to any of the ports, vlan0 is added to both of them.....restricted it or not. Thanks to last changes in the driver it's not ""acting like a switch"" The patch in question enables this in ndo, not me.: Adding vlanid 400 to vlan filter I just propose to extend it later, when it's correct to do. But if no harm (basically no harm, only if someone decides to add vlan0 to both ports and then delete on one of them), at least you should take this into account. The above proposition is only to your change, only for dual-mac.","On Mon, Nov 26, 2018 at 12:57:20PM -0600, Grygorii Strashko wrote: It's not accurate now. This sw bug acting like a switch"" was fixed indirectly in LKML ). And at least for upstream version, not TISDK, desc should be updated, but better do this when it fixed completely and merged with TISDK.  I know about this ""written"" restriction (for tiSDK, and it's not TRM after all ...), it can be avoided and it's partly avoided now ...  Also, for notice, while you add any of the vlans to any of the ports, vlan0 is added to both of them.....restricted it or not. Thanks to last changes in the driver it's not ""acting like a switch"" The patch in question enables this in ndo, not me.  #ip link add link eth0 name eth0.400 type vlan id 400 [  326.538989] 8021q: 802.1Q VLAN Support v1.8 [  326.543217] 8021q: adding VLAN 0 to HW filter on device eth0 [  326.554645] 8021q: adding VLAN 0 to HW filter on device eth1 [  326.572236] net eth0: Adding vlanid 400 to vlan filter  I just propose to extend it later, when it's correct to do. But if no harm (basically no harm, only if someone decides to add vlan0 to both ports and then delete on one of them) , at least you should take this into account.  The above proposition is only to your change, only for dual-mac.   --  Regards, Ivan Khoronzhuk""",technical,Ivan Khoronzhuk,ivan.khoronzhuk@linaro.org,0,0,928,1.0,0.4,0,0.0,0.75,0.0,0.0
850,469469,470958,"Thank you for your review. Seems not everything works as expected with this patch, so ignore it please.","On 11/26/18 2:07 PM, Ivan Khoronzhuk wrote: Thank you for your review. Seems not everything works as expected with this patch, so ignore it please.  regards, -grygorii",technical,Grygorii Strashko,grygorii.strashko@ti.com,1,1,103,0.09859154929577464,0.5,0,0.25,0.75,0.0,0.25
851,469469,473084,"I'd like to clarify point about supporting same VLANs in dual_mac mode, to avoid future misunderstanding, overall: it's *not* supported as adding same VLAN to both net devices will cause unknown unicast packets leaking between interfaces and it can't be avoided - hw limitation. Regarding vid0 - current default configuration of CPSW considersvid0/priority tagged packets as - untagged and assigns pvid to any such ingress packet inside switch. Hence, P0 (Linux host) egress port never modifies packet contents - this behavior is not visible to Linux","On 11/26/18 2:07 PM, Ivan Khoronzhuk wrote:  I'd like to clarify point about supporting same VLANs in dual_mac mode, to avoid future misunderstanding, overall: it's *not* supported as adding same VLAN to both netdevices will cause unknown unicast packets leaking between interfaces and it can't be avoided - hw limitation.  Regarding vid0 - current default configuration of CPSW considers vid0/priority tagged packets as - untagged and assigns pvid to any such ingress packet inside switch. Hence, P0 (Linux host) egress port never modifies packet contents - this behavior is not visible to Linux. (EN_VID0_MODE=0, P1_PASS_PRI_TAGGED=0)     --  regards, -grygorii",technical,Grygorii Strashko,grygorii.strashko@ti.com,1,1,550,0.4647887323943662,0.6,0,0.5,0.25,0.25,0.0
852,469469,474547,"Simple test shows no issues with ucast leaking. But for current buggy ucast vlan implementation it's not possible to verify, not sure but probably leaking in your case caused by hidden toggling of interface to promisc while added ucast to vlans or other reason or so. Anyway I just decided to check specifically ucasts (macst as you know are not normal now). For verification you need to apply ucast fix (including vlans) first: This is generic fix (not sure it will be approved, need try RFC) but implement the same as local fix for vlan ucasts: Any of those are correct. I've used generic one. Applied the following scheme, Observe silence on PC wireshark. Thus, no see issues with this. PS: I'm sure in plget tool, you can use your own. I can't verify everything with vlan0 at this moment (not time), just shared my thoughts adding a notice it has same possible overlap issues(or part of them) after this patch as regular vlans have.","On Wed, Nov 28, 2018 at 03:15:46PM -0600, Grygorii Strashko wrote:  Simple test shows no issues with ucast leaking. But for current buggy ucast vlan implementation it's not possible to verify, not sure but probably leaking in your case cuased by hidden toggling of interface to promisc while added ucast to vlans or other reason or so. Anyway I just decided to check specifically ucasts  (macst as you know are not normal now).  For verification you need to apply ucast fix (including vlans) first: https://git.linaro.org/people/ivan.khoronzhuk/tsn_kernel.git/log/?h=vlan_addr_flt_fix  This is generic fix (not sure it will be approved, need try RFC) but implement the same as local fix for vlan ucasts: https://git.linaro.org/people/ivan.khoronzhuk/tsn_kernel.git/log/?h=ucast_vlan_fix  Any of those are correct. I've used generic one. Applied the following scheme:                       +--------------------------+                      | host 74:DA:EA:47:7D:9C   |                      +--------------------------+                          +---------------------+ 			|       am572 evm     |                         |    eth0      eth1   |                         +----------+----------+                         | eth0.400 | eth1.400 |                         +----------+----------+                             ^          |                             |          |  +-----------+  +-----------------+        |          |  |     PC    |  | BBB eth0.400    |--------+          +->| Wireshark |  +-----------------+                      +-----------+   1) Configure vlans on am572x evm  ip link add link eth0 name eth0.400 type vlan id 400  ip link add link eth1 name eth1.400 type vlan id 400  2) On BBB side:  # ip link add link eth0 name eth0.400 type vlan id 400 Send ucast vlan traffic to the am572 evm, vlan ucast address is unreq on am572.  # ./plget -i eth0.400 -t ptpl2 -m tx-lat -n 160 -s 10 -a 74:DA:EA:47:7D:66  # ./plget -i eth0.400 -t ptpl2 -m tx-lat -n 160 -s 10 -a 18:03:73:66:87:42  3) Observe silence on PC wireshark.  Thus, no see issues with this.  PS: I'm sure in plget tool, you can use your own.   I can't verify everything with vlan0 at this moment (not time), just shared my thoughts adding a notice it has same possible overlap issues (or part of them) after this patch as regular vlans have.   --  Regards, Ivan Khoronzhuk",technical,Ivan Khoronzhuk,ivan.khoronzhuk@linaro.org,0,0,936,0.9295774647887324,0.7,0,0.75,0.0,0.0,0.0
853,469469,475075,I'm using packeth to generate udp packets (vlan) src=PC dst=unknown if there is record in ALE table which looks like this then above udp packet will be forwarded to BBB,"On 11/29/18 9:26 AM, Ivan Khoronzhuk wrote:  I'm using packeth to generate udp packets (vlan) src=PC dst=unknown if there is record in ALE table which looks like: type: vlan , vid = 100, untag_force = 0x0, reg_mcast = 0x7, unreg_mcast = 0x0, member_list = 0x7 then above udp packet will be forwarded to BBB.    --  regards, -grygorii",technical,Grygorii Strashko,grygorii.strashko@ti.com,1,1,168,0.15492957746478872,0.8,0,0.75,0.0,0.0,0.0
854,469469,475693,"Agree, seems no normal way to avoid ucast leak.","On Thu, Nov 29, 2018 at 05:23:09PM -0600, Grygorii Strashko wrote: Agree, seems no normal way to avoid ucast leak.   --  Regards, Ivan Khoronzhuk",technical,Ivan Khoronzhuk,ivan.khoronzhuk@linaro.org,0,0,47,0.051643192488262914,0.9,0,1.0,0.0,0.0,0.0
855,469469,475711,"One of the ways could be removing end ports as memembers, leaving only and allow tagged packets to be received by ports being non members of a vlan. So that only unknown vlans are dropped..","On Fri, Nov 30, 2018 at 03:42:29PM +0200, Ivan Khoronzhuk wrote:  One of the ways could be removing end ports as memembers, leaving only port0:  type: vlan , vid = 100, untag_force = 0x0, reg_mcast = 0x7, unreg_mcast = 0x0, member_list = 0x1  and allow tagged packets to be received by ports beeing non memebers of a vlan:         cpsw_ale_control_set(cpsw->ale, slave_port, 			    ALE_PORT_DROP_UNKNOWN_VLAN, 0),  So that only unknown vlans are dropped...   --  Regards, Ivan Khoronzhuk",technical,Ivan Khoronzhuk,ivan.khoronzhuk@linaro.org,0,0,189,0.1784037558685446,1.0,1,1.0,0.0,0.0,0.0
856,470249,472216,"Nice work Rafael. Minor nits below.. minimum observation latency greater What about a short section for the ladder governor as well ? containing Maybe I missed, but I couldn't find any text that says what state 0, 1, ... Nmean. Like which is the deepest idle state and which one is the shallowest. opening constraints","On 26-11-18, 14:11, Rafael J. Wysocki wrote:  Nice work Rafael. Minor nits below..                                                                minimum                              observation                                               latency                                                                      greater   What about a short section for the ladder governor as well ?                                                                     containing   Maybe I missed, but I couldn't find any text that says what state 0, 1, ... N mean. Like which is the deepest idle state and which one is the shallowest.                                                                             opening                                         constraints   --  viresh",technical,Viresh Kumar,viresh.kumar@linaro.org,1,0,317,0.7325581395348837,0.3333333333333333,0,1.0,0.0,1.0,0.0
857,470249,472335,"Thanks for the typo fixes.  The spellchecker I have here evidently doesn't work.[cut]There is a paragraph on that above. But this part is missing, good catch!","On Wed, Nov 28, 2018 at 6:48 AM Viresh Kumar <viresh.kumar@linaro.org> wrote:  Thanks for the typo fixes.  The spellchecker I have here evidently doesn't work.  [cut]   There is a paragraph on that above.   But this part is missing, good catch!  Thanks, Rafael",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,158,0.4186046511627907,0.5,0,1.0,0.0,0.0,0.0
858,470249,472388,I have this in my .vimrc and I am shown these spelling mistakes somewhat forcefully :) set spell,"On 28-11-18, 10:11, Rafael J. Wysocki wrote:  I have this in my .vimrc and I am shown these spelling mistakes somewhat forcefully :)  set spell spelllang=en_us  --  viresh",technical,Viresh Kumar,viresh.kumar@linaro.org,1,0,96,0.22093023255813954,0.6666666666666666,0,1.0,0.0,0.0,0.0
859,470249,472435,"Interestingly enough, it appears to work when I turn the automatic spell checking, which I don't do for code as a rule, because it distracts me.  I will need to do that for docs going forward it seems, though.BTW, I didn't respond to the remark about the ladder governor.  I have no plans to describe it at this time and that can be done at any time later easily enough if anyone wants to do it.","On Wed, Nov 28, 2018 at 10:31 AM Viresh Kumar <viresh.kumar@linaro.org> wrote:  Interestingly enough, it appears to work when I turn the automatic spellchecking, which I don't do for code as a rule, because it distracts me.  I will need to do that for docs going forward it seems, though.  BTW, I didn't respond to the remark about the ladder governor.  I have no plans to describe it at this time and that can be done at any time later easily enough if anyone wants to do it.  Thanks, Rafael",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,395,1.0,0.8333333333333334,0,1.0,0.0,0.0,0.0
860,470249,472442,"That would have made this doc complete somewhat. But anyway, that's fine with me.","On 28-11-18, 11:01, Rafael J. Wysocki wrote:  That would have made this doc complete somewhat. But anyway, that's fine with me.  --  viresh",technical,Viresh Kumar,viresh.kumar@linaro.org,1,0,81,0.20930232558139536,1.0,1,1.0,0.0,0.0,0.0
861,470341,470877,Same as my previous comments.,"On Mon, Nov 26, 2018 at 9:58 AM Yangtao Li <tiny.windzz@gmail.com> wrote:  Same as my previous comments.    --  paul moore www.paul-moore.com",technical,Paul Moore,paul@paul-moore.com,1,0,29,1.0,1.0,1,0.0,0.0,0.0,0.0
862,474974,474974,"Hi all, After merging the clk tree, today's linux-next build (armmulti_v7_defconfig) failed like this. I have used the clk tree from next-20181129 for today","Hi all,  After merging the clk tree, today's linux-next build (arm multi_v7_defconfig) failed like this:  arm-linux-gnueabi-ld: drivers/clk/imx/clk-frac-pll.o: in function `clk_pll_round_rate': clk-frac-pll.c:(.text+0x50): undefined reference to `__aeabi_uldivmod'  Caused by commit    9fd680d0fafd (clk: imx: add fractional PLL output clock"")  I have used the clk tree from next-20181129 for today.  --  Cheers, Stephen Rothwell """,technical,Stephen Rothwell,sfr@canb.auug.org.au,0,1,156,1.0,0.25,0,0.0,0.0,0.0,0.0
863,474974,475166,"Did you mean to lose the doubling of ""rate"" above?","Hi Abel,  On Thu, 29 Nov 2018 23:50:22 +0000 Abel Vesa <abel.vesa@nxp.com> wrote:  Did you mean to lose the doubling of rate"" above?  --  Cheers, Stephen Rothwell """,technical,Stephen Rothwell,sfr@canb.auug.org.au,0,1,50,0.43333333333333335,1.0,1,0.0,0.0,0.0,0.0
864,476403,476592,Adding the current maintainers on CC.,"On Sat, Dec 01, 2018 at 04:31:49PM +0800, Wen Yang wrote:  Adding the current maintainers on CC.  							Thanx, Paul",technical,Paul E. McKenney,paulmck@linux.ibm.com,1,0,37,0.11475409836065574,0.25,0,0.0,0.5,0.0,0.5
865,476403,477142,"Indeed, and it's actually *worse* to read, as 0/1 stands out more and is more compact than false/true...The only reasonable case where bool is recommended is when functions are returning it, to make sure there's no mishap returning something else. But for a plain .c variable? Nope. Ack.","* Peter Zijlstra <peterz@infradead.org> wrote:   Indeed, and it's actually *worse* to read, as 0/1 stands out more and is  more compact than false/true...  The only reasonable case where bool is recommended is when functions are  returning it, to make sure there's no mishap returning something else.  But for a plain .c variable? Nope.   Ack.  Thanks,  	Ingo",technical,Ingo Molnar,mingo@kernel.org,1,0,287,1.0,0.625,0,1.0,0.0,0.0,0.0
866,476403,477173,"Personally, I would prefer that assignments involving boolean variables use true or false.  It seems more readable.  Potentially better for tools as well.  But if the community really prefers 0 and 1, then the test can be deleted.","On Mon, 3 Dec 2018, Peter Zijlstra wrote:   Personally, I would prefer that assignments involving boolean variables use true or false.  It seems more readable.  Potentially better for tools as well.  But if the community really prefers 0 and 1, then the test can be deleted.  julia",technical,Julia Lawall,julia.lawall@lip6.fr,1,0,230,0.7213114754098361,0.75,0,1.0,0.0,0.0,0.0
867,476403,478346,How about it it were suggested only in files that already use true and false somewhere?,"On Mon, 3 Dec 2018, Peter Zijlstra wrote:   How about it it were suggested only in files that already use true and false somewhere?  julia",technical,Julia Lawall,julia.lawall@lip6.fr,1,0,87,0.2786885245901639,1.0,1,1.0,0.0,0.0,0.0
868,478341,478513,"Now thinking further about this, I actually still need to validate that the L12 EPT for this gfn actually contains the apic_access address. To ensure that I only fixup the fault when the L1 hypervisor sets up both VMCS L12 APIC_ACCESS and L12 EPT to contain the same address.Will fix and send v2","On Mon, 2018-12-03 at 14:59 +0100, KarimAllah Ahmed wrote:    Now thinking further about this, I actually still need to validate that the L12   EPT for this gfn actually contains the apic_access address. To ensure that I   only fixup the fault when the L1 hypervisor sets up both VMCS L12 APIC_ACCESS   and L12 EPT to contain the same address.    Will fix and send v2.       Amazon Development Center Germany GmbH Krausenstr. 38 10117 Berlin Geschaeftsfuehrer: Christian Schlaeger, Ralf Herbrich Ust-ID: DE 289 237 879 Eingetragen am Amtsgericht Charlottenburg HRB 149173 B",technical,"Raslan, KarimAllah",karahmed@amazon.de,0,0,295,1.0,1.0,1,0.0,0.0,0.0,0.0
869,482005,482181,"The ioctl(BC_FREE_BUFFER) frees the buffer memory associated with a transaction that has completed processing in user space. If the buffer contains an FDA object (file-descriptor array), then it closes all of the fds passed in the transaction using ksys_close(). In the case with the issue, the fd associated with the binder driver has been passed in the array. Since the fdget() optimization didn't increment the reference, this makes us vulnerable to the UAF described above since the rules for fdget() are being violated (ksys_close()). This change did prevent the final close during the handling of BC_FREE_BUFFER, but as you point out, may still result in the final close being processed prematurely after the new fput() (no observed negative side-effects right now, but agreed this could be an issue).I'll rework it according to your suggestion. I had hoped to do this in a way that doesn't require adding calls to non-exported functions since we are trying to clean up binder (I hear you snickering) to be a better citizen and not rely on internal functions that drivers shouldn't be using. I presume there are no plans to export task_work_add()...There are indeed many things about the binder interface we'd do differently if we had the chance to start over...","On Wed, Dec 5, 2018 at 2:00 PM Al Viro <viro@zeniv.linux.org.uk> wrote:  The ioctl(BC_FREE_BUFFER) frees the buffer memory associated with a transaction that has completed processing in userspace. If the buffer contains an FDA object (file-descriptor array), then it closes all of the fds passed in the transaction using ksys_close(). In the case with the issue, the fd associated with the binder driver has been passed in the array. Since the fdget() optimization didn't increment the reference, this makes us vulnerable to the UAF described above since the rules for fdget() are being violated (ksys_close()). This change did prevent the final close during the handling of BC_FREE_BUFFER, but as you point out, may still result in the final close being processed prematurely after the new fput() (no observed negative side-effects right now, but agreed this could be an issue).   I'll rework it according to your suggestion. I had hoped to do this in a way that doesn't require adding calls to non-exported functions since we are trying to clean up binder (I hear you snickering) to be a better citizen and not rely on internal functions that drivers shouldn't be using. I presume there are no plans to export task_work_add()...   There are indeed many things about the binder interface we'd do differently if we had the chance to start over...  -Todd",technical,Todd Kjos,tkjos@google.com,1,0,1268,1.0,0.6,0,0.0,0.0,0.0,0.0
870,482005,482220,Thanks for the detailed responses. I'll rework it for v3.,"On Wed, Dec 5, 2018 at 4:40 PM Al Viro <viro@zeniv.linux.org.uk> wrote:  Thanks for the detailed responses. I'll rework it for v3.",technical,Todd Kjos,tkjos@google.com,1,0,57,0.05179282868525897,1.0,1,0.0,0.0,0.0,0.0
871,482005,482187,"Er...  Your variant critically depends upon binder being non-modular, if it*was* built as a module, you could * lose the timeslice just after your fput() * have another process hit the final fput() *and* close the struct file * now that module refcount is not pinned by anything, get rmmod remove your module * have the process in binder_ioctl() regain the timeslice and find the code under it gone. That's one of the reasons why such kludges are brittle as hell - normally you are guaranteed that once fdget() has succeeded, the final fput() won't happen until fdput().  With everything that guarantees in terms of code/data not going away under you.  This patch relies upon the lack of accesses to anything sensitive after that fput() added into binder_ioctl().  Which is actually true, but only because the driver is not modular...At least this variant (task_work_add()-based) doesn't depend on anything subtle - the lack of exports is the only problem there (IOW, it would've worked in a module if not for that).","On Wed, Dec 05, 2018 at 04:21:55PM -0800, Todd Kjos wrote:   Er...  Your variant critically depends upon binder being non-modular, if it *was* built as a module, you could 	* lose the timeslice just after your fput() 	* have another process hit the final fput() *and* close the struct file 	* now that module refcount is not pinned by anything, get rmmod remove your module 	* have the process in binder_ioctl() regain the timeslice and find the code under it gone.  That's one of the reasons why such kludges are brittle as hell - normally you are guaranteed that once fdget() has succeeded, the final fput() won't happen until fdput().  With everything that guarantees in terms of code/data not going away under you.  This patch relies upon the lack of accesses to anything sensitive after that fput() added into binder_ioctl().  Which is actually true, but only because the driver is not modular...  At least this variant (task_work_add()-based) doesn't depend on anything subtle - the lack of exports is the only problem there (IOW, it would've worked in a module if not for that).",technical,Al Viro,viro@zeniv.linux.org.uk,1,0,1016,0.8605577689243028,0.8,0,0.0,0.0,0.0,0.0
872,484126,484128,",That should be s/high/above I suppose. Other than that this seems really useful :-)","Hi Rafael,  On Friday 07 Dec 2018 at 12:57:00 (+0100), Rafael J. Wysocki wrote:  That should be s/high/above I suppose.   Other than that this seems really useful :-)  Thanks, Quentin",technical,Quentin Perret,quentin.perret@arm.com,0,0,84,0.16216216216216217,0.4,0,0.0,1.0,0.0,0.0
873,484126,484139,"Right, thanks for spotting this. :-) Thanks!","On Friday, December 7, 2018 1:02:05 PM CET Quentin Perret wrote:  Right, thanks for spotting this. :-)   Thanks!",technical,Rafael J. Wysocki,rjw@rjwysocki.net,1,1,44,0.10810810810810811,0.6,0,0.0,1.0,0.0,0.0
874,484126,484174,"s/metrics/metrics am probably pointing out something that has been already debated, apologies if so. exit_latency is the *worst* case exit latency for idle states that involve multiple CPUs, we can't say for certain it is the latency that was actually experienced by the idle state exit. It can be microseconds (eg CPU resume) vs milliseconds (eg groups of cpus resume).I think the current approach (which may only understimate the ""below"" by subtracting the worst case value) is reasonable but I pointed this out since I do not know how these stats will be used.","On Fri, Dec 07, 2018 at 12:57:00PM +0100, Rafael J. Wysocki wrote:  s/mertics/metrics   I am probably pointing out something that has been already debated, apologies if so.  exit_latency is the *worst* case exit latency for idle states that involve multiple CPUs, we can't say for certain it is the latency that was actually experienced by the idle state exit.  It can be microseconds (eg CPU resume) vs milliseconds (eg groups of cpus resume).  I think the current approach (which may only understimate the below"" by substracting the worst case value) is reasonable but I pointed this out since I do not know how these stats will be used.  Lorenzo """,technical,Lorenzo Pieralisi,lorenzo.pieralisi@arm.com,1,0,563,1.0,0.8,0,0.0,1.0,0.0,1.0
875,484126,485213,"Right, thanks! Right. This is on purpose. I want to count the cases when the selected state has been off for certain.","On Fri, Dec 7, 2018 at 1:57 PM Lorenzo Pieralisi <lorenzo.pieralisi@arm.com> wrote:  Right, thanks!   Right.   This is on purpose.  I want to count the cases when the selected state has been off for certain.  Thanks, Rafael",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,117,0.24324324324324326,1.0,1,1.0,0.0,1.0,0.0
876,488067,493690,"...[snip]...Same here. Same here.My graphs now include the ""above"" and ""below"" metrics. In particular see Idle State 1 ""above"" (was too deep) graphs in the links below. However, performance is up and power about the same, so O.K. I cherry picked a couple of the mmtests that Giovanni was doing: Test kernels:""stock"" kernel 4.20-rc5 + a couple of rjw patches. Specifically:  (note: slow upload speed from my server) The above results tables are also here: I wanted to also do the tbench on loopback test, but have not been able to get it working on my system yet. I'll supply more test results at a later date....","On 2018.12.11 03:50 Rafael J. Wysocki wrote:  ...[snip]...   Same here.   Same here.   My graphs now include the above"" and ""below"" metrics. In particular see Idle State 1 ""above"" (was too deep) graphs in the links below. However, performance is up and power about the same, so O.K.     I cherry picked a couple of the mmtests that Giovanni was doing:  Test kernels:  ""stock"" kernel 4.20-rc5 + a couple of rjw patches. Specifically:  	    2a110ed cpuidle: poll_state: Disregard disable idle states 	    8f09875 cpuidle: Add 'above' and 'below' idle state metrics 	    d6851a5 Documentation: admin-guide: PM: Add cpuidle document 	    2595646 Linux 4.20-rc5  ""teov6"" above + teov6 patch ""teov7"" above + teov7 patch ""teov8"" above + teov8 patch  1.) mmtests - netperf-unbound test (UDP):                                     4.20-rc5               4.20-rc5               4.20-rc5               4.20-rc5                                       stock                   teo6                   teo7                   teo8 Hmean     send-64         129.64 (   0.00%)      132.45 *   2.17%*      130.55 *   0.71%*      132.87 *   2.49%* Hmean     send-128        259.53 (   0.00%)      264.90 *   2.07%*      261.61 *   0.80%*      264.94 *   2.09%* Hmean     send-256        515.24 (   0.00%)      525.41 *   1.97%*      517.41 *   0.42%*      524.88 *   1.87%* Hmean     send-1024      2041.01 (   0.00%)     2079.22 *   1.87%*     2045.03 *   0.20%*     2077.25 *   1.78%* Hmean     send-2048      3980.04 (   0.00%)     4071.09 *   2.29%*     4041.15 *   1.54%*     4057.09 *   1.94%* Hmean     send-3312      6321.90 (   0.00%)     6460.23 *   2.19%*     6395.71 *   1.17%*     6409.09 *   1.38%* Hmean     send-4096      7695.18 (   0.00%)     7882.81 *   2.44%*     7813.72 *   1.54%*     7832.77 *   1.79%* Hmean     send-8192     13920.53 (   0.00%)    14146.47 *   1.62%*    13986.72 *   0.48%*    14079.07 *   1.14%* Hmean     send-16384    24714.99 (   0.00%)    25225.95 *   2.07%*    24896.10 *   0.73%*    25131.52 *   1.69%* Hmean     recv-64         129.64 (   0.00%)      132.45 *   2.17%*      130.55 *   0.71%*      132.87 *   2.49%* Hmean     recv-128        259.53 (   0.00%)      264.90 *   2.07%*      261.61 *   0.80%*      264.94 *   2.09%* Hmean     recv-256        515.24 (   0.00%)      525.41 *   1.97%*      517.41 *   0.42%*      524.88 *   1.87%* Hmean     recv-1024      2041.01 (   0.00%)     2079.22 *   1.87%*     2045.03 *   0.20%*     2077.25 *   1.78%* Hmean     recv-2048      3980.04 (   0.00%)     4071.09 *   2.29%*     4041.15 *   1.54%*     4057.09 *   1.94%* Hmean     recv-3312      6321.88 (   0.00%)     6460.23 *   2.19%*     6395.71 *   1.17%*     6409.09 *   1.38%* Hmean     recv-4096      7695.15 (   0.00%)     7882.81 *   2.44%*     7813.72 *   1.54%*     7832.75 *   1.79%* Hmean     recv-8192     13920.52 (   0.00%)    14146.43 *   1.62%*    13986.72 *   0.48%*    14079.07 *   1.14%* Hmean     recv-16384    24714.99 (   0.00%)    25225.90 *   2.07%*    24896.07 *   0.73%*    25131.49 *   1.69%*  Graphs: http://www.smythies.com/~doug/linux/idle/teo8/net-pref-udp-unbound/index.html (note: slow upload speed from my server)  2.) mmtests - sockperf-udp-throughput test:                              4.20-rc5               4.20-rc5               4.20-rc5               4.20-rc5                                stock                   teo6                   teo7                   teo8 Hmean     14        24.57 (   0.00%)       25.91 *   5.46%*       25.99 *   5.78%*       25.73 *   4.75%* Hmean     100      175.37 (   0.00%)      185.09 *   5.54%*      185.89 *   6.00%*      184.48 *   5.19%* Hmean     300      523.81 (   0.00%)      553.47 *   5.66%*      554.70 *   5.90%*      550.16 *   5.03%* Hmean     500      870.08 (   0.00%)      918.88 *   5.61%*      924.33 *   6.24%*      914.53 *   5.11%* Hmean     850     1449.44 (   0.00%)     1530.84 *   5.62%*     1535.40 *   5.93%*     1522.53 *   5.04%*  Graphs: http://www.smythies.com/~doug/linux/idle/teo8/sockperf-udp-throughput/index.html (note: slow upload speed from my server)  The above results tables are also here: http://www.smythies.com/~doug/linux/idle/teo8/index.html  I wanted to also do the tbench on loopback test, but have not been able to get it working on my system yet. I'll supply more test results at a later date.  ... Doug""",technical,Doug Smythies,dsmythies@telus.net,0,0,612,1.0,0.6666666666666666,0,0.8333333333333334,0.0,0.8333333333333334,0.0
877,488067,494016,"Thanks a lot for the comprehensive results, much appreciated as always! This basically confirms my own observations, so my overall conclusion is that what we have here is as good as it can get without changing the approach entirely or adding complications that would be difficult to justify in general. And so that's what I'm going to do.","On Mon, Dec 17, 2018 at 2:53 AM Doug Smythies <dsmythies@telus.net> wrote:  Thanks a lot for the comprehensive results, much appreciated as always!  This basically confirms my own observations, so my overall conclusion is that what we have here is as good as it can get without changing the approach entirely or adding complications that would be difficult to justify in general.   And so that's what I'm going to do.  Cheers, Rafael",technical,Rafael J. Wysocki,rafael@kernel.org,1,0,338,0.46099290780141844,1.0,1,1.0,0.0,0.0,0.0
878,490445,491221,"I like that you're getting rid of the extra task let, but the other part of this is properly refilling your rx ring.  The way you have this coded, you always blindly just receive the incoming frame, even if your refill operation fails. If you get a long enough period in which you are memory constrained, you will wind up with an empty rx ring, which isn't good.  With this patch, if your ring becomes empty, then you will stop receiving frames (no buffers to put them in), which in turn will prevent further attempts to refill the ring.  The result is effectively a hang in traffic reception that is only solveable by a NIC reset. Common practice is to, for each skb that you intend to receive: 1) Allocate a replacement buffer/skb2a) If allocation succeeds, receive the buffer currently on the ring, and replace it with the buffer from (1) 2b) If allocation fails, record a frame drop, mark the existing buffer as clean, and move on This process ensures that the ring never has any gaps in it, preventing the above hang condition.","On Wed, Dec 12, 2018 at 05:40:23PM +0000, Xue Chaojing wrote: I like that you're getting rid of the extra tasklet, but the other part of this is properly refilling your rx ring.  The way you have this coded, you always blindly just receive the incomming frame, even if your refill operation fails. If you get a long enough period in which you are memory constrained, you will wind up with an empty rx ring, which isn't good.  With this patch, if your ring becomes empty, then you will stop receiving frames (no buffers to put them in), which in turn will prevent further attempts to refill the ring.  The result is effectively a hang in traffic reception that is only solveable by a NIC reset.  Common practice is to, for each skb that you intend to receive:  1) Allocate a replacement buffer/skb 2a) If allocation succedes, receive the buffer currently on the ring, and replace it with the buffer from (1) 2b) If allocation fails, record a frame drop, mark the existing buffer as clean, and move on  This process ensures that the ring never has any gaps in it, preventing the above hang condition.  Neil",technical,Neil Horman,nhorman@redhat.com,1,0,1032,1.0,1.0,1,0.0,0.0,0.0,0.0
879,491025,491103,It should be Note that The check is not strictly needed in this artificial example because we never read/write any data there. But I agree that we should add the check to promote the the right programming patterns.,"On Thu 2018-12-13 12:09:49, Nicholas Mc Guire wrote:  It should be:  	if (!leak) { 		kfree(d), 		return NULL, 	}  Note that The check is not strictly needed in this artificial example because we never read/write any data there. But I agree that we should add the check to promote the the right programming patterns.  Best Regards, Petr",technical,Petr Mladek,pmladek@suse.com,1,0,214,1.0,0.6,0,0.0,0.0,0.0,0.0
880,491025,491105,The same comments apply here as for PATCH 1/2.,"On Thu 2018-12-13 12:09:50, Nicholas Mc Guire wrote:  The same comments apply here as for PATCH 1/2.  Best Regards, Petr",technical,Petr Mladek,pmladek@suse.com,1,0,46,0.25,0.8,0,0.0,0.0,0.0,0.0
881,491025,491200,thanks for catching this ! will send a V2.,"On Thu, Dec 13, 2018 at 01:31:39PM +0100, Petr Mladek wrote: thanks for catching this ! will send a V2.  thx! hofrat",technical,Nicholas Mc Guire,der.herr@hofr.at,0,0,42,0.25,1.0,1,0.0,0.0,0.0,0.0
882,491848,494643,"This is unusual and the example below lists a clock phandle (which is the common thing), so I guess the description is just wrong. What is the unit? I'd drop ""approx."", that the driver might not be able to exactly hit the specified period is (IMHO) obvious and doesn't need to be mentioned in the property name.","On Fri, Dec 14, 2018 at 11:50:41AM +0530, Yash Shah wrote:  This is unusual and the example below lists a clock phandle (which is the common thing), so I guess the description is just wrong.   What is the unit? I'd drop approx"", that the driver might not be able to exactly hit the specified period is (IMHO) obvious and doesn't need to be mentioned in the property name.   Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |""",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,311,0.1770573566084788,0.4444444444444444,0,0.15,0.85,0.15,0.0
883,491848,494688,"If there is a publically available reference manual, please add a link to it here.@Thierry: You see, this driver is cheating in the same way that I suggested to implement for imx. You must not use / to divide an u64 (unless you're on a 64 bit arch).Also if real_period is for example 10 ms and the consumer requests duty=12 ms + period=100 ms, the hardware is configured for duty=1.2 ms +period=10 ms, right? You should also check polarity (and fail if it's !=PWM_POLARITY_INVERSED?).If state->duty_cycle == state->period, we end up with frac = 0xffff.Does that mean the chip cannot output 100%? Is this the expected behaviour of .apply to update *state? (I think it's a good idea, but I think it misses official blessing.) How does a period start with this PWM hardware. The expected behaviour would be to start with low level for duty_cycle and then high for the rest of the period (given that the polarity is always inversed). Is this what the hardware actually does? If the duty cycle changes, is the currently running period completed before the new setting gets active? If yes, .apply is supposed to block until the new setting is active. A single space before the = please. What happens with the output if you don't set the BIT_PWM_EN_ALWAYS bit? I suggest commenting this assignment with something like: ""As scale <=15 the shift operation cannot overflow."" You must use div64_ul for dividing an unsigned long long variable. Can it happen that the result is too big to be hold by read_period (which is an unsigned int only)?Maybe add a dev_dbg with the new real_period here.Please don't emit an error message if PTR_ERR(pwm->clk) is-EPROBE_DEFER. You're supposed to call clk_get_rate only after you enabled the clk. In probe you setup the clk notifier before calling pwmchip_add. So it's a good habit to do it the other way round in .remove. You're not using the irq that according to the dt binding is required?!","On Fri, Dec 14, 2018 at 11:50:42AM +0530, Yash Shah wrote:  If there is a publically available reference manual, please add a link to it here.   @Thierry: You see, this driver is cheating in the same way that I suggested to implement for imx.   You must not use / to divide an u64 (unless you're on a 64 bit arch).   Also if real_period is for example 10 ms and the consumer requests duty=12 ms + period=100 ms, the hardware is configured for duty=1.2 ms + period=10 ms, right?  You should also check polarity (and fail if it's != PWM_POLARITY_INVERSED?).  If state->duty_cycle == state->period, we end up with frac = 0xffff. Does that mean the chip cannot output 100%?   Is this the expected behaviour of .apply to update *state? (I think it's a good idea, but I think it misses official blessing.)   How does a period start with this PWM hardware. The expected behaviour would be to start with low level for duty_cycle and then high for the rest of the period (given that the polarity is always inversed). Is this what the hardware actually does?  If the duty cycle changes, is the currently running period completed before the new setting gets active? If yes, .apply is supposed to block until the new setting is active.   A single space before the = please.   What happens with the output if you don't set the BIT_PWM_EN_ALWAYS bit?   I suggest commenting this assignment with something like: As scale <= 15 the shift operation cannot overflow."" You must use div64_ul for dividing an unsigned long long variable. Can it happen that the result is too big to be hold by read_period (which is an unsigned int only)?  Maybe add a dev_dbg with the new real_period here.   Please don't emit an error message if PTR_ERR(pwm->clk) is -EPROBE_DEFER.   You're supposed to call clk_get_rate only after you enabled the clk.   In probe you setup the clk notifier before calling pwmchip_add. So it's a good habit to do it the other way round in .remove.   You're not using the irq that according to the dt binding is required?!  Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |""",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,1920,1.0,0.5555555555555556,0,0.15,0.85,0.0,0.0
884,491848,497238,This should reference the common doc Paul has written and not re-explain the versioning scheme again. Needs a unit suffix as defined in property-units.txt,"On Fri, Dec 14, 2018 at 11:50:41AM +0530, Yash Shah wrote:  This should reference the common doc Paul has written and not re-explain  the versioning scheme again.   Needs a unit suffix as defined in property-units.txt",technical,Rob Herring,robh@kernel.org,1,0,154,0.06234413965087282,0.6666666666666666,0,0.2,0.8,0.0,0.8
885,491848,505976,Ok sure. Will be done. Thanks for the comments!,"On Tue, Dec 18, 2018 at 10:50 PM Rob Herring <robh@kernel.org> wrote:  Ok sure.   Will be done.   Thanks for the comments!",technical,Yash Shah,yash.shah@sifive.com,1,1,47,0.029925187032418952,0.7777777777777778,0,1.0,0.0,0.8,0.0
886,491848,505979,"You are right, I will correct the description. The unit is nanoseconds. Will add the unit suffix to the property name. Thanks for the comments!","On Tue, Dec 18, 2018 at 2:46 AM Uwe Kleine-König <u.kleine-koenig@pengutronix.de> wrote:  You are right, I will correct the description.   The unit is nanoseconds. Will add the unit suffix to the property name.   Thanks for the comments!",technical,Yash Shah,yash.shah@sifive.com,1,1,143,0.07481296758104738,0.8888888888888888,0,1.0,0.0,0.0,0.0
887,491848,505981,"Ok will add the link to the reference manual. Will use div_u64(). Right. Will add the check for polarity. No, it does not mean that. The chip can output 100% Ok, will update the *state by calling get_state() from .apply Yes, Correct. No, it is not the case. Sure. If BIT_PWM_EN_ALWAYS is set, the PWM counter increments continuously. If not set, PWM counter will be disabled. There won't be PWM output unless BIT_PWM_EN_ONCE is set. In that case it will generate single PWM cycle and stop. Ok. Will add that comment and also use div64_ul for division. Regarding the result, I don't think so it will be big enough to overflow read_period. Sure, will add it. Will add an ""if"" check. Will fix this. Will change the sequence. Yes, currently there is no use. Thanks for the comments!","On Tue, Dec 18, 2018 at 3:42 AM Uwe Kleine-König <u.kleine-koenig@pengutronix.de> wrote:  Ok will add the link to the reference manual.   Will use div_u64().   Right.   Will add the check for polarity.   No, it does not mean that. The chip can output 100%   Ok, will update the *state by calling get_state() from .apply   Yes, Correct.   No, it is not the case.   Sure.   If BIT_PWM_EN_ALWAYS is set, the PWM counter increments continuously. If not set, PWM counter will be disabled. There won't be PWM output unless BIT_PWM_EN_ONCE is set. In that case it will generate single PWM cycle and stop.   Ok. Will add that comment and also use div64_ul for division. Regarding the result, I don't think so it will be big enough to overflow read_period.   Sure, will add it.   Will add an if"" check.   Will fix this.   Will change the sequence.   Yes, currently there is no use.   Thanks for the comments! """,technical,Yash Shah,yash.shah@sifive.com,1,1,778,0.44139650872817954,1.0,1,1.0,0.0,0.0,0.0
888,492894,492913,"(adding some more people, please remember to run get_maintainer.plto get the full list in the future)","(adding some more people, please remember to run get_maintainer.pl to get the full list in the future) On 12/14/18 10:15 AM, Thomas Schoebel-Theuer wrote:",technical,Laura Abbott,labbott@redhat.com,1,0,101,0.11176470588235295,0.25,0,0.0,0.0,0.0,0.0
889,492894,492960,"Are you talking about the precondition?Because apei_resources_fini()  happens under the same condition check and if arch_apei_filter_addr was false, it should not become true, all of a sudden. Or? Or does that function ptr get set in the meantime on your machine? I.e., this hackery: being called in pci_mmcfg_early_init()...? Hmmm.","On Fri, Dec 14, 2018 at 07:15:14PM +0100, Thomas Schoebel-Theuer wrote:  Are you talking about the precondition  	if (arch_apei_filter_addr)  ?  Because apei_resources_fini() happens under the same condition check and if arch_apei_filter_addr was false, it should not become true, all of a sudden. Or?  Or does that function ptr get set in the meantime on your machine? I.e., this hackery:  #define set_apei_filter() (arch_apei_filter_addr = pci_mmcfg_for_each_region)  being called in pci_mmcfg_early_init()...  ?  Hmmm.  --  Regards/Gruss,     Boris.  Good mailing practices for 400: avoid top-posting and trim the reply.",technical,Borislav Petkov,bp@alien8.de,1,0,332,0.38235294117647056,0.375,0,0.0,0.0,0.0,0.0
890,492894,493068,"please take a look at the stacktrace. For some reason, and only at that specific hardware, the condition is false, there but later the indicated error exit is taken whose message you can see immediately before the stack trace. So this should documents the one observed case where the NULL deref is actually happening. Of course, it would be possible to develop another solution, but this one appears the simplest and safest to me (minimum changes to the logic). I have tested the patch on that specific hardware: I have verified that the patch does not trigger the NULL deref anymore. Of course, on any other hardware we have tested, the bug did not trigger at all. If you don't have that specific hardware, you probably cannot easily trigger / verify the problem. If you need access to the specific hardware, talk to me in a private conversation.","On 12/14/18 21:24, Borislav Petkov wrote:  Hi Borislav,  please take a look at the stacktrace. For some reason, and only at that  specific hardware, the condition is false, there but later the indicated  error exit is taken whose message you can see immediately before the  stack trace.  So this should documents the one observed case where the NULL deref is  actually happening.  Of course, it would be possible to develop another solution, but this  one appears the simplest and safest to me (minimum changes to the logic).  I have tested the patch on that specifc hardware: I have verified that  the patch does not trigger the NULL deref anymore.  Of course, on any other hardware we have tested, the bug did not trigger  at all.  If you don't have that specific hardware, you probably cannot easily  trigger / verify the problem.  If you need access to the specfic hardware, talk to me in a private  conversation.  Cheers,  Thomas",technical,Thomas Schoebel-Theuer,tst@schoebel-theuer.de,0,1,847,1.0,0.5,0,0.0,0.0,0.0,0.0
891,492894,493122,"Yes, but if you say ""for some reason"",  then we still don't know what the root cause is. So before we do any fixing, let's find out what the problem is first. Can you pls run this debugging hunk on top of -rc6, on the box and send me full dmesg? Privately is fine too. Thx","On Fri, Dec 14, 2018 at 10:27:25PM +0100, Thomas Schoebel-Theuer wrote:  Yes, but if you say for some reason"", then we still don't know what the root cause is. So before we do any fixing, let's find out what the problem is first.  Can you pls run this debugging hunk ontop of -rc6, on the box and send me full dmesg? Privately is fine too.  Thx.  --- diff --git a/arch/x86/pci/mmconfig-shared.c b/arch/x86/pci/mmconfig-shared.c index 7389db538c30..5166639b7280 100644 --- a/arch/x86/pci/mmconfig-shared.c +++ b/arch/x86/pci/mmconfig-shared.c @@ -667,6 +667,9 @@ void __init pci_mmcfg_early_init(void)  			acpi_sfi_table_parse(ACPI_SIG_MCFG, pci_parse_mcfg),  		__pci_mmcfg_init(1),   +		pr_info(""%s: setting apei filter\n"", __func__), +		dump_stack(), +  		set_apei_filter(),  	}  } diff --git a/drivers/acpi/apei/apei-base.c b/drivers/acpi/apei/apei-base.c index da370e1d31f4..e87b183ca73d 100644 --- a/drivers/acpi/apei/apei-base.c +++ b/drivers/acpi/apei/apei-base.c @@ -494,6 +494,9 @@ int apei_resources_request(struct apei_resources *resources,  	if (rc)  		goto nvs_res_fini,   +	pr_info(""%s: 1, arch_apei_filter_addr: 0x%px\n"", +		__func__, arch_apei_filter_addr), +  	if (arch_apei_filter_addr) {  		apei_resources_init(&arch_res),  		rc = apei_get_arch_resources(&arch_res), @@ -552,6 +555,9 @@ int apei_resources_request(struct apei_resources *resources,  		release_mem_region(res->start, res->end - res->start),  	}  arch_res_fini: +	pr_info(""%s: arch_res_fini, arch_apei_filter_addr: 0x%px\n"", +		__func__, arch_apei_filter_addr), +  	if (arch_apei_filter_addr)  		apei_resources_fini(&arch_res),  nvs_res_fini:  --  Regards/Gruss,     Boris.  Good mailing practices for 400: avoid top-posting and trim the reply.""",technical,Borislav Petkov,bp@alien8.de,1,0,272,0.4,0.625,0,0.0,0.0,0.0,0.0
892,492894,493136,"Ah, I overlooked that commit e56c92565dfe2 is already providing a different solution to the same problem in newer kernels _only_, as a_side_ effect (not clear to me from the description, but clear from reading the code). But this patch is not present at the 4.4.166 kernel where I found the problem and fixed it internally in a different way. The 4.4.166 code looks like this, without the if-statement you are mentioning, unconditionally trying to free the uninitialized variable under certain circumstances: So another alternative would be backporting to the 4.4 LTSseries. Also fine for me.","On 12/14/18 22:27, Thomas Schoebel-Theuer wrote:  Ah, I overlooked that commit e56c92565dfe2 is already providing a  different solution to the same problem in newer kernels _only_, as a  _side_ effect (not clear to me from the description, but clear from  reading the code).  But this patch is not present at the 4.4.166 kernel where I found the  problem and fixed it internally in a different way.  The 4.4.166 code looks like this, without the if-statement you are  mentioning, unconditionally trying to free the unitinialized variable  under certain circumstances:  d91525eb8ee6a (Chen, Gong     2014-12-10 13:53:26 -0800 553) arch_res_fini: d91525eb8ee6a (Chen, Gong     2014-12-10 13:53:26 -0800 554)  apei_resources_fini(&arch_res), d91525eb8ee6a (Chen, Gong     2014-12-10 13:53:26 -0800 555) nvs_res_fini: 4134b8c8811f2 (Huang Ying     2011-12-08 11:25:50 +0800 556)  apei_resources_fini(&nvs_resources), 23f124ca3dda9 (Huang Ying     2010-09-29 19:53:54 +0800 557) return rc,  So another alternative would be backporting e56c92565dfe2 to the 4.4 LTS  series. Also fine for me.",technical,Thomas Schöbel-Theuer,thomas@schoebel-theuer.de,0,0,592,0.6294117647058823,0.75,0,0.0,0.0,0.0,0.0
893,492894,493142,"Damn, I missed the fact that this is not the upstream kernel: CPU: 0 PID: 1 UID: 0 Comm: swapper/0 Not tainted 4.4.0-ui18344.004-uiabi1-infong-amd64 #1That looks like the right fix. A note for the next time: do not send a fix for a stable kernel which is not upstream: From Documentation/process/stable-kernel-rules.rst:"" - It or an equivalent fix must already exist in Linus' tree (upstream).""The stable kernels track upstream so if a stable kernel has a problem, the first thing one needs to do is to check whether this has been fixed upstream and if so, to backport it. This is the case most of the time. In the very seldom cases where a separate fix is needed, it needs to be handled by asking Greg what to do. :-) Adding stable@ folks to CC to set me straight if I'm missing something.","On Fri, Dec 14, 2018 at 11:42:01PM +0100, Thomas Schöbel-Theuer wrote:  Damn, I missed the fact that this is not the upstream kernel:  CPU: 0 PID: 1 UID: 0 Comm: swapper/0 Not tainted 4.4.0-ui18344.004-uiabi1-infong-amd64 #1   That looks like the right fix.  A note for the next time: do not send a fix for a stable kernel which is not upstream:  From Documentation/process/stable-kernel-rules.rst:   - It or an equivalent fix must already exist in Linus' tree (upstream).""  The stable kernels track upstream so if a stable kernel has a problem, the first thing one needs to do is to check whether this has been fixed upstream and if so, to backport it. This is the case most of the time. In the very seldom cases where a separate fix is needed, it needs to be handled by asking Greg what to do. :-)  Adding stable@ folks to CC to set me straight if I'm missing something.  --  Regards/Gruss,     Boris.  Good mailing practices for 400: avoid top-posting and trim the reply.""",technical,Borislav Petkov,bp@alien8.de,1,0,790,1.0,0.875,0,0.0,0.0,0.0,0.0
894,492894,493258,"Nope, you are correct, thanks!","On Fri, Dec 14, 2018 at 11:54:28PM +0100, Borislav Petkov wrote:  Nope, you are correct, thanks!  greg k-h",technical,Greg KH,gregkh@linuxfoundation.org,1,0,30,0.047058823529411764,1.0,1,0.0,0.0,0.0,0.0
895,492895,497841,"There is no JESD85-B51.  I presume you mean JESD84-B51, but I can't find any reference to DMA in 6.6.39.  All the host controller relevant material seems to be in Annex B.  Can you clarify this reference?","On 14/12/18 8:21 PM, Sowjanya Komatineni wrote:  There is no JESD85-B51.  I presume you mean JESD84-B51, but I can't find any reference to DMA in 6.6.39.  All the host controller relevant material seems to be in Annex B.  Can you clarify this reference?",technical,Adrian Hunter,adrian.hunter@intel.com,1,0,204,1.0,1.0,1,1.0,0.0,1.0,0.0
896,494673,497707,Please just fix the test to use the global object created for this purpose instead of an unnecessary on-stack instance. Thanks.,"From: Bart Van Assche <bvanassche@acm.org> Date: Mon, 17 Dec 2018 13:40:58 -0800   ...  Please just fix the test to use the global object created for this purpose instead of an unnecessary on-stack instance.  Thanks.",technical,David Miller,davem@davemloft.net,1,0,127,1.0,0.6666666666666666,0,0.5,0.5,0.5,0.5
897,494673,499508,Please just fix the test to use the global object created for this purpose instead of an unnecessary on-stack instance. Thanks.,"On Tue, 2018-12-18 at 21:32 -0800, David Miller wrote: +AD4 From: Bart Van Assche +ADw-bvanassche+AEA-acm.org+AD4 +AD4 Date: Mon, 17 Dec 2018 13:40:58 -0800 +AD4  +AD4 +AD4 The test+AF8-insert+AF8-dup() function from lib/test+AF8-rhashtable.c passes a +AD4 +AD4 pointer to a stack object to rhltable+AF8-init(). Avoid that the following +AD4 +AD4 is reported with object debugging enabled while running the selftest +AD4 +AD4 from lib/test+AF8-rhashtable.c: +AD4  ... +AD4 +AD4 Signed-off-by: Bart Van Assche +ADw-bvanassche+AEA-acm.org+AD4 +AD4  +AD4 Please just fix the test to use the global object created for this purpose +AD4 instead of an unnecessary on-stack instance.  Hi Dave,  I will do that. Thanks for the feedback.  Bart.",technical,Bart Van Assche,bvanassche@acm.org,1,1,127,1.0,1.0,1,1.0,0.0,0.5,0.0
898,494810,496723,Do you mean ATF (ARM Trusted Firmware) instead?,"Hi Anson,  On Tue, Dec 18, 2018 at 12:56 AM Anson Huang <anson.huang@nxp.com> wrote:  Do you mean ATF (ARM Trusted Firmware) instead?",technical,Fabio Estevam,festevam@gmail.com,1,0,47,0.04888888888888889,0.23076923076923078,0,0.0,0.0,0.0,0.0
899,494810,496732,"TF-A is the name of the day for what was formerly known as ATF...However I don't think that it's correct to just don't cache the clock settings. Normally the secure world firmware should not change any clock settings at runtime, or it would run into all kinds of conflicts with the clock driver. So there are probably some well known points in time like a suspend or resume event when the firmware might change clock settings, so we could instead use those to trigger an explicit invalidate of the clock caches with much lower overhead.","Am Dienstag, den 18.12.2018, 08:24 -0200 schrieb Fabio Estevam:  TF-A is the name of the day for what was formerly known as ATF...  However I don't think that it's correct to just don't cache the clock settings. Normally the secure world firmware should not change any clock settings at runtime, or it would run into all kinds of conflicts with the clock driver. So there are probably some well known points in time like a suspend or resume event when the firmware might change clock settings, so we could instead use those to trigger an explicit invalidate of the clock caches with much lower overhead.  Regards, Lucas",technical,Lucas Stach,l.stach@pengutronix.de,1,0,536,0.4666666666666667,0.3076923076923077,0,0.0,0.0,0.0,0.0
900,494810,496901,"There is bus-freq feature on imx8m which is to scale ddr clock, this is done in ARM Trusted Firmware, for some setpoints, the DDR PLL clock rate must be changed directly in TF-A, but its child clock like dram core is unaware in Linux kernel, so the clock rate will mismatch with hardware, since ddr related clocks will NOT used by any module in Linux kernel, so it will NOT introduce any conflict. Regarding about the over head, yes, the change in common composite clock register has too many over head for other clocks, what if I ONLY have dram core clock to pass the flag to register the composite clock?","Hi, Lucas    From Anson's iPhone 6        There is bus-freq feature on imx8m which is to scale ddr clock, this is done in ARM Trusted Firmware, for some setpoints, the DDR PLL clock rate must be changed directly in TF-A, but its child clock like dram core is unaware in Linux kernel, so the clock rate will mismatch with hardware, since ddr related clocks will NOT used by any module in Linux kernel, so it will NOT introduce any conflict.    Regarding about the over head, yes, the change in common composite clock register has too many over head for other clocks, what if I ONLY have dram core clock to pass the CLK_GET_RATE_NOCACHE flag to register the composite clock?    Anson.",technical,Anson Huang,anson.huang@nxp.com,0,1,606,0.5466666666666666,0.38461538461538464,0,0.0,0.0,0.0,0.0
901,494810,496915,"I don't think there is anything implementing the bus frequency scaling in main line, right? IMHO marking clocks under TF-A control explicitly as nocache would be much more acceptable than doing it for every composite clock. This seems okay for a short term solution. Still I think that whatever is causing the bus frequency scale to change should have a way to explicitly invalidate the clock cache for the affected clocks eventually.","Hi Anson,  Am Dienstag, den 18.12.2018, 13:35 +0000 schrieb Anson Huang:  I don't think there is anything implementing the bus frequency scaling in mainline, right?   IMHO marking clocks under TF-A control explicitly as nocache would be much more acceptable than doing it for every composite clock. This seems okay for a short term solution.  Still I think that whatever is causing the bus frequency scale to change should have a way to explicitly invalidate the clock cache for the affected clocks eventually.  Regards, Lucas",technical,Lucas Stach,l.stach@pengutronix.de,1,0,434,0.3466666666666667,0.5384615384615384,0,0.0,0.0,0.0,0.0
902,494810,496940,"Yes, mainline has no bus-freq scaling so far, but internally we use same composite clock driver as mainline, and bus-freq clock rate issue/bug reported during our internal test, that is why I create this patch to easy our next kernel upgrade. It is because the DDR PLL/clocks can only be changed with strict DDR freq change flow, and it is done in TF-A, Linux kernel can NOT touch it in runtime, so we have to mark the child clock of DDR PLL to be uncached, in V2 patch, I will just add the flag for the DDR PLL child clocks to be a shorten solution, should be only very few ones, hope it is acceptable, thanks.","Hi, Lucas    From Anson's iPhone 6        Yes, mainline has no bus-freq scaling so far, but internally we use same composite clock driver as mainline, and bus-freq clock rate issue/bug reported during our internal test, that is why I create this patch to easy our next kernel upgrade.      It is because the DDR PLL/clocks can only be changed with strict DDR freq change flow, and it is done in TF-A, Linux kernel can NOT touch it in runtime, so we have to mark the child clock of DDR PLL to be uncached, in V2 patch, I will just add the flag for the DDR PLL child clocks to be a shorten solution, should be only very few ones, hope it is acceptable, thanks.    Anson.",technical,Anson Huang,anson.huang@nxp.com,0,1,611,0.5777777777777777,0.6153846153846154,0,0.0,0.0,0.0,0.0
903,494810,496963,I fully understand why you are doing the frequency change in TF-A and I agree with the reasoning to do so. I also think that using uncached for the few clocks under TF-A control is fine for now. But if/when the bus frequency scaling is actually implemented for upstream I think the flow should look something like that: 1. Bus freq scaling driver determines that a change is necessary 2. Scaling driver calls into TF-A to do the change 3. TF-A reconfigures clock rates 4. Scaling driver calls into clock driver to signal that a clock change might have happened 5. Clock driver invalidates and recalculates cached values for the affected clocks,"Am Dienstag, den 18.12.2018, 13:53 +0000 schrieb Anson Huang: [...]  I fully understand why you are doing the frequency change in TF-A and I agree with the reasoning to do so. I also think that using uncached for the few clocks under TF-A control is fine for now.  But if/when the bus frequency scaling is actually implemented for upstream I think the flow should look something like that:  1. Bus freq scaling driver determines that a change is necessary 2. Scaling driver calls into TF-A to do the change 3. TF-A reconfigures clock rates 4. Scaling driver calls into clock driver to signal that a clock change might have happened 5. Clock driver invalidates and recalculates cached values for the affected clocks  Regards, Lucas",technical,Lucas Stach,l.stach@pengutronix.de,1,0,643,0.5333333333333333,0.7692307692307693,0,0.0,0.0,0.0,0.0
904,494810,497336,"Quoting him, Does any clk consuming driver of the downstream clks that are branched off of the bus clk managed by firmware care about the frequency? Or do they just want the clk to be on. If they don't care then it's possible to break the parent dependency and just not care to tell them what the bus frequency is anymore. I don't know how you would implement #4 above, besides by having the busfreq scaling driver use clk_set_rate() to tell the bus clk that it wants a new rate and then having that clk implementation do #2. That way the rate propagation works without having to notify clk code somehow.","Quoting Lucas Stach (2018-12-18 06:02:28)  Does any clk consuming driver of the downstream clks that are branched off of the bus clk managed by firmware care about the frequency? Or do they just want the clk to be on. If they don't care then it's possible to break the parent dependency and just not care to tell them what the bus frequency is anymore.  I don't know how you would implement #4 above, besides by having the bus freq scaling driver use clk_set_rate() to tell the bus clk that it wants a new rate and then having that clk implementation do #2. That way the rate propagation works without having to notify clk code somehow.",technical,Stephen Boyd,sboyd@kernel.org,1,0,604,0.5555555555555556,0.8461538461538461,0,0.0,0.0,0.0,0.0
905,494810,497664,"In our case, the original clock relationship is as below, when DDR freq needs to be scaled, below things are proceeded: Clock tree Linux kernel do SMC call into TF-A,2. in TF-A, we have to scale the IMX8MQ_DRAM_PLL2 to dedicated frequency along with DDR freq scale flow,3. After TF-A done the DDR freq scale and return back to Linux, all the downstream of IMX8MQ_DRAM_PLL2 clock will  have mismatch clock rate between clock tree and HW settings since they are cached,4. Maybe we can call clk_set_rate in Linux for IMX8MQ_DRAM_PLL2 again (although the HW settings are already expected)  to update the downstream clocks, looks like can fix it, but will the call be skipped by clock framework when clock driver  re-calculate the PLL rate based on HW setting, as HW setting already equal the rate wants to be set, and clk_propagate_rate_change  will be skipped too or it will automatically update all child clocks' rate? Is it correct? if all child clocks' rate can be updated by clk_propagate_rate_change(), then we no need to add any clock flag ,just make sure after TF-A finish the DDR freq scale, make sure calling the clk_set_rate() for this, then everything should be correct?","Hi, Stephen    Best Regards!  Anson Huang      In our case, the original clock relationship is as below, when DDR freq needs to be scaled, below things are proceeded:    Clock tree:  IMX8MQ_DRAM_PLL2  	IMX8MQ_DRAM_PLL2_DIV  		IMX8MQ_DRAM_PLL2_OUT  			IMX8MQ_DRAM_PLL_OUT  				IMX8MQ_CLK_DRAM_CORE    1. Linux kernel do SMC call into TF-A,  2. in TF-A, we have to scale the IMX8MQ_DRAM_PLL2 to dedicated frequency along with DDR freq scale flow,  3. After TF-A done the DDR freq scale and return back to Linux, all the downstream of IMX8MQ_DRAM_PLL2 clock will    have mismatch clock rate between clock tree and HW settings since they are cached,  4. Maybe we can call clk_set_rate in Linux for IMX8MQ_DRAM_PLL2 again (although the HW settings are already expected)    to update the downstream clocks, looks like can fix it, but will the call be skipped by clock framework when clock driver    re-calculate the PLL rate based on HW setting, as HW setting already equal the rate wants to be set, and clk_propagate_rate_change    will be skipped too or it will automatically update all child clocks' rate?    Is it correct? if all child clocks' rate can be updated by clk_propagate_rate_change(), then we no need to add any clock flag,  just make sure after TF-A finish the DDR freq scale, make sure calling the clk_set_rate() for the IMX8MQ_DRAM_PLL2, then everything should be correct?    Anson.",technical,Anson Huang,anson.huang@nxp.com,0,1,1178,1.0,0.9230769230769231,1,0.0,0.0,0.0,0.0
906,494896,496623,"You may want to check linux/next. As far as I can see, the two patches are there already.","On 18/12/2018 06:02, Kuninori Morimoto wrote:  You may want to check linux/next. As far as I can see, the two patches are there already.  Thanks,  	M.-- Jazz is not dead. It just smells funny...",technical,Marc Zyngier,marc.zyngier@arm.com,1,0,89,1.0,0.8,0,0.0,0.0,0.0,0.0
907,494896,497628,Wow !!Thank you !!, Hi Marc   Wow !! Thank you !!  Best regards --- Kuninori Morimoto,technical,Kuninori Morimoto,kuninori.morimoto.gx@renesas.com,0,1,18,0.3333333333333333,1.0,1,0.0,0.0,0.0,0.0
908,498391,498419,This makes it impossible to write a wrapper that turns this mode on for unmodified programs. Do you have a real use case where this behavior is a problem?,"On Wed, Dec 19, 2018 at 02:09:50PM -0500, Waiman Long wrote:  This makes it impossible to write a wrapper that turns this mode on for unmodified programs.  Do you have a real use case where this behavior is a problem?  -Andi",technical,Andi Kleen,ak@linux.intel.com,0,0,154,0.4246575342465753,0.2222222222222222,0,0.0,1.0,0.0,0.0
909,498391,498424,"You can always force disable SSB. In that case, all the child processes will have SSBD on. Yes, we have an enterprise application partner that found that their application slow down up to 10-20% depending on how their application was set up. With the slow setup, the application was spawned by Java processes causing the SSBD bit to stay on when the application was running.","On 12/19/2018 02:38 PM, Andi Kleen wrote:  You can always force disable SSB. In that case, all the child processes will have SSBD on.   Yes, we have an enterprise application partner that found that their application slow down up to 10-20% depending on how their application was set up. With the slow setup, the application was spawned by Java processes causing the SSBD bit to stay on when the application was running.  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,374,1.0,0.3333333333333333,0,0.0,1.0,0.0,0.0
910,498391,498598,"Okay that sounds reasonable, given the below. Thanks.-"," Okay that sounds reasonable, given the below. Thanks.  -Andi ",technical,Andi Kleen,ak@linux.intel.com,0,0,54,0.136986301369863,0.4444444444444444,0,0.0,1.0,0.0,0.6923076923076923
911,498391,508053,Ping! Any comments of objections?,"On 12/19/2018 02:09 PM, Waiman Long wrote:  Ping! Any comments of objections?  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,33,0.0958904109589041,0.5555555555555556,0,0.6923076923076923,0.3076923076923077,0.6923076923076923,0.15384615384615385
912,498391,514679,"Lot's of MAY's here. Aside of that this fundamentally changes the behaviour. I'm not really a fan of doing that. If there are good reasons to have a non-inherited variant, then we rather introduce that instead of changing the existing semantics without a way for existing user space to notice. Thanks","On Wed, 19 Dec 2018, Waiman Long wrote:   Lot's of MAY's here. Aside of that this fundamentally changes the behaviour. I'm not really a fan of doing that.  If there are good reasons to have a non-inherited variant, then we rather introduce that instead of changing the existing semantics without a way for existing userspace to notice.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,300,0.8082191780821918,0.6666666666666666,0,0.8846153846153846,0.11538461538461539,0.15384615384615385,0.11538461538461539
913,498391,516277,"I understand your point. How about adding a "",noexec"" auxiliary option to the spec_store_bypass_disable command line to activate this new behavior without changing the default. Will that be acceptable?","On 01/11/2019 02:52 PM, Thomas Gleixner wrote:  I understand your point. How about adding a ,noexec"" auxillary option to the spec_store_bypass_disable command line to activate this new behavior without changing the default. Will that be acceptable?  Cheers, Longman""",technical,Waiman Long,longman@redhat.com,1,1,201,0.4794520547945205,0.7777777777777778,0,1.0,0.0,0.11538461538461539,0.0
914,498391,516671,I'd rather have an explicit PR_SPEC_DISABLE_NOEXEC argument for the PRCTL so you can decide at the application level what kind of behaviour you want.,"On Mon, 14 Jan 2019, Waiman Long wrote:  I'd rather have an explicit PR_SPEC_DISABLE_NOEXEC argument for the PRCTL so you can decide at the application level what kind of behaviour you want.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,149,0.3561643835616438,0.8888888888888888,0,1.0,0.0,0.0,0.0
915,498391,516986,Thanks for the advice. Will work on a v2 to be sent out later this week.,"On 01/15/2019 04:48 AM, Thomas Gleixner wrote:  Thanks for the advice. Will work on a v2 to be sent out later this week.  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,72,0.2465753424657534,1.0,1,1.0,0.0,0.0,0.0
916,498470,500794,"*groans*  Another one where the MODULE_LICENSE is different. Michael, Analog copyright, so if you want to express a view on the intent that would be great. My feeling would be that the MODULE_LICENSE is the wrong one given it's easier to get that wrong than to add an 'or later' to the text. On these I generally want an ack from the copyright holder anyway just to be sure everything is in order.","On Wed, 19 Dec 2018 13:07:31 -0800 Amir Mahdi Ghorbanian <indigoomega021@gmail.com> wrote:  *groans*  Another one where the MODULE_LICENSE is different. Michael, Analog copyright, so if you want to express a view on the intent that would be great.  My feeling would be that the MODULE_LICENSE is the wrong one given it's easier to get that wrong than to add an 'or later' to the text..  On these I generally want an ack from the copyright holder anyway just to be sure everything is in order.  Thanks,  Jonathan",technical,Jonathan Cameron,jic23@kernel.org,1,0,397,1.0,1.0,1,1.0,0.0,1.0,0.0
917,498594,498594,"Folks, I'm about to vanish for a truly needed break until Jan 7th. Time to lookback to an interesting year. Almost exactly a year ago, all hell broke loose and quite some people were forced to cancel their Christmas and New Year vacation and instead of spending quality time with family and friends they tried to bring the bits and pieces for the Meltdown and Spectre mitigations into shape. While the Meltdown part (KPTI) was in a halfways good shape - at least in mainline - the Spectre mitigations did not make it into mainline on time and caused havoc in distros. The broken microcode updates and other unpleasant issues did not help the situation either. And no, the 6 days extra if the embargo wouldn't have ended early would not have made any difference. It's a wonder that it held up until Jan. 3rd at all. The reasons for this disaster have been pretty much covered in various ways, so no point to go back to that again. Though it's worth to mention that some of the mitigations took quite some time to materialize and the development was not at all driven by those who are responsible for the problem in the first place. Primary examples are KPTI support for 32bit and STIBP which took more than 9 months to get into the mainline. KPTI for 32bit was ignored completely and STIPB only got attention due to performance regressions, though the response was causing more work than help. The next round of speculation-related issues including the scary L1TFhardware bug was a way more ""pleasant"" experience to work on. While forobvious reasons the mitigation development happened behind closed doors in a smaller group of people, we were at least able to collaborate in a way which is somehow close to what we are used to. There were surely a few rough edges with respect to bringing in particular developers and information flow, but both Intel and we as a community have learned how to deal with that and improved a lot. As a consequence, we are going to have a well documented and formalized process for this in the foreseeable future. There are also efforts on th eway to have non-public testing infrastructure available for future events of this kind. No need to speculate whether this makes sense. I'm not overly optimistic that we have seen all of that by now and my gut feeling tells me that we are going to be haunted by that kind of issues for a very long time. For the very unlikely case that I'm proven wrong, then I'm surely not going to shed a tear about the time spent on writing the documentation and getting things prepared. At this point I want to say BIG THANKS to everybody involved for all the great work which was done under not so enjoyable circumstances. Both the required secrecy and the set in stone timelines are pretty different from our normal workflow. At the same time I want to take the opportunity and apologize for any outburst I had. I know that I went overboard occasionally and it's nothing I'm proud of. Looking back, I have to say that all of this certainly had consequences outside of that restricted setting. The coordinated release dates forced quite some people to put a break on other tasks which were piling up nevertheless. The review backlog was from time to time tremendous and I'm sure that we dropped stuff on the way and that we still have things to catch up with on all ends. Though a lot of this pressure and fallout is home-grown and could have been avoided at least to some extent. The underlying reasons are not specific to the mitigation development, the circumstances just emphasized them and made them more observable for everyone - involved or not. 1) Lack of code quality    This is a problem which I observe increasing over many years.    The feature driven duct tape engineering mode is progressing    massively. Proper root cause analysis has become the exception not the    rule.    In our normal kernel development it's just annoying and eats up review    capacity unnecessarily, but in the face of a timeline or real bugs it's    worse. Aside of wasting time for review rounds, at some point other    people have to just drop everything else and get it fixed.    Even if some people don't want to admit it, the increasing complexity    of the hardware technology and as a consequence the increasing    complexity of the kernel code base makes it mandatory to put    correctness and maintainability first and not to fall for the    featuritis and performance chants which are driving this    industry. We've learned painfully what that causes in the last year. 2) Lack of review response    Not addressing review feedback is not a new problem, but again under    time pressure or in the face of real bugs it becomes a real pain and    causes extra work for others and maintainers in particular. 3) Outright refusal    I've seen particularly in this year quite some people who responded to    review feedback with outright and outspoken refusal. The points they    refuse to address are not some esoteric whims of particular    maintainers, no it's refusal to accept that there are documented    process and patch submission rules which apply for everyone.    Again, not a big problem if it's related to features. If it's related    to actual bugs or the timelined mitigation development then it causes    extra burden for others. In other words, if we are exposed to more half-baked patches, sloppy addressing of review feedback or in the worst case refusal to collaborate and then on top getting complaints about maintainers and reviewers being bottlenecks, then this will become a real problem in the not so distant future. Companies have to understand, that the kernel community cannot provide all-inclusive educational programs for their engineers. It's about time, that the companies catch the obvious wreckage before it leaves the house and make sure that feedback is addressed properly and in all points. I'm neither expecting perfect patches nor is there a guarantee that even well thought out and well written code will go into the tree undisputed. Though reviewing and discussing something which is well done is way less time consuming and frustrating than dealing with the above. I know that some people will come forth immediately and educate me once more on maintainer models and the need to bring new maintainers in fast. I'm all for more maintainers, but it's hard to find the right people. All good maintainers - and I've brought quite a few of them into that role myself - had proven themselves in their contributor role before taking that up. Rest assured that I constantly look out for these people and try to get them on board. Picking them out is based on their technical skills but even more so on their mindset. Unfortunately quite some of them don't want to step into that role because they are well aware of the responsibility and the burden which comes with it. I respect that decision and I definitely can understand it. I was more than once on the verge of throwing in the towel during the last year. I'm not opposed to try new things, quite the contrary. But something which worked out for a particular subsystem cannot be applied blindly to everything else in the hope that it works out. That needs a lot more thought and I'm not at all buying that tooling is a crucial part of the solution. Last but not least, I'm not sure whether more maintainers can solve the pain points which bugger me most. I rather think we'd need lots of nursemaids and teachers to address that. Sorry for the lengthy and maybe unpleasant read, but keeping the frustration which built up over the year to myself would just cause me gastric ulcer and a bad mood over Christmas. So I decided to vent and share it with all of you even at the risk that I'm barking up the wrong tree. That said, I'm going to vanish into vacation until Jan. 7th and I'm not going to read any (LKML) mails until then. As I predict from experience that my (filtered) inbox will be a untameable beast by then, don't expect me to actually go through it mail by mail. If your mail will unfortunately end up in the 'lkml/done' folder without being read, I'm sure you'll notice and find a way to resend it.I'm nevertheless looking positively forward to the new challenges of 2019 and I wish you all a Merry Christmas, a Happy New Year and a refreshing break! I wish especially for those who suffered a year ago, that they can enjoy quality time with their families and friends!","Folks,  I'm about to vanish for a truly needed break until Jan 7th. Time to look back to an interesting year.  Almost exactly a year ago, all hell broke loose and quite some people were forced to cancel their Christmas and New Year vacation and instead of spending quality time with family and friends they tried to bring the bits and pieces for the Meltdown and Spectre mitigations into shape.  While the Meltdown part (KPTI) was in a halfways good shape - at least in mainline - the Spectre mitigations did not make it into mainline on time and caused havoc in distros. The broken microcode updates and other unpleasant issues did not help the situation either. And no, the 6 days extra if the embargo wouldn't have ended early would not have made any difference. It's a wonder that it held up until Jan. 3rd at all.  The reasons for this disaster have been pretty much covered in various ways, so no point to go back to that again. Though it's worth to mention that some of the mitigations took quite some time to materialize and the development was not at all driven by those who are responsible for the problem in the first place. Primary examples are KPTI support for 32bit and STIBP which took more than 9 months to get into the mainline. KPTI for 32bit was ignored completely and STIPB only got attention due to performance regressions, though the response was causing more work than help.  The next round of speculation-related issues including the scary L1TF hardware bug was a way more pleasant"" experience to work on. While for obvious reasons the mitigation development happened behind closed doors in a smaller group of people, we were at least able to collaborate in a way which is somehow close to what we are used to.  There were surely a few rough edges with respect to bringing in particular developers and information flow, but both Intel and we as a community have learned how to deal with that and improved a lot.  As a consequence, we are going to have a well documented and formalized process for this in the foreseeable future. There are also efforts on the way to have non-public testing infrastructure available for future events of this kind.  No need to speculate whether this makes sense. I'm not overly optimistic that we have seen all of that by now and my gut feeling tells me that we are going to be haunted by that kind of issues for a very long time. For the very unlikely case that I'm proven wrong, then I'm surely not going to shed a tear about the time spent on writing the documentation and getting things prepared.  At this point I want to say BIG THANKS to everybody involved for all the great work which was done under not so enjoyable circumstances. Both the required secrecy and the set in stone timelines are pretty different from our normal workflow. At the same time I want to take the opportunity and apologize for any outburst I had. I know that I went overboard occasionally and it's nothing I'm proud of.  Looking back, I have to say that all of this certainly had consequences outside of that restricted setting. The coordinated release dates forced quite some people to put a break on other tasks which were piling up nevertheless. The review backlog was from time to time tremendous and I'm sure that we dropped stuff on the way and that we still have things to catch up with on all ends.  Though a lot of this pressure and fallout is home-grown and could have been avoided at least to some extent. The underlying reasons are not specific to the mitigation development, the circumstances just emphasized them and made them more observable for everyone - involved or not.   1) Lack of code quality      This is a problem which I observe increasing over many years.      The feature driven duct tape engineering mode is progressing     massively. Proper root cause analysis has become the exception not the     rule.      In our normal kernel development it's just annoying and eats up review     capacity unnecessarily, but in the face of a timeline or real bugs it's     worse. Aside of wasting time for review rounds, at some point other     people have to just drop everything else and get it fixed.      Even if some people don't want to admit it, the increasing complexity     of the hardware technology and as a consequence the increasing     complexity of the kernel code base makes it mandatory to put     correctness and maintainability first and not to fall for the     featuritis and performance chants which are driving this     industry. We've learned painfully what that causes in the last year.   2) Lack of review response      Not addressing review feedback is not a new problem, but again under     time pressure or in the face of real bugs it becomes a real pain and     causes extra work for others and maintainers in particular.   3) Outright refusal      I've seen particularly in this year quite some people who responded to     review feedback with outright and outspoken refusal. The points they     refuse to address are not some esoteric whims of particular     maintainers, no it's refusal to accept that there are documented     process and patch submission rules which apply for everyone.      Again, not a big problem if it's related to features. If it's related     to actual bugs or the timelined mitigation development then it causes     extra burden for others.  In other words, if we are exposed to more half-baked patches, sloppy addressing of review feedback or in the worst case refusal to collaborate and then on top getting complaints about maintainers and reviewers being bottlenecks, then this will become a real problem in the not so distant future.  Companies have to understand, that the kernel community cannot provide all-inclusive educational programs for their engineers. It's about time, that the companies catch the obvious wreckage before it leaves the house and make sure that feedback is addressed properly and in all points.  I'm neither expecting perfect patches nor is there a guarantee that even well thought out and well written code will go into the tree undisputed. Though reviewing and discussing something which is well done is way less time consuming and frustrating than dealing with the above.  I know that some people will come forth immediately and educate me once more on maintainer models and the need to bring new maintainers in fast.  I'm all for more maintainers, but it's hard to find the right people.  All good maintainers - and I've brought quite a few of them into that role myself - had proven themselves in their contributor role before taking that up. Rest assured that I constantly look out for these people and try to get them on board. Picking them out is based on their technical skills but even more so on their mindset. Unfortunately quite some of them don't want to step into that role because they are well aware of the responsibility and the burden which comes with it. I respect that decision and I definitely can understand it. I was more than once on the verge of throwing in the towel during the last year.  I'm not opposed to try new things, quite the contrary. But something which worked out for a particular subsystem cannot be applied blindly to everything else in the hope that it works out. That needs a lot more thought and I'm not at all buying that tooling is a crucial part of the solution.  Last but not least, I'm not sure whether more maintainers can solve the pain points which bugger me most. I rather think we'd need lots of nursemaids and teachers to address that.  Sorry for the lengthy and maybe unpleasant read, but keeping the frustration which built up over the year to myself would just cause me gastric ulcer and a bad mood over Christmas. So I decided to vent and share it with all of you even at the risk that I'm barking up the wrong tree.  That said, I'm going to vanish into vacation until Jan. 7th and I'm not going to read any (LKML) mails until then. As I predict from experience that my (filtered) inbox will be a untameable beast by then, don't expect me to actually go through it mail by mail. If your mail will unfortunately end up in the 'lkml/done' folder without being read, I'm sure you'll notice and find a way to resend it.  I'm nevertheless looking positively forward to the new challenges of 2019 and I wish you all a Merry Christmas, a Happy New Year and a refreshing break! I wish especially for those who suffered a year ago, that they can enjoy quality time with their families and friends!  Thanks,  	Thomas""",technical,Thomas Gleixner,tglx@linutronix.de,1,1,8425,1.0,0.09090909090909091,0,0.0,1.0,-1.00398406374502,0.0
918,498594,498677,"I totally agree on this point by having been hit by the same problem on another project (haproxy). It turns out that everyone are interested in features, reliability and performance. But these ones cannot come without maintainability, and in practice only these 3 former ones can improve overtime. Maintainability only gets worse and is never ever addressed ""later"" by incremental code updates. Now I tend to be a bastard on this point and to demand properly documented patches, properly named functions/variables and everything that helps other people quickly figure why the code works or doesn't work, knowing that performance/features/reliability area easily addressed afterwards by many other contributors when the code is maintainable. Take your well deserved vacation with your family, ignore e-mails and don't read the news, it will only make you relax better, and you'll come back fully recharged.","Hi Thomas,  [trimmed cc list]  On Thu, Dec 20, 2018 at 01:46:24AM +0100, Thomas Gleixner wrote:  I totally agree on this point by having been hit by the same problem on another project (haproxy). It turns out that everyone are interested in features, reliability and performance. But these ones cannot come without maintainability, and in practice only these 3 former ones can improve over time. Maintainability only gets worse and is never ever addressed later"" by incremental code updates. Now I tend to be a bastard on this point and to demand properly documented patches, properly named functions/variables and everything that helps other people quickly figure why the code works or doesn't work, knowing that performance/features/reliability area easily addressed afterwards by many other contributors when the code is maintainable.   Take your well deserved vacation with your family, ignore e-mails and don't read the news, it will only make you relax better, and you'll come back fully recharged.  Willy""",technical,Willy Tarreau,w@1wt.eu,1,0,905,0.1,0.18181818181818182,0,0.0,1.0,0.0,0.05179282868525897
919,498594,573878,"Ping? Jonathan, can you pick this up?","Ping? Jonathan, can you pick this up?  								Pavel  On Thu 2019-01-03 00:51:52, Pavel Machek wrote:    --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,37,0.006211180124223602,0.36363636363636365,0,0.32270916334661354,0.6772908366533864,0.26693227091633465,0.0
920,500531,500772,"Thanks a lot for the follow-up to our earlier discussion here! Are we actually worried about concurrent writers here? I thought the only problem was a race between writer and reader, which would mean that we could solve it using only a seqcount_t which is cheaper to update than a seqlock_t.","On 12/21/18, Deepa Dinamani <deepa.kernel@gmail.com> wrote:  Hi Deepa,  Thanks a lot for the follow-up to our earlier discussion here!  Are we actually worried about concurrent writers here? I thought the only problem was a race between writer and reader, which would mean that we could solve it using only a seqcount_t which is cheaper to update than a seqlock_t.         Arnd",technical,Arnd Bergmann,arnd@arndb.de,1,0,291,0.4435483870967742,0.3333333333333333,0,0.0,1.0,0.0,0.0
921,500531,500752,"I considered using just the seqcount_t. But, I think we do care about concurrent writers here. A couple of scenarios I can think of:1. When you have 2 concurrent recvmsg() calls on a socket, and they both try to update sk_tstamp.2. If a socket has don't have one of the SO_TIMESTAMP/NS options set and you have a first recvmsg and a concurrent ioctl call on thesocket.These are corner cases and if we don't care aout these then we can use just the sequence counters. I have missed out tstamp update in the sunrcpc code. If everyone is ok with this approach, I will add it in when I post an update","I considered using just the seqcount_t. But, I think we do care about concurrent writers here. A couple of scenarios I can think of:  1. When you have 2 concurrent recvmsg() calls on a socket, and they both try to update sk_tstamp. 2. If a socket has don't have one of the SO_TIMESTAMP/NS options set and you have a first recvmsg and a concurrent ioctl call on the socket.  These are corner cases and if we don't care aout these then we can use just the sequence counters.  I have missed out tstamp update in the sunrcpc code. If everyone is ok with this approach, I will add it in when I post an update  -Deepa",technical,Deepa Dinamani,deepa.kernel@gmail.com,0,1,596,1.0,0.5,0,0.0,1.0,0.0,0.0
922,500531,500930,"Since, regardless of whether this is the final approach we will take, it seems that sunrpc needs to be added to this patch. So I'm definitely waiting for a new version. Thanks.","From: Deepa Dinamani <deepa.kernel@gmail.com> Date: Fri, 21 Dec 2018 12:27:33 -0800   Since, regardless of whether this is the final approach we will take, it seems that sunrpc needs to be added to this patch.  So I'm definitely waiting for a new version.  Thanks.",technical,David Miller,davem@davemloft.net,1,0,176,0.3064516129032258,0.6666666666666666,0,1.0,0.0,0.0,0.0
923,500531,501008,"Please come up with something that has zero added costs for 64bit kernels .Most of us do not really care about 32bit kernels anymore, so we do not want to slow down 64bits kernels for such things. Look at include/linux/u64_stats_sync.h for initial thoughts. Thanks.","On 12/21/2018 12:27 PM, Deepa Dinamani wrote:  Hi Deepa  Please come up with something that has zero added costs for 64bit kernels.  Most of us do not really care about 32bit kernels anymore, so we do not want to slow down 64bits kernels for such things.  Look at include/linux/u64_stats_sync.h for initial thoughts.  Thanks.",technical,Eric Dumazet,eric.dumazet@gmail.com,1,0,265,0.3870967741935484,0.8333333333333334,0,1.0,0.0,0.0,0.0
924,500531,501135,"This is similar to what I did here. But, I can add a few ifdef's to make this code a noop on 64 bit arches. I will include this in my next update. I'm assuming there is no contention on whether writers need exclusive access and hence requiring a lock here.Let me know otherwise.","On Sat, Dec 22, 2018 at 11:31 PM Eric Dumazet <eric.dumazet@gmail.com> wrote:  This is similar to what I did here. But, I can add a few ifdef's to make this code a noop on 64 bit arches. I will include this in my next update.  I'm assuming there is no contention on whether writers need exclusive access and hence requiring a lock here. Let me know otherwise.  Thanks, Deepa",technical,Deepa Dinamani,deepa.kernel@gmail.com,0,1,278,0.49193548387096775,1.0,1,1.0,0.0,0.0,0.0
925,501672,501962,"HI, What makes you think that not matching on this compatible is an error? Have you looked at the rest of the driver?","Hi,  On Tue, Dec 25, 2018 at 07:51:29PM -0600, Kangjie Lu wrote:  What makes you think that not matching on this compatible is an error? Have you looked at the rest of the dirver?   --  Dmitry",technical,Dmitry Torokhov,dmitry.torokhov@gmail.com,1,0,117,1.0,1.0,1,0.0,0.0,0.0,0.0
926,501706,501720,"Btrfs have better error message infrastructures (e.g. distinguish different filesystems). Please use btrfs_error() or btrfs_warn() instead. Despite that, I think the patch looks good.","On 2018/12/26 上午11:46, Kangjie Lu wrote:  Btrfs have better error message infrastructures (e.g. distinguish different filesystems).  Please use btrfs_error() or btrfs_warn() instead.  Despite that, I think the patch looks good.  Thanks, Qu",technical,Qu Wenruo,quwenruo.btrfs@gmx.com,0,0,183,1.0,1.0,1,0.0,0.0,0.0,0.0
927,502474,504194,"Were you able to reproduce?  If so, did you use the syzkaller output or something else?","On Thu, Dec 27, 2018 at 8:08 PM Myungho Jung <mhjungk@gmail.com> wrote:  Hi Myungho,  Were you able to reproduce?  If so, did you use the syzkaller output or something else?  Thanks,                  Ilya",technical,Ilya Dryomov,idryomov@gmail.com,1,0,87,0.18811881188118812,0.3333333333333333,0,0.2777777777777778,0.6666666666666666,0.2777777777777778,0.0
928,502474,504498,I reproduced on vm using syzkaller utils and verified the fix by syzbot.,"On Wed, Jan 02, 2019 at 04:42:47PM +0100, Ilya Dryomov wrote: Hi Ilya,  I reproduced on vm using syzkaller utils and verified the fix by syzbot.  Thanks, Myungho",technical,Myungho Jung,mhjungk@gmail.com,0,1,72,0.13861386138613863,0.5,0,0.3333333333333333,0.6666666666666666,0.0,0.6111111111111112
929,502474,516217,"I think this might be a better fix,WRITE_PENDING can be set without con->mutex held from socket callbacks.This is the reason we use atomic bit ops here, so testing WRITE_PENDING under the lock didn't make sense to me.At the same time, KEEPALIVE_PENDING could have been a non-atomic flag. I spent some time trying to make sense of conditioning queue_con() call on the previous value of KEEPALIVE_PENDING and couldn't see any, so I'm setting it with con_flag_set(), making ceph_con_keepalive() symmetric  with ceph_con_send().","On Thu, Jan 3, 2019 at 4:50 AM Myungho Jung <mhjungk@gmail.com> wrote:  Hi Myungho,  I think this might be a better fix:  diff --git a/net/ceph/messenger.c b/net/ceph/messenger.c index d5718284db57..c5f5313e3537 100644 --- a/net/ceph/messenger.c +++ b/net/ceph/messenger.c @@ -3205,10 +3205,11 @@ void ceph_con_keepalive(struct ceph_connection *con)  {         dout(con_keepalive %p\n"", con),         mutex_lock(&con->mutex), +       con_flag_set(con, CON_FLAG_KEEPALIVE_PENDING),         clear_standby(con),         mutex_unlock(&con->mutex), -       if (con_flag_test_and_set(con, CON_FLAG_KEEPALIVE_PENDING) == 0 && -           con_flag_test_and_set(con, CON_FLAG_WRITE_PENDING) == 0) + +       if (con_flag_test_and_set(con, CON_FLAG_WRITE_PENDING) == 0)                 queue_con(con),  }  EXPORT_SYMBOL(ceph_con_keepalive),  WRITE_PENDING can be set without con->mutex held from socket callbacks. This is the reason we use atomic bit ops here, so testing WRITE_PENDING under the lock didn't make sense to me.  At the same time, KEEPALIVE_PENDING could have been a non-atomic flag. I spent some time trying to make sense of conditioning queue_con() call on the previous value of KEEPALIVE_PENDING and couldn't see any, so I'm setting it with con_flag_set(), making ceph_con_keepalive() symmetric with ceph_con_send().  Thanks,                  Ilya""",technical,Ilya Dryomov,idryomov@gmail.com,1,0,524,1.0,0.6666666666666666,0,1.0,0.0,0.6111111111111112,0.0
930,502474,516585,"Yes, it looks clear and makes sense to have an atomic operation in if statement but it still triggers warning. KEEPALIVE_PENDING should be set afterclear_standby() because con_fault() can be called right before acquiring the lock here which sets the flag in standby state. I tested the change with syzbot and confirmed there was no warning.","On Mon, Jan 14, 2019 at 09:37:25PM +0100, Ilya Dryomov wrote:  Hi Ilya,  Yes, it looks clear and makes sense to have an atomic operation in if statement but it still triggers warning. KEEPALIVE_PENDING should be set after clear_standby() because con_fault() can be called right before acquiring the lock here which sets the flag in standby state. I tesed the change with syzbot and confirmed there was no warning.  Thanks, Myungho",technical,Myungho Jung,mhjungk@gmail.com,0,1,340,0.6237623762376238,0.8333333333333334,0,1.0,0.0,0.0,0.0
931,502474,516719,"Right, it still triggers one of the warnings.  I was too focused on WRITE_PENDING and missed that in plain sight.  I'll update the patch. Thanks for testing!                Ilya","On Tue, Jan 15, 2019 at 7:56 AM Myungho Jung <mhjungk@gmail.com> wrote:  Right, it still triggers one of the warnings.  I was too focused on WRITE_PENDING and missed that in plain sight.  I'll update the patch.  Thanks for testing!                  Ilya",technical,Ilya Dryomov,idryomov@gmail.com,1,0,177,0.33663366336633666,1.0,1,1.0,0.0,0.0,0.0
932,504434,505381,"Really?  Why not?  What keeps you from ""knowing"" this?  Can't the developer of the chip tell you? Shouldn't ""Unknown"" really be the same thing as ""Vulnerable""?  A user should treat it the same way, ""Unknown"" makes it feel like ""maybe I can just ignore this and hope I really am safe"", which is not a good idea at all.","On Wed, Jan 02, 2019 at 06:49:15PM -0600, Jeremy Linton wrote:  Really?  Why not?  What keeps you from knowing"" this?  Can't the developer of the chip tell you?   Shouldn't ""Unknown"" really be the same thing as ""Vulnerable""?  A user should treat it the same way, ""Unknown"" makes it feel like ""maybe I can just ignore this and hope I really am safe"", which is not a good idea at all.  thanks,  greg k-h""",technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,0,317,0.30980392156862746,0.47368421052631576,0,0.0,1.0,0.0,0.0
933,504434,505675,"Do some of the ""Unknown"" cases arise  from the vulnerability detection code being compiled out of the kernel? I wonder whether at least the detection support should be mandatory. sysfs is not very useful as a standard vulnerability reporting interface unless we make best efforts to always populate it with real information. Also, does ""Unknown"" convey anything beyond what is indicated by the sysfs entry being omitted altogether?","On Wed, Jan 02, 2019 at 06:49:15PM -0600, Jeremy Linton wrote:  Do some of the Unknown"" cases arise from the vulnerability detection code being compiled out of the kernel?  I wonder whether at least the detection support should be mandatory. sysfs is not very useful as a standard vulnerability reporting interface unless we make best efforts to always populate it with real information.   Also, does ""Unknown"" convey anything beyond what is indicated by the sysfs entry being omitted altogether?  Cheers ---Dave""",technical,Dave Martin,Dave.Martin@arm.com,0,0,431,0.30196078431372547,0.5263157894736842,0,0.0,0.0,0.0,0.0
934,504434,505677,"There tends to be a few cases, possibly incomplete white/black lists, firmware that isn't responding correctly, or the user didn't build in the code to check the mitigation (possibly because its an embedded system and they know its not vulnerable?).I would hope that it is an exceptional case. I tend to agree its not clear what to do with ""unknown"". OTOH, I think there is a hesitation to declare something vulnerable when it isn't. Meltdown for example, is fairly rare given that it currently only affects a few arm parts, so declaring someone vulnerable when they likely aren't is going to be just as difficult to explain.","On 01/03/2019 03:38 AM, Greg Kroah-Hartman wrote:  There tends to be a few cases, possibly incomplete white/black lists,  firmware that isn't responding correctly, or the user didn't build in  the code to check the mitigation (possibly because its an embedded  system and they know its not vulnerable?).  I would hope that it is an exceptional case.   I tend to agree its not clear what to do with unknown"".  OTOH, I think there is a hesitation to declare something vulnerable when  it isn't. Meltdown for example, is fairly rare given that it currently  only affects a few arm parts, so declaring someone vulnerable when they  likely aren't is going to be just as difficult to explain.""",technical,Jeremy Linton,jeremy.linton@arm.com,0,1,625,0.4980392156862745,0.5789473684210527,0,0.0,0.0,0.0,0.0
935,504434,505684,"Hi, yes, I'm not sure about this one. I tend to think the ""unknown"" case encourages users that really want an answer to dig deeper and call their hardware/os/whoever to get an answer. I would tend to think that if the entry is missing it would tend to encourage the behavior that Greg KH mentions where the user assumes ""hey the system doesn't have a sysfs entry for VUNLERABILITY, that probably means that its not possible on the architecture"".","Hi,  On 01/03/2019 10:37 AM, Dave Martin wrote:  Yes,   I'm not sure about this one. I tend to think the unknown"" case  encourages users that really want an answer to dig deeper and call their  hardware/os/whoever to get an answer. I would tend to think that if the  entry is missing it would tend to encourage the behavior that Greg KH  mentions where the user assumes ""hey the system doesn't have a sysfs  entry for $VUNLERABILITY, that probably means that its not possible on  the architecture"".""",technical,Jeremy Linton,jeremy.linton@arm.com,0,1,445,0.3568627450980392,0.631578947368421,0,0.0,0.0,0.0,0.0
936,504434,505687,"Then fix the lists :) If the firmware doesn't respond, that would imply it is vulnerable :) And if the code isn't built in, again, it's vulnerable. Then have the default be vulnerable, don't give people false hope. If you know it is rare, then you know how to properly detect it so ""unknown"" is not needed, correct? Again, ""unknown"" is not going to help anyone out here, please don't do it.","On Thu, Jan 03, 2019 at 10:38:16AM -0600, Jeremy Linton wrote:  Then fix the lists :)   If the firmware doesn't respond, that would imply it is vulnerable :)  And if the code isn't built in, again, it's vulnerable.   Then have the default be vulnerable, don't give people false hope.   If you know it is rare, then you know how to properly detect it so unknown"" is not needed, correct?  Again, ""unknown"" is not going to help anyone out here, please don't do it.  thanks,  greg k-h""",technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,0,390,0.37254901960784315,0.6842105263157895,0,0.0,0.0,0.0,0.0
937,504434,505777,"i applied your patch series on linux-next-20190103. On my Raspberry Pi 3B+ (defconfig) I'm getting this from sysfs:l1tf:Not affected meltdown: Not affected spec_store_bypass:Unknown spectre_v1:Mitigation: __user pointer sanitizationspectre_v2: Unknown AFAIK it has 4 Cortex-A53 cores (no PSCI firmware), so shouldn't be affected. How can this be fixed?","Hi Jeremy,   i applied your patch series on linux-next-20190103. On my Raspberry Pi 3B+ (defconfig) i'm getting this from sysfs:  l1tf:Not affected meltdown:Not affected spec_store_bypass:Unknown spectre_v1:Mitigation: __user pointer sanitization spectre_v2:Unknown  AFAIK it has 4 Cortex-A53 cores (no PSCI firmware), so shouldn't be affected.  How can this be fixed?  Thanks Stefan",technical,Stefan Wahren,stefan.wahren@i2se.com,0,0,352,0.2627450980392157,0.7368421052631579,0,0.0,0.0,0.0,0.0
938,504434,505816,"HI, So, for spec_store_bypass, as you noted your getting hit by the lack of psci/smccc to report the ssb state, and this patch is just reflecting that. In the case of spectrev2 it may be correct to blame this patch set because its displaying ""unknown"" since your core isn't in the blacklist, and your core isn't new enough to have the csv2 bit indicating its not vulnerable. In this case if we do away with the unknown state, we should probably depend entirely on the black list and simply display ""Not affected"" if the core isn't listed. (meaning we may report cores not affected when they are missing from the blacklist). For ssb, the correct answer is probably fix the firmware, but given the situation, its likely this kind of machine is going to force an additional MIDR list to report the state correctly. Maybe Will or someone can chime in here? For spectrev2, wait for another version of this patch.","Hi,  On 01/03/2019 01:30 PM, Stefan Wahren wrote:  So, for spec_store_bypass, as you noted your getting hit by the lack of  psci/smccc to report the ssb state, and this patch is just reflecting that.  In the case of spectrev2 it may be correct to blame this patch set  because its displaying unknown"" since your core isn't in the black  list, and your core isn't new enough to have the csv2 bit indicating its  not vulnerable. In this case if we do away with the unknown state, we  should probably depend entirely on the black list and simply display  ""Not affected"" if the core isn't listed. (meaning we may report cores  not affected when they are missing from the blacklist).    For ssb, the correct answer is probably fix the firmware, but given the  situation, its likely this kind of machine is going to force an  additional MIDR list to report the state correctly. Maybe Will or  someone can chime in here?  For spectrev2, wait for another version of this patch.""",technical,Jeremy Linton,jeremy.linton@arm.com,0,1,907,0.7333333333333333,0.7894736842105263,0,0.0,0.0,0.0,0.0
939,504434,506117,"Marc Z is already working on this iirc, since we need it to fix the message printed to dmesg about the mitigation status anyway.","On Thu, Jan 03, 2019 at 02:32:44PM -0600, Jeremy Linton wrote:  Marc Z is already working on this iirc, since we need it to fix the message printed to dmesg about the mitigation status anyway.  Will",technical,Will Deacon,will.deacon@arm.com,1,0,128,0.10196078431372549,0.8421052631578947,0,1.0,0.0,0.0,0.0
940,504434,506255,"Thinking about it, ""unknown"" is actually the common case. Kernels that predate the sysfs vulnerabilities interface effectively report this for all vulnerabilities by omitting the sysfs entries entirely. Current kernels also don't know anything about future vulnerabilities that may be added in sysfs later on (but which may nevertheless be discovered subsequently to affect current hardware). So, can we simply omit the sysfs entries for which we can't provide a good answer? IMHO the kernel should make best efforts to provide answers for every vulnerability that it knows about, so the checks should not be Kconfig-dependent without a good reason. There will be cases where whitelists/blacklists are the only source of answers, and we are ultimately reliant on vendors to provide that information.  Upstream Linux is likely to lag, there's not much we can do about that.","On Thu, Jan 03, 2019 at 05:48:31PM +0100, Greg Kroah-Hartman wrote:  Thinking about it, unknown"" is actually the common case.  Kernels that predate the sysfs vulnerabilities interface effectively report this for all vulnerabilities by omitting the sysfs entries entirely.  Current kernels also don't know anything about future vulnerabilities that may be added in sysfs later on (but which may nevertheless be discovered subsequently to affect current hardware).  So, can we simply omit the sysfs entries for which we can't provide a good answer?  IMHO the kernel should make best efforts to provide answers for every vulnerability that it knows about, so the checks should not be Kconfig- dependent without a good reason.  There will be cases where whitelists/blacklists are the only source of answers, and we are ultimately reliant on vendors to provide that information.  Upstream Linux is likely to lag, there's not much we can do about that.  Cheers ---Dave""",technical,Dave Martin,Dave.Martin@arm.com,0,0,872,0.611764705882353,0.8947368421052632,0,1.0,0.0,0.0,0.0
941,504434,506261,"As you say, we already do this for older systems. But don't add new logic to explicitly not create the files just because we ""can not figure it out"".  For those systems, I would default to ""vulnerable"" as I think that's what we do today, right?","On Fri, Jan 04, 2019 at 02:08:32PM +0000, Dave Martin wrote:  As you say, we already do this for older systems.  But don't add new logic to explicitly not create the files just because we can not figure it out"".  For those systems, I would default to ""vulnerable"" as I think that's what we do today, right?  thanks, g reg k-h""",technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,0,244,0.22745098039215686,0.9473684210526315,0,1.0,0.0,0.0,0.0
942,504434,506275,"Nope: currently the vulnerabilities directory doesn't even exist for arm64 because we don't select GENERIC_CPU_VULNERABILITIES.There are also a few other things to consider here:  1. The action to take as an end-user is slightly different in the case     that you know for sure that your system is vulnerable, as opposed to     the case that you don't know whether your system is vulnerable or not.     The former needs a firmware update, the second needs a statement about     the CPU, which could result in a simple whitelist update in Linux.  2. There's an unfortunate political angle to this. Whilst the Arm website     [1] provides information for all of the Arm-designed CPUs (i.e.     Cortex-A*), it doesn't comment on partner implementations. I'm not at     all keen to be seen as branding them all as vulnerable in the Linux     kernel, as this is likely to cause more problems than it solves.     If we had complete whitelist information available in public, that     would be ideal, but it's not the case.  3. The architecture has added some ID registers to determine if a CPU     is affected by Spectre and Meltdown, so a whitelist only needs to     cover existing CPUs. So I agree with Dave that continuing to omit the files when we don't know whether or not the system is affected is the right thing to do.","On Fri, Jan 04, 2019 at 03:18:05PM +0100, Greg Kroah-Hartman wrote:  Nope: currently the vulnerabilities directory doesn't even exist for arm64 because we don't select GENERIC_CPU_VULNERABILITIES.  There are also a few other things to consider here:    1. The action to take as an end-user is slightly different in the case      that you know for sure that your system is vulnerable, as opposed to      the case that you don't know whether your system is vulnerable or not.      The former needs a firmware update, the second needs a statement about      the CPU, which could result in a simple whitelist update in Linux.    2. There's an unfortunate political angle to this. Whilst the Arm website      [1] provides information for all of the Arm-designed CPUs (i.e.      Cortex-A*), it doesn't comment on partner implementations. I'm not at      all keen to be seen as branding them all as vulnerable in the Linux      kernel, as this is likely to cause more problems than it solves.      If we had complete whitelist information available in public, that      would be ideal, but it's not the case.    3. The architecture has added some ID registers to determine if a CPU      is affected by Spectre and Meltdown, so a whitelist only needs to      cover existing CPUs.  So I agree with Dave that continuing to omit the files when we don't know whether or not the system is affected is the right thing to do.  Will  [1] https://developer.arm.com/support/arm-security-updates/speculative-processor-vulnerability",technical,Will Deacon,will.deacon@arm.com,1,0,1320,1.0,1.0,1,1.0,0.0,0.0,0.0
943,505390,505385,Already send a new version My reference is SCSI Block Commands,"On Wed, Jan 02, 2019 at 11:51:33PM -0800, Christoph Hellwig wrote: Already send a new version My reference is SCSI Block Commands – 3 (SBC-3) Revision 25. Section 5.32 WRITE (10) and 5.34 WRITE (16)",technical,Randall Huang,ihhuang@abmail.org,0,1,62,0.07586206896551724,0.1111111111111111,0,0.0,1.0,-1.1428571428571428,0.0
944,505390,505831,"That is the GROUP NUMBER field. Also found in READ(16) at the same location within its cdb. The proposed code deserves at least an explanatory comment. Since it is relatively recent, perhaps the above should only be done iff:    - the REPORT SUPPORTED OPERATION CODES (RSOC) command is supported, and    - in the RSOC entry for WRITE(16), the CDB USAGE DATA field (a bit mask)      indicates the GROUP NUMBER field is supported That check can be done once, at disk attachment time where there is already code to fetch RSOC.Is there a bi_read_hint ? If not then the bi_write_hint should also be applied to READ(16). Makes that variable naming look pretty silly though.","On 2019-01-03 4:47 a.m., Randall Huang wrote:  That is the GROUP NUMBER field. Also found in READ(16) at the same location within its cdb. The proposed code deserves at least an explanatory comment.  Since it is relatively recent, perhaps the above should only be done iff:     - the REPORT SUPPORTED OPERATION CODES (RSOC) command is supported, and     - in the RSOC entry for WRITE(16), the CDB USAGE DATA field (a bit mask)       indicates the GROUP NUMBER field is supported  That check can be done once, at disk attachment time where there is already code to fetch RSOC.   Is there a bi_read_hint ? If not then the bi_write_hint should also be applied to READ(16). Makes that variable naming look pretty silly though.  Doug Gilbert",technical,Douglas Gilbert,dgilbert@interlog.com,1,0,667,0.9379310344827586,0.3333333333333333,0,0.0,0.8571428571428571,0.0,0.0
945,505390,505845,"SBC-5 says that support for the grouping function is indicated by the GROUP_SUP bit in the Extended Inquiry VPD page (86h).  I'm not sure how many devices actually support that page though.  Probably most don't. What devices actually DO support the grouping and do something with it? We'd probably need a blacklist flag to turn this off and/or some code in the error path to discontinue setting the field if the device returns INVALID FIELD IN CDB or something, like we do for disabling discard commands if they don't actually work in sd_done().","On Thu, 2019-01-03 at 16:00 -0500, Douglas Gilbert wrote:  SBC-5 says that support for the grouping function is indicated by the GROUP_SUP bit in the Extended Inquiry VPD page (86h).  I'm not sure how many devices actually support that page though.  Probably most don't.  What devices actually DO support the grouping and do something with it?  We'd probably need a blacklist flag to turn this off and/or some code in the error path to discontinue setting the field if the device returns INVALID FIELD IN CDB or something, like we do for disabling discard commands if they don't actually work in sd_done().  -Ewan",technical,Ewan D. Milne,emilne@redhat.com,0,0,545,0.7379310344827587,0.4444444444444444,0,0.0,0.8571428571428571,0.0,0.0
946,505390,505973,"Several devices support it, albeit for various different purposes. It's one of these wonderful features whose interpretation was left outside the scope of the spec for a long time. So even though we absolutely and positively need to make setting GROUPNUMBER conditional on GROUP_SUP being reported, we also need additional information from the storage about how the field should be interpreted. The official way to report hinting is for the device to implement the IOAdvice Hints Grouping mode page. I wrote some code to support that but no vendors that I know of ended up actually shipping an implementation. A few implemented my older I/O class proposal but didn't ship that either despite really convincing performance results. If Randall has access to a device which implements hinting, I'd love to know more."," Ewan,   Several devices support it, albeit for various different purposes. It's one of these wonderful features whose interpretation was left outside the scope of the spec for a long time.  So even though we absolutely and positively need to make setting GROUP NUMBER conditional on GROUP_SUP being reported, we also need additional information from the storage about how the field should be interpreted.  The official way to report hinting is for the device to implement the IO Advice Hints Grouping mode page. I wrote some code to support that but no vendors that I know of ended up actually shipping an implementation. A few implemented my older I/O class proposal but didn't ship that either despite really convincing performance results.  If Randall has access to a device which implements hinting, I'd love to know more.  --  Martin K. Petersen	Oracle Linux Engineering",technical,Martin K. Petersen,martin.petersen@oracle.com,1,0,813,1.0,0.5555555555555556,0,0.0,0.8571428571428571,0.0,0.0
947,505390,505983,I am working on Android phone. The idea is to enable write hint for Turbo write UFS feature.Turbo write feature in UFS 3.x is under discussion in JEDEC JC-64.This patch is the under-lying framework for supporting this feature.,"On Thu, Jan 03, 2019 at 11:57:38PM -0500, Martin K. Petersen wrote: I am working on Android phone. The idea is to enable write hint for Turbo write UFS feature. Turbo write feature in UFS 3.x is under discussion in JEDEC JC-64.  This patch is the under-lying framework for supporting this feature.",technical,Randall Huang,ihhuang@abmail.org,0,1,226,0.27586206896551724,0.6666666666666666,0,0.0,0.8571428571428571,0.0,0.0
948,505390,506000,"OK, but we can't blindly go setting GROUP NUMBER to a non-zero value. That'll break a massive amount of devices which will fail READ/WRITE commands with INVALID FIELD IN CDB. So aside from requiring the device to report GROUP_SUP=1, we'll need some sort of indication that this device supports the UFS Turbo Write feature. If you are engaged with JEDEC on this, please tell them we'll need a VPD page, a mode page, or something similar to use as trigger to entertain enabling this feature.","Hi Randall,    OK, but we can't blindly go setting GROUP NUMBER to a non-zero value. That'll break a massive amount of devices which will fail READ/WRITE commands with INVALID FIELD IN CDB.  So aside from requiring the device to report GROUP_SUP=1, we'll need some sort of indication that this device supports the UFS Turbo Write feature. If you are engaged with JEDEC on this, please tell them we'll need a VPD page, a mode page, or something similar to use as trigger to entertain enabling this feature.  --  Martin K. Petersen	Oracle Linux Engineering",technical,Martin K. Petersen,martin.petersen@oracle.com,1,0,489,0.6758620689655173,0.7777777777777778,0,0.0,0.8571428571428571,0.0,0.8571428571428571
949,505390,512151,"As far as I know, there are more than one Turbo Write feature proposals in JEDEC, which are currently under discussion. For now, not all proposals are using CDB bytes for enabling TurboWrite. So, maybe making this change in SCSI is still too early.","Hi Randall,    On Fri, 2019-01-04 at 13:22 +0800, Randall Huang wrote:    As far as I know, there are more than one Turbo Write feature  proposals in JEDEC, which are currently under discussion.  For now, not all proposals are using CDB bytes for enabling  TurboWrite.  So, maybe making this change in SCSI is still too early.",technical,Alex Lemberg,Alex.Lemberg@wdc.com,0,0,248,0.35172413793103446,0.8888888888888888,1,1.0,0.0,0.8571428571428571,0.0
950,508181,516512,"Given that keycodes are linux-specific, I think the property should be linux,keycodes. Also, I am not sure we need separate rotary-encoder, relative-keys property as we can infer that we want to generate keys from presence of linux, keycodes property. Rob, any comments?","On Tue, Jan 08, 2019 at 01:42:49AM +0900, Donghoon Han wrote:  Given that keycodes are linux-specific, I think the property should be linux,keycodes. Also, I am not sure we need separate rotary-encoder,relative-keys property as we can infer that we want to generate keys from presence of linux,keycodes property.  Rob, any comments?   Thanks.  --  Dmitry",technical,Dmitry Torokhov,dmitry.torokhov@gmail.com,1,0,270,0.5977011494252874,0.5714285714285714,0,0.0958904109589041,0.8904109589041096,0.0958904109589041,0.0
951,508181,517448,"Yes, I had similar thoughts.","On Mon, Jan 14, 2019 at 07:52:21PM -0800, Dmitry Torokhov wrote:  Yes, I had similar thoughts.",technical,Rob Herring,robh@kernel.org,1,0,28,0.08045977011494253,0.8571428571428571,0,0.1095890410958904,0.8904109589041096,0.0,0.8904109589041096
952,508181,587450,"Hello, i used this rotary-encoder patch in my embedded project and found two errors. I am sorry, I know that E-Mail style is not good. I have no time right now, but I'll be back in two weeks. Someone, maybe Mr. Han, could submit a new version of the patch. If not, I'll try to do it on my return. (it could take some time, since I am new to patchwork)","Hello,  i used this rotary-encoder patch in my embedded project and found two  errors:   First, in drivers/input/misc/rotary_encoder.c, at @@ -237,6 +244,16 @@: instead of  +        if (err) +            dev_err(dev, unable to get keycodes: %d\n"", err), +        return err,  it must be  +        if (err) { +            dev_err(dev, ""unable to get keycodes: %d\n"", err), +            return err, +        }  otherwise successful creation of device is not possible.   Second, a typo in  Documentation/devicetree/bindings/input/rotary-encoder.txt, at @@ -48,3 +52,11 @@: instead of  +            rotary-encoder,relative-keycode = <103>, <108>,  it should be  +            rotary-encoder,relative-keycodes = <103>, <108>,  otherwise keycodes are not found.   I am sorry, I know that E-Mail style is not good. I have no time right now, but I'll be back in two weeks. Someone, maybe Mr. Han, could submit a new version of the patch. If not, I'll try to do it on my return. (it could take some time, since  I am new to patchwork)  Best Regards and thanks Alexey Slepov""",technical,Alexey Slepov,sir-lexa@yandex.ru,0,0,351,1.0,1.0,1,1.0,0.0,0.8904109589041096,0.0
953,508181,516513,"[ resending to Rob... ]Given that keycodes are linux-specific, I think the property should be linux, keycodes. Also, I am not sure we need separate rotary-encoder, relative-keys property as we can infer that we want to generate keys from presence of linux, keycodes property. Rob, any comments?","[ resending to Rob... ] On Tue, Jan 08, 2019 at 01:42:49AM +0900, Donghoon Han wrote:  Given that keycodes are linux-specific, I think the property should be linux,keycodes. Also, I am not sure we need separate rotary-encoder,relative-keys property as we can infer that we want to generate keys from presence of linux,keycodes property.  Rob, any comments?   Thanks.  --  Dmitry",technical,Dmitry Torokhov,dmitry.torokhov@gmail.com,1,0,294,0.6666666666666666,0.7142857142857143,0,0.0958904109589041,0.8904109589041096,0.0,0.0
954,509341,610769,I wonder if you can take this.,"Hi Dave,  I wonder if you can take this.  Thanks -- Gustavo  On 1/8/19 10:13 AM, Gustavo A. R. Silva wrote:",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,30,1.0,1.0,1,1.0,0.0,0.4673913043478261,0.0
955,514377,528953,Any comment on this patch ?,"On Fri, Jan 11, 2019 at 8:37 PM Souptick Joarder <jrdr.linux@gmail.com> wrote:  Any comment on this patch ?",technical,Souptick Joarder,jrdr.linux@gmail.com,0,1,27,1.0,1.0,1,1.0,0.0,1.0,0.0
956,515530,515530,"please take a look at this series, which implements a completely generic set of dma_map_ops for IOMMU drivers.  This is done by taking the existing arm64 code, moving it to drivers/iommu and then massaging it so that it can also work for architectures with DMA remapping.  This should help future ports to support IOMMUs more easily, and also allow to remove various custom IOMMU dma_map_ops implementations, like Tom was planning to for the AMD one. A git tree is also available at:    Gitweb:","Hi Robin,  please take a look at this series, which implements a completely generic set of dma_map_ops for IOMMU drivers.  This is done by taking the existing arm64 code, moving it to drivers/iommu and then massaging it so that it can also work for architectures with DMA remapping.  This should help future ports to support IOMMUs more easily, and also allow to remove various custom IOMMU dma_map_ops implementations, like Tom was planning to for the AMD one.  A git tree is also available at:      git://git.infradead.org/users/hch/misc.git dma-iommu-ops  Gitweb:      http://git.infradead.org/users/hch/misc.git/shortlog/refs/heads/dma-iommu-ops",technical,Christoph Hellwig,hch@lst.de,1,1,494,0.5822784810126582,0.02631578947368421,0,0.0,1.0,-1.0357142857142858,0.0
957,515530,529023,Any chance to get a review on this one?---end quoted text---,"Any chance to get a review on this one?  On Mon, Jan 14, 2019 at 10:41:40AM +0100, Christoph Hellwig wrote: ---end quoted text---",technical,Christoph Hellwig,hch@lst.de,1,1,60,0.10126582278481013,0.5526315789473685,0,0.4642857142857143,0.5,0.4642857142857143,0.14285714285714285
958,515530,536310,"I've been pondering this for a while now, and I still can't really come up with a case where arch_dma_prep_coherent() would need to behave differently from arch_sync_dma_for_device(..., DMA_BIDIRECTIONAL). I wonder if we could just save ourselves this little bit of complexity by using that instead...","On 14/01/2019 09:41, Christoph Hellwig wrote:  I've been pondering this for a while now, and I still can't really come  up with a case where arch_dma_prep_coherent() would need to behave  differently from arch_sync_dma_for_device(..., DMA_BIDIRECTIONAL). I  wonder if we could just save ourselves this little bit of complexity by  using that instead...  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,301,0.35443037974683544,0.5789473684210527,0,0.6428571428571429,0.35714285714285715,0.14285714285714285,0.0
959,515530,536328,"I think the __KERNEL__ and asm/errno.h slip-ups are things I cargo-culted from the arch code as a fresh-faced noob yet to learn the finer details, so ack for those parts. The forward-declarations, though, were a deliberate effort to minimise header dependencies and compilation bloat for includers who absolutely wouldn't care, and specifically to try to avoid setting transitive include expectations since they always seem to end up breaking someone's config somewhere down the line. Admittedly this little backwater is hardly comparable to the likes of the sched.h business, but I'm still somewhat on the fence about that change :/","On 14/01/2019 09:41, Christoph Hellwig wrote:  I think the __KERNEL__ and asm/errno.h slip-ups are things I  cargo-culted from the arch code as a fresh-faced noob yet to learn the  finer details, so ack for those parts. The forward-declarations, though,  were a deliberate effort to minimise header dependencies and compilation  bloat for includers who absolutely wouldn't care, and specifically to  try to avoid setting transitive include expectations since they always  seem to end up breaking someone's config somewhere down the line.  Admittedly this little backwater is hardly comparable to the likes of  the sched.h business, but I'm still somewhat on the fence about that  change :/  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,633,0.6962025316455697,0.6052631578947368,0,0.6428571428571429,0.35714285714285715,0.0,0.0
960,515530,536359,"It also defeats the whole purpose of __iommu_dma_alloc_pages(), so I'm not really buying the simplification angle - you've *seen* that code, right? ,)If you want simple, get rid of the pages array entirely. However, as I've touched on previously, it's all there for a reason, because making the individual iommu_map() calls as large as possible gives significant performance/power benefits in many cases which I'm not too keen to regress. In fact I still have the spark of an idea to sort the filled pages array for optimal physical layout, I've just never had the free time to play with it. FWIW, since iommu_map_sg() was new and promising at the time, using sg_alloc_table_from_pages() actually *was* the simplification over copying arch/arm's __iommu_create_mapping() logic.","On 14/01/2019 09:41, Christoph Hellwig wrote:  It also defeats the whole purpose of __iommu_dma_alloc_pages(), so I'm  not really buying the simplification angle - you've *seen* that code,  right? ,)  If you want simple, get rid of the pages array entirely. However, as  I've touched on previously, it's all there for a reason, because making  the individual iommu_map() calls as large as possible gives significant  performance/power benefits in many cases which I'm not too keen to  regress. In fact I still have the spark of an idea to sort the filled  pages array for optimal physical layout, I've just never had the free  time to play with it. FWIW, since iommu_map_sg() was new and promising  at the time, using sg_alloc_table_from_pages() actually *was* the  simplification over copying arch/arm's __iommu_create_mapping() logic.  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,777,1.0,0.631578947368421,0,0.6428571428571429,0.35714285714285715,0.0,0.0
961,515530,536360,"Agreed - I'd definitely ack a version of this change which didn't depend on patch #3 ,)","On 14/01/2019 09:41, Christoph Hellwig wrote:  Agreed - I'd definitely ack a version of this change which didn't depend  on patch #3 ,)  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,87,0.13291139240506328,0.6578947368421053,0,0.6428571428571429,0.35714285714285715,0.0,0.0
962,515530,536448,"A lot of architectures do really weird stuff in the dma sync routines.So my plan would be to consolidate a lot more logic in there first, and then maybe as a next step we could look into usingarch_sync_dma_for_device eventually.","On Fri, Feb 01, 2019 at 02:22:46PM +0000, Robin Murphy wrote:  A lot of architectures do really weird stuff in the dma sync routines. So my plan would be to consolidate a lot more logic in there first, and then maybe as a next step we could look into using arch_sync_dma_for_device eventually.",technical,Christoph Hellwig,hch@lst.de,1,1,228,0.25949367088607594,0.6842105263157895,0,0.6428571428571429,0.35714285714285715,0.0,0.0
963,515530,536451,As far as I can tell almost all users of this to be enabled anyway..,"On Fri, Feb 01, 2019 at 02:47:17PM +0000, Robin Murphy wrote:  As far as I can tell almost all users of linux/dma-iommu.h require CONFIG_DMA_IOMMU to be enabled anyway..",technical,Christoph Hellwig,hch@lst.de,1,1,68,0.10126582278481013,0.7105263157894737,0,0.6428571428571429,0.35714285714285715,0.0,0.0
964,515530,536452,How does it defeat the purpose of __iommu_dma_alloc_pages?,"On Fri, Feb 01, 2019 at 03:24:45PM +0000, Robin Murphy wrote:  How does it defeat the purpose of __iommu_dma_alloc_pages?",technical,Christoph Hellwig,hch@lst.de,1,1,58,0.056962025316455694,0.7368421052631579,0,0.6428571428571429,0.35714285714285715,0.0,0.10714285714285714
965,515530,539321,"If there's an actual bug fix here, can we make that before all of the other code movement? If it's at all related to other reports of weird mmap behaviour it might warrant backporting, and either way I'm finding it needlessly tough to follow what's going on in this patch :(","On 14/01/2019 09:41, Christoph Hellwig wrote:  If there's an actual bugfix here, can we make that before all of the  other code movement? If it's at all related to other reports of weird  mmap behaviour it might warrant backporting, and either way I'm finding  it needlessly tough to follow what's going on in this patch :(  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,274,0.37341772151898733,0.7631578947368421,0,0.7857142857142857,0.21428571428571427,0.10714285714285714,0.0
966,515530,540896,"Yes there is, namely assembling large buffers without the need for massive CMA areas and compaction overhead under memory fragmentation. That has always been a distinct concern from the DMA_DIRECT_REMAP cases, they've just been able to share a fair few code paths. As far as I'm concerned that splits things the wrong way. Logically, iommu_dma_alloc() should always have done its own vmap() instead of just returning the bare pages array, but that was tricky to resolve with the design of having the caller handle everything to do with coherency (forcing the caller to unpick that mapping just to remap it yet again in the noncoherent case didn't seem sensible).","On 14/01/2019 09:41, Christoph Hellwig wrote:  Yes there is, namely assembling large buffers without the need for  massive CMA areas and compaction overhead under memory fragmentation.  That has always been a distinct concern from the DMA_DIRECT_REMAP cases,  they've just been able to share a fair few code paths.   As far as I'm concerned that splits things the wrong way. Logically,  iommu_dma_alloc() should always have done its own vmap() instead of just  returning the bare pages array, but that was tricky to resolve with the  design of having the caller handle everything to do with coherency  (forcing the caller to unpick that mapping just to remap it yet again in  the noncoherent case didn't seem sensible).  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,662,0.7974683544303798,0.7894736842105263,0,0.8214285714285714,0.17857142857142858,0.0,0.0
967,515530,541114,"Other than dma-iommu.c itself, none of them *require* it - only arch/arm64 selects it (the one from MTK_IOMMU is just bogus), and a lot of the drivers also build for at least one other architecture (and/orarm64 with !IOMMU_API). Either way, I have no vehement objection to the change, I just don't see any positive value in it.","On 01/02/2019 16:13, Christoph Hellwig wrote:  Other than dma-iommu.c itself, none of them *require* it - only  arch/arm64 selects it (the one from MTK_IOMMU is just bogus), and a lot  of the drivers also build for at least one other architecture (and/or  arm64 with !IOMMU_API).  Either way, I have no vehement objection to the change, I just don't see  any positive value in it.  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,327,0.44936708860759494,0.868421052631579,0,0.8214285714285714,0.17857142857142858,0.0,0.0
968,515530,541141,"Because if iommu_map() only gets called at PAGE_SIZE granularity, then the IOMMU PTEs will be created at PAGE_SIZE (or smaller) granularity, so any effort to get higher-order allocations matching larger IOMMU block sizes is wasted, and we may as well have just done this: Really, it's a shame we have to split huge pages for the CPU remap, since in the common case the CPU MMU will have a matching block size, but IIRC there was something in vmap() or thereabouts that explicitly chokes on them.","On 01/02/2019 16:16, Christoph Hellwig wrote:  Because if iommu_map() only gets called at PAGE_SIZE granularity, then  the IOMMU PTEs will be created at PAGE_SIZE (or smaller) granularity, so  any effort to get higher-order allocations matching larger IOMMU block  sizes is wasted, and we may as well have just done this:  	for (i = 0, i < count, i++) { 		struct page *page = alloc_page(gfp), 		... 		iommu_map(..., page_to_phys(page), PAGE_SIZE, ...), 	}  Really, it's a shame we have to split huge pages for the CPU remap,  since in the common case the CPU MMU will have a matching block size,  but IIRC there was something in vmap() or thereabouts that explicitly  chokes on them.  Robin.",technical,Robin Murphy,robin.murphy@arm.com,1,0,495,0.6392405063291139,0.8947368421052632,0,0.8214285714285714,0.17857142857142858,0.0,0.17857142857142858
969,515530,546163,I've moved the idef back down below the includes.,"On Wed, Feb 06, 2019 at 03:08:26PM +0000, Robin Murphy wrote:  I've moved the idef back down below the includes.",technical,Christoph Hellwig,hch@lst.de,1,1,49,0.06962025316455696,0.9210526315789473,0,1.0,0.0,0.17857142857142858,0.0
970,515530,546172,"True.  I've dropped this patch. That just needs a volunteer to fix the implementation, as there is no fundamental reason not to remap large pages.","On Wed, Feb 06, 2019 at 03:28:28PM +0000, Robin Murphy wrote:  True.  I've dropped this patch.   That just needs a volunteer to fix the implementation, as there is no fundamental reason not to remap large pages.",technical,Christoph Hellwig,hch@lst.de,1,1,146,0.189873417721519,0.9473684210526315,0,1.0,0.0,0.0,0.0
971,515530,546190,The bug fix is to handle non-vmalloc pages.  I'll see if I can do a smaller and more bandaid-y fix first.,"On Tue, Feb 05, 2019 at 03:02:23PM +0000, Robin Murphy wrote:  The bug fix is to handle non-vmalloc pages.  I'll see if I can do a smaller and more bandaid-y fix first.",technical,Christoph Hellwig,hch@lst.de,1,1,105,0.1518987341772152,0.9736842105263158,0,1.0,0.0,0.0,0.0
972,515530,546288,"Well, I guess I need to reword this - there is no _requirement_ to remap.  And x86 has been happy to not remap so far and I see absolutely no reason to force anyone to remap. I don't parse this.  In the old code base before this series iommu_dma_alloc is a relatively low-level helper allocating and mapping pages.  And that one should have done the remapping, and in fact does so since (""dma-iommu: refactor page array remap helpers"").  It just happens that the function is now called iommu_dma_alloc_remap.The new iommu_dma_alloc is the high level entry point that handles every possible case of different allocations, including those where we do not have a virtual mapping.","On Wed, Feb 06, 2019 at 11:55:49AM +0000, Robin Murphy wrote:  Well, I guess I need to reword this - there is no _requirement_ to remap.  And x86 has been happy to not remap so far and I see absolutely no reason to force anyone to remap.   I don't parse this.  In the old code base before this series iommu_dma_alloc is a relatively low-level helper allocating and mapping pages.  And that one should have done the remapping, and in fact does so since (dma-iommu: refactor page array remap helpers"").  It just happens that the function is now called iommu_dma_alloc_remap.  The new iommu_dma_alloc is the high level entry point that handles every possible case of different allocations, including those where we do not have a virtual mapping.""",technical,Christoph Hellwig,hch@lst.de,1,1,676,0.8164556962025317,1.0,1,1.0,0.0,0.0,0.0
973,519668,520119,"Should the timeout be set depending on the max transfer size? 10s seems an age if the max transfer size is 4KB. In other words, we should this only be applied for T194+? Furthermore, in tegra_i2c_xfer_msg() we know the len of the message and so maybe it would be better to dynamically set the timeout depending on length?","On 17/01/2019 20:39, Sowjanya Komatineni wrote:  Should the timeout be set depending on the max transfer size? 10s seems an age if the max transfer size is 4KB. In other words, we should this only be applied for T194+?  Furthermore, in tegra_i2c_xfer_msg() we know the len of the message and so maybe it would be better to dynamically set the timeout depending on length?  Cheers Jon  --  nvpublic",technical,Jon Hunter,jonathanh@nvidia.com,1,0,321,0.3127962085308057,0.4,0,0.0,1.0,0.0,0.0
974,519668,520680,"Yes, that's the ideal way to compute timeout based on msg len and bus rate. To do this I had to update TEGRA_I2C_TIMEOUT macro to take arg and there are 3 different patches for tegra i2c under review and all of those will effect as the patch changes use TEGRA_I2C_TIMEOUT.So, Should I hold on to this change for now till those patches are merged?","Yes, that’s the ideal way to compute timeout based on msg len and bus rate.   To do this I had to update TEGRA_I2C_TIMEOUT macro to take arg and there are 3 different patches for tegra i2c under review and all of those will effect as the patch changes use TEGRA_I2C_TIMEOUT.     So, Should I hold on to this change for now till those patches are merged?    Thanks  Sowjanya",technical,Sowjanya Komatineni,skomatineni@nvidia.com,1,1,346,0.32701421800947866,0.6,0,0.0,0.6666666666666666,0.0,0.0
975,519668,521774,"If you have a number of patches with interdependencies, it's best to send them out as a whole series. So you'd typically apply them in order to a single branch, then use:  git format-patch first^..last where first is the SHA1 of the first commit you want to send, and last is the  SHA1 of the last patch. The carret (^) means the parent commit of the specified one and is needed because git format-patch doesn't include the start of the sequence. If the commits are at the top of your branch you can use something like this:  git format-patch -3 which will generate a series for the last three patches in the branch(more specifically starting from HEAD).If you send them as a series, it's immediately obvious in what order they should be applied and generally makes it easier for people to review and test.I think in this case you can probably just have the other two patches first in the series, then apply the timeout patch on top. That way you can resolve the conflicts between patches 1 and 2, and patch 3 before sending out.","On Fri, Jan 18, 2019 at 05:21:28PM +0000, Sowjanya Komatineni wrote:  If you have a number of patches with interdependencies, it's best to send them out as a whole series. So you'd typically apply them in order to a single branch, then use:  	$ git format-patch first^..last  where first is the SHA1 of the first commit you want to send, and last is the the SHA1 of the last patch. The carret (^) means the parent commit of the specified one and is needed because git format-patch doesn't include the start of the sequence.  If the commits are at the top of your branch you can use something like this:  	$ git format-patch -3  which will generate a series for the last three patches in the branch (more specifically starting from HEAD).  If you send them as a series, it's immediately obvious in what order they should be applied and generally makes it easier for people to review and test.  I think in this case you can probably just have the other two patches first in the series, then apply the timeout patch on top. That way you can resolve the conflicts between patches 1 and 2, and patch 3 before sending out.  Thierry",technical,Thierry Reding,thierry.reding@gmail.com,1,0,1029,1.0,1.0,1,1.0,0.0,0.6666666666666666,0.0
976,521268,521541,"This has the effect to ensure that if USER_ACCESS is a module then sois cxgb4, otherwise USER_ACCESS can be enabled or disabled Jason","On Sun, Jan 20, 2019 at 02:27:13AM +0100, Nicholas Mc Guire wrote:   This has the effect to ensure that if USER_ACCESS is a module then so is cxgb4, otherwise USER_ACCESS can be enabled or disabled  Jason",technical,Jason Gunthorpe,jgg@ziepe.ca,1,0,133,0.3116883116883117,0.25,0,0.25,0.5,0.25,0.5
977,521268,525701,"It seems weird to have NOFAIL and then have an error unwind path, what is the deal here?","On Sun, Jan 20, 2019 at 02:27:13AM +0100, Nicholas Mc Guire wrote:  Steve? It seems weird to have NOFAIL and then have an error unwind path, what is the deal here?",technical,Jason Gunthorpe,jgg@ziepe.ca,1,0,88,0.2597402597402597,0.375,0,0.75,0.0,0.5,0.0
978,521268,525724,"The other queue allocations in qp.c don't use __GFP_NOFAIL.  So either leave it and remove the error check as per this patch, or remove the NOFAIL and leave the check. I suggest you remove the __GFP_NOFAIL.","On 1/23/2019 12:30 PM, Jason Gunthorpe wrote:  The other queue allocations in qp.c don't use __GFP_NOFAIL.  So either leave it and remove the error check as per this patch, or remove the NOFAIL and leave the check.  I suggest you remove the __GFP_NOFAIL.   Steve.",technical,Steve Wise,swise@chelsio.com,0,0,206,0.5324675324675324,0.5,0,0.75,0.0,0.0,0.0
979,521268,525727,"The other queue allocations in qp.c don't use __GFP_NOFAIL.  So either leave it and remove the error check as per this patch, or remove the NOFAIL and leave the check. I suggest you remove the __GFP_NOFAIL, since I have a recollection that using it was frowned upon.  In this case, if there is no memory available it is reasonable to return that error to the user creating the srq...","On 1/23/2019 12:30 PM, Jason Gunthorpe wrote:  The other queue allocations in qp.c don't use __GFP_NOFAIL.  So either leave it and remove the error check as per this patch, or remove the NOFAIL and leave the check.  I suggest you remove the __GFP_NOFAIL, since I have a recollection that using it was frowned upon.  In this case, if there is no memory available it is reasonable to return that error to the user creating the srq...   Steve.",technical,Steve Wise,swise@opengridcomputing.com,0,0,383,1.0,0.625,0,0.75,0.0,0.0,0.0
980,521268,525837,As per steve's remarks I revised this to the below and applied it to for-next,"On Sun, Jan 20, 2019 at 02:27:13AM +0100, Nicholas Mc Guire wrote:  As per steve's remarkes I revised this to the below and applied it to for-next",technical,Jason Gunthorpe,jgg@ziepe.ca,1,0,77,0.2077922077922078,0.75,0,0.75,0.0,0.0,0.0
981,521268,525839,Thanks Jason!,"On 1/23/2019 3:44 PM, Jason Gunthorpe wrote:  Thanks Jason!",technical,Steve Wise,swise@opengridcomputing.com,0,0,13,0.03896103896103896,0.875,0,0.75,0.0,0.0,0.0
982,521268,525971,thanks for taking care of this - I simply did not have enough context to decide if there would be some special reason for this allocation to need __GFP_NOFAIL - keeping its use to a minimum though is the best solution. thx!,"On Wed, Jan 23, 2019 at 12:45:11PM -0600, Steve Wise wrote: thanks for taking care of this - I simply did not have enough context to decide if there would be some special reason for this allocation to need __GFP_NOFAIL - keeping its use to a minimum though is the best solution.  thx! hofrat",technical,Nicholas Mc Guire,der.herr@hofr.at,0,0,223,0.5714285714285714,1.0,1,1.0,0.0,0.0,0.0
983,521470,533830,"Just to clarify to the new Cc'ed list, I'm waiting on one of the Chromium guys to review before I put my mucky paws over it.","On Mon, 21 Jan 2019, Pi-Hsun Shih wrote:   Just to clarify to the new Cc'ed list, I'm waiting on one of the Chromium guys to review before I put my mucky paws over it.  --  Lee Jones [李琼斯] Linaro Services Technical Lead Linaro.org │ Open source software for ARM SoCs Follow Linaro: Facebook | Twitter | Blog",technical,Lee Jones,lee.jones@linaro.org,1,0,124,0.6744186046511628,0.5,0,1.0,0.0,1.0,0.0
984,521470,534676,"I don't think we need this to be merged ASAP.I feel that most of the todos are done though, so I'll drop the RFCtag and resend a v4 (which also contains some bug fixes found when testing).","Hi Enric,  On Wed, Jan 30, 2019 at 11:06 PM Enric Balletbo Serra <eballetbo@gmail.com> wrote:  I don't think we need this to be merged ASAP.  I feel that most of the todos are done though, so I'll drop the RFC tag and resend a v4 (which also contains some bug fixes found when testing).",technical,Pi-Hsun Shih,pihsun@chromium.org,0,1,188,1.0,1.0,1,1.0,0.0,0.0,0.0
985,522705,522787,"Boqun had previously pointed this out, you need to WRITE_ONCE() node->head too.","On Tue, 31 Oct 2017, Waiman Long wrote:   Boqun had previously pointed this out, you need to WRITE_ONCE() node->head too.  Thanks, Davidlohr",technical,Davidlohr Bueso,dave@stgolabs.net,1,0,79,0.12162162162162163,0.28125,0,0.0,1.0,0.0,0.0
986,522705,531212,The patch looks good to me. You can add: Reviewed-by: Jan Kara,"On Tue 31-10-17 14:50:58, Waiman Long wrote:  The patch looks good to me. You can add:  Reviewed-by: Jan Kara <jack@suse.cz>  								Honza  --  Jan Kara <jack@suse.com> SUSE Labs, CR",technical,Jan Kara,jack@suse.cz,1,0,62,0.10135135135135136,0.3125,0,0.0,0.9914529914529915,0.0,0.0
987,522705,531213,"Hum, do we have any users for this API? And wouldn't they also need to control how many lists are allocated then?","On Tue 31-10-17 14:50:59, Waiman Long wrote:  Hum, do we have any users for this API? And wouldn't they also need to control how many lists are allocated then?  								Honza  --  Jan Kara <jack@suse.com> SUSE Labs, CR",technical,Jan Kara,jack@suse.cz,1,0,113,0.17567567567567569,0.34375,0,0.0,0.9914529914529915,0.0,0.0
988,522705,531391,"This patch is supposed to be used by the epoll patch from Davidlohr. Ashe has retracted the patch, I can drop this patch also. The number of lists scale with the number of CPU cores in the system whether it is used one way or the others.","On 11/01/2017 04:40 AM, Jan Kara wrote:  This patch is supposed to be used by the epoll patch from Davidlohr. As he has retracted the patch, I can drop this patch also. The number of lists scale with the number of CPU cores in the system whether it is used one way or the others.  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,237,0.34459459459459457,0.375,0,0.0,0.9914529914529915,0.0,0.0
989,522705,531665,"Right, I will get that into the next version of the patch.","On 10/31/2017 05:37 PM, Davidlohr Bueso wrote:  Right, I will get that into the next version of the patch.  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,58,0.0945945945945946,0.40625,0,0.0,0.9914529914529915,0.0,0.0
990,522705,548954,"I vote for doing this in the original version. How about the following? Instead of the current O(N) implementation, at the cost of adding an atomic counter. We also need to add a heads pointer to the node structure such that we can unaccount a thread doing list_del(). The lock has somehow changed. Retry again if it is--2.13.6","On Tue, 31 Oct 2017, Waiman Long wrote:   I vote for doing this in the original version. How about the following?   ----------8<----------------------------------------------- From: Davidlohr Bueso <dave@stgolabs.net> Subject: [PATCH] lib/dlock-list: Scale dlock_lists_empty()  Instead of the current O(N) implementation, at the cost of adding an atomic counter. We also need to add a heads pointer to the node structure such that we can unaccount a thread doing list_del().  Signed-off-by: Davidlohr Bueso <dbueso@suse.de> ---  include/linux/dlock-list.h |  2 ++  lib/dlock-list.c           | 40 ++++++++++++++++++++++++++++------------  2 files changed, 30 insertions(+), 12 deletions(-)  diff --git a/include/linux/dlock-list.h b/include/linux/dlock-list.h index c00c7f92ada4..dd73d5787885 100644 --- a/include/linux/dlock-list.h +++ b/include/linux/dlock-list.h @@ -36,6 +36,7 @@ struct dlock_list_head {    struct dlock_list_heads {  	struct dlock_list_head *heads, +	atomic_t waiters,  },    /* @@ -44,6 +45,7 @@ struct dlock_list_heads {  struct dlock_list_node {  	struct list_head list,  	struct dlock_list_head *head, +	struct dlock_list_heads *heads,  },    /* diff --git a/lib/dlock-list.c b/lib/dlock-list.c index a4ddecc01b12..bd11fc0da254 100644 --- a/lib/dlock-list.c +++ b/lib/dlock-list.c @@ -124,6 +124,8 @@ int __alloc_dlock_list_heads(struct dlock_list_heads *dlist,  		head->lock = __SPIN_LOCK_UNLOCKED(&head->lock),  		lockdep_set_class(&head->lock, key),  	} + +	atomic_set(&dlist->waiters, 0),  	return 0,  }  EXPORT_SYMBOL(__alloc_dlock_list_heads), @@ -139,29 +141,23 @@ void free_dlock_list_heads(struct dlock_list_heads *dlist)  {  	kfree(dlist->heads),  	dlist->heads = NULL, +	atomic_set(&dlist->waiters, 0),  }  EXPORT_SYMBOL(free_dlock_list_heads),    /**   * dlock_lists_empty - Check if all the dlock lists are empty   * @dlist: Pointer to the dlock_list_heads structure - * Return: true if list is empty, false otherwise.   * - * This can be a pretty expensive function call. If this function is required - * in a performance critical path, we may have to maintain a global count - * of the list entries in the global dlock_list_heads structure instead. + * Return: true if all dlock lists are empty, false otherwise.   */  bool dlock_lists_empty(struct dlock_list_heads *dlist)  { -	int idx, -  	/* Shouldn't be called before nr_dlock_lists is initialized */  	WARN_ON_ONCE(!nr_dlock_lists),   -	for (idx = 0, idx < nr_dlock_lists, idx++) -		if (!list_empty(&dlist->heads[idx].list)) -			return false, -	return true, +	smp_mb__before_atomic(), +	return !atomic_read(&dlist->waiters),  }  EXPORT_SYMBOL(dlock_lists_empty),   @@ -179,10 +175,30 @@ void dlock_lists_add(struct dlock_list_node *node,  	struct dlock_list_head *head = &dlist->heads[this_cpu_read(cpu2idx)],    	/* +	 * Serialize dlist->waiters such that a 0->1 transition is not missed, +	 * by another thread checking if any of the dlock lists are used. +	 * +	 * CPU0				    CPU1 +	 * dlock_list_add()                 dlock_lists_empty() +	 *   [S] atomic_inc(waiters), +	 *	 smp_mb__after_atomic(), +	 *				      smp_mb__before_atomic(), +	 *				      [L] atomic_read(waiters) +	 *       list_add() +	 * +	 * Bump the waiters counter _before_ taking the head->lock such that we +	 * don't miss a thread adding itself to a list while spinning for the +	 * lock. +	 */ +	atomic_inc(&dlist->waiters), +	smp_mb__after_atomic(), + +	/*  	 * There is no need to disable preemption  	 */  	spin_lock(&head->lock),  	node->head = head, +	node->heads = dlist,  	list_add(&node->list, &head->list),  	spin_unlock(&head->lock),  } @@ -199,8 +215,7 @@ EXPORT_SYMBOL(dlock_lists_add),   * a bug.   */  void dlock_lists_del(struct dlock_list_node *node) -{ -	struct dlock_list_head *head, +{	struct dlock_list_head *head,  	bool retry,    	do { @@ -214,6 +229,7 @@ void dlock_lists_del(struct dlock_list_node *node)  			list_del_init(&node->list),  			node->head = NULL,  			retry = false, +			atomic_dec(&node->heads->waiters),  		} else {  			/*  			 * The lock has somehow changed. Retry again if it is --  2.13.6",technical,Davidlohr Bueso,dave@stgolabs.net,1,0,327,0.4797297297297297,0.4375,0,0.008547008547008548,0.9829059829059829,0.0,0.0
991,522705,548985,"The counter will then become the single contention point for all concurrent updates to the dlock-list. So it will have a big impact on performance. On the other hand, instead of being a counter of # of items, we can make that a counter of # of non-empty lists. So its value will only be changed when a list go from empty to non-empty and vice versa. That will greatly reduce the number of updates to that counter. I don't want to add a new data item into dlock_list_node as there can be thousands or even of them in the system. Instead, I prefer increasing the size of dlock_list_head which only have a limited number of them and they have unused space because they are cache line aligned.","On 11/02/2017 01:04 PM, Davidlohr Bueso wrote:  The counter will then become the single contention point for all concurrent updates to the dlock-list. So it will have a big impact on performance. On the other hand, instead of being a counter of # of items, we can make that a counter of # of non-empty lists. So its value will only be changed when a list go from empty to non-empty and vice versa. That will greatly reduce the number of updates to that counter.    I don't want to add a new data item into dlock_list_node as there can be thousands or even of them in the system. Instead, I prefer increasing the size of dlock_list_head which only have a limited number of them and they have unused space because they are cacheline aligned.  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,689,0.9391891891891891,0.46875,0,0.008547008547008548,0.9829059829059829,0.0,0.0
992,522705,549576,"Both are good points. Thanks. Instead of the current O(N) implementation, at the cost of adding an atomic counter, we can convert the call to an atomic_read(). The counter only serves for accounting empty to non-empty transitions, and vice versa, therefore only modified twice for each of the lists, during the lifetime of the dlock -- thus 2*nr_dlock_lists. In addition, to be able to unaccount a list_del(), we add a dlist pointer to each head, thus minimizing the overall memory footprint.","On Thu, 02 Nov 2017, Waiman Long wrote:   Both are good points. Thanks.  ----8<-------------------------------------------------------- Subject: [PATCH] [PATCH v2] lib/dlock-list: Scale dlock_lists_empty()  Instead of the current O(N) implementation, at the cost of adding an atomic counter, we can convert the call to an atomic_read(). The counter only serves for accounting empty to non-empty transitions, and vice versa, therefore only modified twice for each of the lists, during the lifetime of the dlock -- thus 2*nr_dlock_lists.  In addition, to be able to unaccount a list_del(), we add a dlist pointer to each head, thus minimizing the overall memory footprint.  Signed-off-by: Davidlohr Bueso <dbueso@suse.de> ---  include/linux/dlock-list.h |  2 ++  lib/dlock-list.c           | 56 +++++++++++++++++++++++++++++++++++-----------  2 files changed, 45 insertions(+), 13 deletions(-)  diff --git a/include/linux/dlock-list.h b/include/linux/dlock-list.h index c00c7f92ada4..d176a2d00cd1 100644 --- a/include/linux/dlock-list.h +++ b/include/linux/dlock-list.h @@ -32,10 +32,12 @@  struct dlock_list_head {  	struct list_head list,  	spinlock_t lock, +	struct dlock_list_heads *dlist,  } ____cacheline_aligned_in_smp,    struct dlock_list_heads {  	struct dlock_list_head *heads, +	atomic_t waiters,  },    /* diff --git a/lib/dlock-list.c b/lib/dlock-list.c index a4ddecc01b12..a84f42e800d5 100644 --- a/lib/dlock-list.c +++ b/lib/dlock-list.c @@ -122,8 +122,11 @@ int __alloc_dlock_list_heads(struct dlock_list_heads *dlist,    		INIT_LIST_HEAD(&head->list),  		head->lock = __SPIN_LOCK_UNLOCKED(&head->lock), +		head->dlist = dlist,  		lockdep_set_class(&head->lock, key),  	} + +	atomic_set(&dlist->waiters, 0),  	return 0,  }  EXPORT_SYMBOL(__alloc_dlock_list_heads), @@ -138,30 +141,36 @@ EXPORT_SYMBOL(__alloc_dlock_list_heads),  void free_dlock_list_heads(struct dlock_list_heads *dlist)  {  	kfree(dlist->heads), -	dlist->heads = NULL, +	atomic_set(&dlist->waiters, 0),  }  EXPORT_SYMBOL(free_dlock_list_heads),    /**   * dlock_lists_empty - Check if all the dlock lists are empty   * @dlist: Pointer to the dlock_list_heads structure - * Return: true if list is empty, false otherwise.   * - * This can be a pretty expensive function call. If this function is required - * in a performance critical path, we may have to maintain a global count - * of the list entries in the global dlock_list_heads structure instead. + * Return: true if all dlock lists are empty, false otherwise.   */  bool dlock_lists_empty(struct dlock_list_heads *dlist)  { -	int idx, -  	/* Shouldn't be called before nr_dlock_lists is initialized */  	WARN_ON_ONCE(!nr_dlock_lists),   -	for (idx = 0, idx < nr_dlock_lists, idx++) -		if (!list_empty(&dlist->heads[idx].list)) -			return false, -	return true, +	/* +	 * Serialize dlist->waiters such that a 0->1 transition is not missed +	 * by another thread checking if any of the dlock lists are used. +	 * +	 * CPU0				    CPU1 +	 * dlock_list_add()                 dlock_lists_empty() +	 *   [S] atomic_inc(waiters), +	 *       smp_mb__after_atomic(), +	 *					  smp_mb__before_atomic(), +	 *				      [L] atomic_read(waiters) +	 *       list_add() +	 * +	 */ +	smp_mb__before_atomic(), +	return !atomic_read(&dlist->waiters),  }  EXPORT_SYMBOL(dlock_lists_empty),   @@ -179,6 +188,16 @@ void dlock_lists_add(struct dlock_list_node *node,  	struct dlock_list_head *head = &dlist->heads[this_cpu_read(cpu2idx)],    	/* +	 * Bump the waiters counter _before_ taking the head->lock +	 * such that we don't miss a thread adding itself to a list +	 * while spinning for the lock. +	 */ +	if (list_empty_careful(&head->list)) { +		atomic_inc(&dlist->waiters), +		smp_mb__after_atomic(), +	} + +	/*  	 * There is no need to disable preemption  	 */  	spin_lock(&head->lock), @@ -199,8 +218,7 @@ EXPORT_SYMBOL(dlock_lists_add),   * a bug.   */  void dlock_lists_del(struct dlock_list_node *node) -{ -	struct dlock_list_head *head, +{	struct dlock_list_head *head,  	bool retry,    	do { @@ -212,6 +230,18 @@ void dlock_lists_del(struct dlock_list_node *node)  		spin_lock(&head->lock),  		if (likely(head == node->head)) {  			list_del_init(&node->list), +			/* +			 * We still hold the head->lock, a normal list_empty() +			 * check will do. +			 */ +			if (list_empty(&head->list)) { +				struct dlock_list_heads *dlist, +				dlist = node->head->dlist, + +				atomic_dec(&dlist->waiters), +				smp_mb__after_atomic(), +			} +  			node->head = NULL,  			retry = false,  		} else { --  2.13.6",technical,Davidlohr Bueso,dave@stgolabs.net,1,0,492,0.6959459459459459,0.5,0,0.017094017094017096,0.9743589743589743,0.0,0.0
993,522705,557923,"We are tracking number of non-empty lists here. So I think we need a better name and maybe some documentation of what it is.That is racy. You will need to remember that you have opportunistically incremented the count. After acquiring the spinlock, the code will have to decrement it if the list is no longer empty. You will also have to increment it after lock if the list is empty now, but not previously. Do you need that while the code will do a spin_unlock soon?","On 11/03/2017 10:22 AM, Davidlohr Bueso wrote:  We are tracking number of non-empty lists here. So I think we need a better name and maybe some documentation of what it is.   That is racy. You will need to remember that you have opportunistically incremented the count. After acquiring the spinlock, the code will have to decrement it if the list is no longer empty. You will also have to increment it after lock if the list is empty now, but not previously.   Do you need that while the code will do a spin_unlock soon?  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,467,0.6351351351351351,0.5625,0,0.017094017094017096,0.9743589743589743,0.0,0.02564102564102564
994,522705,575707,The patch looks good to me. You can add: Reviewed-by: Jan Kara,"On 11/06/2017 01:47 PM, Davidlohr Bueso wrote:   This patch looks good to me.  Acked-by: Waiman Long <longman@redhat.com>",technical,Waiman Long,longman@redhat.com,1,1,62,0.10135135135135136,0.625,0,0.05128205128205128,0.9487179487179487,0.0,0.0
995,522705,585725,The patch looks good to me. You can add: Reviewed-by: Jan Kara,"On Mon 06-11-17 10:47:08, Davidlohr Bueso wrote:  Looks good to me. You can add:  Reviewed-by: Jan Kara <jack@suse.cz>  								Honza Kara  --  Jan Kara <jack@suse.com> SUSE Labs, CR",technical,Jan Kara,jack@suse.cz,1,0,62,0.10135135135135136,0.65625,0,0.05128205128205128,0.9401709401709402,0.0,0.0
996,522705,594084,"Just a general kernel programming question here - I thought the whole point of atomics is that they are, well, atomic across all CPUs so there is no need for a memory barrier?  If there is a need for a memory barrier for each atomic access (assuming it isn't accessed under another lock, which would make the use of atomic types pointless, IMHO) then I'd think there is a lot of code in the kernel that isn't doing this properly. What am I missing here? I don't see how this helps if the operations are executed like: then the same problem would exist, unless those functions/macros are somehow bound to the atomic operations themselves?  In that case, what makes the use in other parts of the code safe without them?","On Nov 7, 2017, at 4:59 AM, Jan Kara <jack@suse.cz> wrote:  Just a general kernel programming question here - I thought the whole point of atomics is that they are, well, atomic across all CPUs so there is no need for a memory barrier?  If there is a need for a memory barrier for each atomic access (assuming it isn't accessed under another lock, which would make the use of atomic types pointless, IMHO) then I'd think there is a lot of code in the kernel that isn't doing this properly.  What am I missing here?  I don't see how this helps if the operations are executed like:  	 * CPU0				    CPU1 	 * dlock_list_add()                 dlock_lists_empty() 	 *   [S] atomic_inc(used_lists), 	 *					  smp_mb__before_atomic(), 	 *       smp_mb__after_atomic(), 	 *				      [L] atomic_read(used_lists)  or alternately like:  	 * CPU0				    CPU1 	 * dlock_list_add()                 dlock_lists_empty() 	 *					  smp_mb__before_atomic(), 	 *   [S] atomic_inc(used_lists), 	 *       smp_mb__after_atomic(), 	 *				      [L] atomic_read(used_lists)  then the same problem would exist, unless those functions/macros are somehow bound to the atomic operations themselves?  In that case, what makes the use of atomic_{inc,dec,read}() in other parts of the code safe without them?  Cheers, Andreas",technical,Andreas Dilger,adilger@dilger.ca,1,0,717,1.0,0.6875,0,0.05128205128205128,0.9401709401709402,0.0,0.0
997,522705,594127,"Atomic update and memory barrier are 2 different things. Atomic update means other CPUs see either the value before or after the update. They won't see anything in between. For a counter, it means we won't miss any counts. However, not all atomic operations give an ordering guarantee. The atomic_read() and atomic_inc() are examples that do not provide memory ordering guarantee. See Documentation/memory-barriers.txt for more information about it.A CPU can perform atomic operations 1 & 2 in program order, but other CPUs may see operation 2 first before operation 1. Here memory barrier can be used to guarantee that other CPUs see the memory updates in certain order. Hope this help.","On 11/07/2017 12:59 PM, Andreas Dilger wrote:  Atomic update and memory barrier are 2 different things. Atomic update means other CPUs see either the value before or after the update. They won't see anything in between. For a counter, it means we won't miss any counts. However, not all atomic operations give an ordering guarantee. The atomic_read() and atomic_inc() are examples that do not provide memory ordering guarantee. See Documentation/memory-barriers.txt for more information about it.  A CPU can perform atomic operations 1 & 2 in program order, but other CPUs may see operation 2 first before operation 1. Here memory barrier can be used to guarantee that other CPUs see the memory updates in certain order.  Hope this help. Longman",technical,Waiman Long,longman@redhat.com,1,1,687,0.8716216216216216,0.71875,0,0.05982905982905983,0.9401709401709402,0.0,0.0
998,522705,594150,"There's an omission here which I think Andreas may have been referring to:   atomic_inc/dec operations *are* strongly ordered with respect to each other, so if two CPUs are simultaneously executing atomic_inc, the order in which they execute isn't guaranteed, but it is guaranteed that the losing atomic_inc will not begin until the winning one is completed, so after both are done the value will have +2.   So although atomic_read and atomic_inc have no ordering guarantee at all (the point of the barrier above), if you're looking at the return values of atomic_inc/dec you don't need a barrier because regardless of which order the CPUs go in, they'll see distinct values (we use this for reference counting).","On Tue, 2017-11-07 at 13:57 -0500, Waiman Long wrote:  There's an omission here which I think Andreas may have been referring to:  atomic_inc/dec operations *are* strongly ordered with respect to each other, so if two CPUs are simultaneously executing atomic_inc, the order in which they execute isn't guaranteed, but it is guaranteed that the losing atomic_inc will not begin until the winning one is completed, so after both are done the value will have +2.  So although atomic_read and atomic_inc have no ordering guarantee at all (the point of the barrier above), if you're looking at the return values of atomic_inc/dec you don't need a barrier because regardless of which order the CPUs go in, they'll see distinct values (we use this for reference counting).  James",technical,James Bottomley,James.Bottomley@HansenPartnership.com,1,0,712,0.918918918918919,0.75,0,0.05982905982905983,0.9401709401709402,0.0,0.0
999,522705,594344,"Or worse:  in which case dlock_lists_empty() misses a increment of used_lists.That said, this may be fine for the usage of dlock_list. But I think the comments are confusing or wrong. The reason is you can not use barriers to order two memory ops on different CPUs, and usually we add comments like:this. could you try to improve the comments by finding both memory ops preceding and following the barriers? Maybe you will find those barriers are not necessary in the end.","On Tue, Nov 07, 2017 at 10:59:29AM -0700, Andreas Dilger wrote:  Or worse:   	 * CPU0				    CPU1  	 * dlock_list_add()                 dlock_lists_empty()  	 *					  smp_mb__before_atomic(),  	 *				      [L] atomic_read(used_lists)  	 *   [S] atomic_inc(used_lists),  	 *       smp_mb__after_atomic(),  , in which case dlock_lists_empty() misses a increment of used_lists.  That said, this may be fine for the usage of dlock_list. But I think the comments are confusing or wrong. The reason is you can not use barriers to order two memory ops on different CPUs, and usually we add comments like:  	[S] ...			[S] ... 	<barrier>		<barrier> 	[L] ...			[L] ...  Davidlohr, could you try to improve the comments by finding both memory ops preceding and following the barriers? Maybe you will find those barriers are not necessary in the end.  Regards, Boqun",technical,Boqun Feng,boqun.feng@gmail.com,1,0,472,0.6283783783783784,0.78125,0,0.05982905982905983,0.9401709401709402,0.0,0.008547008547008548
1000,522705,612410,"So I think that case is OK as CPU1 legitimately reads a 0, the problem would be if we miss the inc it because we are doing spin_lock().That is true.Ok so for the dlock_list_add() the first barrier is so that the atomic_inc()is not done inside the critical region, after the head->lock is taken. The other barriers that follow I put such that the respective atomic opis done before the list_add(), however thinking about it I don't think they are really needed. Regarding dlock_list_empty(), the smp_mb__before_atomic() is mainly for pairing reasons, but at this point I don't see a respective counterpart for the above diagram so I'm unclear.","On Wed, 08 Nov 2017, Boqun Feng wrote:  So I think that case is OK as CPU1 legitimately reads a 0, the problem would be if we miss the inc it because we are doing spin_lock().   That is true.   Ok so for the dlock_list_add() the first barrier is so that the atomic_inc() is not done inside the critical region, after the head->lock is taken. The other barriers that follow I put such that the respective atomic op is done before the list_add(), however thinking about it I don't think they are really needed.  Regarding dlock_list_empty(), the smp_mb__before_atomic() is mainly for pairing reasons, but at this point I don't see a respective counterpart for the above diagram so I'm unclear.  Thanks, Davidlohr",technical,Davidlohr Bueso,dave@stgolabs.net,1,0,642,0.9054054054054054,0.8125,0,0.06837606837606838,0.9230769230769231,0.008547008547008548,0.0
1001,522705,612455,Note that this is broken is not valid onatomic_read().,"On Thu, Nov 09, 2017 at 09:24:08AM -0800, Davidlohr Bueso wrote:  Note that this is broken, smp_mb__before_atomic() is not valid on atomic_read().",technical,Peter Zijlstra,peterz@infradead.org,1,0,54,0.08108108108108109,0.84375,0,0.06837606837606838,0.9230769230769231,0.0,0.1623931623931624
1002,522705,782103,Are you planning on sending a v9 with the discussed changes? Drop last two patches- Fix tearing (WRITE/READ_ONCE())- Reduce barrier usage for dlock_lists_empty() -- which I'll be sending  you shortly.,"Are you planning on sending a v9 with the discussed changes? afaict:  - Drop last two patches - Fix tearing (WRITE/READ_ONCE()) - Reduce barrier usage for dlock_lists_empty() -- which I'll be sending   you shortly.  Thanks, Davidlohr  On Tue, 31 Oct 2017, Waiman Long wrote:",technical,Davidlohr Bueso,dave@stgolabs.net,1,0,200,0.2702702702702703,0.875,0,0.23931623931623933,0.7521367521367521,0.1623931623931624,0.0
1003,522705,782104,"Yes, I am planning to do so when I have some free time as I am working on a high-priority task at the moment.","On 11/29/2017 10:26 AM, Davidlohr Bueso wrote:  Yes, I am planning to do so when I have some free time as I am working on a high-priority task at the moment.  Regards, Longman",technical,Waiman Long,longman@redhat.com,1,1,109,0.17567567567567569,0.9375,0,0.23931623931623933,0.7521367521367521,0.0,0.7521367521367521
1004,522705,205880,"What's happened to this patchset? Any plans to repost a more recent version?FYI, I just ran a workload that hit 60% CPU usage on sb inode listlock contention - a multithreaded bulkstat scan of an XFS filesystem with millions of inodes on SSDs. last time I ran this (about 18months ago now!) I saw rates of about 600,000 inodes/s being scanned from userspace. The run I did earlier today made 300,000 inodes/s on the same 16p machine and was completely CPU bound...","Hi Waiman,  What's happened to this patchset? Any plans to repost a more recent version?  FYI, I just ran a workload that hit 60% CPU usage on sb inode list lock contention - a multithreaded bulkstat scan of an XFS filesystem with millions of inodes on SSDs. last time I ran this (about 18 months ago now!) I saw rates of about 600,000 inodes/s being scanned from userspace. The run I did earlier today made 300,000 inodes/s on the same 16p machine and was completely CPU bound....  Cheers,  Dave.  On Tue, Oct 31, 2017 at 02:50:54PM -0400, Waiman Long wrote:  --  Dave Chinner david@fromorbit.com",technical,Dave Chinner,david@fromorbit.com,0,0,464,0.6351351351351351,0.96875,0,1.0,0.0,0.7521367521367521,0.0
1005,522705,205910,"I was planning to repost the patchset with the latest change last November and then Meltdown/Spectre happened. I was drafted into backporting fixes to RHEL6. Hopefully, I can finish up the work in early March and work on my upstream patches again.","On 02/25/2018 09:47 PM, Dave Chinner wrote:  I was planning to repost the patchset with the latest change last November and then Meltdown/Spectre happened. I was drafted into backporting fixes to RHEL6. Hopefully, I can finish up the work in early March and work on my upstream patches again.  Cheers, Longman",technical,Waiman Long,longman@redhat.com,1,1,247,0.3108108108108108,1.0,1,1.0,0.0,0.0,0.0
1006,524110,533387,"fixes -> fixes Note sure, maybe you didn't mean to add 'one' here ? Why not just say that that firmware expect values in Q16 ? Looking toward testing it, but I had the bad luck of using an USB storage rootfs, and apparently USB no longer works on 5.0rc+, if you have a baseline tree to suggest, I'll take it. Thanks for this patch.","Le mardi 22 janvier 2019 à 12:53 +0200, Stanimir Varbanov a écrit :  ixes -> fixes   Note sure, maybe you didn't mean to add 'one' here ? Why not just say that that firmware expect values in Q16 ?   Looking toward testing it, but I had the bad luck of using an USB storage rootfs, and apparently USB no longer works on 5.0rc+, if you have a baseline tree to suggest, I'll take it. Thanks for this patch.",technical,Nicolas Dufresne,nicolas@ndufresne.ca,0,0,331,1.0,0.6666666666666666,0,1.0,0.0,1.0,0.0
1007,524110,533688,"yes, thanks for the suggestion. Try  release branch at [1].","Hi Nicolas,  On 1/30/19 5:28 AM, Nicolas Dufresne wrote:  yes, thanks for the suggestion.   try qcomlt-4.14 release branch at [1].  --  regards, Stan  [1] https://git.linaro.org/landing-teams/working/qualcomm/kernel.git/log/?h=release/qcomlt-4.14",technical,Stanimir Varbanov,stanimir.varbanov@linaro.org,1,1,59,0.19736842105263158,1.0,1,1.0,0.0,0.0,0.0
1008,524301,524335,"Noise, I am wrong...","Noise, I am wrong...",technical,Frank Lee,tiny.windzz@gmail.com,1,0,20,1.0,1.0,1,0.0,0.0,0.0,0.0
1009,524351,547076,"This name does not match up with the ""From:"" line :(Please fix up and resend.","On Tue, Jan 22, 2019 at 10:33:57PM +0800, Bo Wang wrote:  This name does not match up with the From:"" line :(  Please fix up and resend.  thanks  greg k-h""",technical,Greg KH,gregkh@linuxfoundation.org,1,0,77,1.0,1.0,1,1.0,0.0,1.0,0.0
1010,527579,541531,Do you have an oops report or test case for this?,Geliang Tang <geliangtang@gmail.com> wrote:   Do you have an oops report or test case for this?  David,technical,David Howells,dhowells@redhat.com,1,0,49,0.75,0.5,0,0.375,0.59375,0.375,0.59375
1011,527579,563081,"Here is the test module code. Insmod it, we can get the oops.","On Wed, Feb 06, 2019 at 10:26:53PM +0000, David Howells wrote:  Here is the test module code. Insmod it, we can get the oops.  #include <linux/init.h> #include <linux/module.h> #include <linux/key.h> #include <linux/key-type.h> #include <linux/cred.h> #include <linux/seq_file.h>  static int test_instantiate(struct key *key, struct key_preparsed_payload *prep) { 	return 0, }  static void test_describe(const struct key *key, struct seq_file *m) { 	seq_puts(m, key->description), }  struct key_type test_key_type = {         .name           = test"", 	//.instantiate	= test_instantiate,         .describe       = test_describe, },  static int __init test_init(void) { 	const struct cred *cred = current_cred(), 	struct key *key, 	int ret,  	register_key_type(&test_key_type),  	key = key_alloc(&test_key_type, ""test"", 			GLOBAL_ROOT_UID, GLOBAL_ROOT_GID, cred, 			KEY_POS_SEARCH, KEY_ALLOC_NOT_IN_QUOTA, NULL), 	if (IS_ERR(key)) 		return -1,  	pr_info(""keyring allocated successfully.\n""),  	ret = key_instantiate_and_link(key, 				       NULL, 				       sizeof(int), 				       current->cred->user->session_keyring, 				       NULL), 	if (ret < 0) { 		key_revoke(key), 		key_put(key), 		return ret, 	}  	return 0, }  static void __exit test_exit(void) { 	unregister_key_type(&test_key_type), }  module_init(test_init), module_exit(test_exit),  MODULE_LICENSE(""GPL""),""",technical,Geliang Tang,geliangtang@gmail.com,0,1,61,1.0,0.75,0,1.0,0.0,0.59375,0.0
1012,527579,563083,"Here is the test module code. Insmod it, we can get the oops.","On Wed, Feb 06, 2019 at 10:26:53PM +0000, David Howells wrote:  Here is the test module code. Insmod it, we can get the oops.  #include <linux/init.h> #include <linux/module.h> #include <linux/key.h> #include <linux/key-type.h> #include <linux/cred.h> #include <linux/seq_file.h>  static int test_instantiate(struct key *key, struct key_preparsed_payload *prep) { 	return 0, }  static void test_describe(const struct key *key, struct seq_file *m) { 	seq_puts(m, key->description), }  struct key_type test_key_type = {         .name           = test"", 	//.instantiate	= test_instantiate,         .describe       = test_describe, },  static int __init test_init(void) { 	const struct cred *cred = current_cred(), 	struct key *key, 	int ret,  	register_key_type(&test_key_type),  	key = key_alloc(&test_key_type, ""test"", 			GLOBAL_ROOT_UID, GLOBAL_ROOT_GID, cred, 			KEY_POS_SEARCH, KEY_ALLOC_NOT_IN_QUOTA, NULL), 	if (IS_ERR(key)) 		return -1,  	pr_info(""keyring allocated successfully.\n""),  	ret = key_instantiate_and_link(key, 				       NULL, 				       sizeof(int), 				       current->cred->user->session_keyring, 				       NULL), 	if (ret < 0) { 		key_revoke(key), 		key_put(key), 		return ret, 	}  	return 0, }  static void __exit test_exit(void) { 	unregister_key_type(&test_key_type), }  module_init(test_init), module_exit(test_exit),  MODULE_LICENSE(""GPL""),""",technical,Geliang Tang,geliangtang@gmail.com,0,1,61,1.0,1.0,1,1.0,0.0,0.0,0.0
1013,528089,528385,"You do not need explicitly unregister input device if it is managed (allocated with devm). This is however is wrong, as you can't shutdown hardware before disapling/freeing IRQ, etc. Given that there are no users ofgp2a_platform_data in kernel I'd recommend creating a preparatory patch removing platform data support from the driver.","On Fri, Jan 25, 2019 at 06:50:42PM +0100, Paweł Chmiel wrote:  You do not need explicitly unregister input device if it is managed (allocated with devm).   This is however is wrong, as you can't shutdown hardware before disapling/freeing IRQ, etc. Given that there are no users of gp2a_platform_data in kernel I'd recommend creating a preparatory patch removing platform data support from the driver.  Thanks.  --  Dmitry",technical,Dmitry Torokhov,dmitry.torokhov@gmail.com,1,0,334,0.9375,0.5454545454545454,0,0.0,0.6666666666666666,0.0,0.0
1014,528089,528386,"No, light sensor is not an input device, keep it in IIO please.","On Fri, Jan 25, 2019 at 06:50:43PM +0100, Paweł Chmiel wrote:  No, light sensor is not an input device, keep it in IIO please.  --  Dmitry",technical,Dmitry Torokhov,dmitry.torokhov@gmail.com,1,0,63,0.25,0.6363636363636364,0,0.0,0.6666666666666666,0.0,0.0
1015,528089,528393,Do you know what it is for?,"On Fri, Jan 25, 2019 at 06:50:45PM +0100, Paweł Chmiel wrote:  Do you know what it is for?   --  Dmitry",technical,Dmitry Torokhov,dmitry.torokhov@gmail.com,1,0,27,0.125,0.7272727272727273,0,0.0,0.6666666666666666,0.0,0.0
1016,528089,528411,"Thanks for your review of the patches. Considering that the light sensor part should be in IIO, should the entire driver be rewritten as an IIO driver?  There's already the driver for gp2ap020a00f there which is presumably the gp2ap002a00f's successor and does the same functions. It's the control of the main power supply to the chip.","Hi Dmitry,    Thanks for your review of the patches.    Considering that the light sensor part should be in IIO, should the entire driver be rewritten as an IIO driver?  There's already the driver for gp2ap020a00f there which is presumably the gp2ap002a00f's successor and does the same functions.    On 2019-01-25 5:32 p.m., Dmitry Torokhov wrote:  It's the control of the main power supply to the chip.  Thanks,  Jonathan",technical,Jonathan Bakker,xc-racer2@live.ca,0,0,335,1.0,0.8181818181818182,0,0.0,0.6666666666666666,0.0,0.0
1017,528089,530799,"I'd be fine with that. In this case it should be a power supply (regulator), not gpio.","On Sat, Jan 26, 2019 at 03:14:14AM +0000, Jonathan Bakker wrote:  I'd be fine with that.   In this case it should be a power supply (regulator), not gpio.  Thanks.  --  Dmitry",technical,Dmitry Torokhov,dmitry.torokhov@gmail.com,1,0,86,0.359375,1.0,1,1.0,0.0,0.6666666666666666,0.0
1018,529166,529168,Not sure why it never made it. I created it for use with a wl1271 which wan't properly reset in case of a fault. Also combined with imx28.,"Hi Martin,  On Mon, 28 Jan 2019 11:20:22 +0100 Martin Kepplinger <martink@posteo.de> wrote:   Not sure why it never made it. I created it for use with a wl1271 which wan't properly reset in case of a fault. Also combined with imx28.  Regards, Robin van der Gracht",technical,Robin van der Gracht,robin@protonic.nl,1,0,138,0.3855421686746988,0.6666666666666666,0,0.0,0.0,0.0,0.0
1019,529166,529223,Isn't there a GPIO that needs to be toggled and separate clk for the WIFI chip that needs to be enabled as well? You don't need this here as this is already part of the generic struct mmc_host (via the struct mmc_supply). Ditto. You should use mmc_regulator_set_ocr() instead. Ditto. This isn't needed. The state is already controlled by the mmc core. You should use mmc_regulator_get_supply() instead.,"On Mon, 28 Jan 2019 at 11:20, Martin Kepplinger <martink@posteo.de> wrote:  Isn't there a GPIO that needs to be toggled and separate clk for the WiFi chip that needs to be enabled as well?   You don't need this here as this is already part of the generic struct mmc_host (via the struct mmc_supply).   Ditto.   You should use mmc_regulator_set_ocr() instead.   Ditto.   This isn't needed. The state is already controlled by the mmc core.   You should use mmc_regulator_get_supply() instead.   Kind regards Uffe",technical,Ulf Hansson,ulf.hansson@linaro.org,1,0,402,1.0,1.0,1,0.0,0.0,0.0,0.0
1020,532681,532860,Coding style is off here. But really I don't think these inlines are needed here. Put them in qemu or something.,"On Tue, Jan 29, 2019 at 02:52:36PM +0200, Yuri Benditovich wrote:  Coding style is off here. But really I don't think these inlines are needed here. Put them in qemu or something.",technical,Michael S. Tsirkin,mst@redhat.com,1,0,112,0.1736111111111111,0.6666666666666666,0,0.0,1.0,0.0,1.0
1021,532681,534079,I also wonder why do we want to put this code in the legacy section and use the legacy virtio_net_hdr as opposed to the new. coding style says: Descendants are always substantially shorter than the parent and are placed substantially to the right. placing a line to the left of ( doesn't count as substantially to the right :) Maybe start a new line at this. Lack of documentation is also a problem. Okay but this assumes specific usage. E.g. someone might want an offset and not a pointer. Or have a struct instance on stack. Given all above issues (and also header version issues described above) I'm inclined to say macros are better. But please in any case also add documentation same as we have for fields.,"On Wed, Jan 30, 2019 at 09:42:07AM +0200, Yuri Benditovich wrote:  I also wonder why do we want to put this code in the legacy section and use the legacy virtio_net_hdr as opposed to the new virtio_net_hdr_v1.    coding style says:  	Descendants are always substantially shorter than the parent and 	are placed substantially to the right.  placing a line to the left of ( doesn't count as substantially to the right :) Maybe start a new line at virtio_net_rsc_ext_num_dupacks.  Lack of documentation is also a problem.   Okay but this assumes specific usage. E.g. someone might want an offset and not a pointer. Or have a struct instance on stack.  Given all above issues (and also header version issues described above) I'm inclined to say macros are better:   #define virtio_net_rsc_ext_num_packets csum_start #define virtio_net_rsc_ext_num_dupacks csum_offset   But please in any case also add documentation same as we have for fields.",technical,Michael S. Tsirkin,mst@redhat.com,1,0,711,1.0,1.0,1,1.0,0.0,1.0,0.0
1022,533420,533462,Nit: does not seem to be required,"On Tue, Jan 29, 2019 at 09:02:16PM -0800, Dan Williams wrote:  [ ... ]    ...   Nit: does not seem to be required   --  Sincerely yours, Mike.",technical,Mike Rapoport,rppt@linux.ibm.com,1,0,33,0.03162055335968379,0.38461538461538464,0,0.0,1.0,0.0,0.0
1023,533420,534248,"Good catch. Notch one more line saved in the incremental diffstat fromv8. I'll wait for Michal's thumbs up on the rest before re-spinning,or perhaps Andrew can drop this line on applying?","On Tue, Jan 29, 2019 at 10:49 PM Mike Rapoport <rppt@linux.ibm.com> wrote:  Good catch. Notch one more line saved in the incremental diffstat from v8. I'll wait for Michal's thumbs up on the rest before re-spinning, or perhaps Andrew can drop this line on applying?",technical,Dan Williams,dan.j.williams@intel.com,1,1,187,0.15019762845849802,0.46153846153846156,0,0.0,1.0,0.0,0.0
1024,533420,534296,"The last part is not true with this version anymore, right? I find mm_shuffle_ctl a bit confusing because the mode of operation is either AUTO (enabled when the HW is present) or FORCE_ENABLE when explicitly enabled by the command line. Nothing earth shattering though. Other than that, I haven't spotted any fundamental issues. The feature is a hack but I do agree that it might be useful for the specific HW it is going to be used for. I still think that shuffling only top orders has close to zero security benefits because it is not that hard to control the memory fragmentation.","On Tue 29-01-19 21:02:16, Dan Williams wrote:  The last part is not true with this version anymore, right?   I find mm_shuffle_ctl a bit confusing because the mode of operation is either AUTO (enabled when the HW is present) or FORCE_ENABLE when explicitly enabled by the command line. Nothing earth shattering though.   Other than that, I haven't spotted any fundamental issues. The feature is a hack but I do agree that it might be useful for the specific HW it is going to be used for. I still think that shuffling only top orders has close to zero security benefits because it is not that hard to control the memory fragmentation.  With that Acked-by: Michal Hocko <mhocko@suse.com>   --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,583,0.4505928853754941,0.5384615384615384,0,0.0,1.0,0.0,0.0
1025,533420,534298,I have asked in v7 but didn't get any response. Do we really ned perfree_area random pool? Why a global one is not sufficient?,"On Tue 29-01-19 21:02:26, Dan Williams wrote:  I have asked in v7 but didn't get any response. Do we really ned per free_area random pool? Why a global one is not sufficient?   --  Michal Hocko SUSE Labs",technical,Michal Hocko,mhocko@kernel.org,1,0,126,0.11067193675889328,0.6153846153846154,0,0.0,1.0,0.0,0.0
1026,533420,534311,"Ah, yes, sorry, overlooked that feedback. A global one is sufficient. Will rework.","On Wed, Jan 30, 2019 at 11:11 AM Michal Hocko <mhocko@kernel.org> wrote:  Ah, yes, sorry, overlooked that feedback. A global one is sufficient. Will rework.",technical,Dan Williams,dan.j.williams@intel.com,1,1,82,0.07509881422924901,0.6923076923076923,0,0.0,1.0,0.0,0.0
1027,533420,534553,"True, and given that page_alloc_init_late() is waiting for it complete the impact is no different from v8 to v9. I'll drop that sentence from the changelog. Yeah, it's named from the perspective of the kernel internal usage which is flipped from the user facing interaction. ENABLE is called from the command line handler and in a follow-on patch the parser of the platform-firmware table indicating the presence of a cache. FORCE_DISABLE is only called from the command line handler. I'll add a comment to this effect. Much appreciated.","On Wed, Jan 30, 2019 at 11:08 AM Michal Hocko <mhocko@kernel.org> wrote:  True, and given that page_alloc_init_late() is waiting for it complete the impact is no different from v8 to v9. I'll drop that sentence from the changelog.   Yeah, it's named from the perspective of the kernel internal usage which is flipped from the user facing interaction. ENABLE is called from the command line handler and in a follow-on patch the parser of the platform-firmware table indicating the presence of a cache. FORCE_DISABLE is only called from the command line handler. I'll add a comment to this effect.   Much appreciated.",technical,Dan Williams,dan.j.williams@intel.com,1,1,537,0.4031620553359684,0.7692307692307693,0,0.0,0.0,0.0,0.0
1028,533420,535666,"This is unfortunate from a testing and coverage point of view.  Atleast initially it is desirable that all testers run this feature. Also, it's unfortunate that enabling the feature requires a reboot. What happens if we do away with the boot-time (and maybe hot plug-time) randomization and permit the feature to be switched on/off at runtime? Can we get a Documentation update for the new kernel parameter? Does shuffle.h need to be available to the whole kernel or can we put it in mm/?Can this be __meminit? Reflowing the comment to use all 80 cols would save a line :) Second sentence is hard to parse. Reflow the comment...Reflow.","On Tue, 29 Jan 2019 21:02:16 -0800 Dan Williams <dan.j.williams@intel.com> wrote:   This is unfortunate from a testing and coverage point of view.  At least initially it is desirable that all testers run this feature.  Also, it's unfortunate that enableing the feature requires a reboot.  What happens if we do away with the boot-time (and maybe hotplug-time) randomization and permit the feature to be switched on/off at runtime?   Can we get a Documentation update for the new kernel parameter?   Does shuffle.h need to be available to the whole kernel or can we put it in mm/?   Can this be __meminit?   Reflowing the comment to use all 80 cols would save a line :)   Second sentence is hard to parse.   Reflow the comment...   Reflow.",technical,Andrew Morton,akpm@linux-foundation.org,1,0,635,0.4980237154150198,0.8461538461538461,0,1.0,0.0,0.0,0.0
1029,533420,535667,A static inline would be nicer. Well that's nice and simple.,"On Tue, 29 Jan 2019 21:02:26 -0800 Dan Williams <dan.j.williams@intel.com> wrote:   A static inline would be nicer.   Well that's nice and simple.",technical,Andrew Morton,akpm@linux-foundation.org,1,0,60,0.05533596837944664,0.9230769230769231,0,1.0,0.0,0.0,0.0
1030,533420,535681,"Currently there's the 'shuffle' at memory online time and a random front-back freeing of max_order pages to the free lists at runtime. The random front-back freeing behavior would be trivial to toggle at runtime, however testing showed that the entropy it injects is only enough to preserve the randomization of the initial 'shuffle', but not enough entropy to improve cache utilization on its own. The shuffling could be done dynamically at runtime, but it only shuffles free memory, the effectiveness is diminished if the workload has already taken pages off the free list. It's also diminished if the free lists are polluted with sub MAX_ORDER pages. The number of caveats that need to be documented makes me skeptical that runtime triggered shuffling would be reliable. That said, I see your point about experimentation and validation. What about allowing it to be settable as a sysfs parameter for memory-blocks that are being hot-added? That way we know the shuffle will be effective and the administrator can validate shuffling with a hot-unplug/replug?Yes.The wider kernel just needs page_alloc_shuffle() so that platform-firmware parsing code that detects a memory-side-cache can enable the shuffle. The rest can be constrained to an mm/ local header. Yes. Will do. Earlier versions only arranged to shuffle over non-hole ranges, but the SHUFFLE_RETRY works around that now. I'll update the comment. yup. ok.","On Thu, Jan 31, 2019 at 2:15 PM Andrew Morton <akpm@linux-foundation.org> wrote: [..]  Currently there's the 'shuffle' at memory online time and a random front-back freeing of max_order pages to the free lists at runtime. The random front-back freeing behavior would be trivial to toggle at runtime, however testing showed that the entropy it injects is only enough to preserve the randomization of the initial 'shuffle', but not enough entropy to improve cache utilization on its own.  The shuffling could be done dynamically at runtime, but it only shuffles free memory, the effectiveness is diminished if the workload has already taken pages off the free list. It's also diminished if the free lists are polluted with sub MAX_ORDER pages.  The number of caveats that need to be documented makes me skeptical that runtime triggered shuffling would be reliable.  That said, I see your point about experimentation and validation. What about allowing it to be settable as a sysfs parameter for memory-blocks that are being hot-added? That way we know the shuffle will be effective and the administrator can validate shuffling with a hot-unplug/replug?   Yes.   The wider kernel just needs page_alloc_shuffle() so that platform-firmware parsing code that detects a memory-side-cache can enable the shuffle. The rest can be constrained to an mm/ local header.   Yes.   WIll do.   Earlier versions only arranged to shuffle over non-hole ranges, but the SHUFFLE_RETRY works around that now. I'll update the comment.   yup.   ok.",technical,Dan Williams,dan.j.williams@intel.com,1,1,1417,1.0,1.0,1,1.0,0.0,0.0,0.0
1031,534110,548813,"Hello, Gentle reminder for this new driver review","On 1/30/19 5:38 PM, Fabrice Gasnier wrote:  Hello,  Gentle reminder for this new driver review  Best Regards, Fabrice",technical,Fabrice Gasnier,fabrice.gasnier@st.com,1,1,49,0.07692307692307693,0.5,0,0.48148148148148145,0.48148148148148145,0.48148148148148145,0.0
1032,534110,548821,"Nvmem provider driver itself looks fine for me, but I am unable to take his as 5.1 material, as I normally take nvmem patches which are reviewed and ready before rc5.dt bindings patch needs an ack from DT maintainers.","On 13/02/2019 10:15, Fabrice Gasnier wrote: Nvmem provider driver itself looks fine for me, but I am unable to take  this as 5.1 material, as I normally take nvmem patches which are  reviewed and ready before rc5.  dt bindings patch needs an ack from DT maintainers.  Thanks, srini",technical,Srinivas Kandagatla,srinivas.kandagatla@linaro.org,1,0,217,0.358974358974359,0.5833333333333334,0,0.48148148148148145,0.48148148148148145,0.0,0.037037037037037035
1033,534110,551233,Thanks for the feedback. I hope Rob cant take a look at it.,"On 2/13/19 11:34 AM, Srinivas Kandagatla wrote:  Hi Srini, Thanks for the feedback.   I hope Rob cant take a look at it.  Best Regards, Fabrice",technical,Fabrice Gasnier,fabrice.gasnier@st.com,1,1,59,0.1282051282051282,0.6666666666666666,0,0.5185185185185185,0.4444444444444444,0.037037037037037035,0.14814814814814814
1034,534110,562078,"Several distinct types here. Does s/w need to know the difference rather than just one generic-ish compatible? Access size restrictions maybe? Ability to unlock and program? If not, then why even make this stm32 specific?","On Wed, Jan 30, 2019 at 05:38:53PM +0100, Fabrice Gasnier wrote:  Several distinct types here. Does s/w need to know the difference  rather than just one generic-ish compatible? Access size restrictions  maybe? Ability to unlock and program?  If not, then why even make this stm32 specific?",technical,Rob Herring,robh@kernel.org,1,0,221,0.3504273504273504,0.8333333333333334,0,0.9629629629629629,0.037037037037037035,0.25925925925925924,0.0
1035,534110,563086,"The reading part is represented here as ""st,stm32-romem"" compatible, to simply handle read only access. I agree this could be a generic-ish. BUT the specifics are regarding the ability to unlock/lock and program. Access size can vary from one part to another (e.g. reference manual sates: OTP area is divided into 16 OTP data blocks of32 bytes.  OTP area is divided into 16 OTP data blocks of 64bytes.) In STM32MP15, both the read & write access through the BSEC are  specific, represented by dedicated compatible. Do you wish I update the compatible to something like? Thanks for reviewing,","On 2/25/19 5:53 PM, Rob Herring wrote:  Hi Rob,  The reading part is represented here as st,stm32-romem"" compatible, to simply handle read only access. I agree this could be a generic-ish.  BUT the specifics are regarding the ability to unlock/lock and program. Access size can vary from one part to another (e.g. on stm32f4, reference manual sates: OTP area is divided into 16 OTP data blocks of 32 bytes. on stm32f7, OTP area is divided into 16 OTP data blocks of 64 bytes.)  In STM32MP15, both the read & write access through the BSEC are specific, represented by dedicated compatible.  Do you wish I update the compatible to something like: ""st,stm32f4-otp"" ""st,stm32mp15-bsec"" ?  Thanks for reviewing, Best regards, Fabrice """,technical,Fabrice Gasnier,fabrice.gasnier@st.com,1,1,591,1.0,0.9166666666666666,0,0.9629629629629629,0.0,0.0,0.0
1036,534110,563561,"Yes, I think given the above that makes sense. We can always map specific bindings to generic drivers, but not the reverse.","On Tue, Feb 26, 2019 at 3:14 AM Fabrice Gasnier <fabrice.gasnier@st.com> wrote:  Yes, I think given the above that makes sense. We can always map specific bindings to generic drivers, but not the reverse.  Rob",technical,Rob Herring,robh@kernel.org,1,0,123,0.2222222222222222,1.0,1,1.0,0.0,0.0,0.0
1037,534110,554531,"DT Maintainers, Gentlemen reminder for new DT bindings review.","On 1/30/19 5:38 PM, Fabrice Gasnier wrote:  Hello Rob, DT Maintainers,  Gentlemen reminder for new DT bindings review.  Best Regards, Fabrice",technical,Fabrice Gasnier,fabrice.gasnier@st.com,1,1,62,0.09401709401709402,0.75,0,0.6666666666666666,0.2962962962962963,0.14814814814814814,0.25925925925925924
1038,534860,535350,Thanks but I submitted a different patch:,"On Thu, 2019-01-31 at 17:56 +0800, Pi-Hsun Shih wrote:  Thanks but I submitted a different patch:  https://lore.kernel.org/lkml/8b02899853247a2c67669561761f354dd3bd110e.camel@perches.com/",technical,Joe Perches,joe@perches.com,1,0,41,0.34782608695652173,0.6666666666666666,0,0.0,1.0,0.0,1.0
1039,534860,547040,"I tried your patch, but when running checkpatch.pl on I still get. Feels that these two patches fix different issues.","On Fri, Feb 1, 2019 at 12:54 AM Joe Perches <joe@perches.com> wrote:  I tried your patch, but when running checkpatch.pl on https://patchwork.kernel.org/patch/10790203/, I still get: WARNING: 'SPDX-License-Identifier: GPL-2.0 */' is not supported in LICENSES/... #75: FILE: drivers/remoteproc/mtk_common.h:1: +/* SPDX-License-Identifier: GPL-2.0 */  Feels that these two patches fix different issues.",technical,Pi-Hsun Shih,pihsun@chromium.org,0,1,117,1.0,1.0,1,1.0,0.0,1.0,0.0
1040,535440,536756,"Hello everyone, If it's cleared with ptep_clear_flush_notify, change_pte still won't work. The above text needs updating with""ptep_clear_flush"". set_pte_at_notify is all about having ptep_clear_flush only before it or it's the same as having a range invalidate preceding it. With regard to the code, it relies on the ptep_clear_flush preceding the set_pte_at_notify that will make sure if the secondary MMU mapping randomly disappears between ptep_clear_flush and set_pte_at_notify, gup_fast will wait and block on the PT lock until afterset_pte_at_notify is completed before trying to re-establish a secondary MMU mapping. So then we've only to worry about what happens because we left the secondary MMU mapping potentially intact despite we flushed the primary MMU mapping with ptep_clear_flush (as opposed to ptep_clear_flush_notify which would teardown the secondary MMU mapping too). In you wording above at least the ""with a different pfn"" is superfluous. I think it's ok if the protection changes from read-only to read-write and the pfn remains the same. Like when we takeover a page because it's not shared anymore (fork child quit). It's also ok to change pfn if the mapping is read-only and remains read-only, this is what KSM does in replace_page.The read-write to read-only case must not change pfn to avoid losing coherency from the secondary MMU point of view. This isn't so much about change_pte itself, but the fact that the page-copy generally happens well before the pte mangling starts. This case never presents itself in the code because KSM is first write protecting the page and only later merging it, regardless of change_pte or not. The important thing is that the secondary MMU must be updated first (unlike the invalidates) to be sure the secondary MMU already points to the new page when the pfn changes and the protection changes from read-only to read-write (COW fault). The primary MMU cannot read/write to the page anyway while we update the secondary MMU because we did ptep_clear_flush() before calling set_pte_at_notify(). So this ordering of this ensures whenever the CPU can access the memory, the access is synchronous with the secondary MMUs because they've all been updated already. If (in set_pte_at_notify) we were to call change_pte() afterset_pte_at() what would happen is that the CPU could write to the page through a TLB fill without page fault while the secondary MMUs still read the old memory in the old read-only page.","Hello everyone,  On Thu, Jan 31, 2019 at 01:37:02PM -0500, Jerome Glisse wrote:  If it's cleared with ptep_clear_flush_notify, change_pte still won't work. The above text needs updating with ptep_clear_flush"". set_pte_at_notify is all about having ptep_clear_flush only before it or it's the same as having a range invalidate preceding it.  With regard to the code, wp_page_copy() needs s/ptep_clear_flush_notify/ptep_clear_flush/ before set_pte_at_notify.  change_pte relies on the ptep_clear_flush preceding the set_pte_at_notify that will make sure if the secondary MMU mapping randomly disappears between ptep_clear_flush and set_pte_at_notify, gup_fast will wait and block on the PT lock until after set_pte_at_notify is completed before trying to re-establish a secondary MMU mapping.  So then we've only to worry about what happens because we left the secondary MMU mapping potentially intact despite we flushed the primary MMU mapping with ptep_clear_flush (as opposed to ptep_clear_flush_notify which would teardown the secondary MMU mapping too).  In you wording above at least the ""with a different pfn"" is superflous. I think it's ok if the protection changes from read-only to read-write and the pfn remains the same. Like when we takeover a page because it's not shared anymore (fork child quit).  It's also ok to change pfn if the mapping is read-only and remains read-only, this is what KSM does in replace_page.  The read-write to read-only case must not change pfn to avoid losing coherency from the secondary MMU point of view. This isn't so much about change_pte itself, but the fact that the page-copy generally happens well before the pte mangling starts. This case never presents itself in the code because KSM is first write protecting the page and only later merging it, regardless of change_pte or not.  The important thing is that the secondary MMU must be updated first (unlike the invalidates) to be sure the secondary MMU already points to the new page when the pfn changes and the protection changes from read-only to read-write (COW fault). The primary MMU cannot read/write to the page anyway while we update the secondary MMU because we did ptep_clear_flush() before calling set_pte_at_notify(). So this ordering of ""ptep_clear_flush, change_pte, set_pte_at"" ensures whenever the CPU can access the memory, the access is synchronous with the secondary MMUs because they've all been updated already.  If (in set_pte_at_notify) we were to call change_pte() after set_pte_at() what would happen is that the CPU could write to the page through a TLB fill without page fault while the secondary MMUs still read the old memory in the old readonly page.  Thanks, Andrea""",technical,Andrea Arcangeli,aarcange@redhat.com,0,0,2469,1.0,0.3333333333333333,0,0.05555555555555555,0.9444444444444444,0.05555555555555555,0.0
1041,535440,536767,"Oops, the above two statements were incorrect because ptep_clear_flush_notify doesn't interfere with change_pte and will only invalidate secondary MMU mappings on those secondary MMUs that shares the same page tables with the primary MMU and that in turn won't ever implement a change_pte method.","On Fri, Feb 01, 2019 at 06:57:38PM -0500, Andrea Arcangeli wrote:  Oops, the above two statements were incorrect because ptep_clear_flush_notify doesn't interfere with change_pte and will only invalidate secondary MMU mappings on those secondary MMUs that shares the same pagetables with the primary MMU and that in turn won't ever implement a change_pte method.",technical,Andrea Arcangeli,aarcange@redhat.com,0,0,296,0.10786516853932585,0.3888888888888889,0,0.05555555555555555,0.9444444444444444,0.0,0.0
1042,535440,536788,"This seems racy by design in the way it copies the page, if the vma mapping isn't read only to begin with (in which case it'd be ok to change the pfn with change_pte too, it'd be a from read-only to read-only change which is ok). If the code copies a writable page there's no much issue if coherency is lost by other means too. Said that this isn't a worthwhile optimization for uprobes so because of the lack of explicit read-only enforcement, I agree it's simpler to skip change_pte above. It's orthogonal, but in this function it can be optimized, otherwise there's no point to retain the _notify.","On Thu, Jan 31, 2019 at 01:37:03PM -0500, Jerome Glisse wrote:  This seems racy by design in the way it copies the page, if the vma mapping isn't readonly to begin with (in which case it'd be ok to change the pfn with change_pte too, it'd be a from read-only to read-only change which is ok).  If the code copies a writable page there's no much issue if coherency is lost by other means too.  Said that this isn't a worthwhile optimization for uprobes so because of the lack of explicit read-only enforcement, I agree it's simpler to skip change_pte above.  It's orthogonal, but in this function the mmu_notifier_invalidate_range_end(&range), can be optimized to mmu_notifier_invalidate_range_only_end(&range), otherwise there's no point to retain the _notify in ptep_clear_flush_notify.",technical,Andrea Arcangeli,aarcange@redhat.com,0,0,600,0.2876404494382023,0.4444444444444444,0,0.05555555555555555,0.9444444444444444,0.0,0.0
1043,535440,536797,"This is only allocated in the stack, so saving RAM by mixing bitfields with enum in the same 4 bytes to save 4 bytes isn't of maximum priority. A possibly cleaner way to save those 4 bytes without mixing enum with bitfields by hand, is to add a ""unsigned short flags"" which will make ""event/flags/blockable"" fit in the same 8 bytes (bool only needs 1byte) as before the patch (the first bitfield can start from 0 then). Yet another way is to drop blockable and convert it to a flag in ""unsigned int flags"".","On Thu, Jan 31, 2019 at 01:37:04PM -0500, Jerome Glisse wrote:  This is only allocated in the stack, so saving RAM by mixing bitfields with enum in the same 4 bytes to save 4 bytes isn't of maximum priority.  A possibly cleaner way to save those 4 bytes without mixing enum with bitfields by hand, is to add a unsigned short flags"" which will make ""event/flags/blockable"" fit in the same 8 bytes (bool only needs 1 byte) as before the patch (the first bitfield can start from 0 then).  Yet another way is to drop blockable and convert it to a flag in ""unsigned int flags"".""",technical,Andrea Arcangeli,aarcange@redhat.com,0,0,506,0.24719101123595505,0.5,0,0.05555555555555555,0.9444444444444444,0.0,0.5
1044,535440,546460,"So all the above is moot since as you pointed out in the other emailptep_clear_flush_notify does not invalidate kvm secondary mmu hence.Yes i thought this was obvious i will reword and probably just do a list of every case that is fine.Yeah, between do you have any good workload for me to test this ? I was thinking of running few same VM and having KSM work on them. Is there some way to trigger KVM to fork ? As the other case is breaking COW after fork.","On Fri, Feb 01, 2019 at 06:57:38PM -0500, Andrea Arcangeli wrote:  So all the above is moot since as you pointed out in the other email ptep_clear_flush_notify does not invalidate kvm secondary mmu hence.    Yes i thought this was obvious i will reword and probably just do a list of every case that is fine.   Yeah, between do you have any good workload for me to test this ? I was thinking of running few same VM and having KSM work on them. Is there some way to trigger KVM to fork ? As the other case is breaking COW after fork.  Cheers, Jrme",technical,Jerome Glisse,jglisse@redhat.com,1,0,457,0.20449438202247192,0.5555555555555556,0,0.6111111111111112,0.3888888888888889,0.5,0.0
1045,535440,546490,Background we are discussing __replace_page() in it and whether this can be call on page that can be written too through its virtual address mapping. I am not sure the race exist but i am not familiar with the uprobe code so maybe the page is already write protected and thus the copy is fine and in fact that is likely the case but there is not check for that. Maybe there should be a check ? Maybe someone familiar with this code can chime in. We need to keep the _notify for IOMMU otherwise it would break IOMMU. IOMMU can walk the page table at any time and thus we need to first clear the table then notify the IOMMU to flush TLB on all the devices that might have a TLB entry. Only then can we set the new pte. But yes the mmu_notifier_invalidate_range_end can be optimized to only end. I will do a separate patch for this. As it is orthogonal as you pointed out :) ],"Background we are discussing __replace_page() in:     kernel/events/uprobes.c  and wether this can be call on page that can be written too through its virtual address mapping.  On Fri, Feb 01, 2019 at 07:50:22PM -0500, Andrea Arcangeli wrote:  I am not sure the race exist but i am not familiar with the uprobe code so maybe the page is already write protected and thus the copy is fine and in fact that is likely the case but there is not check for that. Maybe there should be a check ?  Maybe someone familiar with this code can chime in.   We need to keep the _notify for IOMMU otherwise it would break IOMMU. IOMMU can walk the page table at any time and thus we need to first clear the table then notify the IOMMU to flush TLB on all the devices that might have a TLB entry. Only then can we set the new pte.  But yes the mmu_notifier_invalidate_range_end can be optimized to only end. I will do a separate patch for this. As it is orthogonal as you pointed out :)  Cheers, Jrme",technical,Jerome Glisse,jglisse@redhat.com,1,0,873,0.4067415730337079,0.6111111111111112,0,0.6111111111111112,0.3888888888888889,0.0,0.0
1046,535440,546523,"KVM can fork on guest pci-hotplug events or network init to run hostscripts and re-init the signals before doing the exec, but it won't move the needle because all guest memory registered in the MMUnotifier is set as MADV_DONTFORK... so fork() is a noop unless qemu is also modified not to call MADV_DONTFORK. Calling if (!fork()) exit(0) from a timer at regular intervals during qemu runtime after turning off MADV_DONTFORK in qemu would allow to exercise fork against the KVM MMU Notifier methods. The optimized change_pte code in copy-on-write code is the same post-fork or post-KSM merge and fork() itself doesn't use change_ptewhile KSM does, so with regard to change_pte it should already provide a good test coverage to test with only KSM without fork(). It'll cover the read-write -> read-only transition with same PFN(write_protect_page), the read-only to read-only changing PFN(replace_page) as well as the read-only -> read-write transition changing PFN (wp_page_copy) all three optimized with change_pte. Fork would not leverage change_pte for the first two cases.","On Mon, Feb 11, 2019 at 02:09:31PM -0500, Jerome Glisse wrote:  KVM can fork on guest pci-hotplug events or network init to run host scripts and re-init the signals before doing the exec, but it won't move the needle because all guest memory registered in the MMU notifier is set as MADV_DONTFORK... so fork() is a noop unless qemu is also modified not to call MADV_DONTFORK.  Calling if (!fork()) exit(0) from a timer at regular intervals during qemu runtime after turning off MADV_DONTFORK in qemu would allow to exercise fork against the KVM MMU Notifier methods.  The optimized change_pte code in copy-on-write code is the same post-fork or post-KSM merge and fork() itself doesn't use change_pte while KSM does, so with regard to change_pte it should already provide a good test coverage to test with only KSM without fork(). It'll cover the read-write -> readonly transition with same PFN (write_protect_page), the read-only to read-only changing PFN (replace_page) as well as the readonly -> read-write transition changing PFN (wp_page_copy) all three optimized with change_pte. Fork would not leverage change_pte for the first two cases.",technical,Andrea Arcangeli,aarcange@redhat.com,0,0,1076,0.45617977528089887,0.6666666666666666,0,0.6111111111111112,0.3888888888888889,0.0,0.3333333333333333
1047,535440,555240,"So i run 2 exact same VMs side by side (copy of same COW image) and built the same kernel tree inside each (that is the only important workload that exist ,)) but the change_pte did not have any impact: before  mean  Above is time taken by make inside each VM for all yes config. npages is the number of page shared reported on the host at the end of the build. There is no change before and after the patchset to restore changepte. I tried removing the change_pte callback all together to see if that did have any effect (without above) and it did not have any effect either. Should we still restore change_pte() ? It does not hurt, but it does not seems to help in anyway. Maybe you have a better benchmark i could run ?","On Mon, Feb 11, 2019 at 03:02:00PM -0500, Andrea Arcangeli wrote:  So i run 2 exact same VMs side by side (copy of same COW image) and built the same kernel tree inside each (that is the only important workload that exist ,)) but the change_pte did not have any impact:  before  mean  {real: 1358.250977, user: 16650.880859, sys: 839.199524, npages: 76855.390625} before  stdev {real:    6.744010, user:   108.863762, sys:   6.840437, npages:  1868.071899} after   mean  {real: 1357.833740, user: 16685.849609, sys: 839.646973, npages: 76210.601562} after   stdev {real:    5.124797, user:    78.469360, sys:   7.009164, npages:  2468.017578} without mean  {real: 1358.501343, user: 16674.478516, sys: 837.791992, npages: 76225.203125} without stdev {real:    5.541104, user:    97.998367, sys:   6.715869, npages:  1682.392578}  Above is time taken by make inside each VM for all yes config. npages is the number of page shared reported on the host at the end of the build.  There is no change before and after the patchset to restore change pte. I tried removing the change_pte callback alltogether to see if that did have any effect (without above) and it did not have any effect either.  Should we still restore change_pte() ? It does not hurt, but it does not seems to help in anyway. Maybe you have a better benchmark i could run ?  Cheers, Jrme",technical,Jerome Glisse,jglisse@redhat.com,1,0,722,0.34831460674157305,0.7222222222222222,0,0.9444444444444444,0.0,0.3333333333333333,0.0
1048,535440,555344,"Did you set /sys/kernel/mm/ksm/sleep_millisecs to 0?It would also help to remove the checksum check from mm/ksm.c:- if (rmap_item->old checksum != checksum) {-  rmap_item->old checksum = checksum,-  return,- }One way or another, /sys/kernel/mm/ksm/pages_shared and/or pages_sharing need to change significantly to be sure we're exercising the COW/merging code that uses change_pte. KSM is smart enough to merge only not frequently changing pages, and with the default KSMcode this probably works too well for a kernel build. We could also try a micro benchmark based onlap/testcases/kernel/mem/ksm/ksm02.c that already should trigger a merge flood and a COW flood during its internal processing.","On Mon, Feb 18, 2019 at 11:04:13AM -0500, Jerome Glisse wrote:  Did you set /sys/kernel/mm/ksm/sleep_millisecs to 0?  It would also help to remove the checksum check from mm/ksm.c:  -	if (rmap_item->oldchecksum != checksum) { -		rmap_item->oldchecksum = checksum, -		return, -	}  One way or another, /sys/kernel/mm/ksm/pages_shared and/or pages_sharing need to change significantly to be sure we're exercising the COW/merging code that uses change_pte. KSM is smart enough to merge only not frequently changing pages, and with the default KSM code this probably works too well for a kernel build.   We could also try a microbenchmark based on ltp/testcases/kernel/mem/ksm/ksm02.c that already should trigger a merge flood and a COW flood during its internal processing.  Thanks, Andrea",technical,Andrea Arcangeli,aarcange@redhat.com,0,0,695,0.27191011235955054,0.7777777777777778,0,0.9444444444444444,0.0,0.0,0.0
1049,535440,555378,No but i have increase the pages_to_scan to 10000 and during the kernel build i see the number of page that are shared increase steadily so itis definitely merging thing. Will try with that. Will try that.,"On Mon, Feb 18, 2019 at 12:45:05PM -0500, Andrea Arcangeli wrote:  No but i have increase the pages_to_scan to 10000 and during the kernel build i see the number of page that are shared increase steadily so it is definitly merging thing.   Will try with that.   Will try that.  Cheers, Jrme",technical,Jerome Glisse,jglisse@redhat.com,1,0,205,0.0898876404494382,0.8333333333333334,0,0.9444444444444444,0.0,0.0,0.0
1050,535440,555680,"Would it also make sense to track how many pages are really affected by change_pte (say, in kvm_set_pte_rmapp, count available SPTEs that are correctly rebuilt)?  I'm thinking even if many pages are merged by KSM it's still possible that these pages are not actively shadowed by KVM MMU, meanwhile change_pte should only affect actively shadowed SPTEs.  In other words, IMHO we might not be able to observe obvious performance differences if the pages we are accessing are not merged by KSM.  In our case (building the kernel), IIUC the mostly possible shared pages are system image pages, however when building the kernel I'm thinking whether these pages will be frequently accesses, and whether this could lead to similar performance numbers.","On Mon, Feb 18, 2019 at 12:45:05PM -0500, Andrea Arcangeli wrote:  Would it also make sense to track how many pages are really affected by change_pte (say, in kvm_set_pte_rmapp, count avaliable SPTEs that are correctly rebuilt)?  I'm thinking even if many pages are merged by KSM it's still possible that these pages are not actively shadowed by KVM MMU, meanwhile change_pte should only affect actively shadowed SPTEs.  In other words, IMHO we might not be able to observe obvious performance differeneces if the pages we are accessing are not merged by KSM.  In our case (building the kernel), IIUC the mostly possible shared pages are system image pages, however when building the kernel I'm thinking whether these pages will be frequently accesses, and whether this could lead to similar performance numbers.  Thanks,  --  Peter Xu",technical,Peter Xu,peterx@redhat.com,0,0,744,0.3101123595505618,0.8888888888888888,0,1.0,0.0,0.0,0.0
1051,535440,555681,"I checked that, if no KVM is running KSM never merge anything (after bumping KSM page to scan to 10000 and sleep to 0). It starts merging once i start KVM. Then i wait a bit for KSM to stabilize (i.e. to merge the stock KVM pages). It is only once KSM count is somewhat stable that i run the test and check that KSM count goes up significantly while test is running. KSM will definitely go through the change_pte path for KVM so i am definitely testing the change_pte path. I have been running the micro benchmark and on that i do see a perf improvement i will report shortly once i am done gathering enough data.","On Tue, Feb 19, 2019 at 10:37:01AM +0800, Peter Xu wrote:  I checked that, if no KVM is running KSM never merge anything (after bumping KSM page to scan to 10000 and sleep to 0). It starts merging once i start KVM. Then i wait a bit for KSM to stabilize (ie to merge the stock KVM pages). It is only once KSM count is somewhat stable that i run the test and check that KSM count goes up significantly while test is running.  KSM will definitly go through the change_pte path for KVM so i am definitly testing the change_pte path.  I have been running the micro benchmark and on that i do see a perf improvement i will report shortly once i am done gathering enough data.  Cheers, Jrme",technical,Jerome Glisse,jglisse@redhat.com,1,0,613,0.29213483146067415,0.9444444444444444,0,1.0,0.0,0.0,0.0
1052,535440,555715,"So using that and the checksum test removed there is an improvement, roughly 7%~8% on time spends in the kernel. I am guessing for kernel build this get lost in the noise and that KSM changes do not have that much of an impact. So i will reposting the mmu notifier changes shortly in hope to get them in 5.1 and I will post the KVM part separately shortly there after. If there is any more testing you wish me to do let me know.","On Mon, Feb 18, 2019 at 12:45:05PM -0500, Andrea Arcangeli wrote:  So using that and the checksum test removed there is an improvement, roughly 7%~8% on time spends in the kernel:  before  mean  {real: 675.460632, user: 857.771423, sys: 215.929657, npages: 4773.066895} before  stdev {real:  37.035435, user:   4.395942, sys:   3.976172, npages:  675.352783} after   mean  {real: 672.515503, user: 855.817322, sys: 200.902710, npages: 4899.000000} after   stdev {real:  37.340954, user:   4.051633, sys:   3.894153, npages:  742.413452}  I am guessing for kernel build this get lost in the noise and that KSM changes do not have that much of an impact. So i will reposting the mmu notifier changes shortly in hope to get them in 5.1 and i will post the KVM part separatly shortly there after.  If there is any more testing you wish me to do let me know.  Cheers, Jrme",technical,Jerome Glisse,jglisse@redhat.com,1,0,428,0.20898876404494382,1.0,1,1.0,0.0,0.0,0.0
1053,538810,550556,"What determines when you want to use polling mode? I'm not sure DTis the best way to control this unless it's really a property of the h/w. Driver behavior is really outside the scope of the DT. u-boot would use polling even if an interrupt is specified, for example.","On Mon, Feb 04, 2019 at 03:15:49PM -0800, Ray Jui wrote:  What determines when you want to use polling mode? I'm not sure DT  is the best way to control this unless it's really a property of  the h/w. Driver behavior is really outside the scope of the DT. u-boot  would use polling even if an interrupt is specified, for example.  Rob",technical,Rob Herring,robh@kernel.org,1,0,267,0.7368421052631579,0.5882352941176471,0,0.8888888888888888,0.0,0.8888888888888888,0.0
1054,538810,550558,What's the data type and size? What are valid values?,"On Mon, Feb 04, 2019 at 03:15:52PM -0800, Ray Jui wrote:  What's the data type and size? What are valid values?",technical,Rob Herring,robh@kernel.org,1,0,53,0.17105263157894737,0.6470588235294118,0,0.8888888888888888,0.0,0.0,0.0
1055,538810,550624,"It's tied to the particular revision of the I2C controller, i.e., the iProc NIC i2c controller does not have interrupt line wired. In this case, the behavior is determined by the DT compatible string of the iProc I2C device. I thought that it makes sense to now move the 'interrupts' property to be under ""Optional"" than ""Required"" which is basically what this change is.","Hi Rob,  On 2/13/2019 1:16 PM, Rob Herring wrote: It's tied to the particular revision of the I2C controller, i.e., the iProc NIC i2c controller does not have interrupt line wired. In this case, the behavior is determined by the DT compatible string of the iProc I2C device. I thought that it makes sense to now move the 'interrupts' property to be under Optional"" than ""Required"" which is basically what this change is. """,technical,Ray Jui,ray.jui@broadcom.com,1,1,371,1.0,0.7058823529411765,0,0.8888888888888888,0.0,0.0,0.0
1056,538810,550635,It's an unsigned u32 mask value. An example of a valid value is for example. Do you want any of these added to the paragraph above?,"On 2/13/2019 1:18 PM, Rob Herring wrote:  It's an unsigned u32 mask value. An example of a valid value is for example 0x03400000. Do you want any of these added to the paragraph above?",technical,Ray Jui,ray.jui@broadcom.com,1,1,131,0.39473684210526316,0.7647058823529411,0,0.8888888888888888,0.0,0.0,0.0
1057,538810,551451,Bindings should define constraints.,"On Wed, Feb 13, 2019 at 4:09 PM Ray Jui <ray.jui@broadcom.com> wrote:  Yes. Bindings should define constraints.  Rob",technical,Rob Herring,robh@kernel.org,1,0,35,0.06578947368421052,0.8235294117647058,0,1.0,0.0,0.0,0.0
1058,538810,551468,"Okay, please put this detail into the commit msg.","On Wed, Feb 13, 2019 at 4:06 PM Ray Jui <ray.jui@broadcom.com> wrote:  Okay, please put this detail into the commit msg.  Rob",technical,Rob Herring,robh@kernel.org,1,0,49,0.14473684210526316,0.8823529411764706,0,1.0,0.0,0.0,0.0
1059,538810,551833,Okay will do! Thanks.,"On 2/14/2019 6:02 AM, Rob Herring wrote:  Okay will do! Thanks.",technical,Ray Jui,ray.jui@broadcom.com,1,1,21,0.07894736842105263,0.9411764705882353,0,1.0,0.0,0.0,0.0
1060,539613,548095,"OK, cool. Prefer slash for TPM specific commits. I can merge this after those extra logs are removed.","On Mon, Feb 11, 2019 at 09:55:36AM -0800, Tadeusz Struk wrote:  OK, cool.   Prefer slash for TPM specific commits.  I can merge this after those extra logs are removed.  /Jarkko",technical,Jarkko Sakkinen,jarkko.sakkinen@linux.intel.com,1,0,101,1.0,1.0,1,1.0,0.0,0.14285714285714285,0.0
1061,539687,539945,"HI, I guess we don't need to initialize it anymore with the check you add?","Hi,  On Tue, Feb 05, 2019 at 02:15:59PM -0800, Yizhuo wrote:  I guess we don't need to initialize it anymore with the check you add?  Maxime  --  Maxime Ripard, Bootlin Embedded Linux and Kernel engineering https://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,74,0.8181818181818182,0.6666666666666666,0,0.0,0.0,0.0,0.0
1062,539687,541634,I agree with the other reviewer that since you check 'ret' the initialization of 'val' is no longer needed.,"From: Yizhuo <yzhai003@ucr.edu> Date: Tue,  5 Feb 2019 14:15:59 -0800   I agree with the other reviewer that since you check 'ret' the initialization of 'val' is no longer needed.",technical,David Miller,davem@davemloft.net,1,0,107,1.0,1.0,1,1.0,0.0,0.0,0.0
1063,541259,552567,Could you please help to review and provide your comments to this patch series when you have time?,"Hi Kishon,  Could you please help to review and provide your comments to this patch series when you have time?  Regards, Srinath.  On Wed, Feb 6, 2019 at 11:03 PM Srinath Mannam <srinath.mannam@broadcom.com> wrote:",technical,Srinath Mannam,srinath.mannam@broadcom.com,0,1,98,0.25333333333333335,0.7142857142857143,0,0.6666666666666666,0.25,0.6666666666666666,0.25
1064,541259,555302,"SoC specific compatibles are preferred. Version numbers can be used but should follow some documented scheme and be meaningful. What we don't want is just Linux developers making up numbering. Unless there's DT resources for each child node, you don't need these. Just make #phy-cells 1 in the parent.","On Wed, Feb 06, 2019 at 11:02:25PM +0530, Srinath Mannam wrote:  SoC specific compatibles are preferred. Version numbers can be used but  should follow some documented scheme and be meaningful. What we don't  want is just Linux developers making up numbering.   Unless there's DT resources for each child node, you don't need these.  Just make #phy-cells 1 in the parent.  Rob",technical,Rob Herring,robh@kernel.org,1,0,301,0.7866666666666666,0.8571428571428571,0,0.9166666666666666,0.0,0.25,0.0
1065,541259,555718,"Thanks for review, please see my comments below inline. Both versions are different phy controllers and also have separate register offsets. I will provide more meaningful compatible IDs and their documentation in next patchset. With the use of phy-cell, PHY argument is available with xlate function , But controller specific assignments required to be done while probe. So I will take your first option given in previous comment.","Hi Rob,  Thanks for review, please see my comments below inline.  On Mon, Feb 18, 2019 at 10:52 PM Rob Herring <robh@kernel.org> wrote: Both versions are different phy controllers and also have separate register offsets. I will provide more meaningful compatible IDs and their documentation in next patchset. With the use of phy-cell, PHY argument is available with xlate function, But controller specific assignments required to be done while probe. So I will take your first option given in previous comment.  Regards, Srinath.",technical,Srinath Mannam,srinath.mannam@broadcom.com,0,1,431,1.0,1.0,1,1.0,0.0,0.0,0.0
1066,541450,541454,"Not sure if we support backwards compatibility like this? My issue with this change is that by doing this, application will have no clue if the new bits were ignored or not and it may think that an event is enabled while it is not. A workaround would be to do a getsockopt and check the size that was returned. But then, it might as well use the right struct here in the first place. I'm seeing current implementation as an implicitly versioned argument: it will always accept setsockopt calls with an old struct (v4.11 orv4.12), but if the user tries to use v3 on a v1-only system, it will be rejected. Pretty much like using a newer setsockopt on an old system.","On Wed, Feb 06, 2019 at 12:14:30PM -0800, Julien Gomes wrote:  Not sure if we support backwards compatibility like this?  My issue with this change is that by doing this, application will have no clue if the new bits were ignored or not and it may think that an event is enabled while it is not.  A workaround would be to do a getsockopt and check the size that was returned. But then, it might as well use the right struct here in the first place.  I'm seeing current implementation as an implicitly versioned argument: it will always accept setsockopt calls with an old struct (v4.11 or v4.12), but if the user tries to use v3 on a v1-only system, it will be rejected. Pretty much like using a newer setsockopt on an old system.",technical,Marcelo Ricardo Leitner,marcelo.leitner@gmail.com,1,0,663,0.20601503759398496,0.07692307692307693,0,0.0,1.0,0.0,0.0
1067,541450,541460,"With the current implementation, given sources that say are supposed to run on a 4.9 kernel (no use of any newer field added in 4.11 or 4.12), we can't rebuild the exact same sources on a 4.19 kernel and still run them on 4.9 without messing with structures re-definition. I understand your point, but this still looks like a sort of uapi breakage to me. I also had another way to work-around this in mind, by copying optlenbytes and checking that any additional field (not included in the ""current"" kernel structure definition) is not set, returning EINVAL in such case to keep a similar to current behavior. The issue with this is that I didn't find a suitable (i.e. not totally arbitrary such as ""twice the existing structure size"") upper limit to optlen.","On 2/6/19 12:37 PM, Marcelo Ricardo Leitner wrote:  With the current implementation, given sources that say are supposed to run on a 4.9 kernel (no use of any newer field added in 4.11 or 4.12), we can't rebuild the exact same sources on a 4.19 kernel and still run them on 4.9 without messing with structures re-definition.  I understand your point, but this still looks like a sort of uapi breakage to me.   I also had another way to work-around this in mind, by copying optlen bytes and checking that any additional field (not included in the current"" kernel structure definition) is not set, returning EINVAL in such case to keep a similar to current behavior. The issue with this is that I didn't find a suitable (ie not totally arbitrary such as ""twice the existing structure size"") upper limit to optlen. """,technical,Julien Gomes,julien@arista.com,0,1,758,0.23308270676691728,0.11538461538461539,0,0.0,1.0,0.0,0.0
1068,541450,541462,"I'm not sure I like this.  If you have a userspace application built against more recent uapi headers than the kernel you are actually running on, then by definition you won't have this check in place, and you'll get EINVAL returns anyway.  If you just backport this patch to an older kernel, you'll not get the EINVAL return, but you will get silent failures on event subscriptions that your application thinks exists, but the kernel doesn't recognize. This would make sense if you had a way to communicate back to user space the unrecognized options, but since we don't (currently) have that, I would rather see the EINVAL returned than just have things not work.","On Wed, Feb 06, 2019 at 12:14:30PM -0800, Julien Gomes wrote: I'm not sure I like this.  If you have a userspace application built against more recent uapi headers than the kernel you are actually running on, then by defintion you won't have this check in place, and you'll get EINVAL returns anyway.  If you just backport this patch to an older kernel, you'll not get the EINVAL return, but you will get silent failures on event subscriptions that your application thinks exists, but the kernel doesn't recognize.    This would make sense if you had a way to communicate back to user space the unrecognized options, but since we don't (currently) have that, I would rather see the EINVAL returned than just have things not work.  Neil",technical,Neil Horman,nhorman@tuxdriver.com,1,0,665,0.20150375939849624,0.15384615384615385,0,0.0,1.0,0.0,0.0
1069,541450,541474,"Maybe what we want(ed) here then is explicit versioning, to have the 3 definitions available. Then the application is able to use, say structsctp_event_subscribe, and be happy with it, while there is structsctp_event_subscribe_v2 and struct sctp_event_subscribe_v3 there too. But it's too late for that now because that would break applications already using the new fields in sctp_event_subscribe. Not disagreeing. I really just don't know how supported that is. Willing to know so I can pay more attention to this on future changes. Btw, is this the only occurrence? Seems interesting. Why would it need that upper limit to optlen?S ay struct v1 had 4 bytes, v3 now had 12. The user supplies 12 bytes to the kernel that only knows about 4 bytes. It can check that (12-4) bytes in the end, make sure no bit is on and use only the first 4. The fact that it was 12 or 200 shouldn't matter, should it? As long as the (200-4) bytes are 0'ed, only the first 4 will be used and it should be ok, otherwise EINVAL. No need to know how big the current current actually is because it wouldn't be validating that here: just that it can safely use the first 4 bytes.","On Wed, Feb 06, 2019 at 12:48:38PM -0800, Julien Gomes wrote:  Maybe what we want(ed) here then is explicit versioning, to have the 3 definitions available. Then the application is able to use, say struct sctp_event_subscribe, and be happy with it, while there is struct sctp_event_subscribe_v2 and struct sctp_event_subscribe_v3 there too.  But it's too late for that now because that would break applications already using the new fields in sctp_event_subscribe.   Not disagreeing. I really just don't know how supported that is. Willing to know so I can pay more attention to this on future changes.  Btw, is this the only occurrence?   Seems interesting. Why would it need that upper limit to optlen?  Say struct v1 had 4 bytes, v3 now had 12. The user supplies 12 bytes to the kernel that only knows about 4 bytes. It can check that (12-4) bytes in the end, make sure no bit is on and use only the first 4.  The fact that it was 12 or 200 shouldn't matter, should it? As long as the (200-4) bytes are 0'ed, only the first 4 will be used and it should be ok, otherwise EINVAL. No need to know how big the current current actually is because it wouldn't be validating that here: just that it can safely use the first 4 bytes.",technical,Marcelo Ricardo Leitner,marcelo.leitner@gmail.com,1,0,1155,0.3669172932330827,0.19230769230769232,0,0.0,1.0,0.0,0.0
1070,541450,541476,"What given sources say that?  I understand it might be expected, but this is a common concern with setsockopt method on many protocols, it just so happens that sctp extends them more than other protocols. Right, put another way, we support backward compatibility with older userspace applications, but not newer one.  I.e. if you build an application against the 4.9 SCTP API, it should work with the 4.19 UAPI, but not vice versa, which it seems is like what you are trying to do here.There is no real upper limit to the size of the structure in this case, and IIRC this isn't the only sockopt structure that can be extended for SCTP in this way. I really don't see a sane way to allow newer userspaces to be compatible withholder kernels here.  If we were to do it I would suggest moving the responsibility for that feature into lksctp-tools, versioning that library such that corollary symbols are versioned to translate the application view of the socket options structs to the size and format that the running kernel understands.  Note that I'm not really advocating for that, as it seems like a fast moving target, but if we were to do it I think that would be the most sane way to handle it.","On Wed, Feb 06, 2019 at 12:48:38PM -0800, Julien Gomes wrote: What given sources say that?  I understand it might be expected, but this is an common concern with setsockopt method on many protocols, it just so happens that sctp extends them more than other protocols.  Right, put another way, we support backward compatibility with older userspace applications, but not newer one.  I.e. if you build an application against the 4.9 SCTP API, it should work with the 4.19 UAPI, but not vice versa, which it seems is like what you are trying to do here.  There is no real uppper limit to the size of the structure in this case, and IIRC this isn't the only sockopt structure that can be exentded for SCTP in this way.  I really don't see a sane way to allow newer userspaces to be compatible with older kernels here.  If we were to do it I would suggest moving the responsibility for that feature into lksctp-tools, versioning that library such that correlary symbols are versioned to translate the application view of the socket options structs to the size and format that the running kernel undertands.  Note that I'm not really advocating for that, as it seems like a fast moving target, but if we were to do it I think that would be the most sane way to handle it.  Neil",technical,Neil Horman,nhorman@tuxdriver.com,1,0,1198,0.35639097744360904,0.23076923076923078,0,0.0,1.0,0.0,0.0
1071,541450,541493,"Was looking for that. Thanks. Speaking of that, recent lksctp-tools got some defines to help knowing which features the available kernel headers have as it now probes if specific struct members are available or not. Though yeah, it also wouldn't help in this case, just mentioning it.","On Wed, Feb 06, 2019 at 04:08:27PM -0500, Neil Horman wrote:  Was looking for that. Thanks.   Speaking of that, recent lksctp-tools got some defines to help knowing which features the available kernel headers have as it now probes if specific struct members are available or not. Though yeah, it also wouldn't help in this case, just mentioning it.",technical,Marcelo Ricardo Leitner,marcelo.leitner@gmail.com,1,0,284,0.08270676691729323,0.2692307692307692,0,0.0,1.0,0.0,0.0
1072,541450,541500,"Yeah, I'm not supportive of codifying that knowledge in the kernel.  If we were to support bi-directional versioning, I would encode it into lksctp-tools rather than the kernel. No, I think there are a few others (maybe paddr params?) I think the thought was to differentiate between someone passing a legit larger structure from a newer UAPI, from someone just passing in a massive inappropriately sized buffer (even if the return on both is the same). I'm less than excited about making the kernel check an unbounded user spacebuffer, that's seems like a potential DOS attack from an unprivileged user tome.  I'm also still hung up on the notion that, despite how we do this, this patch is going into the latest kernel, so it will only work on a kernel that already understands the most recent set of subscriptions.  It would work if we, again someday in the future extended this struct, someone built against that newer UAPI, and then tried to run it on a kernel that had this patch.FWIW, there is an existing implied method to determine the available subscription events. sctp_getsockopt_events does clamp the size of the output buffer, and returns that information in the optlen field via put_user.  An application that was build against UAPIs from 4.19 could pass in the 4.19sctp_event_subscribe struct to sctp_getsockopt_events, and read the output length, which would inform the application of the events that the kernel is capable of reporting, and limit itself to only using those events.  Its not a perfect solution, but its direct, understandable and portable.","On Wed, Feb 06, 2019 at 07:07:23PM -0200, Marcelo Ricardo Leitner wrote: Yeah, I'm not supportive of codifying that knoweldge in the kernel.  If we were to support bi-directional versioning, I would encode it into lksctp-tools rather than the kernel.  No, I think there are a few others (maybe paddrparams?)  I think the thought was to differentiate between someone passing a legit larger structure from a newer UAPI, from someone just passing in a massive inappropriately sized buffer (even if the return on both is the same).  I'm less than excited about making the kernel check an unbounded user space buffer, thats seems like a potential DOS attack from an unpriviledged user to me.  I'm also still hung up on the notion that, despite how we do this, this patch is going into the latest kernel, so it will only work on a kernel that already understands the most recent set of subscriptions.  It would work if we, again someday in the future extended this struct, someone built against that newer UAPI, and then tried to run it on a kernel that had this patch.  FWIW, there is an existing implied method to determine the available subscription events. sctp_getsockopt_events does clamp the size of the output buffer, and returns that information in the optlen field via put_user.  An application that was build against UAPIs from 4.19 could pass in the 4.19 sctp_event_subscribe struct to sctp_getsockopt_events, and read the output length, whcih would inform the application of the events that the kernel is capable of reporting, and limit itself to only using those events.  Its not a perfect solution, but its direct, understandable and portable.  Neil",technical,Neil Horman,nhorman@tuxdriver.com,1,0,1572,0.44360902255639095,0.3076923076923077,0,0.0,1.0,0.0,0.0
1073,541450,541502,"Right. Can't really say, this is one I witnessed, I haven't really looked for others. The upper limit concern is more regarding the call to copy_from_userwith an unrestricted/unchecked value. I am not sure of how much of a risk/how exploitable this could be, that's why I cautiously wanted to limit it in the first place just in case.","On 2/6/19 1:07 PM, Marcelo Ricardo Leitner wrote:  Right.   Can't really say, this is one I witnessed, I haven't really looked for others.   The upper limit concern is more regarding the call to copy_from_user with an unrestricted/unchecked value. I am not sure of how much of a risk/how exploitable this could be, that's why I cautiously wanted to limit it in the first place just in case.   --  Julien Gomes",technical,Julien Gomes,julien@arista.com,0,1,334,0.10225563909774436,0.34615384615384615,0,0.0,1.0,0.0,0.0
1074,541450,541509,"Copy_from_user should be safe to copy an arbitrary amount, the only restriction is that optlen can't exceed the size of the buffer receiving the data in the kernel.  From that standpoint your patch is safe.  However,  that exposes the problem of checking any tail data on the userspace buffer.  That is to say, if you want to ensure that any extra data that gets sent from userspace isn't 'set', you would have to copy that extra data in consumable chunks and check them individually, and that screams DOS to me (i.e. imagine a user passing in a 4GB buffer, and having to wait for the kernel to copy each X sized chunk, looking for non-zero values).","On Wed, Feb 06, 2019 at 01:26:55PM -0800, Julien Gomes wrote: Copy_from_user should be safe to copy an arbitrary amount, the only restriction is that optlen can't exceed the size of the buffer receiving the data in the kernel.  From that standpoint your patch is safe.  However,  that exposes the problem of checking any tail data on the userspace buffer.  That is to say, if you want to ensure that any extra data that gets sent from userspace isn't 'set', you would have to copy that extra data in consumable chunks and check them individaully, and that screams DOS to me (i.e. imagine a user passing in a 4GB buffer, and having to wait for the kernel to copy each X sized chunk, looking for non-zero values).",technical,Neil Horman,nhorman@tuxdriver.com,1,0,649,0.2,0.38461538461538464,0,0.0,1.0,0.0,0.0
1075,541450,541519,"And I was just reminded about huge pages. But still, my point of finding a compromise still stands.","On 2/6/19 1:48 PM, Julien Gomes wrote:  And I was just reminded about huge pages. But still, my point of finding a compromise still stands.",technical,Julien Gomes,julien@arista.com,0,1,99,0.031578947368421054,0.5,0,0.0,1.0,0.0,0.0
1076,541450,542131,"Probably not, but I'm not going to pick a magic number to gate what's ok and what's not for sockopt validation. We really don't have to, I refer you to my previous not referencing the fact that the getsockopt variant of this call will return the expected length of this option for the running kernel, allowing userspace to know explicitly what the buffer size should be, and by extension, what options are supported","On Wed, Feb 06, 2019 at 01:48:44PM -0800, Julien Gomes wrote: Probably not, but I'm not going to pick a magic number to gate whats ok and whats not for sockopt validation.  We really don't have to, I refer you to my previous not referencing the fact that the getsockopt variant of this call will return the expected length of this option for the running kernel, allowing userspace to know explicitly what the buffer size should be, and by extension, what options are supported  Neil",technical,Neil Horman,nhorman@tuxdriver.com,1,0,415,0.12481203007518797,0.5769230769230769,0,0.0,1.0,0.0,0.0
1077,541450,542319,"It is probably better to break the recompilation of the few programs that use the new fields (and have them not work on old kernels) than to stop recompilations of old programs stop working on old kernels or have requested new options silently ignored. There are all sorts of reasons why programs get built on systems that are newer than the ones they need to run on. I'm currently planning to get around the glibc 'mercy()' fubar so I can retire some very old build systems before their disks die. Fortunately our sctp code is in the kernel - so has to be compiled with the correct headers. Agreed, these structures should never be changed.","From: Marcelo Ricardo Leitner  It is probably better to break the recompilation of the few programs that use the new fields (and have them not work on old kernels) than to stop recompilations of old programs stop working on old kernels or have requested new options silently ignored.  There are all sorts of reasons why programs get built on systems that are newer than the ones they need to run on. I'm currently planning to get around the glibc 'memcpy()' fubar so I can retire some very old build systems before their disks die.  Fortunately our sctp code is in the kernel - so has to be compiled with the correct headers.   Agreed, these structures should never be changed.  	David  - Registered Address Lakeside, Bramley Road, Mount Farm, Milton Keynes, MK1 1PT, UK Registration No: 1397386 (Wales)",technical,David Laight,David.Laight@ACULAB.COM,0,0,641,0.19097744360902255,0.6153846153846154,0,0.0,0.8333333333333334,0.0,0.0
1078,541450,542328,"I got confused here, not sure what you mean. Seems there is one ""stop"" word too many. You can virtualize those. That's not really a good reason for building with newer kernel and running on old systems, as virtually any old system can be virtualized.","On Thu, Feb 07, 2019 at 05:33:07PM +0000, David Laight wrote:  I got confused here, not sure what you mean. Seems there is one stop"" word too many.   You can virtualize those. That's not really a good reason for building with newer kernel and running on old systems, as virtually any old system can be virtualized.    Marcelo """,technical,'Marcelo Ricardo Leitner',marcelo.leitner@gmail.com,1,0,250,0.081203007518797,0.6538461538461539,0,0.0,0.8333333333333334,0.0,0.0
1079,541450,542900,More confusing than I intended...With the current kernel and headers a 'new program' (one that needs the new options) will fail to run on an old kernel - which is good. However a recompilation of an 'old program' (that doesn't use the new options) will also fail to run on an old kernel - which is bad. Changing the kernel to ignore extra events flags breaks the 'new 'program. Versioning the structure now (even though it should have been done earlier) won't change the behaviour of existing binaries. However a recompilation of an 'old' program would use the 'old 'structure and work on old kernels. Attempts to recompile a 'new' program will fail - until the structure  name (or some #define to enable the extra fields) is changed. Breaking compilations is much better than unexpected run-time behaviour.,"From: 'Marcelo Ricardo Leitner' ...  More confusing than I intended...  With the current kernel and headers a 'new program' (one that needs the new options) will fail to run on an old kernel - which is good. However a recompilation of an 'old program' (that doesn't use the new options) will also fail to run on an old kernel - which is bad.  Changing the kernel to ignore extra events flags breaks the 'new' program.  Versioning the structure now (even though it should have been done earlier) won't change the behaviour of existing binaries.  However a recompilation of an 'old' program would use the 'old' structure and work on old kernels. Attempts to recompile a 'new' program will fail - until the structure name (or some #define to enable the extra fields) is changed.  Breaking compilations is much better than unexpected run-time behaviour.  	David  - Registered Address Lakeside, Bramley Road, Mount Farm, Milton Keynes, MK1 1PT, UK Registration No: 1397386 (Wales)",technical,David Laight,David.Laight@ACULAB.COM,0,0,807,0.24210526315789474,0.6923076923076923,0,0.16666666666666666,0.8333333333333334,0.0,0.0
1080,541450,544331,"What about reusing the same socket option, but defining a new struct? Say, MYSOCKOPT supports struct mysockopt, struct mysockopt2, structmysockopt3...That way we have a clear definition of the user's intent. I'm afraid clearing out may not be enough, though seems it's the best we can do so far. If the struct is allocated but not fully initialized via a memset, but by setting its fields one by one, the remaining newfields will be left uninitialized. One use case here is: a given distro is using kernel X and app Foo is built against it. Then upgrades to X+1, Foo is patched to fix an issue and is rebuilt against X+1. The user upgrades Foo package but for whatever reason, doesn't upgrade kernel or reboot the system. Here,Foo doesn't work anymore until the new kernel is also running.","On Sat, Feb 09, 2019 at 03:12:17PM -0800, David Miller wrote:  What about reusing the same socket option, but defining a new struct? Say, MYSOCKOPT supports struct mysockopt, struct mysockopt2, struct mysockopt3...  That way we have a clear definition of the user's intent.   I'm afraid clearing out may not be enough, though seems it's the best we can do so far. If the struct is allocated but not fully initialized via a memset, but by setting its fields one by one, the remaining new fields will be left uninitinialized.   One use case here is: a given distro is using kernel X and app Foo is built against it. Then upgrades to X+1, Foo is patched to fix an issue and is rebuilt against X+1. The user upgrades Foo package but for whatever reason, doesn't upgrade kernel or reboot the system. Here, Foo doesn't work anymore until the new kernel is also running.",technical,Marcelo Ricardo Leitner,marcelo.leitner@gmail.com,1,0,789,0.24661654135338346,0.8076923076923077,0,0.5,0.5,0.0,0.0
1081,541450,544430,"Need to clarify the ""clearing out"", I think it was meant differently. It was more about on how to ensure that the 16-bytes long of the v3 supplied to a v1-only kernel is compatible with the 12-bytes long v1. The kernel would have to check the trailing 4 bytes after v1-size and make sure they are all zeroed in order for the old kernel to accept it as a v1. But, as I said above, there are situations that this will not be enough. We have issues on read path too.","On Sun, Feb 10, 2019 at 10:46:16AM -0200, Marcelo Ricardo Leitner wrote:  Need to clarify the clearing out"", I think it was meant differently. It was more about on how to ensure that the 16-bytes long of the v3 supplied to a v1-only kernel is compatible with the 12-bytes long v1. The kernel would have to check the trailing 4 bytes after v1-size and make sure they are all zeroed in order for the old kernel to accept it as a v1. But, as I said above, there are situations that this will not be enough.   We have issues on read path too. 52ccb8e90c0a (""[SCTP]: Update SCTP_PEER_ADDR_PARAMS socket option to the latest api draft."") extended struct sctp_paddrparams and its getsockopt goes with:  sctp_getsockopt_peer_addr_params() ...         if (len < sizeof(struct sctp_paddrparams))                 return -EINVAL,         len = sizeof(struct sctp_paddrparams),  By then, we didn't have the /uapi/ folder yet. There may other cases.""",technical,Marcelo Ricardo Leitner,marcelo.leitner@gmail.com,1,0,463,0.1518796992481203,0.8461538461538461,0,0.6666666666666666,0.3333333333333333,0.0,0.0
1082,541450,545750,"That's possible, but I think that's pretty equivalent to what daves saying, in that he wants us to identify all the sizes of this struct and the git history and act on them accordingly.  Having internal versions of the struct seems like a fine way to get there, but I think we need to consider how we got to this situations before we go down the implementation path. I'm not sure this even makes sense.  Currently (as I understood it), the issue we are facing is the one in which an application is built against a newer kernel and run on an older one, the implication there being that the application will pass in a buffer that is larger than what the kernel expects.  In that situation, clearing isn't needed, all that's needed (I think), is a memcmp of the space between the sizeof(kernel struct version), and sizeof(userspace struct version) to see if any bits are non-zero.  If they are, we error out, otherwise, we ignore the space and move forward as though that overage doesn't exist. Mind you, I'm not (yet) advocating for that approach, just trying to clarify what's needed. Yes, that's the use case that we're trying to address. FWIW, before we decide on a course of action, I think I need to point out that, over the last 10 years, we've extended this structure 6 times, in the following commits. The first two I believe were modifications during a period when sctp was actually getting integrated to the kernel, but the last 4 were definitely done during more recent development periods and won't in without any commentary about the impact to UAPI compatibility.  The check for optlen > sizeof(structsctp_event_subscribe) was made back in 2008, and while not spelled out, seems pretty clearly directed at enforcing compatibility with older applications, not compatibility with newer applications running on older kernels. I really worry about situations in which we need to support applications expecting features that the running kernel doesn't have.  In this particular situation it seems like a fixable thing, but I could envision situations in which we just can't do it, and I don't want to set that expectation when we can't consistently meet it.So, if the consensus is that we need to support applications built on newer kernels, but run on older kernels (and I'd like to get verbal consensus on that), then we need to identify a method to fix this.  I'm still hesitant to do anything that involves us accepting any size buffer over the kernel expected size, as that puts us in a position to have to read large amounts of user data(i.e. possible DOS), and just picking an arbitrary large number to limit the buffer size seems wrong.  What if, on receipt of a structure from a newer kernel (implying a size larger than what the kernel expects), we clamp optlen to the kernel size, and put_user it back to the application?  i.e. we don't check any data above and beyond what the the kernel knows about, but we use the optlen as an indicator to user space that not all the data was processed?  That allows the kernel to ignore the overage safely, and while its not in the socket api extension RFC, its not violating anything, and is something we can document in the sctp(7) man page as a linux only behavior. Thoughts?","On Sun, Feb 10, 2019 at 10:46:16AM -0200, Marcelo Ricardo Leitner wrote: Thats possible, but I think thats pretty equivalaent to what daves saying, in that he wants us to identify all the sizes of this struct and the git history and act on them accordingly.  Having internal versions of the struct seems like a fine way to get there, but I think we need to consider how we got to this situations before we go down the implementation path.   I'm not sure this even makes sense.  Currently (as I understood it), the issue we are facing is the one in which an application is built against a newer kernel and run on an older one, the implication there being that the application will pass in a buffer that is larger than what the kernel expects.  In that situation, clearing isn't needed, all thats needed (I think), is a memcmp of the space between the sizeof(kernel struct version), and sizeof(userspace struct version) to see if any bits are non-zero.  If they are, we error out, otherwise, we ignore the space and move forward as though that overage doesn't exist.  Mind you, I'm not (yet) advocating for that approach, just trying to clarify whats needed. Yes, thats the use case that we're trying to address.  FWIW, before we decide on a course of action, I think I need to point out that, over the last 10 years, we've extended this structure 6 times, in the following commits: 0f3fffd8ab1db 7e8616d8e7731 e1cdd553d482c 35ea82d611da5 c95129d127c6d b444153fb5a64  The first two I believe were modifications during a period when sctp was actually getting integrated to the kernel, but the last 4 were definately done during more recent development periods and wen't in without any commentary about the impact to UAPI compatibility.  The check for optlen > sizeof(struct sctp_event_subscribe) was made back in 2008, and while not spelled out, seems pretty clearly directed at enforcing compatibility with older appliations, not compatibility with newer applications running on older kernels.  I really worry about situations in which we need to support applications expecting features that the running kernel doesn't have.  In this particular situation it seems like a fixable thing, but I could envision situations in which we just can't do it, and I don't want to set that expectation when we can't consistently meet it.  So, if the consensus is that we need to support applications built on newer kernels, but run on older kernels (and I'd like to get verbal consensus on that), then we need to identify a method to fix this.  I'm still hesitant to do anything that involves us accepting any size buffer over the kernel expected size, as that puts us in a position to have to read large amounts of user data (i.e. possible DOS), and just picking an arbitrary large number to limit the buffer size seems wrong.  What if, on receipt of a structure from a newer kernel (implying a size larger than what the kernel expects), we clamp optlen to the kernel size, and put_user it back to the application?  i.e. we don't check any data above and beyond what the the kernel knows about, but we use the optlen as an indicator to user space that not all the data was processed?  That allows the kernel to ignore the overage safely, and while its not in the socket api extension RFC, its not violating anything, and is something we can document in the sctp(7) man page as a linux only behavior.  Thoughts? Neil",technical,Neil Horman,nhorman@tuxdriver.com,1,0,3233,1.0,0.8846153846153846,0,0.6666666666666666,0.3333333333333333,0.0,0.0
1083,541450,546322,"I was more referring to future stuff, but yes. I find it a bit easier to handle than having to switch the sockopt too and so far I couldn't find drawbacks to it.That is, when using a new sockopt, we could accept a buffer larger than the needed, but I'm not considering that as a valid point anymore. Putting this compatibility aside for a moment, that pretty much means the user doesn't know what it wants and so we also don't. That's exactly what I tried to mean. :-)Ok. Yes from my side. We can't do that on setsockopt calls, as optlen is R/O there. Returning > 0 is not specified on setsockopt(2). I also need to dig deeper on this, but in general what if we draw a line based on the current implementation:- Current struct is X bytes long- Patch current and older kernels to accept up to X bytes, as long as  the trailing bytes are zeroed. Otherwise, EINVAL.  X may be a magic number for old kernel, but this way we avoid unbounded buffers and the limit is not random.- On further changes, create a new, explicitly versioned struct.  Older kernels will EINVAL if this new struct is used, which is  expected.  Newer kernels will then have to cope with the different  sizes/structs accordingly.","On Mon, Feb 11, 2019 at 10:04:32AM -0500, Neil Horman wrote:  I was more referring to future stuff, but yes. I find it a bit easier to handle than having to switch the sockopt too and so far I couldn't find drawbacks to it.  That is, when using a new sockopt, we could accept a buffer larger than the needed, but I'm not considering that as a valid point anymore. Putting this compatibility aside for a moment, that pretty much means the user doesn't know what it wants and so we also don't.   That's exactly what I tried to mean. :-)   Ok.   Yes from my side.   We can't do that on setsockopt calls, as optlen is R/O there. Returning > 0 is not specified on setsockopt(2).   I also need to dig deeper on this, but in general what if we draw a line based on the current implementation: - Current struct is X bytes long - Patch current and older kernels to accept up to X bytes, as long as   the trailing bytes are zeroed. Otherwise, EINVAL.   X may be a magic number for old kernel, but this way we avoid   unbounded buffers and the limit is not random. - On further changes, create a new, explicitly versioned struct.   Older kernels will EINVAL if this new struct is used, which is   expected.   Newer kernels will then have to cope with the different   sizes/structs accordingly.     Marcelo",technical,Marcelo Ricardo Leitner,marcelo.leitner@gmail.com,1,0,1196,0.3879699248120301,0.9230769230769231,0,0.6666666666666666,0.3333333333333333,0.0,0.16666666666666666
1084,541450,550047,The API shouldn't change like this at all. Is this from the RFC or elsewhere??If the structure changes the socket option name and value should also change. IMHO large chunks of the sctp rfc are just horrid. In particular all the places where is states that API functions are implemented using setsockopt() - that should be an implementation detail. Also ISTR that some of the structures are defined to have holes in them...,"From: Marcelo Ricardo Leitner ...  The API shouldn't change like this at all. Is this from the RFC or elsewhere??  If the structure changes the socket option name and value should also change.  IMHO large chunks of the sctp rfc are just horrid. In particular all the places where is states that API functions are implemented using setsockopt() - that should be an implementation detail. Also ISTR that some of the structures are defined to have holes in them...  	David  - Registered Address Lakeside, Bramley Road, Mount Farm, Milton Keynes, MK1 1PT, UK Registration No: 1397386 (Wales)",technical,David Laight,David.Laight@ACULAB.COM,0,0,423,0.12631578947368421,0.9615384615384616,0,1.0,0.0,0.16666666666666666,0.0
1085,541450,550115,"I would think so. That commit is from 2005, pretty close to initial SCTP RFCs. That's what is at the core of this thread.","On Wed, Feb 13, 2019 at 04:17:41PM +0000, David Laight wrote:  I would think so. That commit is from 2005, pretty close to initial SCTP RFCs.   That's what is at the core of this thread.    Marcelo",technical,'Marcelo Ricardo Leitner',marcelo.leitner@gmail.com,1,0,121,0.04360902255639098,1.0,1,1.0,0.0,0.0,0.0
1086,541575,542252,Hello!    field's.,"Hello!  On 07.02.2019 3:03, Andrey Smirnov wrote:       field's.  [...]  MBR, Sergei",technical,Sergei Shtylyov,sergei.shtylyov@cogentembedded.com,0,0,18,0.5,0.6666666666666666,0,0.0,0.0,0.0,0.0
1087,541575,542510,"Oops, missed that. Will fix in v2.","On Thu, Feb 7, 2019 at 8:44 AM Sergei Shtylyov <sergei.shtylyov@cogentembedded.com> wrote:  Oops, missed that. Will fix in v2.  Thanks, Andrey Smirnov",technical,Andrey Smirnov,andrew.smirnov@gmail.com,0,1,34,1.0,1.0,1,0.0,0.0,0.0,0.0
1088,541978,542695,"You probably can have a __init buffer somewhere in ppc code, append data to it, step by step, and call dump_stack_set_arch_desc() all the time. But no real objections, dump_stack_add_arch_desc() can do.","Cc-ing Steven    https://lore.kernel.org/lkml/20190207124635.3885-1-mpe@ellerman.id.au/T/#u  On (02/07/19 23:46), Michael Ellerman wrote:  You probably can have a __init buffer somewhere in ppc code, append data to it, step by step, and call dump_stack_set_arch_desc() all the time.  But no real objections, dump_stack_add_arch_desc() can do.  FWIW, Reviewed-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>  	-ss",technical,Sergey Senozhatsky,sergey.senozhatsky.work@gmail.com,1,0,202,0.25153374233128833,0.5,0,0.0,1.0,0.0,0.0
1089,541978,543599,This shows me that this can be called at a time when more than one CPU is active. What happens if we have two CPUs calling dump_stack_add_arch_desc() at the same time? Can't that corrupt the dump_stack_arch_desc_str?,"On Thu, Feb 07, 2019 at 11:46:29PM +1100, Michael Ellerman wrote:  This shows me that this can be called at a time when more than one CPU is active. What happens if we have two CPUs calling dump_stack_add_arch_desc() at the same time? Can't that corrupt the dump_stack_arch_desc_str?  -- Steve",technical,Steven Rostedt,rostedt@goodmis.org,1,0,216,0.25766871165644173,0.5625,0,0.07692307692307693,0.9230769230769231,0.0,0.15384615384615385
1090,541978,544613,"Can overwrite part of it, I guess (but it seems that Michael is OK with this). The string is still NULL terminated. The worst case scenario I can think of is not the one when two CPUs call dump_stack_add_arch_desc(), but when CPUA calls dump_stack_add_arch_desc() to append some data and at the same time CPUB calls dump_stack_set_arch_desc() and simply overwrites dump_stack_arch_desc_str. Not sure if this is critical (or possible).","On (02/08/19 13:55), Steven Rostedt wrote: [..]  Can overwrite part of it, I guess (but it seems that Michael is OK with this). The string is still NULL terminated.  The worst case scenario I can think of is not the one when two CPUs call dump_stack_add_arch_desc(), but when CPUA calls dump_stack_add_arch_desc() to append some data and at the same time CPUB calls dump_stack_set_arch_desc() and simply overwrites dump_stack_arch_desc_str. Not sure if this is critical (or possible).  	-ss",technical,Sergey Senozhatsky,sergey.senozhatsky.work@gmail.com,1,0,434,0.5153374233128835,0.625,0,0.23076923076923078,0.7692307692307693,0.15384615384615385,0.0
1091,541978,544931,"The comment doesn't say _why_ we need to order these stores: IOW, what will or can go wrong without this order?  This isn't clear to me. Another good practice when adding constructs is to indicate the matching construct/synch. mechanism.","Hi Michael,   On Thu, Feb 07, 2019 at 11:46:29PM +1100, Michael Ellerman wrote:  The comment doesn't say _why_ we need to order these stores: IOW, what will or can go wrong without this order?  This isn't clear to me.  Another good practice when adding smp_*-constructs (as discussed, e.g., at KS'18) is to indicate the matching construct/synch. mechanism.    Andrea",technical,Andrea Parri,andrea.parri@amarulasolutions.com,1,0,237,0.2883435582822086,0.6875,0,0.3076923076923077,0.6923076923076923,0.0,0.0
1092,541978,546100,"Yes, one barrier without a counter-part is suspicious. If the parallel access is really needed then we could define the current length as atomic_t and use: + atomic_cmpxchg() to reserve the space for the string + %*s to limit the printed length In the worst case, we would print an incomplete string. See below for a sample code.BTW: There are very few users of dump_stack_set_arch_desc(). I would use dump_stack_add_arch_desc() everywhere to keep it simple and have a reasonable semantic. This is what I mean (only compile tested)","On Mon 2019-02-11 13:50:35, Andrea Parri wrote:  Yes, one barrier without a counter-part is suspicious.  If the parallel access is really needed then we could define the current length as atomic_t and use:  	+ atomic_cmpxchg() to reserve the space for the string 	+ %*s to limit the printed length  In the worst case, we would print an incomplete string. See below for a sample code.   BTW: There are very few users of dump_stack_set_arch_desc(). I would use dump_stack_add_arch_desc() everywhere to keep it simple and have a reasonable semantic.   This is what I mean (only compile tested):  diff --git a/lib/dump_stack.c b/lib/dump_stack.c index 5cff72f18c4a..311dd20cc6a7 100644 --- a/lib/dump_stack.c +++ b/lib/dump_stack.c @@ -14,9 +14,10 @@  #include <linux/utsname.h>    static char dump_stack_arch_desc_str[128], +static atomic_t arch_desc_str_len,    /** - * dump_stack_set_arch_desc - set arch-specific str to show with task dumps + * dump_stack_set_arch_desc - add arch-specific str to show with task dumps   * @fmt: printf-style format string   * @...: arguments for the format string   * @@ -25,13 +26,32 @@ static char dump_stack_arch_desc_str[128],   * arch wants to make use of such an ID string, it should initialize this   * as soon as possible during boot.   */ -void __init dump_stack_set_arch_desc(const char *fmt, ...) +void __init dump_stack_add_arch_desc(const char *fmt, ...)  { -	va_list args, +	va_list args, args2, +	int len, cur_len, old_len,    	va_start(args, fmt), -	vsnprintf(dump_stack_arch_desc_str, sizeof(dump_stack_arch_desc_str), + +	va_copy(args2, args), +	len = vsnprintf(NULL, sizeof(dump_stack_arch_desc_str), +			fmt, args2), +	va_end(args2), + +try_again: +	cur_len = atomic_read(&arch_desc_str_len), +	if (cur_len + len > sizeof(dump_stack_arch_desc_str)) +		goto out, + +	old_len = atomic_cmpxchg(&arch_desc_str_len, +				 cur_len, cur_len + len), +	if (old_len != cur_len) +		goto try_again, + +	vsnprintf(dump_stack_arch_desc_str + old_len, +		  sizeof(dump_stack_arch_desc_str) - old_len,  		  fmt, args), +out:  	va_end(args),  }   @@ -44,6 +64,8 @@ void __init dump_stack_set_arch_desc(const char *fmt, ...)   */  void dump_stack_print_info(const char *log_lvl)  { +	int len, +  	printk(%sCPU: %d PID: %d Comm: %.20s %s%s %s %.*s\n"",  	       log_lvl, raw_smp_processor_id(), current->pid, current->comm,  	       kexec_crash_loaded() ? ""Kdump: loaded "" : "", @@ -52,9 +74,11 @@ void dump_stack_print_info(const char *log_lvl)  	       (int)strcspn(init_utsname()->version, "" ""),  	       init_utsname()->version),   -	if (dump_stack_arch_desc_str[0] != '\0') -		printk(""%sHardware name: %s\n"", -		       log_lvl, dump_stack_arch_desc_str), +	len = atomic_read(&arch_desc_str_len), +	if (len) { +		printk(""%sHardware name: %*s\n"", +		       log_lvl, len, dump_stack_arch_desc_str), +	}    	print_worker_info(log_lvl, current),  }  Best Regards, Petr""",technical,Petr Mladek,pmladek@suse.com,1,0,531,0.6441717791411042,0.75,0,0.3076923076923077,0.6923076923076923,0.0,0.6153846153846154
1093,541978,556653,"As is this silence..., what happened to this patch? did you submit a new version? Seems worth exploring, IMO, but I'd like to first hear _clear about the _intended semantics (before digging into alternatives)...,  who first raised the question about ""parallel accesses""","On Mon, Feb 11, 2019 at 03:38:59PM +0100, Petr Mladek wrote:  As is this silence...,  Michael, what happened to this patch? did you submit a new version?    Seems worth exploring, IMO, but I'd like to first hear _clear about the _intended semantics (before digging into alternatives)...  +rostedt,  who first raised the question about parallel accesses""    http://lkml.kernel.org/r/20190208185515.r6vkrezbd3odhpxt@home.goodmis.org    Andrea  """,technical,Andrea Parri,andrea.parri@amarulasolutions.com,1,0,269,0.3374233128834356,0.8125,0,0.9230769230769231,0.07692307692307693,0.6153846153846154,0.0
1094,541978,557001,"No, I'm just busy, it's the merge window next week :) I thought the comment was pretty clear, if the stores are observed out of order we might print the uninitialized tail. And the barrier on the read side would need to be in printk somewhere, which is obviously unpleasant. It is not my intention to support concurrent updates of the string. The idea is you setup the string early in boot. The concern with a concurrent reader is simply that the string is dumped in the panic path, and you never really know when you're going to panic. Even if you only write to the string before doing SMP bring up you might still have another CPU go rogue and panic before then. But I probably should have just not added the barrier, it's over paranoid and will almost certainly never matter in practice.","Andrea Parri <andrea.parri@amarulasolutions.com> writes:  No, I'm just busy, it's the merge window next week :)  I thought the comment was pretty clear, if the stores are observed out of order we might print the uninitialised tail.  And the barrier on the read side would need to be in printk somewhere, which is obviously unpleasant.   It is not my intention to support concurrent updates of the string. The idea is you setup the string early in boot.  The concern with a concurrent reader is simply that the string is dumped in the panic path, and you never really know when you're going to panic. Even if you only write to the string before doing SMP bringup you might still have another CPU go rogue and panic before then.  But I probably should have just not added the barrier, it's over paranoid and will almost certainly never matter in practice.  cheers",technical,Michael Ellerman,mpe@ellerman.id.au,1,1,790,1.0,0.875,0,0.9230769230769231,0.0,0.0,0.0
1095,541978,557292,"Got it. Indeed. Understood, thanks for the clarification. OH, well, I can only echo you: if you don't care about the stores being_observed_ out of order, you could simply remove the barrier, if you do care, then you need ""more paranoid"" on the readers side.  ,-)"," Got it.    Indeed.    Understood, thanks for the clarification.    Oh, well, I can only echo you: if you don't care about the stores being _observed_ out of order, you could simply remove the barrier, if you do care, then you need more paranoid"" on the readers side.  ,-)    Andrea  """,technical,Andrea Parri,andrea.parri@amarulasolutions.com,1,0,262,0.3803680981595092,0.9375,0,1.0,0.0,0.0,0.0
1096,541978,558914,"Hmm, the barrier might be fine and actually useful. The purpose is to make sure that the later '\0' is written before the existing one is replaced by ' '.The reader does not need the barrier as long as it reads the string sequentially. I would expect that it is always the case. But who knows with all the speculation-related CPU bugs around. In each case, any race could never crash the kernel. The dump_stack_arch_desc_str is zeroed out of box and the very last '\0' is never rewritten.","On Wed 2019-02-20 14:44:33, Andrea Parri wrote:  Hmm, the barrier might be fine and actually useful. The purpose is to make sure that the later '\0' is written before the existing one is replaced by ' '.  The reader does not need the barrier as long as it reads the string sequentially. I would expect that it is always the case. But who knows with all the speculation-related CPU bugs around.  In each case, any race could never crash the kernel. The dump_stack_arch_desc_str is zeroed out of box and the very last '\0' is never rewritten.  Best Regards, Petr",technical,Petr Mladek,pmladek@suse.com,1,0,488,0.6012269938650306,1.0,1,1.0,0.0,0.0,0.0
1097,543147,724889,"Please don't make this a kconfig option. According to you description this is a HW feature, so please add the maxnumber of messe objects to ""struct c_can_driver_data"" and adjust the drvdata accordingly. You probably have to pass it via ""struct c_can_priv"" so that it's available in the c_can.","On 2/8/19 2:17 PM, Andrejs Cainikovs wrote:  Please don't make this a kconfig option.  According to you description this is a HW feature, so please add the max number of messe objects to struct c_can_driver_data"" and adjust the drvdata accordingly.   You probably have to pass it via ""struct c_can_priv"" so that it's available in the c_can.c  Marc  --  Pengutronix e.K.                  | Marc Kleine-Budde           | Industrial Linux Solutions        | Phone: +49-231-2826-924     | Vertretung West/Dortmund          | Fax:   +49-5121-206917-5555 | Amtsgericht Hildesheim, HRA 2686  | http://www.pengutronix.de   |  """,technical,Marc Kleine-Budde,mkl@pengutronix.de,1,0,292,1.0,1.0,1,1.0,0.0,1.0,0.0
1098,546823,546993,This still lacks a prototype.,"On Tue, 12 Feb 2019, Aubrey Li wrote:   This still lacks a prototype.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,29,0.07228915662650602,0.2857142857142857,0,0.0,0.0,0.0,0.0
1099,546823,547004,Aside of that this doesn't apply to tip or Linus tree....,"On Tue, 12 Feb 2019, Thomas Gleixner wrote:   Aside of that this doesn't apply to tip or Linus tree....  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,57,0.1566265060240964,0.35714285714285715,0,0.0,0.0,0.0,0.0
1100,546823,547010,Sigh. This is absolutely the wrong place. The weak function is declared and used in fs/proc/... So the prototype wants to be in a header which is included from there independent of x86…,"On Tue, 12 Feb 2019, Aubrey Li wrote:  Sigh. This is absolutely the wrong place. The weak function is declared and used in fs/proc/... So the prototype wants to be in a header which is included from there independent of x86...  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,185,0.43373493975903615,0.42857142857142855,0,0.0,0.0,0.0,0.0
1101,546823,547072,Can the prototype be in the architecture header if they want to call the function? Like the following? Meminfo is used in this as well.,"On 2019/2/12 16:22, Thomas Gleixner wrote:  Can the prototype be in the architecture header if they want to call the function? Like the following? arch_report_meminfo() is used in fs/proc/... as well.  $ find . -name *.h | xargs grep arch_report_meminfo ./arch/s390/include/asm/pgtable.h:void arch_report_meminfo(struct seq_file *m), ./arch/x86/include/asm/pgtable_types.h:extern void arch_report_meminfo(struct seq_file *m), ./arch/parisc/include/asm/pgtable.h:extern void arch_report_meminfo(struct seq_file *m),  Thanks, -Aubrey",technical,"Li, Aubrey",aubrey.li@linux.intel.com,0,0,135,0.3373493975903614,0.5,0,0.0,0.0,0.0,0.0
1102,546823,547105,"Actually both way exist in the current kernel, the reason I chose to put the prototype into architecture header file is that I found some architectures rename the function name by a micro definition while others use prototype. See below, This looks more flexible than it in the common header file.  Anyway, putting the prototype into the common header file like include/linux/proc_fs.his also acceptable to me if you persist, please just let me know, :)","On 2019/2/12 17:14, Li, Aubrey wrote:  Actually both way exist in the current kernel,the reason I chose to put the prototype into architecture header file is that I found some architectures rename the function name by a micro definition while others use prototype. See below:  $ find . -name *.h | xargs grep arch_irq_stat ./arch/arm64/include/asm/hardirq.h:#define arch_irq_stat_cpu	smp_irq_stat_cpu ./arch/arm/include/asm/hardirq.h:#define arch_irq_stat_cpu	smp_irq_stat_cpu ./arch/x86/include/asm/hardirq.h:extern u64 arch_irq_stat_cpu(unsigned int cpu),  This looks more flexible than it in the common header file.  Anyway, putting the prototype into the common header file like include/linux/proc_fs.h is also acceptable to me if you persist, please just let me know, :)  Thanks, -Aubrey",technical,"Li, Aubrey",aubrey.li@linux.intel.com,0,0,453,1.0,0.5714285714285714,0,0.0,0.0,0.0,0.0
1103,546823,547224,Basic C programming course: The prototype must be available before the declaration of the global function.fs/proc/array.c: Oh well....,"On Tue, 12 Feb 2019, Li, Aubrey wrote:  Basic C programming course:   The prototype must be available before the declaration of the global  function.  fs/proc/array.c:404:13: warning: no previous prototype for ‘arch_proc_pid_status’ [-Wmissing-prototypes]  void __weak arch_proc_pid_status(struct seq_file *m, struct task_struct *task)  Oh well....  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,134,0.25301204819277107,0.6428571428571429,0,0.0,0.0,0.0,0.0
1104,546823,547253,"Is this because patch 1/3 applied alone? If the whole patch set are applied,the prototype is included, which is at the beginning of array.c file, so it is available before the declaration.","On 2019/2/12 19:19, Thomas Gleixner wrote:  Is this because patch 1/3 applied alone? If the whole patch set are applied, the prototype is included in <asm/processor.h>, which is at the beginning of array.c file, so it is available before the declaration.  Thanks, -Aubrey",technical,"Li, Aubrey",aubrey.li@linux.intel.com,0,0,188,0.4578313253012048,0.7857142857142857,0,0.0,0.0,0.0,0.0
1105,546823,547254,"Okay, will cook a new version to put it into the common header.","On 2019/2/12 19:27, Thomas Gleixner wrote:  Okay, will cook a new version to put it into the common header.  Thanks, -Aubrey",technical,"Li, Aubrey",aubrey.li@linux.intel.com,0,0,63,0.18072289156626506,0.8571428571428571,0,0.0,0.0,0.0,0.0
1106,546823,547259,1) Each patch has to be correct stand alone 2) This file is compiled for every architecture the kernel supports and how   many of them are including this?   There is a world outside x86 and it's rather large.,"On Tue, 12 Feb 2019, Li, Aubrey wrote:  1) Each patch has to be correct stand alone  2) This file is compiled for every architecture the kernel supports and how    many of them are including arch/x86/include/asm/processor.h ?     There is a world outside x86 and it's rather large.  Thanks,  	tglx",technical,Thomas Gleixner,tglx@linutronix.de,1,0,208,0.5180722891566265,0.9285714285714286,0,0.0,0.0,0.0,0.0
1107,546823,547264,"Oh, I like this, thanks!","On 2019/2/12 19:55, Thomas Gleixner wrote:  Got it, thanks!",technical,"Li, Aubrey",aubrey.li@linux.intel.com,0,0,24,0.0963855421686747,1.0,1,0.0,0.0,0.0,0.0
1108,548527,548665,"I'm not exactly sure how Linux switch driver works, but from DT perspective I think we should rather have *hardware* described instead of a common Linux case. If I'm right, we should rather have all 3 switch ports described (5, 7,8) and have Linux just use the one it needs.","On 13.02.2019 05:25, Florian Fainelli wrote:  I'm not exactly sure how Linux switch driver works, but from DT perspective I think we should rather have *hardware* described instead of a common Linux case.  If I'm right, we should rather have all 3 switch ports described (5, 7, 8) and have Linux just use the one it needs.",technical,Rafał Miłecki,zajec5@gmail.com,1,0,274,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
1109,548527,550116,"Yes, let's do that.","On 2/13/19 12:07 AM, Rafał Miłecki wrote:  Yes, let's do that. --  Florian",technical,Florian Fainelli,florian.fainelli@broadcom.com,1,0,19,0.11475409836065574,1.0,1,0.0,0.0,0.0,0.0
1110,550227,550241,"Which I'm making it a priority to go and review and apply (if there's no issues) right now ,-)","On Wed, 13 Feb 2019 12:17:51 -0600 Tom Zanussi <zanussi@kernel.org> wrote:     Which I'm making it a priority to go and review and apply (if there's no issues) right now ,-)  -- Steve",technical,Steven Rostedt,rostedt@goodmis.org,1,0,94,0.5555555555555556,0.6363636363636364,0,0.0,0.0,0.0,0.0
1111,550227,550882,"Thank you for your great work! I like this very much! One point I would like to comment is to add a kind of entry number tag, so that user distinguish the error message, e.g. What would you think?","Hi Tom,  Thank you for your great work!  On Wed, 13 Feb 2019 12:17:51 -0600 Tom Zanussi <zanussi@kernel.org> wrote:   I like this very much! One point I would like to comment is to add a kind of entry number tag, so that user distinguish the error message, e.g.    # cat /sys/kernel/debug/tracing/error_log   [1] hist:sched:sched_wakeup: error: Variable already defined     Command: keys=pid:ts0=common_timestamp.usecs if comm==cyclictest""                     ^   [2] hist:sched:sched_waking: error: Couldn't find onmax or onchange variable     Command: key=comm:p=prio:onchange($q).snapshot()                                        ^   [3] hist:sched:sched_wakeup: error: Hist trigger already exists     Command: keys=pid              ^   ...  What would you think?  Thank you,  --  Masami Hiramatsu <mhiramat@kernel.org>""",technical,Masami Hiramatsu,mhiramat@kernel.org,1,0,196,1.0,0.7272727272727273,0,0.0,0.0,0.0,0.0
1112,550227,550881,"OK, can I take this over? I would like to try to use this framework. Yes, that is much better, especially for ftrace test.","Hi Tom,  On Wed, 13 Feb 2019 12:17:55 -0600 Tom Zanussi <zanussi@kernel.org> wrote:   OK, can I take this over? I would like to try to use this framework.   Yes, that is much better, especially for ftracetest.   Thank you,     --  Masami Hiramatsu <mhiramat@kernel.org>",technical,Masami Hiramatsu,mhiramat@kernel.org,1,0,122,0.6666666666666666,0.8181818181818182,0,0.0,0.0,0.0,0.0
1113,550227,551590,Thanks![snip]I think that makes sense and would be simple to add - will do in the next version.,"Hi Masami,  On Thu, 2019-02-14 at 12:13 +0900, Masami Hiramatsu wrote:  Thanks!   [snip]   I think that makes sense and would be simple to add - will do in the next version.  Thanks,  Tom",technical,Tom Zanussi,zanussi@kernel.org,0,1,95,0.5333333333333333,0.9090909090909091,0,0.0,0.0,0.0,0.0
1114,550227,551592,"Sure, be my guest. ,-)","Hi Masami,  On Thu, 2019-02-14 at 12:13 +0900, Masami Hiramatsu wrote:  Sure, be my guest. ,-)  Thanks,  Tom",technical,Tom Zanussi,zanussi@kernel.org,0,1,22,0.2,1.0,1,0.0,0.0,0.0,0.0
1115,551005,555177,What's the range of valid values? What's the range of valid values? Should be 2 entries here since there are 2 channels? Or the description above is wrong.,"On Thu, Feb 14, 2019 at 03:40:58PM +0800, shun-chih.yu@mediatek.com wrote:  Drop this.   What's the range of valid values?   What's the range of valid values?   Should be 2 entries here since there are 2 channels? Or the description  above is wrong.",technical,Rob Herring,robh@kernel.org,1,0,155,1.0,1.0,1,1.0,0.0,1.0,0.0
1116,551211,555279,"As you get access to the struct udevice, prefer dev_err() here. Ditto","On Thu, 14 Feb 2019 11:25:49 +0100, Fabrice Gasnier wrote:  Reviewed-by: Rob Herring <robh@kernel.org>",technical,Rob Herring,robh@kernel.org,1,0,69,0.3333333333333333,0.5555555555555556,0,1.0,0.0,1.0,0.0
1117,551211,555303,"Hello, Maybe use this? broken indention.","Hello,  On Thu, Feb 14, 2019 at 11:25:51AM +0100, Fabrice Gasnier wrote:  Maybe use dev_warn(pwm->chip->dev, ...) ?   broken indention.   Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,40,0.1875,0.6666666666666666,0,1.0,0.0,0.0,0.0
1118,551211,555899,"I'm wondering a bit about this: In this case, the caller that doesn't provide a struct device *, PWM provider isn't responsible for that. So I just hope this wouldn't be miss-leading ? Oops, I'll fix it.","On 2/18/19 6:22 PM, Uwe Kleine-König wrote:  Hi Uwe,  I'm wondering a bit about this: In this case, the caller that doesn't provide a struct device *, PWM provider isn't responsible for that. So I just hope this wouldn't be miss-leading ?   Oops, I'll fix it.  Thanks, Fabrice",technical,Fabrice Gasnier,fabrice.gasnier@st.com,1,1,203,1.0,0.7777777777777778,0,1.0,0.0,0.0,0.0
1119,551211,555904,"IMHO it's more the wording that might make the message misleading. If you use this ""No consumer device specified to create a device link to\n""), that's completely fine in my eyes.","On Tue, Feb 19, 2019 at 09:46:32AM +0100, Fabrice Gasnier wrote:  IMHO it's more the wording that might make the message misleading. If you use  	dev_warn(pwm->chip->dev, No consumer device specified to create a device link to\n""),  that's completely fine in my eyes.   Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |""",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,179,0.8125,0.8888888888888888,0,1.0,0.0,0.0,0.0
1120,551211,555971,"Thanks for the suggestion, I'll update this as well in v5.","On 2/19/19 9:55 AM, Uwe Kleine-König wrote:  Thanks for the suggestion, I'll update this as well in v5.  Best regards, Fabrice",technical,Fabrice Gasnier,fabrice.gasnier@st.com,1,1,58,0.2916666666666667,1.0,1,1.0,0.0,0.0,0.0
1121,552948,554177,The patch looks good to me. You can add: Reviewed-by: Jan Kara,"Hi Lukasz,  On 19. 2. 15. 오후 10:05, Lukasz Luba wrote:  Looks good to me. Reviewed-by: Chanwoo Choi <cw00.choi@samsung.com>  --  Best Regards, Chanwoo Choi Samsung Electronics",technical,Chanwoo Choi,cw00.choi@samsung.com,1,0,62,0.21428571428571427,0.6666666666666666,0,0.6666666666666666,0.0,0.6666666666666666,0.0
1122,552948,554178,"I case of __entry-> total_time is zero, why do you show '100' instead of '0'(zero)?I think that it might make the some confusion for user. If it show the '100' in case of ""__entry->total_time is zero"", it cannot distinguish between the real 100% utilization and ""total_time is zero"".","Hi Lukasz,  On 19. 2. 15. 오후 10:05, Lukasz Luba wrote:  I case of __entry->total_time is zero, why do you show '100' instead of '0'(zero)? I think that it might make the some confusion for user.  If it show the '100' in case of __entry->total_time is zero"", it cannot distinguish between the real 100% utilization and ""total_time is zero"".    --  Best Regards, Chanwoo Choi Samsung Electronics""",technical,Chanwoo Choi,cw00.choi@samsung.com,1,0,283,1.0,0.8333333333333334,0,0.6666666666666666,0.0,0.0,0.0
1123,552948,555358,"Good point, I will change it.","Hi Chanwoo,  On 2/18/19 6:40 AM, Chanwoo Choi wrote: Good point, I will change it.  Regards, Lukasz",technical,Lukasz Luba,l.luba@partner.samsung.com,1,1,29,0.11428571428571428,1.0,1,1.0,0.0,0.0,0.0
1124,556356,556889,This driver is quite old and we are not actively using/testing it. Are you wiring that up with qemu? Maybe it should be labeled differently in MAINTAINERS file. Anyway whatever fix is fine for me. Acked-by: Michal Simek,"On 19. 02. 19 17:49, Guenter Roeck wrote:  This driver is quite old and we are not actively using/testing it. Are you wiring that up with qemu?  Maybe it should be labeled differently in MAINTAINERS file.  Anyway whatever fix is fine for me.  Acked-by: Michal Simek <michal.simek@xilinx.com>  Thanks, Michal",technical,Michal Simek,michal.simek@xilinx.com,1,0,219,1.0,0.25,0,0.0,0.9787234042553191,0.0,0.0
1125,556356,557121,"Not specifically, it may be wired up as uninitialized device. Presumably that is why it reports version 0, which causes the driver to bail out. It is instantiated by the device tree file (I think).","On 2/19/19 11:09 PM, Michal Simek wrote:  Not specifically, it may be wired up as uninitialized device. Presumably that is why it reports version 0, which causes the driver to bail out. It is instantiated by the devicetree file (I think).  Guenter",technical,Guenter Roeck,linux@roeck-us.net,1,1,197,0.9767441860465116,0.5,0,0.0,0.9787234042553191,0.0,0.9361702127659575
1126,556356,607101,"Hi, This crash is now quite persistent in mainline. The fix didn't make it. Should I stop testing virtex-ml507 with qemu ?","Hi,  On Tue, Feb 19, 2019 at 08:49:56AM -0800, Guenter Roeck wrote:  This crash is now quite persistent in mainline. The fix didn't make it. Should I stop testing virtex-ml507 with qemu ?  Guenter",technical,Guenter Roeck,linux@roeck-us.net,1,1,122,0.6046511627906976,0.625,0,0.9574468085106383,0.0425531914893617,0.9361702127659575,0.0
1127,556356,607109,"I've applied the fix now. But given Michal's comments, should we kill the driver for 5.2?","On 4/5/19 4:44 PM, Guenter Roeck wrote:  I've applied the fix now.  But given Michal's comments, should we kill the driver for 5.2?  --  Jens Axboe",technical,Jens Axboe,axboe@kernel.dk,1,0,89,0.4883720930232558,0.75,0,0.9574468085106383,0.0425531914893617,0.0,0.0
1128,556356,607316,"If the driver is no longer used or maintained, removing it would indeed make sense.","On 4/5/19 4:06 PM, Jens Axboe wrote:  If the driver is no longer used or maintained, removing it would indeed make sense.  Guenter",technical,Guenter Roeck,linux@roeck-us.net,1,1,83,0.3953488372093023,0.875,0,0.9787234042553191,0.02127659574468085,0.0,0.02127659574468085
1129,556356,607616,I have no problem with it.,"On 06. 04. 19 21:34, Guenter Roeck wrote:  I have no problem with it.  Thanks, Michal  --  Michal Simek, Ing. (M.Eng), OpenPGP -> KeyID: FE3D1F91 w: www.monstr.eu p: +42-0-721842854 Maintainer of Linux kernel - Xilinx Microblaze Maintainer of Linux kernel - Xilinx Zynq ARM and ZynqMP ARM64 SoCs U-Boot custodian - Xilinx Microblaze/Zynq/ZynqMP/Versal SoCs",technical,Michal Simek,monstr@monstr.eu,1,0,26,0.16279069767441862,1.0,1,1.0,0.0,0.02127659574468085,0.0
1130,556373,556433,"oof, great bug report by the way!  Thanks for the fix. Note that ""sparse"" designated initialization zero initializes unnamed members: This transform you've done is safe because hexium was zero initialized via kzalloc, and struct hexium contains a struct i2c_adapter (as opposed to  a pointer to a struct i2c_adapter).  The same is true for both translation units you've touched.","On Tue, Feb 19, 2019 at 9:02 AM Arnd Bergmann <arnd@arndb.de> wrote:  oof, great bug report by the way!  Thanks for the fix. Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>   Note that sparse"" designated initialization zero initializes unnnamed members: https://godbolt.org/z/LkSpJp This transform you've done is safe because hexium was zero initialized via kzalloc, and struct hexium contains a struct i2c_adapter (as opposed to  a pointer to a struct i2c_adapter).  The same is true for both translation units you've touched. LGTM    --  Thanks, ~Nick Desaulniers""",technical,Nick Desaulniers,ndesaulniers@google.com,1,0,378,0.47368421052631576,0.5714285714285714,0,0.0,0.0,0.0,0.0
1131,556373,556453,"Thanks for the fix! In general, for -Wstack-frame-larger-than=warnings, is it possible that these sets of stack frames are already too large if entered?  Sure, in lining was a little aggressive, causing more stack space use than maybe otherwise necessary at runtime, but isn't it also possible that ""no in lining"" a stack frame can still be a problem should the stack frame be entered?  Doesn't the kernel have away of estimating the stack depth for any given frame?  I guess I was always curious if the best fix for these kind of warnings was to non-stack allocate (kmalloc) certain locally allocated structs, or no-inline the function.  Surely there's cases where no-in lining is safe, but I was curious if it's still maybe dangerous to enter the problematic child most stack frame?","On Tue, Feb 19, 2019 at 9:02 AM Arnd Bergmann <arnd@arndb.de> wrote:  Thanks for the fix! In general, for -Wstack-frame-larger-than= warnings, is it possible that these sets of stack frames are already too large if entered?  Sure, inlining was a little aggressive, causing more stack space use than maybe otherwise necessary at runtime, but isn't it also possible that no inlining"" a stack frame can still be a problem should the stack frame be entered?  Doesn't the kernel have a way of estimating the stack depth for any given frame?  I guess I was always curious if the best fix for these kind of warnings was to non-stack allocate (kmalloc) certain locally allocated structs, or no-inline the function.  Surely there's cases where no-inlining is safe, but I was curious if it's still maybe dangerous to enter the problematic child most stack frame?    --  Thanks, ~Nick Desaulniers""",technical,Nick Desaulniers,ndesaulniers@google.com,1,0,784,1.0,0.7142857142857143,0,0.0,0.0,0.0,0.0
1132,556373,556465,"What I think is happening here is that llvm fails to combine the stack allocations for the inclined functions in certain conditions, while gcc can reuse it here. We had similar issues in gcca few years ago, and they got fixed there, but I have not looked at this one in more detail. My guess is that it's related to the bug I mentioned in patch 3.","On Tue, Feb 19, 2019 at 8:02 PM Nick Desaulniers <ndesaulniers@google.com> wrote:  What I think is happening here is that llvm fails to combine the stack allocations for the inlined functions in certain conditions, while gcc can reuse it here. We had similar issues in gcc a few years ago, and they got fixed there, but I have not looked at this one in more detail. My guess is that it's related to the bug I mentioned in patch 3.        Arnd",technical,Arnd Bergmann,arnd@arndb.de,1,1,347,0.4868421052631579,0.8571428571428571,0,0.0,0.0,0.0,0.0
1133,556373,556478,"Please add it for fwht as well. It makes no sense to have it for this but not the fwht function. Got to say this is all very magic...I think it would be good to perhaps have a comment at the start of the source that explains why no inline_for_stack is added to selected functions. Patches 1 & 3 are fine, BTW.","On 2/19/19 6:01 PM, Arnd Bergmann wrote:  Please add it for fwht as well. It makes no sense to have it for fwht16, ifwht but not the fwht function.  Got to say this is all very magic...  I think it would be good to perhaps have a comment at the start of the source that explains why noinline_for_stack is added to selected functions.  Patches 1 & 3 are fine, BTW.  Regards,  	Hans",technical,Hans Verkuil,hverkuil@xs4all.nl,1,0,309,0.45394736842105265,1.0,1,0.0,0.0,0.0,0.0
1134,559592,561010,"Your SoB is missing for patch 1. And besides my ack, I'd still like some feedback from the actual driver maintainers.","On Thu, Feb 21, 2019 at 04:10:57PM +0000, Gareth Williams wrote:  Your SoB is missing for patch 1. And besides my ack, I'd still like some feedback from the actual driver maintainers.",technical,Wolfram Sang,wsa@the-dreams.de,1,0,117,1.0,0.6,0,0.3333333333333333,0.3333333333333333,0.3333333333333333,0.3333333333333333
1135,559592,561776,Thanks for the feedback Wolfram. I will wait for input from the driver maintainers .,"Thanks for the feedback Wolfram. I will wait for input from the driver maintainers.   Renesas Electronics Europe GmbH,Geschaeftsfuehrer/President : Michael Hannawald, Sitz der Gesellschaft/Registered office: Duesseldorf, Arcadiastrasse 10, 40472 Duesseldorf, Germany,Handelsregister/Commercial Register: Duesseldorf, HRB 3708 USt-IDNr./Tax identification no.: DE 119353406 WEEE-Reg.-Nr./WEEE reg. no.: DE 14978647",technical,Gareth Williams,gareth.williams.jx@renesas.com,0,1,84,0.64,0.8,1,1.0,0.0,0.3333333333333333,0.0
1136,559783,561676,"FYI, I have patches I plan to submit soon that gets rid of the struct scatter list use in this code to simplify it:","On Thu, Feb 21, 2019 at 11:27 AM Francesco Ruggeri <fruggeri@arista.com> wrote:  Any comments?  Thanks, Francesco Ruggeri",technical,Francesco Ruggeri,fruggeri@arista.com,0,1,115,1.0,1.0,1,1.0,0.0,1.0,0.0
1137,560287,562054,this is turned on by default for 32-bit. So all you have to do is remove this line.,"Hi Andrey,  On 2/22/19 6:21 AM, Andrey Zhizhikin wrote:  CONFIG_LDBAF is turned on by default for 32-bit. So all you have to do is remove this line.  Thanks, Dinh",technical,Dinh Nguyen,dinguyen@kernel.org,1,0,83,0.38461538461538464,0.5,0,0.6,0.2,0.6,0.0
1138,560287,562160,"Thanks a lot for clarifications here! I suspected that it was the case, just needed a confirmation here. Is it possible we would have this config option enabled? If you would agree, then I prepare a separate patch to have it removed. Thanks a lot!","Hello Dinh,  On Mon, Feb 25, 2019 at 5:35 PM Dinh Nguyen <dinguyen@kernel.org> wrote:  Thanks a lot for clarifications here! I suspected that it was the case, just needed a confirmation here. Is it possible we would have this config option enabled for socfpga_defconfig? If you would agree, then I prepare a separate patch to have the CONFIG_LBDAF is not set"" removed.   Thanks a lot!  -- Andrey.""",technical,Andrey Zhizhikin,andrey.z@gmail.com,0,1,247,1.0,0.75,0,0.6,0.2,0.0,0.2
1139,560287,564276,"Yes, that's what I was alluding to.","On 2/25/19 12:55 PM, Andrey Zhizhikin wrote:  Yes, that's what I was alluding to.  Dinh",technical,Dinh Nguyen,dinguyen@kernel.org,1,0,35,0.19230769230769232,1.0,1,1.0,0.0,0.2,0.0
1140,560350,564249,"Nice! I appreciate open-source hardware! Please send the patch via git send-email, it's a bit broken: Applying this, trailing whitespace.error. It's unusual to have uppercase letters here.Is ctu already registered in Documentation?Is this needed or not? Is this needed or not?","On 2/22/19 2:20 PM, Pavel Pisa wrote:  Nice! I appreciate open-source hardware!  Please send the patch via git send-email, it's a bit broken:  Applying: dt-bindings: net: can: binding for CTU CAN FD open-source IP core. .git/rebase-apply/patch:29: trailing whitespace.  error: corrupt patch at line 30 Patch failed at 0001 dt-bindings: net: can: binding for CTU CAN FD open-source IP core. hint: Use 'git am --show-current-patch' to see the failed patch    It's unusual to have uppercase letters here.   Is ctu already registered in Documentation/devicetree/bindings/vendor-prefixes.txt ?   Is this needed or not?   Is this needed or not?   Marc  --  Pengutronix e.K.                  | Marc Kleine-Budde           | Industrial Linux Solutions        | Phone: +49-231-2826-924     | Vertretung West/Dortmund          | Fax:   +49-5121-206917-5555 | Amtsgericht Hildesheim, HRA 2686  | http://www.pengutronix.de   |",technical,Marc Kleine-Budde,mkl@pengutronix.de,1,0,276,1.0,1.0,1,1.0,0.0,1.0,0.0
1141,560670,586104,"I had actually applied this to pci/msi with the intent of merging it for v5.1, but by coincidence I noticed [1], where Jon was basically solving another piece of the same problem, this time in nvme-pci.AFAICT, the consensus there was that it would be better to find some sort of platform solution instead of dealing with it in individual drivers.  The PCI core isn't really a driver, but I think the same argument applies to it: if we had a better way to recover from readl()errors, that way would work equally well in nvme-pci and the PCI core. It sounds like the problem has two parts: the PCI core part and the individual driver part.  Solving only the first (eg, with this patch)isn't enough by itself, and solving the second via some platform solution would also solve the first.  If that's the case, I don't think it's worth applying this one, but please correct me if I'm wrong.","[+cc Jon, Jens, Christoph, Sagi, Linus, linux-nvme from related discussion] [+cc Greg, Oliver, who responded to v2 of this patch]  On Fri, Feb 22, 2019 at 01:48:06PM -0600, Alexandru Gagniuc wrote:  I had actually applied this to pci/msi with the intent of merging it for v5.1, but by coincidence I noticed [1], where Jon was basically solving another piece of the same problem, this time in nvme-pci.  AFAICT, the consensus there was that it would be better to find some sort of platform solution instead of dealing with it in individual drivers.  The PCI core isn't really a driver, but I think the same argument applies to it: if we had a better way to recover from readl() errors, that way would work equally well in nvme-pci and the PCI core.  It sounds like the problem has two parts: the PCI core part and the individual driver part.  Solving only the first (eg, with this patch) isn't enough by itself, and solving the second via some platform solution would also solve the first.  If that's the case, I don't think it's worth applying this one, but please correct me if I'm wrong.  Bjorn  [1] https://lore.kernel.org/lkml/20190222010502.2434-1-jonathan.derrick@intel.com/T/#u",technical,Bjorn Helgaas,helgaas@kernel.org,1,0,885,0.3810483870967742,0.4,0,1.0,0.0,1.0,0.0
1142,560670,586148,"I think that patches with the pattern ""if (disconnected) don't do IO""are fundamentally broken and we should look for alternatives in all cases. They are fundamentally broken because they are racy: if it's an actual sudden disconnect in the middle of IO, there's no guarantee that we'll even be notified in time. They are fundamentally broken because they add new magic special cases that very few people will ever test, and the people who do test them tend to do so with old irrelevant kernels. Finally, they are fundamentally broken because they always end upbeing just special cases. One or two special case accesses in adriver, or perhaps all accesses of a particular type in just _one_special driver. Yes, yes, I realize that people want error reporting, and that hot-removal can cause various error conditions (perhaps just parity errors for the IO, but also perhaps other random errors caused by firmware perhaps doing special HW setup). But the ""you get a fatal interrupt, so avoid the IO"" kind of model is completely broken, and needs to just be fixed differently. See above why it's so completely broken. So if the hw is set up to send some kind of synchronous interrupt or machine check that cannot sanely be handled (perhaps because it will just repeat forever), we should try to just disable said thing. PCIe allows for just polling for errors on the bridges, afaik. It's been years since I looked at it, and maybe I'm wrong. And I bet there are various ""platform-specific value add"" registers etc that may need tweaking outside of any standard spec for PCIe error reporting. But let's do that in a platform driver, to set up the platform to not do the silly ""I'm just going to die if I see an error"" thing. It's way better to have a model where you poll each bridge once a minute (or one an hour) and let people know ""guys, your hardware reports errors"", than make random crappy changes to random drivers because the hardware was set up to die on said errors. And if some MIS person wants the ""hardware will die"" setting, then they can damn well have that, and then it's not out problem, but it also means that we don't start changing random drivers for that insane setting. It's dead, Jim, and it was the users choice. Notice how in neither case does it make sense to try to do some ""if(disconnected) dont_do_io()"" model for the drivers.","On Wed, Mar 20, 2019 at 1:52 PM Bjorn Helgaas <helgaas@kernel.org> wrote:  I think that patches with the pattern if (disconnected) don't do IO"" are fundamentally broken and we should look for alternatives in all cases.  They are fundamentally broken because they are racy: if it's an actual sudden disconnect in the middle of IO, there's no guarantee that we'll even be notified in time.  They are fundamentally broken because they add new magic special cases that very few people will ever test, and the people who do test them tend to do so with old irrelevant kernels.  Finally, they are fundamentally broken because they always end up being just special cases. One or two special case accesses in a driver, or perhaps all accesses of a particular type in just _one_ special driver.  Yes, yes, I realize that people want error reporting, and that hot-removal can cause various error conditions (perhaps just parity errors for the IO, but also perhaps other random errors caused by firmware perhaps doing special HW setup).  But the ""you get a fatal interrupt, so avoid the IO"" kind of model is completely broken, and needs to just be fixed differently. See above why it's so completely broken.  So if the hw is set up to send some kinf of synchronous interrupt or machine check that cannot sanely be handled (perhaps because it will just repeat forever), we should try to just disable said thing.  PCIe allows for just polling for errors on the bridges, afaik. It's been years since I looked at it, and maybe I'm wrong. And I bet there are various ""platform-specific value add"" registers etc that may need tweaking outside of any standard spec for PCIe error reporting. But let's do that in a platform driver, to set up the platform to not do the silly ""I'm just going to die if I see an error"" thing.  It's way better to have a model where you poll each bridge once a minute (or one an hour) and let people know ""guys, your hardware reports errors"", than make random crappy changes to random drivers because the hardware was set up to die on said errors.  And if some MIS person wants the ""hardware will die"" setting, then they can damn well have that, and then it's not out problem, but it also means that we don't start changing random drivers for that insane setting. It's dead, Jim, and it was the users choice.  Notice how in neither case does it make sense to try to do some ""if (disconnected) dont_do_io()"" model for the drivers.                    Linus""",technical,Linus Torvalds,torvalds@linux-foundation.org,1,0,2351,1.0,0.6,0,1.0,0.0,0.0,0.0
1143,560670,586258,"I disagree with the idea of doing something you know can cause an error to propagate. That being said, in this particular case we have come to rely a little too much on the if (disconnected) model. You mentioned in the other thread that fixing the GHES driver will pay much higher dividends. I'm working on reviving a couple of changes to do just that. Some industry folk were very concerned about the ""don't panic()"" approach, and I want to make sure I fairly present their arguments in the cover letter. I'm hoping one day we'll have the ability to use page tables to prevent the situations that we're trying to fix today in less than ideal ways. Although there are other places in msi.c that use if (disconnected), I'm okay with dropping this change -- provided we come up with an equivalent fix. But even if FFS doesn't crash, do we really want to lose hundreds of milliseconds to SMM --on all cores-- when all it takes is a couple of cycles to check a flag?","On 3/20/19 4:44 PM, Linus Torvalds wrote:  I disagree with the idea of doing something you know can cause an error  to propagate. That being said, in this particular case we have come to  rely a little too much on the if (disconnected) model.  You mentioned in the other thread that fixing the GHES driver will pay  much higher dividends. I'm working on reviving a couple of changes to do  just that. Some industry folk were very concerned about the don't  panic()"" approach, and I want to make sure I fairly present their  arguments in the cover letter.  I'm hoping one day we'll have the ability to use page tables to prevent  the situations that we're trying to fix today in less than ideal ways.  Although there are other places in msi.c that use if (disconnected), I'm  okay with dropping this change -- provided we come up with an equivalent  fix.  But even if FFS doesn't crash, do we really want to lose hundreds of  milliseconds to SMM --on all cores-- when all it takes is a couple of  cycles to check a flag?  Alex""",technical,Alex G,mr.nuke.me@gmail.com,0,0,962,0.4153225806451613,0.8,0,1.0,0.0,0.0,0.0
1144,560670,586338,"My main gripe with the if (disconnected) model is that it's only really good for inactive devices. If a device is being used then odds are the driver will do an MMIO before the pci core has had a chance to mark the device as broken so you crash anyway. What's the idea there? Scan the io remap space for mappings over the device BARs and swap them with a normal memory page? Using pci_dev_is_disconnected() to opportunistically avoid waiting for MMIO timeouts is fair enough IMO, even if it's a bit ugly. It would help your case if you did some measurements to show the improvement and look for other cases it might help. It might also be a good idea to document when it is appropriate to use pci_is_dev_disconnected() so we aren't stuck having the same argument again and again, but that's probably a job for Bjorn though.","On Thu, Mar 21, 2019 at 12:27 PM Alex G <mr.nuke.me@gmail.com> wrote:  My main gripe with the if (disconnected) model is that it's only really good for inactive devices. If a device is being used then odds are the driver will do an MMIO before the pci core has had a chance to mark the device as broken so you crash anyway.   What's the idea there? Scan the ioremap space for mappings over the device BARs and swap them with a normal memory page?   Using pci_dev_is_disconnected() to opportunistically avoid waiting for MMIO timeouts is fair enough IMO, even if it's a bit ugly. It would help your case if you did some measurements to show the improvement and look for other cases it might help. It might also be a good idea to document when it is appropriate to use pci_is_dev_disconnected() so we aren't stuck having the same argument again and again, but that's probably a job for Bjorn though.  Oliver",technical,Oliver,oohall@gmail.com,1,0,823,0.3407258064516129,1.0,1,1.0,0.0,0.0,0.0
1145,560699,560753,Are all versions of the MDIO controller capable of doing C45?,"On Fri, Feb 22, 2019 at 08:12:42PM +0000, Parshuram Thombare wrote:  Hi Parshuram  Are all versions of the MDIO controller capable of doing C45?      Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,61,0.12903225806451613,0.13043478260869565,0,0.0,1.0,0.0,0.0
1146,560699,560954,Now driver support c22 and c45 PHY. Are you suggesting to add check for C45 PHY using is_c45 in phydev ?,"Regards, Parshuram Thombare  Now driver support c22 and c45 PHY.  Are you suggesting to add check for C45 PHY using is_c45 in phydev ?  Regards, Parshuram Thombare",technical,Parshuram Raju Thombare,pthombar@cadence.com,0,0,104,0.23655913978494625,0.17391304347826086,0,0.0,0.9642857142857143,0.0,0.0
1147,560699,561099,"You are unconditionally supporting C45. Are there versions of the hardware which don't actually support C45? You have this endless loop: If there is hardware which does not support C45, will this loop forever?","You are unconditionally supporting C45. Are there versions of the hardware which don't actually support C45? You have this endless loop:  +       /* wait for end of transfer */ +       while (!MACB_BFEXT(IDLE, macb_readl(bp, NSR))) +               cpu_relax(),  If there is hardware which does not support C45, will this loop forever?  	Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,209,0.43010752688172044,0.2608695652173913,0,0.0,0.9642857142857143,0.0,0.0
1148,560699,561100,"You need a timeout here, and anywhere you wait for the hardware to complete. Try to make use of readx_poll_timeout() variants.   Andrew","On Fri, Feb 22, 2019 at 08:12:42PM +0000, Parshuram Thombare wrote:  You need a timeout here, and anywhere you wait for the hardware to complete. Try to make use of readx_poll_timeout() variants.  	  Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,135,0.2903225806451613,0.30434782608695654,0,0.0,0.9642857142857143,0.0,0.03571428571428571
1149,560699,561736,"There is controller which don't support C45. I will add check for that using is_c45. Yes, this bit is supposed to be set. I will add timeout here.","There is controller which don't support C45. I will add check for that using is_c45. Yes, this bit is supposed to be set. I will add timeout here.  Regards, Parshuram Thombare",technical,Parshuram Raju Thombare,pthombar@cadence.com,0,0,146,0.3655913978494624,0.34782608695652173,0,0.07142857142857142,0.8928571428571429,0.03571428571428571,0.0
1150,560699,561739,"Yes, I will add timeout here.","Yes, I will add timeout here.  Regards, Parshuram Thombare",technical,Parshuram Raju Thombare,pthombar@cadence.com,0,0,29,0.08602150537634409,0.43478260869565216,0,0.07142857142857142,0.8928571428571429,0.0,0.0
1151,560699,581997,"You have 3 patches in your series, you need to resend all of them even if there is only one to which you are making changes, this is not documented in netdev-FAQ.rst though, let's update that.","On 3/18/19 10:42 AM, Parshuram Thombare wrote:  You have 3 patches in your series, you need to resend all of them even if there is only one to which you are making changes, this is not documented in netdev-FAQ.rst though, let's update that.    --  Florian",technical,Florian Fainelli,f.fainelli@gmail.com,1,0,192,0.44086021505376344,0.6086956521739131,0,0.8214285714285714,0.14285714285714285,0.0,0.0
1152,560699,582006,"Thanks for quick reply. I am still working on other patches, so I think I will send all of them in another single mail chain.","Thanks for quick reply.  I am still working on other patches, so I think I will send all of them in another single mail chain.    Regards,  Parshuram Thombare    ",technical,Parshuram Raju Thombare,pthombar@cadence.com,0,0,125,0.3010752688172043,0.6521739130434783,0,0.8214285714285714,0.14285714285714285,0.0,0.0
1153,560699,586758,"subject line should be this. And the commit message before should look like: ""Modify MDIO read/write functions to support communication with C45 PHY in Cadence ethernet controller driver. ""You changed tabs to spaces here: please conform to preceding driver's style and other lines of this .h file. Otherwise it looks correct to me on the macb point-of-view. Please correct the little things noted before, re-send independently as a v3 and we also make sure that things are good on the phy side.","$subject line should be:    net: macb: Add c45 PHY support in MDIO read/write functions""      On 18/03/2019 at 18:42, Parshuram Thombare wrote:    Most of this must go below the ""---"" line hereunder...        There ^.    And the commit message before should look like:  ""  Modify MDIO read/write functions to support communication with C45 PHY   in Cadence ethernet controller driver.  ""        You changed tabs to spaces here: please conform to preceding driver's   style and other lines of this .h file.      Otherwise it looks correct to me on the macb point-of-view.    Please correct the little things noted before, re-send independently as   a v3 and we also make sure that things are good on the phy side.    Best regards,  --   Nicolas Ferre  """,technical,Unkown Name,Nicolas.Ferre@microchip.com,0,0,494,1.0,0.7391304347826086,0,0.9285714285714286,0.03571428571428571,0.07142857142857142,0.0
1154,560699,586788,"Something i asked last time, but I'm not sure i got an answer.  Do all generations of the MDIO controller support C45? If the older versions only support C22, we need to be sure it does the right thing when asked to do a C45 transfer.","On Mon, Mar 18, 2019 at 05:42:28PM +0000, Parshuram Thombare wrote:  Hi Parshuram  Something i asked last time, but i'm not sure i got an answer.  Do all generations of the MDIO controller support C45? If the older versions only support C22, we need to be sure it does the right thing when asked to do a C45 transfer.        Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,234,0.5591397849462365,0.8260869565217391,0,0.9285714285714286,0.03571428571428571,0.0,0.03571428571428571
1155,560699,589813,"Ok, I will change subject in v3 of this patch. Ok. I will change spaces by tab. Thank you for comments. I will make above mentioned changes and re-send this patch independently as a v3.","Ok, I will change subject in v3 of this patch.      Ok.      I will change spaces by tab.      Thank you for comments. I will make above mentioned changes and re-send this   patch independently as a v3.      Regards,  Parshuram Thombare",technical,Parshuram Raju Thombare,pthombar@cadence.com,0,0,185,0.44086021505376344,0.8695652173913043,0,1.0,0.0,0.03571428571428571,0.0
1156,560699,589814,There are some older versions of controller which doesn't support C45. I will make sure that driver return error for C45 transfer requests when it is not supported by controller.,"Hi Andrew,  There are some older versions of controller which doesn't support C45. I will make sure that driver return error for C45 transfer requests when it is not supported by controller.   Regards, Parshuram Thombare",technical,Parshuram Raju Thombare,pthombar@cadence.com,0,0,178,0.3548387096774194,0.9565217391304348,1,1.0,0.0,0.0,0.0
1157,562973,565910,"I like your name :) If you post a v2 for any reason, please update the subject like to this. Add support for sdm845 PCIe controller That way it matches the existing conventions and it looks nice.  If there's no need for a v2, Lorenzo will likely fix that up for you.","On Tue, Feb 26, 2019 at 1:00 AM Bjorn Andersson <bjorn.andersson@linaro.org> wrote:  Hi Bjorn,  I like your name :)  If you post a v2 for any reason, please update the subject like to:    PCI: qcom: Add support for sdm845 PCIe controller  That way it matches the existing conventions and git log --oneline drivers/pci/controller/dwc/pcie-qcom.c"" looks nice.  If there's no need for a v2, Lorenzo will likely fix that up for you.  Bjorn""",technical,Bjorn Helgaas,bhelgaas@google.com,1,0,266,0.3172043010752688,0.5,0,0.6666666666666666,0.0,0.6666666666666666,0.0
1158,562973,566427,"Thanks for the patch! What means Q2A? It'd be nice to describe it. Is TBU related to SMMU or to something else? please drop the blank line. you should use usleep_range, please fix it. check for errors, please.","Hi Bjorn,  Thanks for the patch!  On 2/26/19 9:01 AM, Bjorn Andersson wrote:  What means Q2A? It'd be nice to describe it.   Is TBU related to SMMU or to something else?   please drop the blank line.   you could add dev_err() as well (to be aligned with regulator_bulk_enable and reset_control_assert).   for sleeping interval 10us - 20ms you should use usleep_range, please fix it.   check for errors, please.   <cut>  --  regards, Stan",technical,Stanimir Varbanov,svarbanov@mm-sol.com,1,0,209,0.25806451612903225,0.75,0,1.0,0.0,0.0,0.0
1159,562973,567539,"I'll see what I can do. Yes, the ARM SMMU is split in a centralized controller (TCU) and translation blocks (TBU) for each hardware peripheral. So this is th eclock for the translation block sitting between the two PCIe controllers and the system NOC.The clock is described here, rather than in the SMMU node in the upstream way of representing client-related resources - although we don't use the device_link to toggle it in the implementation below. Sure thing, and I'll throw in some defines for the two 2s, per your feedback on the pending patch, will print an error indicating which of the multiple clocks that failed to turn on, so I don't think adding another- more generic - error line will add value. Looking again shows that I missed that regulator_bulk_enable() does the same thing, so I will align the two by removing above error print.10ms looks arbitrary as well, I'll review this. Of course. Thanks for your review!","On Fri 01 Mar 04:53 PST 2019, Stanimir Varbanov wrote:   I'll see what I can do.   Yes, the ARM SMMU is split in a centralized controller (TCU) and translation blocks (TBU) for each hardware peripheral. So this is the clock for the translation block sitting between the two PCIe controllers and the system NOC.  The clock is described here, rather than in the SMMU node in the upstream way of representing client-related resources - although we don't use the device_link to toggle it in the implementation below.   Sure thing, and I'll throw in some defines for the two 2s, per your feedback on the pending QCS404 patch.   clk_bulk_prepare_enable() will print an error indicating which of the multiple clocks that failed to turn on, so I don't think adding another - more generic - error line will add value.   Looking again shows that I missed that regulator_bulk_enable() does the same thing, so I will align the two by removing above error print.   10ms looks arbitrary as well, I'll review this.   Of course.   Thanks for your review!  Regards, Bjorn",technical,Bjorn Andersson,bjorn.andersson@linaro.org,1,1,930,1.0,1.0,1,1.0,0.0,0.0,0.0
1160,563291,563489,"Here you are reporting a bug. (More on this below.)Here you are introducing a new behavior. (Also discussed below.)Although the two are similar, they are really 2 different things. Wouldn't the fix to the bug be to move the ""skip"" target here?skip:My only objection to this is that the ""messages dropped"" only comes if anon-supressed message comes. So information about dropped information may never get printed unless some task prints something non-supressed.Imagine a situation where I am expecting a message to come, but don't see it because it was dropped. But if no more non-supressed messages come, I see neither the expected message nor the dropped message.","On 2019-02-26, Petr Mladek <pmladek@suse.com> wrote:  Here you are reporting a bug. (More on this below.)   Here you are introducing a new behavior. (Also discussed below.) Although the two are similar, they are really 2 different things.   Wouldn't the fix to the bug be to move the skip"" target here?  skip:   My only objection to this is that the ""messages dropped"" only comes if a non-supressed message comes. So information about dropped information may never get printed unless some task prints something non-supressed.  Imagine a situation where I am expecting a message to come, but don't see it because it was dropped. But if no more non-supressed messages come, I see neither the expected message nor the dropped message.  John Ogness""",technical,John Ogness,john.ogness@linutronix.de,1,0,664,0.7204301075268817,0.25,0,0.0,1.0,0.0,0.0
1161,563291,563848,"[..]I think this is exactly the problem (and thus the patch) we discussed some3 years ago. I had a number of rather strangely looking serial logs, which clearly had lost messages but no ""%llu printk messages dropped"" markers. SoI added `static bool lost_messages' to console_unlock(), set it when printing loop would discover lost messages, then print ""%llu printk messages dropped""attached to whatever msg was next in the logbuf, regardless of msg->level.IOW, if lost_messages was set then suppress_message_printing(msg->level)was not even invoked. Yes, that would sometimes print several ""debugging noise"" messages, but the main part was that I would have ""%llu printk messages dropped"" markers in the logs, which was much more important to me.P.S. I'm very sorry, I'm overloaded with work at the moment, will start     looking at pending patches in a day or two, or three, or four...","On (02/26/19 17:26), John Ogness wrote: [..]  I think this is exactly the problem (and thus the patch) we discussed some 3 years ago. I had a number of rather strangely looking serial logs, which clearly had lost messages but no %llu printk messages dropped"" markers. So I added `static bool lost_messages' to console_unlock(), set it when printing loop would discover lost messages, then print ""%llu printk messages dropped"" attached to whatever msg was next in the logbuf, regardless of msg->level. IOW, if lost_messages was set then suppress_message_printing(msg->level) was not even invoked. Yes, that would sometimes print several ""debugging noise"" messages, but the main part was that I would have ""%llu printk messages dropped"" markers in the logs, which was much more important to me.  P.S. I'm very sorry, I'm overloaded with work at the moment, will start      looking at pending patches in a day or two, or three, or four...  	-ss""",technical,Sergey Senozhatsky,sergey.senozhatsky.work@gmail.com,1,0,886,1.0,0.375,0,0.0,1.0,0.0,0.0
1162,563291,563971,"I guess you are referring to this [0] thread. I would agree with the proposed solution from 2016. My experience is that the dropped messages are very important. Yes, printing them could lead to the loss of even more messages. But still, it is important information that needs to get out.","On 2019-02-27, Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com> wrote:  I guess you are referring to this [0] thread.   I would agree with the proposed solution from 2016. My experience is that the dropped messages are very important. Yes, printing them could lead to the loss of even more messages. But still, it is important information that needs to get out.  John Ogness  [0] https://lkml.kernel.org/r/20161224140902.1962-3-sergey.senozhatsky@gmail.com",technical,John Ogness,john.ogness@linutronix.de,1,0,287,0.3225806451612903,0.5,0,0.0,0.8,0.0,0.0
1163,563291,563988,"Yes. No, I am replacing random behavior with a predictable one to fix the bug. The above paragraph explains why the fix looks like it looks. Maybe I should have written something like: A solution would be to print the warning regardless the log level. But it might cause loosing more important messages because of delay caused by the warnings. A better solution is to count all dropped messages until there is a non-suppressed one. Then we could print the summary together with the message. No, the entire loop skiping suppressed messages is done under the logbuf_lock. No old messages can be lost inside this loop. Good point! There is a simple fix for this. We could print the warning also when all messages are proceed and we are about to leave the for-cycle.","On Tue 2019-02-26 17:26:57, John Ogness wrote:  Yes.   No, I am replacing random behavior with a predictable one to fix the bug. The above paragraph explains why the fix looks like it looks. Maybe I should have written somethink like:  A solution would be to print the warning regardless the log level. But it might cause loosing more important messages because of delay caused by the warnings.  A better solution is to count all dropped messages until there is a non-suppressed one. Then we could print the summary together with the message.    No, the entire loop skiping suppressed messages is done under the logbuf_lock. No old messages can be lost inside this loop.   Good point! There is a simple fix for this. We could print the warning also when all messages are proceed and we are about to leave the for-cycle.  Best Regards, Petr",technical,Petr Mladek,pmladek@suse.com,1,1,762,0.8010752688172043,0.625,0,0.0,0.8,0.0,0.0
1164,563291,563990,"Right. Yes, printing out messages does take time. But I think it's easier to start losing messages due to pre-emption under console_sem than due to call_console_drivers() latencies. I'd agree. A summary ""you lost %d messages somewhere between current and previous messages"" is surely better than what we have now, but is still a bit less informative than ""you lost %d messages just now"".","On (02/27/19 09:12), John Ogness wrote:  Right.  [..]  Yes, printing out messages does take time. But I think it's easier to start losing messages due to preemption under console_sem than due to call_console_drivers() latencies.   I'd agree. A summary you lost %d messages somewhere between current and previous messages"" is surely better than what we have now, but is still a bit less informative than ""you lost %d messages just now"".  	-ss""",technical,Sergey Senozhatsky,sergey.senozhatsky.work@gmail.com,1,0,387,0.43010752688172044,0.75,0,0.0,0.8,0.0,0.0
1165,563291,564001,Yes! That is the piece that was missing!,"On 2019-02-27, Petr Mladek <pmladek@suse.com> wrote:  Yes! That is the piece that was missing!  John Ogness",technical,John Ogness,john.ogness@linutronix.de,1,0,40,0.053763440860215055,0.875,0,0.0,0.8,0.0,0.8
1166,563291,568274,"I'd prefer to have lost-messages reporting be less of a summary and more of an ""error"". I think the sooner we report it the better. I don't think that the time we need to print lost-messages on the consoles is significant enough to worry about it, if call_console_drivers() becomes such a problem that any extra char we print causes message-drop then we can disable printk_time or/and printk caller_id prefix printout (can save us more CPU cycles).","On (02/27/19 09:30), Petr Mladek wrote:  I'd prefer to have lost-messages reporting be less of a summary and more of an error"". I think the sooner we report it the better. I don't think that the time we need to print lost-messages on the consoles is significant enough to worry about it, if call_console_drivers() becomes such a problem that any extra char we print causes message-drop then we can disable printk_time or/and printk caller_id prefix printout (can save us more CPU cycles).  	-ss""",technical,Sergey Senozhatsky,sergey.senozhatsky.work@gmail.com,1,0,448,0.4731182795698925,1.0,1,1.0,0.0,0.8,0.0
1167,565592,587229,Any comments on this set?,"Hi Boris,  Any comments on this set?  Thanks, Yazen ",technical,"Ghannam, Yazen",Yazen.Ghannam@amd.com,0,1,25,0.375,0.65,0,0.9130434782608695,0.043478260869565216,0.9130434782608695,0.0
1168,565592,589399,I've tested this patch and it gets rid of a slew of these message.,"Hi Boris,  I've tested this patch and it gets rid of a slew of these messages:  kernel: EDAC amd64: Error: F0 not found, device 0x1460 (broken BIOS?) kernel: EDAC amd64: Error: Error probing instance: 0  So please add my:  Tested-by: Kim Phillips <kim.phillips@amd.com>  Thank you,  Kim  On 3/21/19 3:30 PM, Ghannam, Yazen wrote:",technical,Kim Phillips,kphillips@amd.com,0,0,66,1.0,0.75,0,0.9565217391304348,0.043478260869565216,0.0,0.0
1169,565592,589433,Thanks but please do not top-post when replying to mails on public mailing lists,"On Fri, Mar 22, 2019 at 04:33:48PM +0000, Kim Phillips wrote:  Thanks but please do not top-post when replying to mails on public mailing lists.  --  Regards/Gruss,     Boris.  Good mailing practices for 400: avoid top-posting and trim the reply.",technical,Borislav Petkov,bp@alien8.de,1,0,80,0.875,0.85,0,0.9565217391304348,0.043478260869565216,0.0,0.0
1170,565592,589902,Please don't do silly things like that. I fixed it up now.,"On Thu, Feb 28, 2019 at 03:36:10PM +0000, Ghannam, Yazen wrote:  Please don't do silly things like that:  In file included from drivers/edac/amd64_edac_inj.c:2:0: drivers/edac/amd64_edac.h:279:11: warning: ‘num_umcs’ defined but not used [-Wunused-variable]  static u8 num_umcs,            ^~~~~~~~  I fixed it up now.  --  Regards/Gruss,     Boris.  Good mailing practices for 400: avoid top-posting and trim the reply.",technical,Borislav Petkov,bp@alien8.de,1,0,58,0.9375,0.9,0,0.9565217391304348,0.0,0.0,0.0
1171,565592,589974,Thanks. Sorry I missed that.-,  Thanks. Sorry I missed that.    -Yazen  ,technical,"Ghannam, Yazen",Yazen.Ghannam@amd.com,0,1,29,0.375,0.95,1,1.0,0.0,0.0,0.0
1172,567612,567656,"Awesome. :)I was looking for that link, but I didn't find it.  That's why I sent this patch. Happy to see it is fixed now.","On 3/1/19 5:54 PM, Alex Deucher wrote:  Awesome. :)  I was looking for that link, but I didn't find it.  That's why I sent this patch.  Happy to see it is fixed now.  Thanks, Alex.  -- Gustavo",technical,Gustavo A. R. Silva,gustavo@embeddedor.com,0,1,122,1.0,0.6,0,0.0,1.0,0.0,1.0
1173,568810,568847,"Although this fixes the warning, i suspect there i something wrong with the original patch adding mv88e6390x_port_set_cmode(). It should also be used without CONFIG_NET_DSA_LEGACY.","On Mon, Mar 04, 2019 at 08:43:01PM +0800, Shaokun Zhang wrote:  Hi Shaokun, Heiner  Although this fixes the warning, i suspect there i something wrong with the original patch adding mv88e6390x_port_set_cmode(). It should also be used without CONFIG_NET_DSA_LEGACY.       Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,180,0.23015873015873015,0.2222222222222222,0,0.0,0.0,0.0,0.0
1174,568810,568887,I checked the commit by Florian. Do you mean that CONFIG_NET_DSA_LEGACY shall be removed completely? :-),"Hi Andrew,  On 2019/3/4 21:26, Andrew Lunn wrote:  I checked the commit-id 2a93c1a3651f (net: dsa: Allow compiling out legacy support"") by Florian. Do you mean that CONFIG_NET_DSA_LEGACY shall be removed completely? :-)  Thanks, Shaokun """,technical,Zhangshaokun,zhangshaokun@hisilicon.com,1,0,104,0.15873015873015872,0.3333333333333333,0,0.0,0.0,0.0,0.0
1175,568810,568928,"No, i suspect it is being called from the wrong place, or needs to be called from a second location.[Goes and looks at the code] Yes, it should also be called in the probe. I would call it just after the call to mv88e6xxx_detect(), so that it is the same as in mv88e6xxx_drv_probe(). There are two ways DSA drivers can be probed. The legacy way, which is optional, and is slowly getting removed, and the current way. Heiner is new to DSA and probably missed that, and only handled the legacy probe method. I also missed checking when i reviewed to patch :-(","On Mon, Mar 04, 2019 at 10:16:08PM +0800, Zhangshaokun wrote:  No, i suspect mv88e6390x_ports_cmode_init() is being called from the wrong place, or needs to be called from a second location.  [Goes and looks at the code]  Yes, it should also be called in mv88e6xxx_probe(). I would call it just after the call to mv88e6xxx_detect(), so that it is the same as in mv88e6xxx_drv_probe().  There are two ways DSA drivers can be probed. The legacy way, which is optional, and is slowly getting removed, and the current way. Heiner is new to DSA and probably missed that, and only handled the legacy probe method. I also missed checking when i reviewed to patch :-(     Andrew",technical,Andrew Lunn,andrew@lunn.ch,1,0,557,1.0,0.4444444444444444,0,0.0,0.0,0.0,0.0
1176,568810,569057,"Right, I missed that, will submit a fix. I just saw that the Kconfig entry comment for NET_DSA_LEGACY says:"" This feature is scheduled for removal in 4.17."" Was forgotten to remove it or did somebody scream loud enough ""But I depend on it"" ?","On 04.03.2019 15:57, Andrew Lunn wrote: Right, I missed that, will submit a fix.  I just saw that the Kconfig entry comment for NET_DSA_LEGACY says: This feature is scheduled for removal in 4.17.""  Was forgotten to remove it or did somebody scream loud enough ""But I depend on it"" ?  Heiner""",technical,Heiner Kallweit,hkallweit1@gmail.com,1,0,241,0.42063492063492064,0.5555555555555556,0,0.0,0.0,0.0,0.0
1177,568810,569062,"The intent was to remove it by that kernel version but the 88e6060 driver still depends on it, and there appears to be some active users that Andrew worked with.","On 3/4/19 10:18 AM, Heiner Kallweit wrote:  The intent was to remove it by that kernel version but the 88e6060 driver still depends on it, and there appears to be some active users that Andrew worked with. --  Florian",technical,Florian Fainelli,f.fainelli@gmail.com,1,0,161,0.25396825396825395,0.6666666666666666,0,0.0,0.0,0.0,0.0
1178,568810,569070,"I see, thanks. And migrating this driver to the new DSA framework version isn't possible or not worth the effort?","On 04.03.2019 19:24, Florian Fainelli wrote: I see, thanks. And migrating this driver to the new DSA framework version isn't possible or not worth the effort?",technical,Heiner Kallweit,hkallweit1@gmail.com,1,0,113,0.19047619047619047,0.7777777777777778,0,0.0,0.0,0.0,0.0
1179,568810,569073,Andrew is working on it actually. We could have 88E6060 select NET_DSA_LEGACY and drop legacy probing from mv88e6xxx as an in between solution.,"On 3/4/19 10:31 AM, Heiner Kallweit wrote:  Andrew is working on it actually. We could have 88E6060 select NET_DSA_LEGACY and drop legacy probing from mv88e6xxx as an in between solution. --  Florian",technical,Florian Fainelli,f.fainelli@gmail.com,1,0,143,0.1984126984126984,0.8888888888888888,0,0.0,0.0,0.0,0.0
1180,568810,569396,"Ok, I didn't check it carefully. Hmm, I am not familiar with it,  please feel free to fix it","Hi Andrew,  On 2019/3/4 22:57, Andrew Lunn wrote:  Ok, I didn't check it carefully.   Hmm, I am not familiar wit it, please feel free to fix it.  Thanks, Shaokun",technical,Zhangshaokun,zhangshaokun@hisilicon.com,1,0,92,0.19047619047619047,1.0,1,0.0,0.0,0.0,0.0
1181,568933,568976,All files being opened by the kernel should be calling one of these helper routines.  Has that changed?,"On Mon, 2019-03-04 at 15:11 +0000, David Howells wrote:  All files being opened by the kernel should be calling one of these helper routines.  Has that changed?  Mimi",technical,Mimi Zohar,zohar@linux.ibm.com,1,0,103,0.3333333333333333,0.5,0,0.0,0.0,0.0,0.0
1182,568933,568979,"prepare_binprm() uses kernel_read() and has done since at least 2014.  The binfmt drivers also use kernel_read(). Since kernel_read_file() is used by a bunch of things that aren't exec, even if we switch exec to it, it should probably still go in fs/read_write.c since it seems generic.","Mimi Zohar <zohar@linux.ibm.com> wrote:   prepare_binprm() uses kernel_read() and has done since at least 2014.  The binfmt drivers also use kernel_read().  Since kernel_read_file() is used by a bunch of things that aren't exec, even if we switch exec to it, it should probably still go in fs/read_write.c since it seems generic.  David",technical,David Howells,dhowells@redhat.com,1,1,286,1.0,0.75,0,0.0,0.0,0.0,0.0
1183,568933,568990,Oh this commit moved kernel_read() to fs/read_write.c without moving the helpers. Definitely makes sense to move the helpers.  Please include a reference to the commit in this patch. r,"On Mon, 2019-03-04 at 16:22 +0000, David Howells wrote:  Oh, commit bdd1d2d3d251 (fs: fix kernel_read prototype"") moved kernel_read() to fs/read_write.c without moving the helpers.  Definitely makes sense to move the helpers.  Please include a reference to the commit in this patch.   Reviewed-by: Mimi Zohar <zohar@linux.ibm.com>""",technical,Mimi Zohar,zohar@linux.ibm.com,1,0,184,0.5666666666666667,1.0,1,0.0,0.0,0.0,0.0
1184,570451,571735,"These look good to me. Perhaps they can go via -mm, maybe with Eric and/or Peter's ack/review? I'd really like to get these done so we can get closer to finishing the refcount_t conversions…","On Wed, Mar 6, 2019 at 3:05 AM Elena Reshetova <elena.reshetova@intel.com> wrote:  These look good to me. Perhaps they can go via -mm, maybe with Eric and/or Peter's ack/review? I'd really like to get these done so we can get closer to finishing the refcount_t conversions...  Thanks!  -Kees    --  Kees Cook",technical,Kees Cook,keescook@chromium.org,1,0,190,1.0,1.0,1,1.0,0.0,1.0,0.0
1185,570944,620345,"Some of these patches probably fix issues, like making PASID work correctly on VFs.  But that needs to be made explicit in the commit logs. The current commit logs read more like ""make XYZ follow the spec"", and that's not really what we need to know.  The commit log needs to tell us why we need the change, not just what the spec says. For example, maybe VFs can't use PASID because Linux incorrectly tries to use the PASID capability on the VF when it should be using the capability on the PF.  The commit log should say that explicitly and also say what the current behavior is, e.g., does it cause IOMMU faults, does it cause data corruption, does some DMA mapping interface called by the VF driver fail when it shouldn't, etc?","On Wed, Mar 06, 2019 at 02:11:13PM -0800, sathyanarayanan.kuppuswamy@linux.intel.com wrote:  Some of these patches probably fix issues, like making PASID work correctly on VFs.  But that needs to be made explicit in the commit logs.  The current commit logs read more like make XYZ follow the spec"", and that's not really what we need to know.  The commit log needs to tell us why we need the change, not just what the spec says.  For example, maybe VFs can't use PASID because Linux incorrectly tries to use the PASID capability on the VF when it should be using the capability on the PF.  The commit log should say that explicitly and also say what the current behavior is, e.g., does it cause IOMMU faults, does it cause data corruption, does some DMA mapping interface called by the VF driver fail when it shouldn't, etc?  Bjorn """,technical,Bjorn Helgaas,helgaas@kernel.org,1,0,731,1.0,1.0,1,1.0,0.0,1.0,0.0
1186,571150,573601,Is there something need do for this patch? Pls let me know. I saw the patchwork status labelled to  'Not Applicable',"Hi David,  Is there something need do for this patch? Pls let me know.  I saw the patchwork status labled to  'Not Applicable'  https://patchwork.ozlabs.org/patch/1052624/   On 2019/3/7 10:22, Yue Haibing wrote:",technical,YueHaibing,yuehaibing@huawei.com,0,0,116,1.0,1.0,1,1.0,0.0,0.6666666666666666,0.0
1187,571310,573881,"I am afraid of this change. Besides the rate there might be other reasons to choose one mux input over another, consider for example low power audio playback where we need one specific mux setting because it provides a clock which runs at low power mode. On the IPU on i.MX5/6 there are clocks being used as pixel clocks derived from different muxes. I don't think you want to pick an input clock just because it happens to deliver the best clock rate at that point in time, but really is shared with some other clock that changes its rate in the next moment. I have no concrete examples for things that break with this change, but I would be more confident if we change the behaviour explicitly only for the muxes that we have reviewed to cope with this change.","Hi Abel,  On Thu, Mar 07, 2019 at 09:20:37AM +0000, Abel Vesa wrote:  I am afraid of this change. Besides the rate there might be other reasons to choose one mux input over another, consider for example low power audio playback where we need one specific mux setting because it provides a clock which runs at low power mode. On the IPU on i.MX5/6 there are clocks being used as pixel clocks derived from different muxes. I don't think you want to pick an input clock just because it happens to deliver the best clock rate at that point in time, but really is shared with some other clock that changes its rate in the next moment.  I have no concrete examples for things that break with this change, but I would be more confident if we change the behaviour explicitly only for the muxes that we have reviewed to cope with this change.  Sascha  --  Pengutronix e.K.                           |                             | Industrial Linux Solutions                 | http://www.pengutronix.de/  | Peiner Str. 6-8, 31137 Hildesheim, Germany | Phone: +49-5121-206917-0    | Amtsgericht Hildesheim, HRA 2686           | Fax:   +49-5121-206917-5555 |",technical,Sascha Hauer,s.hauer@pengutronix.de,1,0,762,1.0,0.42857142857142855,0,0.6666666666666666,0.3333333333333333,0.6666666666666666,0.0
1188,571310,573886,Fair enough. We could replace all the imx_clk_mux with imx_clk_mux_noreparent and after that we can independently switch the clocks that are safe (to switch) to imx_clk_mux (which would not have the noreparent flag set).The main idea is to simplify the clock control from drivers point of view. The end goal here would be to only have noreparent flag set for the clocks that need specific mux control.,"On 19-03-11 11:28:25, Sascha Hauer wrote:   Fair enough. We could replace all the imx_clk_mux with imx_clk_mux_noreparent and after that we can independently switch the clocks that are safe (to switch) to imx_clk_mux (which would not have the noreparent flag set).  The main idea is to simplify the clock control from drivers point of view.  The end goal here would be to only have noreparent flag set for the clocks that need specific mux control.",technical,Abel Vesa,abel.vesa@nxp.com,1,1,401,0.5,0.5714285714285714,0,0.6666666666666666,0.3333333333333333,0.0,0.0
1189,571310,577757,"Ok with me.Which drivers do you have in mind? I hardly ever missed reparenting on rate changes, so where is this feature useful?","On Mon, Mar 11, 2019 at 10:41:40AM +0000, Abel Vesa wrote:  Ok with me.   Which drivers do you have in mind? I hardly ever missed reparenting on rate changes, so where is this feature useful?  Sascha  --  Pengutronix e.K.                           |                             | Industrial Linux Solutions                 | http://www.pengutronix.de/  | Peiner Str. 6-8, 31137 Hildesheim, Germany | Phone: +49-5121-206917-0    | Amtsgericht Hildesheim, HRA 2686           | Fax:   +49-5121-206917-5555 |",technical,Sascha Hauer,s.hauer@pengutronix.de,1,0,128,0.17333333333333334,0.8571428571428571,0,1.0,0.0,0.3333333333333333,0.0
1190,571310,577777,"Reparenting is very useful for Audio IPs (SAI, SPDIF, PDM, FSL DSP).We often need clock rates that are multiples of 8000 or 11025 which can be obtain from different PLLs. But in order for this to work we need to have the correct parent. In our current tree we do the re parenting manually inside drivers. For example: This is far from optimal. Abel's patch will be of a great help for Audio.","On Wed, Mar 13, 2019 at 12:42 PM Sascha Hauer <s.hauer@pengutronix.de> wrote:  Hi Sascha,  Reparenting is very useful for Audio IPs (SAI, SPDIF, PDM, FSL DSP). We often need clock rates that are multiples of 8000 or 11025 which can be obtain from different PLLs. But in order for this to work we need to have the correct parent.  In our current tree we do the re parenting manually inside drivers. For example:  https://source.codeaurora.org/external/imx/linux-imx/tree/sound/soc/fsl/fsl_sai.c?h=imx_4.14.78_1.0.0_ga#n244  This is far from optimal. Abel's patch will be of a great help for Audio.  thanks, Daniel.",technical,Daniel Baluta,daniel.baluta@gmail.com,1,0,391,0.5733333333333334,1.0,1,1.0,0.0,0.0,0.0
1191,571417,571437,"Take for instance this one (.config attached), it has both and it compiles: Which, afaict is correct given the asm, but is absolute nonsense given the original C. If you follow that code path, it appears to do the memops without STAC, and then complains it does CLAC. Which is of course complete crap. Maybe I've been staring at this too long and am (again) missing the obvious :/","On Thu, Mar 07, 2019 at 12:45:11PM +0100, Peter Zijlstra wrote:  Take for instance this one (.config attached), it has both CONFIG_PROFILE_ALL_BRANCHES=y and CONFIG_TRACE_BRANCH_PROFILING=y and it compiles:  (from kernel/exit.c)  SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *, 		infop, int, options, struct rusage __user *, ru) { 	struct rusage r, 	struct waitid_info info = {.status = 0}, 	long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL), 	int signo = 0,  	if (err > 0) { 		signo = SIGCHLD, 		err = 0, 		if (ru && copy_to_user(ru, &r, sizeof(struct rusage))) 			return -EFAULT, 	} 	if (!infop) 		return err,  	if (!user_access_begin(infop, sizeof(*infop))) 		return -EFAULT,  	unsafe_put_user(signo, &infop->si_signo, Efault), 	unsafe_put_user(0, &infop->si_errno, Efault), 	unsafe_put_user(info.cause, &infop->si_code, Efault), 	unsafe_put_user(info.pid, &infop->si_pid, Efault), 	unsafe_put_user(info.uid, &infop->si_uid, Efault), 	unsafe_put_user(info.status, &infop->si_status, Efault), 	user_access_end(), 	return err, Efault: 	user_access_end(), 	return -EFAULT, }  into this atrocious crap (grep -v ffffffffffffe0eb, if you want the src to go away):  $ objdump -Sdr randconfig-build/kernel/exit.o | awk />:\$/ { P=0, } /__do_sys_waitid>:\$/ { P=1, O=strtonum(\""0x\"" \$1), } { if (P) { o=strtonum(\""0x\"" \$1), printf(\""%04x \"", o-O), print \$0, } }""  0000 0000000000001f15 <__do_sys_waitid>: ffffffffffffe0eb  ffffffffffffe0eb SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *, ffffffffffffe0eb 		infop, int, options, struct rusage __user *, ru) ffffffffffffe0eb { 0000     1f15:	55                   	push   %rbp ffffffffffffe0eb 	struct rusage r, ffffffffffffe0eb 	struct waitid_info info = {.status = 0}, ffffffffffffe0eb 	long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL), 0001     1f16:	b8 00 00 00 00       	mov    $0x0,%eax ffffffffffffe0eb { 0006     1f1b:	48 89 e5             	mov    %rsp,%rbp 0009     1f1e:	41 57                	push   %r15 000b     1f20:	41 56                	push   %r14 000d     1f22:	41 55                	push   %r13 000f     1f24:	41 54                	push   %r12 0011     1f26:	4d 89 c4             	mov    %r8,%r12 0014     1f29:	53                   	push   %rbx 0015     1f2a:	48 89 d3             	mov    %rdx,%rbx 0018     1f2d:	48 83 e4 f0          	and    $0xfffffffffffffff0,%rsp 001c     1f31:	48 81 ec a0 00 00 00 	sub    $0xa0,%rsp ffffffffffffe0eb 	long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL), 0023     1f38:	4d 85 e4             	test   %r12,%r12 0026     1f3b:	4c 8d 44 24 10       	lea    0x10(%rsp),%r8 002b     1f40:	48 89 e2             	mov    %rsp,%rdx ffffffffffffe0eb 	struct waitid_info info = {.status = 0}, 002e     1f43:	48 c7 04 24 00 00 00 	movq   $0x0,(%rsp) 0035     1f4a:	00  0036     1f4b:	48 c7 44 24 08 00 00 	movq   $0x0,0x8(%rsp) 003d     1f52:	00 00  ffffffffffffe0eb 	long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL), 003f     1f54:	4c 0f 44 c0          	cmove  %rax,%r8 0043     1f58:	e8 9c fe ff ff       	callq  1df9 <kernel_waitid> ffffffffffffe0eb 	int signo = 0, ffffffffffffe0eb  ffffffffffffe0eb 	if (err > 0) { 0048     1f5d:	48 85 c0             	test   %rax,%rax ffffffffffffe0eb 	long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL), 004b     1f60:	49 89 c7             	mov    %rax,%r15 ffffffffffffe0eb 	if (err > 0) { 004e     1f63:	0f 9f c0             	setg   %al 0051     1f66:	0f b6 c0             	movzbl %al,%eax 0054     1f69:	48 83 c0 02          	add    $0x2,%rax 0058     1f6d:	48 ff 04 c5 00 00 00 	incq   0x0(,%rax,8) 005f     1f74:	00  005c 			1f71: R_X86_64_32S	_ftrace_branch+0x1c0 0060     1f75:	4d 85 ff             	test   %r15,%r15 0063     1f78:	7e 7b                	jle    1ff5 <__do_sys_waitid+0xe0> ffffffffffffe0eb 		signo = SIGCHLD, ffffffffffffe0f9 		err = 0, ffffffffffffe0eb 		if (ru && copy_to_user(ru, &r, sizeof(struct rusage))) 0065     1f7a:	31 d2                	xor    %edx,%edx 0067     1f7c:	4d 85 e4             	test   %r12,%r12 006a     1f7f:	74 53                	je     1fd4 <__do_sys_waitid+0xbf> ffffffffffffe0eb  ffffffffffffe0eb static __always_inline bool ffffffffffffe0f7 check_copy_size(const void *addr, size_t bytes, bool is_source) ffffffffffffe0eb { ffffffffffffe0eb 	int sz = __compiletime_object_size(addr), ffffffffffffe0eb 	if (unlikely(sz >= 0 && sz < bytes)) { 006c     1f81:	31 f6                	xor    %esi,%esi 006e     1f83:	b9 01 00 00 00       	mov    $0x1,%ecx 0073     1f88:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi 0076 			1f8b: R_X86_64_32S	_ftrace_annotated_branch+0xcc0 007a     1f8f:	e8 00 00 00 00       	callq  1f94 <__do_sys_waitid+0x7f> 007b 			1f90: R_X86_64_PLT32	ftrace_likely_update-0x4 ffffffffffffe0eb } ffffffffffffe0eb  ffffffffffffe0eb static __always_inline unsigned long __must_check ffffffffffffe0f7 copy_to_user(void __user *to, const void *from, unsigned long n) ffffffffffffe0eb { ffffffffffffe0eb 	if (likely(check_copy_size(from, n, true))) 007f     1f94:	31 c9                	xor    %ecx,%ecx 0081     1f96:	ba 01 00 00 00       	mov    $0x1,%edx 0086     1f9b:	be 01 00 00 00       	mov    $0x1,%esi 008b     1fa0:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi 008e 			1fa3: R_X86_64_32S	_ftrace_annotated_branch+0xae0 0092     1fa7:	48 ff 05 00 00 00 00 	incq   0x0(%rip)        # 1fae <__do_sys_waitid+0x99> 0095 			1faa: R_X86_64_PC32	_ftrace_branch+0x1fcc 0099     1fae:	e8 00 00 00 00       	callq  1fb3 <__do_sys_waitid+0x9e> 009a 			1faf: R_X86_64_PLT32	ftrace_likely_update-0x4 ffffffffffffe0eb 		n = _copy_to_user(to, from, n), 009e     1fb3:	ba 90 00 00 00       	mov    $0x90,%edx 00a3     1fb8:	4c 89 e7             	mov    %r12,%rdi ffffffffffffe0eb 	if (likely(check_copy_size(from, n, true))) 00a6     1fbb:	48 ff 05 00 00 00 00 	incq   0x0(%rip)        # 1fc2 <__do_sys_waitid+0xad> 00a9 			1fbe: R_X86_64_PC32	_ftrace_branch+0x1d7c ffffffffffffe0eb 		n = _copy_to_user(to, from, n), 00ad     1fc2:	48 8d 74 24 10       	lea    0x10(%rsp),%rsi 00b2     1fc7:	e8 00 00 00 00       	callq  1fcc <__do_sys_waitid+0xb7> 00b3 			1fc8: R_X86_64_PLT32	_copy_to_user-0x4 00b7     1fcc:	31 d2                	xor    %edx,%edx 00b9     1fce:	48 85 c0             	test   %rax,%rax 00bc     1fd1:	0f 95 c2             	setne  %dl 00bf     1fd4:	48 63 c2             	movslq %edx,%rax ffffffffffffe0eb 		signo = SIGCHLD, 00c2     1fd7:	41 be 11 00 00 00    	mov    $0x11,%r14d ffffffffffffe0f9 		err = 0, 00c8     1fdd:	45 31 ff             	xor    %r15d,%r15d ffffffffffffe0eb 		if (ru && copy_to_user(ru, &r, sizeof(struct rusage))) 00cb     1fe0:	48 83 c0 02          	add    $0x2,%rax 00cf     1fe4:	48 ff 04 c5 00 00 00 	incq   0x0(,%rax,8) 00d6     1feb:	00  00d3 			1fe8: R_X86_64_32S	_ftrace_branch+0x198 00d7     1fec:	85 d2                	test   %edx,%edx 00d9     1fee:	74 08                	je     1ff8 <__do_sys_waitid+0xe3> 00db     1ff0:	e9 33 01 00 00       	jmpq   2128 <__do_sys_waitid+0x213> ffffffffffffe0eb 	int signo = 0, 00e0     1ff5:	45 31 f6             	xor    %r14d,%r14d ffffffffffffe0eb 			return -EFAULT, ffffffffffffe0eb 	} ffffffffffffe0eb 	if (!infop) 00e3     1ff8:	31 c0                	xor    %eax,%eax 00e5     1ffa:	48 85 db             	test   %rbx,%rbx 00e8     1ffd:	0f 94 c0             	sete   %al 00eb     2000:	48 83 c0 02          	add    $0x2,%rax 00ef     2004:	48 ff 04 c5 00 00 00 	incq   0x0(,%rax,8) 00f6     200b:	00  00f3 			2008: R_X86_64_32S	_ftrace_branch+0x170 00f7     200c:	48 85 db             	test   %rbx,%rbx 00fa     200f:	0f 84 1a 01 00 00    	je     212f <__do_sys_waitid+0x21a> ffffffffffffe0eb 	return raw_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED, 0100     2015:	65 44 8b 25 00 00 00 	mov    %gs:0x0(%rip),%r12d        # 201d <__do_sys_waitid+0x108> 0107     201c:	00  0104 			2019: R_X86_64_PC32	__preempt_count-0x4 ffffffffffffe0eb  * checking before using them, but you have to surround them with the ffffffffffffe0eb  * user_access_begin/end() pair. ffffffffffffe0eb  */ ffffffffffffe0eb static __must_check __always_inline bool user_access_begin(const void __user *ptr, size_t len) ffffffffffffe0eb { ffffffffffffe0eb 	if (unlikely(!access_ok(ptr,len))) 0108     201d:	45 31 ed             	xor    %r13d,%r13d 010b     2020:	41 81 e4 00 01 1f 00 	and    $0x1f0100,%r12d 0112     2027:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi 0115 			202a: R_X86_64_32S	_ftrace_annotated_branch+0xb70 0119     202e:	41 0f 95 c5          	setne  %r13b 011d     2032:	31 c9                	xor    %ecx,%ecx 011f     2034:	31 d2                	xor    %edx,%edx 0121     2036:	44 89 ee             	mov    %r13d,%esi 0124     2039:	e8 00 00 00 00       	callq  203e <__do_sys_waitid+0x129> 0125 			203a: R_X86_64_PLT32	ftrace_likely_update-0x4 0129     203e:	49 63 c5             	movslq %r13d,%rax 012c     2041:	48 83 c0 02          	add    $0x2,%rax 0130     2045:	48 ff 04 c5 00 00 00 	incq   0x0(,%rax,8) 0137     204c:	00  0134 			2049: R_X86_64_32S	_ftrace_branch+0x1d90 0138     204d:	45 85 e4             	test   %r12d,%r12d 013b     2050:	74 02                	je     2054 <__do_sys_waitid+0x13f> 013d     2052:	0f 0b                	ud2     013f     2054:	31 c9                	xor    %ecx,%ecx 0141     2056:	31 d2                	xor    %edx,%edx 0143     2058:	44 89 ee             	mov    %r13d,%esi 0146     205b:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi 0149 			205e: R_X86_64_32S	_ftrace_annotated_branch+0xb40 014d     2062:	e8 00 00 00 00       	callq  2067 <__do_sys_waitid+0x152> 014e 			2063: R_X86_64_PLT32	ftrace_likely_update-0x4 ffffffffffffe0eb 		return unlikely(addr > limit - size), 0152     2067:	45 31 e4             	xor    %r12d,%r12d 0155     206a:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi 0158 			206d: R_X86_64_32S	_ftrace_annotated_branch+0xbd0 015c     2071:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax 0163     2078:	00 00  0161 			2076: R_X86_64_32S	current_task 0165     207a:	4c 8b a8 d8 1d 00 00 	mov    0x1dd8(%rax),%r13 016c     2081:	49 83 c5 80          	add    $0xffffffffffffff80,%r13 0170     2085:	4c 39 eb             	cmp    %r13,%rbx 0173     2088:	41 0f 97 c4          	seta   %r12b 0177     208c:	31 c9                	xor    %ecx,%ecx 0179     208e:	31 d2                	xor    %edx,%edx 017b     2090:	44 89 e6             	mov    %r12d,%esi 017e     2093:	e8 00 00 00 00       	callq  2098 <__do_sys_waitid+0x183> 017f 			2094: R_X86_64_PLT32	ftrace_likely_update-0x4 ffffffffffffe0eb 	if (unlikely(!access_ok(ptr,len))) 0183     2098:	31 f6                	xor    %esi,%esi 0185     209a:	4c 39 eb             	cmp    %r13,%rbx 0188     209d:	ba 01 00 00 00       	mov    $0x1,%edx 018d     20a2:	40 0f 96 c6          	setbe  %sil 0191     20a6:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi 0194 			20a9: R_X86_64_32S	_ftrace_annotated_branch+0xb10 0198     20ad:	31 c9                	xor    %ecx,%ecx 019a     20af:	e8 00 00 00 00       	callq  20b4 <__do_sys_waitid+0x19f> 019b 			20b0: R_X86_64_PLT32	ftrace_likely_update-0x4 019f     20b4:	44 89 e6             	mov    %r12d,%esi 01a2     20b7:	31 c9                	xor    %ecx,%ecx 01a4     20b9:	31 d2                	xor    %edx,%edx 01a6     20bb:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi 01a9 			20be: R_X86_64_32S	_ftrace_annotated_branch+0xba0 01ad     20c2:	49 83 c4 02          	add    $0x2,%r12 01b1     20c6:	e8 00 00 00 00       	callq  20cb <__do_sys_waitid+0x1b6> 01b2 			20c7: R_X86_64_PLT32	ftrace_likely_update-0x4 01b6     20cb:	4a ff 04 e5 00 00 00 	incq   0x0(,%r12,8) 01bd     20d2:	00  01ba 			20cf: R_X86_64_32S	_ftrace_branch+0x1db8 ffffffffffffe0eb 		return 0, 01be     20d3:	31 c0                	xor    %eax,%eax ffffffffffffe0eb 	if (unlikely(!access_ok(ptr,len))) 01c0     20d5:	4c 39 eb             	cmp    %r13,%rbx 01c3     20d8:	77 08                	ja     20e2 <__do_sys_waitid+0x1cd> ffffffffffffe0eb 	__uaccess_begin_nospec(), 01c5     20da:	90                   	nop 01c6     20db:	90                   	nop 01c7     20dc:	90                   	nop ffffffffffffe0eb } ffffffffffffe0eb  ffffffffffffe0eb static __always_inline void stac(void) ffffffffffffe0eb { ffffffffffffe0eb 	/* Note: a barrier is implicit in alternative() */ ffffffffffffe0f5 	alternative("", __stringify(__ASM_STAC), X86_FEATURE_SMAP), 01c8     20dd:	90                   	nop 01c9     20de:	90                   	nop 01ca     20df:	90                   	nop ffffffffffffe0eb 	return 1, 01cb     20e0:	b0 01                	mov    $0x1,%al ffffffffffffe0eb 		return err, ffffffffffffe0eb  ffffffffffffe0eb 	if (!user_access_begin(infop, sizeof(*infop))) 01cd     20e2:	83 f0 01             	xor    $0x1,%eax 01d0     20e5:	48 89 c2             	mov    %rax,%rdx 01d3     20e8:	83 e2 01             	and    $0x1,%edx 01d6     20eb:	48 83 c2 02          	add    $0x2,%rdx 01da     20ef:	48 ff 04 d5 00 00 00 	incq   0x0(,%rdx,8) 01e1     20f6:	00  01de 			20f3: R_X86_64_32S	_ftrace_branch+0x148 01e2     20f7:	84 c0                	test   %al,%al 01e4     20f9:	75 2d                	jne    2128 <__do_sys_waitid+0x213> ffffffffffffe0eb 		return -EFAULT, ffffffffffffe0eb  ffffffffffffe0eb 	unsafe_put_user(signo, &infop->si_signo, Efault), 01e6     20fb:	44 89 33             	mov    %r14d,(%rbx) ffffffffffffe0eb 	unsafe_put_user(0, &infop->si_errno, Efault), 01e9     20fe:	c7 43 04 00 00 00 00 	movl   $0x0,0x4(%rbx) ffffffffffffe0eb 	unsafe_put_user(info.cause, &infop->si_code, Efault), 01f0     2105:	8b 44 24 0c          	mov    0xc(%rsp),%eax 01f4     2109:	89 43 08             	mov    %eax,0x8(%rbx) ffffffffffffe0eb 	unsafe_put_user(info.pid, &infop->si_pid, Efault), 01f7     210c:	8b 04 24             	mov    (%rsp),%eax 01fa     210f:	89 43 10             	mov    %eax,0x10(%rbx) ffffffffffffe0eb 	unsafe_put_user(info.uid, &infop->si_uid, Efault), 01fd     2112:	8b 44 24 04          	mov    0x4(%rsp),%eax 0201     2116:	89 43 14             	mov    %eax,0x14(%rbx) ffffffffffffe0eb 	unsafe_put_user(info.status, &infop->si_status, Efault), 0204     2119:	8b 44 24 08          	mov    0x8(%rsp),%eax 0208     211d:	89 43 18             	mov    %eax,0x18(%rbx) ffffffffffffe0eb 	user_access_end(), 020b     2120:	90                   	nop 020c     2121:	90                   	nop 020d     2122:	90                   	nop ffffffffffffe0eb 	return err, 020e     2123:	eb 0a                	jmp    212f <__do_sys_waitid+0x21a> ffffffffffffefe5 Efault: ffffffffffffe0eb 	user_access_end(), 0210     2125:	90                   	nop 0211     2126:	90                   	nop 0212     2127:	90                   	nop ffffffffffffe0eb 	return -EFAULT, 0213     2128:	49 c7 c7 f2 ff ff ff 	mov    $0xfffffffffffffff2,%r15 ffffffffffffe0eb } 021a     212f:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp 021e     2133:	4c 89 f8             	mov    %r15,%rax 0221     2136:	5b                   	pop    %rbx 0222     2137:	41 5c                	pop    %r12 0224     2139:	41 5d                	pop    %r13 0226     213b:	41 5e                	pop    %r14 0228     213d:	41 5f                	pop    %r15 022a     213f:	5d                   	pop    %rbp 022b     2140:	c3                   	retq    ffffffffffffe0eb   And then complains about:  $ tools/objtool/objtool check --no-fp --uaccess --backtrace randconfig-build/kernel/exit.o randconfig-build/kernel/exit.o: warning: objtool: .altinstr_replacement+0xc: redundant UACCESS disable randconfig-build/kernel/exit.o: warning: objtool:   __do_sys_waitid()+0x210: (alt) randconfig-build/kernel/exit.o: warning: objtool:   __do_sys_waitid()+0x1e6: (alt) randconfig-build/kernel/exit.o: warning: objtool:   __do_sys_waitid()+0x1c3: (branch) randconfig-build/kernel/exit.o: warning: objtool:   __do_sys_waitid()+0x13b: (branch) randconfig-build/kernel/exit.o: warning: objtool:   __do_sys_waitid()+0x63: (branch) randconfig-build/kernel/exit.o: warning: objtool:   __do_sys_waitid()+0x0: <=== (func)  Which, afaict is correct given the asm, but is absolute nonsense given the original C. If you follow that code path, it appears to do the memops without STAC, and then complains it does CLAC. Which is of course complete crap.  Maybe I've been staring at this too long and am (again) missing the obvious :/ """,technical,Peter Zijlstra,peterz@infradead.org,1,1,380,0.3952380952380952,0.22,0,0.0,1.0,0.0,0.0
1192,571417,571450,"No, this is a hypercall with parameters passed through from user land(Xen tools). So setting AC=1 is not related to hysterical reasons, but to indicate that unprivileged buffers are okay here. So please change the commit message to something like: Some Xen hypercalls allow parameter buffers in user land, so they need to set AC=1. Avoid the warning for those cases. Mind Cc-ing the maintainers of the code you are touching in future patches? With the commit message adjusted: Reviewed-by: Juergen Gross","On 07/03/2019 12:45, Peter Zijlstra wrote:  No, this is a hypercall with parameters passed through from user land (Xen tools). So setting AC=1 is not related to hysterical reasons, but to indicate that unprivileged buffers are okay here. So please change the commit message to something like:  Some Xen hypercalls allow parameter buffers in user land, so they need to set AC=1. Avoid the warning for those cases.   Mind Cc-ing the maintainers of the code you are touching in future patches?  With the commit message adjusted:  Reviewed-by: Juergen Gross <jgross@suse.com>   Juergen",technical,Juergen Gross,jgross@suse.com,1,0,503,0.45714285714285713,0.23,0,0.0,1.0,0.0,0.0
1193,571417,571459,"*yuck* that's gross...And I understood from Andrew yesterday (but perhaps I misunderstood) that the whole privcmd thing was a bit of a hack. But sure, I'll change the message. Sure, my bad, I forgot to ask get_maintainers about this.","On Thu, Mar 07, 2019 at 01:22:52PM +0100, Juergen Gross wrote:  *yuck* that's gross...  And I understood from Andrew yesterday (but perhaps I misunderstood) that the whole privcmd thing was a bit of a hack.  But sure, I'll change the message.   Sure, my bad, I forgot to ask get_maintainers about this.",technical,Peter Zijlstra,peterz@infradead.org,1,1,233,0.2523809523809524,0.24,0,0.0,1.0,0.0,0.0
1194,571417,571464,"That was an unwise design decision in the past, yes. Changing it will require a lot of effort. Thanks. :-)","On 07/03/2019 13:32, Peter Zijlstra wrote:  That was an unwise design decision in the past, yes. Changing it will require a lot of effort.   Thanks. :-)   Juergen",technical,Juergen Gross,jgross@suse.com,1,0,106,0.12380952380952381,0.25,0,0.0,1.0,0.0,0.0
1195,571417,571484,"And that is the error, I think. We should've taken it and went to:  return -EFAULT, because: is an unconditional code sequence, but there's no way objtool can do that without becoming a full blown interpreter :/","On Thu, Mar 07, 2019 at 01:55:26PM +0100, Peter Zijlstra wrote:    randconfig-build/kernel/exit.o: warning: objtool:   __do_sys_waitid()+0x1c3: (branch)   And that is the error, I think. We should've taken it and went to:    return -EFAULT,  because:   +1be	xor  %eax,%eax	eax=0  +1cd   xor  $0x1,%eax	eax=1  +1e2   test %al,%al	1&1==1 -> ZF=0  +1e4   jnz  Is an unconditional code sequence, but there's no way objtool can do that without becoming a full blown interpreter :/",technical,Peter Zijlstra,peterz@infradead.org,1,1,211,0.21904761904761905,0.27,0,0.0,0.9767441860465116,0.0,0.0
1196,571417,571652,"How about just turning off SMAP checking for the really odd cases? At some point it's not worth worrying about excessive debug infrastructure, I think. Just give up and say ok.. Or just make preempt_schedule() check AC and not schedule. It already does that for IF.","On Thu, Mar 7, 2019 at 4:03 AM Peter Zijlstra <peterz@infradead.org> wrote:  How about just turning off SMAP checking for the really odd cases?  At some point it's not worth worrying about excessive debug infrastructure, I think. Just give up and say ok. we'll save/restore in preempt_schedule()"".  Or just make preempt_schedule() check AC and not schedule. It already does that for IF.                     Linus""",technical,Linus Torvalds,torvalds@linux-foundation.org,1,0,265,0.2619047619047619,0.29,0,0.0,0.9767441860465116,0.0,0.0
1197,571417,571653,"It's certainly one of the safer functions to call with AC set, but it sounds wrong anyway. It's not like it's likely to leak kernel data(most memset's are with 0, and even the non-zero ones I can't imagine are sensitive - more like poison values etc).What's the call site that made you go ""just add __memset() to the list""?","On Thu, Mar 7, 2019 at 3:52 AM Peter Zijlstra <peterz@infradead.org> wrote:  It's certainly one of the safer functions to call with AC set, but it sounds wrong anyway. It's not like it's likely to leak kernel data (most memset's are with 0, and even the non-zero ones I can't imagine are sensitive - more like poison values etc).  What's the call site that made you go just add __memset() to the list""?              Linus""",technical,Linus Torvalds,torvalds@linux-foundation.org,1,0,323,0.36666666666666664,0.3,0,0.0,0.9767441860465116,0.0,0.0
1198,571417,571673,"Although this fixes the warning, i suspect there i something wrong with the original patch adding mv88e6390x_port_set_cmode(). It should also be used without CONFIG_NET_DSA_LEGACY.","On Thu, Mar 07, 2019 at 02:13:12PM +0100, Peter Zijlstra wrote:  This fixes"" it, and also seems to help -Os make much code:  diff --git a/include/linux/compiler.h b/include/linux/compiler.h index 445348facea9..8de63db58fdd 100644 --- a/include/linux/compiler.h +++ b/include/linux/compiler.h @@ -67,7 +67,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,  				.line = __LINE__,			\  			},						\  		______r = !!(cond),					\ -		______f.miss_hit[______r]++,					\ +		if (______r) ______f.miss_hit[1]++, else ______f.miss_hit[0]++, \  		______r,						\  	}))  #endif /* CONFIG_PROFILE_ALL_BRANCHES */""",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,180,0.1380952380952381,0.31,0,0.0,0.9767441860465116,0.0,0.0
1199,571417,571674,much *smaller* code,"On Thu, Mar 07, 2019 at 10:47:05AM -0600, Josh Poimboeuf wrote:  much *smaller* code   --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,19,0.023809523809523808,0.32,0,0.0,0.9767441860465116,0.0,0.0
1200,571417,571691,"Yeah, considering that this __trace_if() macro from hell is doing an 'if()' on the result of that inner thing, it makes sense to *not*  use that ""looks simpler and shorter"" array sequence, but depend on the compiler then noticing that the conditionals are the same and joining the two branches together. The compiler has to do much more work to either generate the actual dynamic array thing, or notice that the _index_ of the array matches the _conditional_ on the branch, and undo that thing. But that macro really is the macro from hell regardless. Do people really use CONFIG_PROFILE_ALL_BRANCHES?","On Thu, Mar 7, 2019 at 8:47 AM Josh Poimboeuf <jpoimboe@redhat.com> wrote:  Yeah, considering that this __trace_if() macro from hell is doing an 'if()' on the result of that inner thing, it makes sense to *not*  use that looks simpler and shorter"" array sequence, but depend on the compiler then noticing that the conditionals are the same and joining the two branches together.  The compiler has to do much more work to either generate the actual dynamic array thing, or notice that the _index_ of the array matches the _conditional_ on the branch, and undo that thing.  But that macro really is the macro from hell regardless.  Do people really use CONFIG_PROFILE_ALL_BRANCHES?                  Linus""",technical,Linus Torvalds,torvalds@linux-foundation.org,1,0,601,0.5619047619047619,0.33,0,0.0,0.9767441860465116,0.0,0.0
1201,571417,571699,if (cond)?  Or is ___r used elsewhere?,"On March 7, 2019 8:47:05 AM PST, Josh Poimboeuf <jpoimboe@redhat.com> wrote:  if (cond)?  Or is ___r used elsewhere? --  Sent from my Android device with K-9 Mail. Please excuse my brevity.",technical,Unkown Name,hpa@zytor.com,0,0,38,0.05238095238095238,0.34,0,0.0,0.9767441860465116,0.0,0.0
1202,571417,571704,"Yeah, agreed.  Now it doesn't have to do the funky xor thing to convert the conditional to an int.IIRC, Steven runs it once a year or so…","On Thu, Mar 07, 2019 at 09:00:49AM -0800, Linus Torvalds wrote:  Yeah, agreed.  Now it doesn't have to do the funky xor thing to convert the conditional to an int.   IIRC, Steven runs it once a year or so...  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,137,0.14761904761904762,0.37,0,0.0,0.9767441860465116,0.0,0.0
1203,571417,571706,______r is also the return value.  And it's needed because cond should only be evaluated once.,"On Thu, Mar 07, 2019 at 09:04:36AM -0800, hpa@zytor.com wrote:  ______r is also the return value.  And it's needed because cond should only be evaluated once.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,94,0.09047619047619047,0.38,0,0.0,0.9767441860465116,0.0,0.0
1204,571417,571718,"So put a true, and false, inside the if.","On March 7, 2019 9:18:29 AM PST, Josh Poimboeuf <jpoimboe@redhat.com> wrote:  So put a true, and false, inside the if. --  Sent from my Android device with K-9 Mail. Please excuse my brevity.",technical,Unkown Name,hpa@zytor.com,0,0,40,0.05714285714285714,0.39,0,0.0,0.9767441860465116,0.0,0.0
1205,571417,571727,"That's I don't know, that mostly just gets in the way I think. Also, it seems to me that something PT, or maybe even simply, this would get you similar or sufficient information.","On Thu, Mar 07, 2019 at 11:17:09AM -0600, Josh Poimboeuf wrote:  That's TRACE_BRANCH_PROFILING, PROFILE_ALL_BRANCHES I don't know, that mostly just gets in the way I think.  Also, it seems to me that something PT, or maybe even simply:    perf -e branches -e branch-misses  would get you similar or sufficient information.",technical,Peter Zijlstra,peterz@infradead.org,1,1,178,0.19523809523809524,0.4,0,0.0,0.9767441860465116,0.0,0.0
1206,571417,571734,"Excellent, let me put the kids to bed and then I'll have a poke.","On Thu, Mar 07, 2019 at 10:47:05AM -0600, Josh Poimboeuf wrote:   Excellen, let me put the kids to bed and then I'll have a poke.",technical,Peter Zijlstra,peterz@infradead.org,1,1,64,0.08095238095238096,0.42,0,0.0,0.9767441860465116,0.0,0.0
1207,571417,571753,"Yeah, I'm not really seeing a lot of upside to PROFILE_ALL_BRANCHES. Particularly since it doesn't actually profile all branches at all. It only basically profiles ""if ()"" statements, which obviously misses loops etc, but then also _does_ hit things where people turned loops into, which happens in (for example) low-level locking code etc that often has a fast-case ""first try ""thing followed by a slow-case ""ok, let's loop for it"" thing. So I think PROFILE_ALL_BRANCHES tends to have very random coverage. I'd love to get rid of it, because it seems so random.","On Thu, Mar 7, 2019 at 9:38 AM Peter Zijlstra <peterz@infradead.org> wrote:  Yeah, I'm not really seeing a lot of upside to PROFILE_ALL_BRANCHES.  Particularly since it doesn't actually profile all branches at all. It only basically profiles if ()"" statements, which obviously misses loops etc, but then also _does_ hit things where people turned loops into ""if (unlikely()) loop()"", which happens in (for example) low-level locking code etc that often has a fast-case ""first try"" thing followed by a slow-case ""ok, let's loop for it"" thing.  So I think PROFILE_ALL_BRANCHES tends to have very random coverage. I'd love to get rid of it, because it seems so random.                 Linus""",technical,Linus Torvalds,torvalds@linux-foundation.org,1,0,562,0.5571428571428572,0.43,0,0.0,0.9767441860465116,0.0,0.0
1208,571417,571775,Is that possible to do in a C macro?  Doesn't seem to work for me…,"On Thu, Mar 07, 2019 at 09:29:17AM -0800, hpa@zytor.com wrote:  Is that possible to do in a C macro?  Doesn't seem to work for me...  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,66,0.08095238095238096,0.44,0,0.0,0.9767441860465116,0.0,0.0
1209,571417,571777,"Here's a proper patch. Subject: [PATCH] tracing: Improve ""if"" macro code generation With CONFIG_PROFILE_ALL_BRANCHES, the ""if"" macro converts the conditional to an array index.  This can cause GCC to create horrible code.  When there are nested ifs, the generated code uses register values to encode branching decisions. Make it easier for GCC to optimize by keeping the conditional as a conditional rather than converting it to an integer.  This shrinks the generated code quite a bit, and also makes the code sane enough for objtool to understand.","On Thu, Mar 07, 2019 at 06:43:22PM +0100, Peter Zijlstra wrote:  Here's a proper patch.  From: Josh Poimboeuf <jpoimboe@redhat.com> Subject: [PATCH] tracing: Improve if"" macro code generation  With CONFIG_PROFILE_ALL_BRANCHES, the ""if"" macro converts the conditional to an array index.  This can cause GCC to create horrible code.  When there are nested ifs, the generated code uses register values to encode branching decisions.  Make it easier for GCC to optimize by keeping the conditional as a conditional rather than converting it to an integer.  This shrinks the generated code quite a bit, and also makes the code sane enough for objtool to understand.  Reported-by: Peter Zijlstra <peterz@infradead.org> Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com> ---  include/linux/compiler.h | 2 +-  1 file changed, 1 insertion(+), 1 deletion(-)  diff --git a/include/linux/compiler.h b/include/linux/compiler.h index 445348facea9..d58aa0db05f9 100644 --- a/include/linux/compiler.h +++ b/include/linux/compiler.h @@ -67,7 +67,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,  				.line = __LINE__,			\  			},						\  		______r = !!(cond),					\ -		______f.miss_hit[______r]++,					\ +		______r ? ______f.miss_hit[1]++ : ______f.miss_hit[0]++,\  		______r,						\  	}))  #endif /* CONFIG_PROFILE_ALL_BRANCHES */ --  2.17.2""",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,549,0.5,0.45,0,0.0,0.9767441860465116,0.0,0.0
1210,571417,571778,"The meat of that macro could easily be done as a helper inline function. But as mentioned, I think a better option would be to remove it entirely, if at all possible. The best part about that config option is the comment, and while cute I don't think that's really worth saving it for...","On Thu, Mar 7, 2019 at 9:46 AM Josh Poimboeuf <jpoimboe@redhat.com> wrote:  The meat of that macro could easily be done as a helper inline function.  But as mentioned, I think a better option would be to remove it entirely, if at all possible.  The best part about that config option is the comment, and while cute I don't think that's really worth saving it for...                 Linus",technical,Linus Torvalds,torvalds@linux-foundation.org,1,0,287,0.29523809523809524,0.46,0,0.0,0.9767441860465116,0.0,0.0
1211,571417,571812,"Ugh. I think I almost just agree with your decision to just let that memset go unchecked. I'm not saying it's right, but it doesn't seem to be a fight worth fighting. Again, maybe we could avoid the static checking entirely for the complex configs, and just make preempt_schedule() not do it for AC regions. Because AC vs KASAN in general ends up smelling like ""not a fight worth fighting"" to me. You've done a herculean job, but..","On Thu, Mar 7, 2019 at 9:41 AM Peter Zijlstra <peterz@infradead.org> wrote:  Ugh. I think I almost just agree with your decision to just let that memset go unchecked.  I'm not saying it's right, but it doesn't seem to be a fight worth fighting.  Again, maybe we could avoid the static checking entirely for the complex configs, and just make preempt_schedule() not do it for AC regions.  Because AC vs KASAN in general ends up smelling like not a fight worth fighting"" to me. You've done a herculean job, but..                   Linus""",technical,Linus Torvalds,torvalds@linux-foundation.org,1,0,431,0.45714285714285713,0.47,0,0.0,0.9767441860465116,0.0,0.0
1212,571417,571816,Let's move it instead.  I looked at the code and it should be fine.,"On Thu, Mar 7, 2019 at 3:52 AM Peter Zijlstra <peterz@infradead.org> wrote:  Let's move it instead.  I looked at the code and it should be fine.  --Andy",technical,Andy Lutomirski,luto@amacapital.net,1,0,67,0.08095238095238096,0.48,0,0.0,0.9767441860465116,0.0,0.0
1213,571417,571833,"As Josh said, I run it once a year on two production (real use)machines for 2 to 4 weeks and collect the data to see if there are places that can be optimized better. I currently have one of my engineers looking at the data and may be sending patches soon. It's basically an entry level way to get into kernel development. Note, no patch will be sent just because of the data from the profiling. The task is to look at and understand the code, and see if it can be optimized (with likely/unlikely or flow changes). It's a way to get a better understanding of the kernel in various locations. It is by no means ""profiler said this, lets change it."" All changes must be rational, and make sense. The profiler is only used to help find those places. The data that was run at the end of January can be found here","On Thu, 7 Mar 2019 09:45:35 -0800 Linus Torvalds <torvalds@linux-foundation.org> wrote:   As Josh said, I run it once a year on two production (real use) machines for 2 to 4 weeks and collect the data to see if there are places that can be optimized better.  I currently have one of my engineers looking at the data and may be sending patches soon. It's basically an entry level way to get into kernel development. Note, no patch will be sent just because of the data from the profiling. The task is to look at and understand the code, and see if it can be optimized (with likely/unlikely or flow changes). It's a way to get a better understanding of the kernel in various locations. It is by no means profiler said this, lets change it."" All changes must be rational, and make sense. The profiler is only used to help find those places.  The data that was run at the end of January can be found here:   http://rostedt.homelinux.com/branches/mammoth-branches-2019/  http://rostedt.homelinux.com/branches/gandalf-branches-2019/  -- Steve""",technical,Steven Rostedt,rostedt@goodmis.org,1,0,808,0.8476190476190476,0.49,0,0.0,0.9767441860465116,0.0,0.0
1214,571417,571857,"One think I could do, is add a filter to each function and only allow__memset from the kasan code, and not from anywhere else. Another thing I need to look at is why objtool only found memset_orig(from __memset) but not memset_erms, which if I read the code right, is a possible alternative there. I know,.. I've been so close to doing that so many times, but it seems like defeat, esp. since I'm so close now :-)","On Thu, Mar 07, 2019 at 09:54:14AM -0800, Linus Torvalds wrote:  One think I could do, is add a filter to each function and only allow __memset from the kasan code, and not from anywhere else.  Another thing I need to look at is why objtool only found memset_orig (from __memset) but not memset_erms, which if I read the code right, is a possible alternative there.   I know,.. I've been so close to doing that so many times, but it seems like defeat, esp. since I'm so close now :-)",technical,Peter Zijlstra,peterz@infradead.org,1,1,413,0.4523809523809524,0.5,0,0.0,0.9767441860465116,0.0,0.0
1215,571417,571872,"Ah.. how about I feed objtool a text file with all these symbol names, and I have Makefile compose file that from fragments. Then only KASAN builds will have memset whitelisted, and any other build will still flag memset abuse. Now I only have to figure out how to make Makefile do something like that :-)","On Thu, Mar 07, 2019 at 07:48:13PM +0100, Peter Zijlstra wrote:  Ah.. how about I feed objtool a text file with all these symbol names, and I have Makefile compose file that from fragments.  Then only KASAN builds will have memset whitelisted, and any other build will still flag memset abuse.  Now I only have to figure out how to make Makefile do something like that :-)",technical,Peter Zijlstra,peterz@infradead.org,1,1,305,0.3,0.53,0,0.0,0.9767441860465116,0.0,0.0
1216,571417,571876,"Also, this stuff is pretty well covered by the x86 self tests, mostly because getting it right in the first place was way too subtle for comfort.","On Thu, Mar 7, 2019 at 10:59 AM Peter Zijlstra <peterz@infradead.org> wrote:  Reviewed-by: Andy Lutomirski <luto@kernel.org>  Also, this stuff is pretty well covered by the x86 selftests, mostly because getting it right in the first place was way too subtle for comfort.  --Andy",technical,Andy Lutomirski,luto@kernel.org,1,0,145,0.14285714285714285,0.54,0,0.0,0.9767441860465116,0.0,0.0
1217,571417,571910,"These two can be called only with CONFIG_KASAN_EXTRA=y which was removed very recently, so it should be safe to delete these functions.","On 3/7/19 8:41 PM, Peter Zijlstra wrote:  These two can be called only with CONFIG_KASAN_EXTRA=y which  was removed very recently, so it should be safe to delete these functions.",technical,Andrey Ryabinin,aryabinin@virtuozzo.com,1,0,135,0.11428571428571428,0.55,0,0.0,0.9767441860465116,0.0,0.0
1218,571417,571917,"Turns out we only look for sibling calls in the original instruction stream, not in any alternatives, which in general seems like a fair enough assumption.","On Thu, Mar 07, 2019 at 07:48:13PM +0100, Peter Zijlstra wrote:  Turns out we only look for sibling calls in the original instruction stream, not in any alternatives, which in general seems like a fair enough assumption.",technical,Peter Zijlstra,peterz@infradead.org,1,1,155,0.1380952380952381,0.56,0,0.0,0.9767441860465116,0.0,0.0
1219,571417,571923,"Ooh shiny. Clearly my tree still has them, what commit do I need to look for?","On Thu, Mar 07, 2019 at 11:15:42PM +0300, Andrey Ryabinin wrote:  Ooh shiny. Clearly my tree still has them, what commit do I need to look for?",technical,Peter Zijlstra,peterz@infradead.org,1,1,77,0.09047619047619047,0.57,0,0.0,0.9767441860465116,0.0,0.0
1220,571417,571930,"And while I'm looking at memset_64.S, why are memset_erms and memset_orig global functions? At the very least they should be local, and ideally not even functions.","On Thu, Mar 07, 2019 at 09:23:19PM +0100, Peter Zijlstra wrote:  And while I'm looking at memset_64.S, why are memset_erms and memset_orig global functions? At the very least they should be local, and ideally not even functions.",technical,Peter Zijlstra,peterz@infradead.org,1,1,163,0.14761904761904762,0.59,0,0.0,0.9767441860465116,0.0,0.0
1221,571417,571933,kasan: remove use after scope bugs detection.,"On Thu, Mar 07, 2019 at 09:33:18PM +0100, Peter Zijlstra wrote:   30be39d1e1dc (kasan: remove use after scope bugs detection."")""",technical,Peter Zijlstra,peterz@infradead.org,1,1,45,0.04285714285714286,0.6,0,0.0,0.9767441860465116,0.0,0.0
1222,571417,572696,"Instead of adding all those additional moving parts, I would much rather either: a) have kasan call a special whitelisted version of memset (like hpa   suggested), orb) just don't use the objtool --uaccess flag for KASAN builds.","On Thu, Mar 07, 2019 at 08:03:13PM +0100, Peter Zijlstra wrote:  Instead of adding all those additional moving parts, I would much rather either:  a) have kasan call a special whitelisted version of memset (like hpa    suggested), or  b) just don't use the objtool --uaccess flag for KASAN builds.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,228,0.22380952380952382,0.61,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1223,571417,572700,"I think the only benefit is that they would show up better on stacktraces, but that could also be solved by just making them local labels inside memset.  Which is what I think they should be. The general rule is that ENDPROC is only used for callable functions, so yeah, I think the current setup isn't ideal, and also prevents objtool from properly doing the AC analysis as you pointed out earlier.","On Thu, Mar 07, 2019 at 09:40:21PM +0100, Peter Zijlstra wrote:  I think the only benefit is that they would show up better on stack traces, but that could also be solved by just making them local labels inside memset.  Which is what I think they should be.  The general rule is that ENDPROC is only used for callable functions, so yeah, I think the current setup isn't ideal, and also prevents objtool from properly doing the AC analysis as you pointed out earlier.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,399,0.38095238095238093,0.62,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1224,571417,572701,"The problem went away. That is, the problematic part of KASAN just got removed in this merge window. Also, I just about had hpa's __memset_kasan implemented when I got that email.","On Fri, Mar 08, 2019 at 09:01:11AM -0600, Josh Poimboeuf wrote:   The problem went away. That is, the problematic part of KASAN just got removed in this merge window.  Also, I just about had hpa's __memset_kasan implemented when I got that email.",technical,Peter Zijlstra,peterz@infradead.org,1,1,179,0.1761904761904762,0.63,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1225,571417,572711,"Boris wanted to use alternative_call_2, just like copy_user_generic(). Which makes more sense to me still.","On Fri, Mar 08, 2019 at 09:07:03AM -0600, Josh Poimboeuf wrote:  Boris wanted to use alternative_call_2, just like copy_user_generic(). Which makes more sense to me still.",technical,Peter Zijlstra,peterz@infradead.org,1,1,106,0.09523809523809523,0.64,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1226,571417,572889,"This is an unstructured piece of code rather than a callable function, END would probably be more appropriate.  Or maybe it should just be a local label (.Lcopy_user_handle_tail) because I don't think the alignment and ELF symbol size are even needed.","On Thu, Mar 07, 2019 at 12:45:14PM +0100, Peter Zijlstra wrote:  This is an unstructured piece of code rather than a callable function, END would probably be more appropriate.  Or maybe it should just be a local label (.Lcopy_user_handle_tail) because I don't think the alignment and ELF symbol size are even needed.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,251,0.22380952380952382,0.65,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1227,571417,572892,Shouldn't these be using SMAP-based alternatives like stac()/clac() do?,"On Thu, Mar 07, 2019 at 12:45:16PM +0100, Peter Zijlstra wrote:  Shouldn't these be using SMAP-based alternatives like stac()/clac() do?  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,71,0.0761904761904762,0.66,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1228,571417,572896,Also this you could get rid of the comment if there were an ANNOTATE_AC_SAFE macro which does a similar thing.,"On Fri, Mar 08, 2019 at 01:00:38PM -0600, Josh Poimboeuf wrote:  Also this you could get rid of the comment if there were an ANNOTATE_AC_SAFE macro which does a similar thing.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,110,0.1,0.67,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1229,571417,572905,Can you also say _why_?  I presume it's so the warning messages will be saner.,"On Thu, Mar 07, 2019 at 12:45:23PM +0100, Peter Zijlstra wrote:  Can you also say _why_?  I presume it's so the warning messages will be saner.   --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,78,0.08571428571428572,0.68,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1230,571417,572906,Also you can clarify why we need to track aliases?,"On Thu, Mar 07, 2019 at 12:45:24PM +0100, Peter Zijlstra wrote:  s/Hande/Handle/ in $SUBJECT  Also you can clarify why we need to track aliases?  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,50,0.05238095238095238,0.69,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1231,571417,572915,We should have done this a long time ago.  Thank you for this!,"On Thu, Mar 07, 2019 at 12:45:26PM +0100, Peter Zijlstra wrote:  We should have done this a long time ago.  Thank you for this!  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,62,0.07142857142857142,0.71,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1232,571417,572927,ENDPROC makes it STT_FUNC and gets us stricter AC tests.,"On Fri, Mar 08, 2019 at 12:53:21PM -0600, Josh Poimboeuf wrote:  ENDPROC makes it STT_FUNC and gets us stricter AC tests.",technical,Peter Zijlstra,peterz@infradead.org,1,1,56,0.05238095238095238,0.72,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1233,571417,572929,"Right, because I'll introduce function attributes later on, so it becomes important to know what function an instruction belongs to. I'll update the Changelog.","On Fri, Mar 08, 2019 at 01:16:29PM -0600, Josh Poimboeuf wrote:  Right, because I'll introduce function attributes later on, so it becomes important to know what function an instruction belongs to.  I'll update the Changelog.",technical,Peter Zijlstra,peterz@infradead.org,1,1,159,0.14285714285714285,0.74,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1234,571417,572930,"Whoops :-) Function attributes (as per that previous patch) and: git grep ""alias ""mm/kasan.","On Fri, Mar 08, 2019 at 01:23:58PM -0600, Josh Poimboeuf wrote:  Whoops :-)   Function attributes (as per that previous patch) and: git grep alias"" mm/kasan.""",technical,Peter Zijlstra,peterz@infradead.org,1,1,91,0.10476190476190476,0.75,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1235,571417,572931,How so?  I would have thought the opposite.  Doesn't objtool only follow a jump if its destination is to a non-function?  Otherwise it's considered a sibling call.,"On Fri, Mar 08, 2019 at 08:48:35PM +0100, Peter Zijlstra wrote:  How so?  I would have thought the opposite.  Doesn't objtool only follow a jump if its destination is to a non-function?  Otherwise it's considered a sibling call.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,163,0.15714285714285714,0.76,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1236,571417,572933,"Ok, such details would be good in the patch description.","On Fri, Mar 08, 2019 at 08:52:54PM +0100, Peter Zijlstra wrote:  Ok, such details would be good in the patch description.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,56,0.05714285714285714,0.77,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1237,571417,572941,I like this approach.  I wonder if we can do something similar to get rid of the nasty fake jumps,"On Thu, Mar 07, 2019 at 12:45:27PM +0100, Peter Zijlstra wrote:  I like this approach.  I wonder if we can do something similar to get rid of the nasty fake jumps.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,97,0.1,0.78,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1238,571417,572946,and we need to know who we're calling because...s/class/call/ I would rather have 2-3 duplicated lines of code than complicating the control flow like this.,"On Thu, Mar 07, 2019 at 12:45:28PM +0100, Peter Zijlstra wrote:  and we need to know who we're calling because...   s/class/call/   I would rather have 2-3 duplicated lines of code than complicating the control flow like this.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,156,0.1380952380952381,0.79,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1239,571417,572965,"Style nit, no need for all those brackets and newlines. A short comment would be good here, something describing why a function might be added to the list. This won't work if the function name changes due to IPA optimizations. I assume these are all global functions so maybe it's fine? These goto's make my head spin.  Again I would much prefer a small amount of code duplication over this. Why is this a problem? Same question here.--","On Thu, Mar 07, 2019 at 12:45:29PM +0100, Peter Zijlstra wrote:  Style nit, no need for all those brackets and newlines.   A short comment would be good here, something describing why a function might be added to the list.   This won't work if the function name changes due to IPA optimizations. I assume these are all global functions so maybe it's fine?   These gotos make my head spin.  Again I would much prefer a small amount of code duplication over this.   Why is this a problem?   Same question here.   --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,436,0.43333333333333335,0.8,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1240,571417,572975,"Needs more description -- namely, what and why.","On Thu, Mar 07, 2019 at 12:45:30PM +0100, Peter Zijlstra wrote:  Needs more description -- namely, what and why.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,47,0.047619047619047616,0.81,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1241,571417,572978,Can you elaborate on why (in the patch description)?  Did this actually find any occurrences?,"On Thu, Mar 07, 2019 at 12:45:31PM +0100, Peter Zijlstra wrote:  Can you elaborate on why (in the patch description)?  Did this actually find any occurrences?  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,93,0.09047619047619047,0.82,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1242,571417,572991,"There is, but I'm thinking it might be too short? Those are gone. That is the main kasan_report function, and is for, as the comment says: kasan :-) These, are, again as the comment suggests, the out-of-line KASAN ABIcalls.The in-line KASAN ABI Also, can I just say that this bugs the hell out of me? the logger function KCOV ABI implementation UBSAN ABI With one or two exceptions, yep.I didn't think the code was that bad once you see the end result, but sure, I can try something else. 'obvious' violation? If we do the STAC we must also do the CLAC. If we don't do the STAC we must also not do the CLAC. In general, validating NOPs isn't too interesting, so all NOP/INSN binary alternatives could be forced on. I've actually changed this to depend on --uaccess, when set we force on FEATURE_SMAP, otherwise we force off.","On Fri, Mar 08, 2019 at 03:02:09PM -0600, Josh Poimboeuf wrote:  There is, but I'm thinking it might be too short?   Those are gone.   That is the main kasan_report function, and is for, as the comment says: kasan :-)   for __asan_{load,store}N   These, are, again as the comment suggests, the out-of-line KASAN ABI calls.   The in-line KASAN ABI  Also, can I just say that {load,store}N vs {load,store}_n bugs the hell out of me?   the logger function   KCOV ABI   implementation   UBSAN ABI    With one or two exceptions, yep.   I didn't think the code was that bad once you see the end result, but sure, I can try something else.   'obvious' violation?  STAC, .... RET, # an AC=1 leak  .... CLAC, # spurious CLAC  If we do the STAC we must also do the CLAC. If we don't do the STAC we must also not do the CLAC.    In general, validating NOPs isn't too interesting, so all NOP/INSN binary alternatives could be forced on.   I've actually changed this to depend on --uaccess, when set we force on FEATURE_SMAP, otherwise we force off.",technical,Peter Zijlstra,peterz@infradead.org,1,1,824,0.8666666666666667,0.83,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1243,571417,572993,"Nope, didn't find anything. Also, all DF users are in asm so I didn't really expect any. Having it escape would probably result in fairly instant wreckage though. DF=1 results in things like rep mov going _backwards_.","On Fri, Mar 08, 2019 at 03:16:03PM -0600, Josh Poimboeuf wrote:  Nope, didn't find anything. Also, all DF users are in asm so I didn't really expect any. Having it escape would probably result in fairly instant wreckage though.  DF=1 results in things like rep mov going _backwards_.",technical,Peter Zijlstra,peterz@infradead.org,1,1,217,0.21428571428571427,0.84,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1244,571417,572994,This actually hard relies on those fake jumps. Or am I missing the point?,"On Fri, Mar 08, 2019 at 02:15:56PM -0600, Josh Poimboeuf wrote:  This actually hard relies on those fake jumps. Or am I missing the point?",technical,Peter Zijlstra,peterz@infradead.org,1,1,73,0.0761904761904762,0.85,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1245,571417,573000,"I actually just meant a comment above uaccess_safe_builtin describing what the purpose of the list is and what the expectations are for the listed functions.  i.e. these are functions which are allowed to be called with the AC flag on, and they should not clear it unless they're saving/restoring. Makes sense now, can you add that last sentence to the paragraph? Right, but it doesn't sound like there's any real benefit to adding extra logic. Ok.","On Fri, Mar 08, 2019 at 10:31:56PM +0100, Peter Zijlstra wrote:  I actually just meant a comment above uaccess_safe_builtin describing what the purpose of the list is and what the expectations are for the listed functions.  i.e. these are functions which are allowed to be called with the AC flag on, and they should not clear it unless they're saving/restoring.   Makes sense now, can you add that last sentence to the paragraph?   Right, but it doesn't sound like there's any real benefit to adding extra logic.   Ok.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,448,0.41904761904761906,0.86,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1246,571417,573002,"Ok, I wonder if we really need to add this then.","On Fri, Mar 08, 2019 at 10:33:43PM +0100, Peter Zijlstra wrote:  Ok, I wonder if we really need to add this then.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,48,0.06190476190476191,0.87,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1247,571417,573015,"I was just wondering out loud if we could somehow keep the same ""fake jump"" functionality, but do it in a cleaner way that doesn't require creating fake instructions.  I'll might give it a try.","On Fri, Mar 08, 2019 at 10:34:18PM +0100, Peter Zijlstra wrote:  I was just wondering out loud if we could somehow keep the same fake jump"" functionality, but do it in a cleaner way that doesn't require creating fake instructions.  I'll might give it a try.  --  Josh""",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,193,0.2,0.88,0,0.023255813953488372,0.9534883720930233,0.0,0.023255813953488372
1248,571417,573421,"Please note that there is nothing wrong in the generated code, just that it confuses objtool. Clang has simply omitted the statement where NULL is returned since the pointer was always dereferenced post in lining. Note that GCC will also remove the NULL pointers if it knows that the pointer is dereferenced. Here is an example.","On Fri, Mar 08, 2019 at 03:02:09PM -0600, Josh Poimboeuf wrote:   something like so then?  --- a/tools/objtool/check.c +++ b/tools/objtool/check.c @@ -1888,6 +1888,23 @@ static inline const char *insn_dest_name  	return {dynamic}"",  }   +static int validate_call(struct instruction *insn, struct insn_state *state) +{ +	if (state->uaccess && !func_uaccess_safe(insn->call_dest)) { +		WARN_FUNC(""call to %s() with UACCESS enabled"", +				insn->sec, insn->offset, insn_dest_name(insn)), +		return 1, +	} + +	if (state->df) { +		WARN_FUNC(""call to %s() with DF set"", +				insn->sec, insn->offset, insn_dest_name(insn)), +		return 1, +	} + +	return 0, +} +  /*   * Follow the branch starting at the given instruction, and recursively follow   * any other branches (jumps).  Meanwhile, track the frame pointer state at @@ -2036,25 +2053,9 @@ static int validate_branch(struct objtoo    		case INSN_CALL:  		case INSN_CALL_DYNAMIC: -do_call: -			if (state.uaccess && !func_uaccess_safe(insn->call_dest)) { -				WARN_FUNC(""call to %s() with UACCESS enabled"", -					  sec, insn->offset, insn_dest_name(insn)), -				return 1, -			} - -			if (state.df) { -				WARN_FUNC(""call to %s() with DF set"", -					  sec, insn->offset, insn_dest_name(insn)), -				return 1, -			} - -			if (insn->type == INSN_JUMP_UNCONDITIONAL || -			    insn->type == INSN_JUMP_DYNAMIC) -				return 0, - -			if (insn->type == INSN_JUMP_CONDITIONAL) -				break, +			ret = validate_call(insn, &state), +			if (ret) +				return ret,    			if (insn->type == INSN_CALL) {  				if (is_fentry_call(insn)) @@ -2077,13 +2078,15 @@ static int validate_branch(struct objtoo  		case INSN_JUMP_CONDITIONAL:  		case INSN_JUMP_UNCONDITIONAL:  			if (func && !insn->jump_dest) { -do_sibling_call: +				/* sibling call */  				if (has_modified_stack_frame(&state)) {  					WARN_FUNC(""sibling call from callable instruction with modified stack frame"",  							sec, insn->offset),  					return 1,  				} -				goto do_call, +				ret = validate_call(insn, &state), +				if (ret) +					return ret,    			} else if (insn->jump_dest &&  				   (!func || !insn->jump_dest->func || @@ -2104,8 +2107,17 @@ static int validate_branch(struct objtoo  			break,    		case INSN_JUMP_DYNAMIC: -			if (func && list_empty(&insn->alts)) -				goto do_sibling_call, +			if (func && list_empty(&insn->alts)) { +				/* sibling call */ +				if (has_modified_stack_frame(&state)) { +					WARN_FUNC(""sibling call from callable instruction with modified stack frame"", +							sec, insn->offset), +					return 1, +				} +				ret = validate_call(insn, &state), +				if (ret) +					return ret, +			}    			return 0,""",technical,Peter Zijlstra,peterz@infradead.org,1,1,328,0.2904761904761905,0.89,0,0.06976744186046512,0.9069767441860465,0.023255813953488372,0.0
1249,571417,573422,"See patch 8. This really should be folded back in the previous patch, but it is easier to look at without all that mixed in.","On Fri, Mar 08, 2019 at 03:11:22PM -0600, Josh Poimboeuf wrote:  See patch 8. This really should be folded back in the previous patch, but it is easier to look at without all that mixed in.",technical,Peter Zijlstra,peterz@infradead.org,1,1,124,0.13333333333333333,0.9,0,0.06976744186046512,0.9069767441860465,0.0,0.0
1250,571417,573423,"Well, Linus asked for it, and it was a fairly trivial add-on :-)","On Fri, Mar 08, 2019 at 03:56:02PM -0600, Josh Poimboeuf wrote:   Well, Linus asked for it, and it was a fairly trivial add-on :-)",technical,Peter Zijlstra,peterz@infradead.org,1,1,64,0.08095238095238096,0.91,0,0.06976744186046512,0.9069767441860465,0.0,0.0
1251,571417,573424,Can't you just have those same engineers look at perf data? This seems like a very expensive and convoluted way of getting something.,"On Thu, Mar 07, 2019 at 01:18:41PM -0500, Steven Rostedt wrote:   Can't you just have those same engineers look at perf data? This seems like a very expensive and convoluted way of getting something.",technical,Peter Zijlstra,peterz@infradead.org,1,1,133,0.12380952380952381,0.92,0,0.06976744186046512,0.9069767441860465,0.0,0.0
1252,571417,573425,How about we just rename the one annotation we have? I figured it was a waste of LoC to do yet another annotation that does the very same thing.,"On Fri, Mar 08, 2019 at 01:03:34PM -0600, Josh Poimboeuf wrote:    How about we just rename the one annotation we have? I figured it was a waste of LoC to do yet another annotation that does the very same thing.",technical,Peter Zijlstra,peterz@infradead.org,1,1,144,0.14761904761904762,0.93,0,0.06976744186046512,0.9069767441860465,0.0,0.0
1253,571417,573426,"Normally yes, but we don't do that for .fixup I think. And by setting STT_FUNC we enable the 'redundant CLAC' warning, which is ignored for !STT_FUNC.","On Fri, Mar 08, 2019 at 01:53:02PM -0600, Josh Poimboeuf wrote:  Normally yes, but we don't do that for .fixup I think. And by setting STT_FUNC we enable the 'redundant CLAC' warning, which is ignored for !STT_FUNC.",technical,Peter Zijlstra,peterz@infradead.org,1,1,150,0.15714285714285714,0.94,0,0.06976744186046512,0.9069767441860465,0.0,0.023255813953488372
1254,571417,574440,"I think ANNOTATE_IGNORE_ALTERNATIVE would hurt readability too.  How about just see this or, make both ANNOTATE_UACCESS_SAFE and ANNOTATE_NOSPEC_ALTERNATIVE defined to the same OBJTOOL_IGNORE_ALTERNATIVE macro.","On Sun, Mar 10, 2019 at 02:19:29PM +0100, Peter Zijlstra wrote:  I think ANNOTATE_IGNORE_ALTERNATIVE would hurt readability too.  How about just    #define ANNOTATE_UACCESS_SAFE ANNOTATE_NOSPEC_ALTERNATIVE  or, make both ANNOTATE_UACCESS_SAFE and ANNOTATE_NOSPEC_ALTERNATIVE defined to the same OBJTOOL_IGNORE_ALTERNATIVE macro.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,210,0.12857142857142856,0.95,0,0.09302325581395349,0.8837209302325582,0.023255813953488372,0.0
1255,571417,574442,"If it doesn't matter much either way, I'd rather err on the side of less code.  Your call though.","On Sun, Mar 10, 2019 at 02:13:31PM +0100, Peter Zijlstra wrote:  If it doesn't matter much either way, I'd rather err on the side of less code.  Your call though.  --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,97,0.11428571428571428,0.96,0,0.09302325581395349,0.8837209302325582,0.0,0.0
1256,571417,574444,"Yeah, that looks a lot nicer to me.  Thanks.","On Sun, Mar 10, 2019 at 02:10:46PM +0100, Peter Zijlstra wrote:  Yeah, that looks a lot nicer to me.  Thanks.   --  Josh",technical,Josh Poimboeuf,jpoimboe@redhat.com,1,0,44,0.05714285714285714,0.97,0,0.09302325581395349,0.8837209302325582,0.0,0.5116279069767442
1257,571417,602444,"Commit-ID:  tracing: Improve ""if"" macro code generation With CONFIG_PROFILE_ALL_BRANCHES=y, the ""if"" macro converts the conditional to an array index.  This can cause GCC to create horrible code.  When there are nested ifs, the generated code uses register values to encode branching decisions. Make it easier for GCC to optimize by keeping the conditional as a conditional rather than converting it to an integer.  This shrinks the generated code quite a bit, and also makes the code sane enough for objtool to understand.","Commit-ID:  37686b1353cfc30e127cef811959cdbcd0495d98 Gitweb:     https://git.kernel.org/tip/37686b1353cfc30e127cef811959cdbcd0495d98 Author:     Josh Poimboeuf <jpoimboe@redhat.com> AuthorDate: Thu, 7 Mar 2019 11:48:02 -0600 Committer:  Ingo Molnar <mingo@kernel.org> CommitDate: Wed, 3 Apr 2019 09:36:27 +0200  tracing: Improve if"" macro code generation  With CONFIG_PROFILE_ALL_BRANCHES=y, the ""if"" macro converts the conditional to an array index.  This can cause GCC to create horrible code.  When there are nested ifs, the generated code uses register values to encode branching decisions.  Make it easier for GCC to optimize by keeping the conditional as a conditional rather than converting it to an integer.  This shrinks the generated code quite a bit, and also makes the code sane enough for objtool to understand.  Reported-by: Peter Zijlstra <peterz@infradead.org> Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com> Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> Cc: Borislav Petkov <bp@alien8.de> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: brgerst@gmail.com Cc: catalin.marinas@arm.com Cc: dvlasenk@redhat.com Cc: dvyukov@google.com Cc: hpa@zytor.com Cc: james.morse@arm.com Cc: julien.thierry@arm.com Cc: luto@amacapital.net Cc: luto@kernel.org Cc: rostedt@goodmis.org Cc: valentin.schneider@arm.com Cc: will.deacon@arm.com Link: https://lkml.kernel.org/r/20190307174802.46fmpysxyo35hh43@treble Signed-off-by: Ingo Molnar <mingo@kernel.org> ---  include/linux/compiler.h | 2 +-  1 file changed, 1 insertion(+), 1 deletion(-)  diff --git a/include/linux/compiler.h b/include/linux/compiler.h index 445348facea9..d58aa0db05f9 100644 --- a/include/linux/compiler.h +++ b/include/linux/compiler.h @@ -67,7 +67,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,  				.line = __LINE__,			\  			},						\  		______r = !!(cond),					\ -		______f.miss_hit[______r]++,					\ +		______r ? ______f.miss_hit[1]++ : ______f.miss_hit[0]++,\  		______r,						\  	}))  #endif /* CONFIG_PROFILE_ALL_BRANCHES */""",technical,tip-bot for Josh Poimboeuf,tipbot@zytor.com,0,0,523,0.45714285714285713,0.98,0,0.6046511627906976,0.37209302325581395,0.5116279069767442,0.37209302325581395
1258,571417,620383,"I haven't tried the perf data. How well does it work with running over a 2 weeks to a month period? That's what I do yearly. Here's the results of my last run: I have a cron job that runs nightly that copies the current state, and if the machine reboots, it starts a new file (which is why there's multiple files for mammoth - it rebooted). I've never used it, so I have no idea if it is suitable or not. But is it a real burden? It's been in the kernel for over 10 years with very little issue. Only when we do something drastic does it showup, and it's usually a quick fix to get it working again. I believe Josh even told me that it found a bug in the objtool code, so it does still have benefit staying in the kernel even without people using it for profiling. Note, I'm in the middle of writing a LWN article about learning the kernel from branch profiling and it would be a shame if it disappears before I finish it.","On Fri, 19 Apr 2019 12:08:37 +0200 Ingo Molnar <mingo@kernel.org> wrote:   I haven't tried the perf data. How well does it work with running over a 2 weeks to a month period? That's what I do yearly. Here's the results of my last run:    http://rostedt.homelinux.com/branches/gandalf-branches-2019/brach_all-2019-02-05    http://rostedt.homelinux.com/branches/mammoth-branches-2019/branch_all-2019-01-02   http://rostedt.homelinux.com/branches/mammoth-branches-2019/branch_all-2019-01-03   http://rostedt.homelinux.com/branches/mammoth-branches-2019/branch_all-2019-01-17   http://rostedt.homelinux.com/branches/mammoth-branches-2019/branch_all-2019-02-05  I have a cron job that runs nightly that copies the current state, and if the machine reboots, it starts a new file (which is why there's multiple files for mammoth - it rebooted).   I've never used it, so I have no idea if it is suitable or not.   But is it a real burden? It's been in the kernel for over 10 years with very little issue. Only when we do something drastic does it show up, and it's usually a quick fix to get it working again.  I believe Josh even told me that it found a bug in the objtool code, so it does still have benefit staying in the kernel even without people using it for profiling.  Note, I'm in the middle of writing a LWN article about learning the kernel from branch profiling and it would be a shame if it disappears before I finish it.  -- Steve",technical,Steven Rostedt,rostedt@goodmis.org,1,0,922,1.0,1.0,1,1.0,0.0,0.0,0.0
1259,571417,571505,"These two can be called only with CONFIG_KASAN_EXTRA=y which was removed very recently, so it should be safe to delete these functions.","On Thu, Mar 07, 2019 at 12:45:18PM +0100, Peter Zijlstra wrote:  arch/x86/kernel/signal.o: warning: objtool: do_signal()+0x384: call to frame_uc_flags.isra.0() with UACCESS enabled  @@ -441,7 +441,7 @@ static int __setup_rt_frame(int sig, str  	return 0,  }  #else /* !CONFIG_X86_32 */ -static unsigned long frame_uc_flags(struct pt_regs *regs) +static __always_inline unsigned long frame_uc_flags(struct pt_regs *regs)  {  	unsigned long flags,",technical,Peter Zijlstra,peterz@infradead.org,1,1,135,0.11428571428571428,0.28,0,0.0,0.9767441860465116,0.0,0.0
1260,571417,572928,"They are, __ASM_{ST,CL}AC are the raw instructions ASM_{ST,CL}AC are the alternatives.","On Fri, Mar 08, 2019 at 01:00:38PM -0600, Josh Poimboeuf wrote:  They are, __ASM_{ST,CL}AC are the raw instructions ASM_{ST,CL}AC are the alternatives.",technical,Peter Zijlstra,peterz@infradead.org,1,1,86,0.11904761904761904,0.73,0,0.023255813953488372,0.9534883720930233,0.0,0.0
1261,571989,572637,"Hello After thinking about this dt header I am not happy about having it. I am thinking of removing this file completely and doing some calculations in the driverfile for the impedance, averaging time and ramp times. I would still like to get comments on the rest of the code but in v2 the dt bindings header will go away.","Hello  On 3/7/19 4:09 PM, Dan Murphy wrote:  After thinking about this dt header I am not happy about having it. I am thinking of removing this file completely and doing some calculations in the driver file for the impedance, averaging time and ramp times.  I would still like to get comments on the rest of the code but in v2 the dt bindings header will go away.  Dan  <snip> --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,322,0.5765765765765766,0.3125,0,0.0,1.0,0.0,0.0
1262,571989,572710,"Hi! I guess dt people will have some comments here. I'd expect ramp-up-us = <1024> would be more natural. Ok, so we'll have lm3532::backlight. That is not too useful, as it does not tell userland what kaclight it is.main_display::backlight ?OTOH this one is not too important as backlight subsystem should handle this. I guess best variant would be inputX::backlight here, but that might be tricky to implement.","Hi!   I guess dt people will have some comments here. I'd expect  ramp-up-us = <1024> would be more natural.   Ok, so we'll have lm3532::backlight. That is not too useful, as it does not tell userland what kaclight it is.  main_display::backlight ?  OTOH this one is not too important as backlight subsystem should handle this.   I guess best variant would be inputX::backlight here, but that might be tricky to implement.  									Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html",technical,Pavel Machek,pavel@ucw.cz,1,0,411,0.7837837837837838,0.375,0,0.0,1.0,0.0,0.0
1263,571989,572723,Thanks for the review. Actually ramp-up-us/ramp-down-us is more correct this is an error in this dt definition and will be fixed in v2For Droid4 I am not particular to any specific label. And if the series in is ever implemented then the label may change as well. The driver forces the lm3532 string to be part of the label. This was a discussion a while back with Jacek when I submitted other drivers.Yeah because we don't know what input the keyboard would be on. Unless there are APIs to retrieve that info.  I have not looked at the input framework in a while.,"Pavel  Thanks for the review.  On 3/8/19 9:14 AM, Pavel Machek wrote:  Actually ramp-up-us/ramp-down-us is more correct this is an error in this dt definition and will be fixed in v2    For Droid4 I am not particular to any specific label.  And if the series in https://lkml.org/lkml/2018/11/7/144 is ever implemented then the label may change as well.  The driver forces the lm3532 string to be part of the label.  This was a discussion a while back with Jacek when I submitted other drivers.   Yeah because we don't know what input the keyboard would be on. Unless there are APIs to retrieve that info.  I have not looked at the input framework in a while.  Dan    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,564,1.0,0.4375,0,0.0,1.0,0.0,0.0
1264,571989,572968,"Lets just make it ""platform::kbd_backlight"". *:kbd_backlight is already used by quite a few drivers.","  Lets just make it platform::kbd_backlight"". *:kbd_backlight is already used by quite a few drivers.  									Pavel --  (english) http://www.livejournal.com/~pavelmachek (cesky, pictures) http://atrey.karlin.mff.cuni.cz/~pavel/picture/horses/blog.html """,technical,Pavel Machek,pavel@ucw.cz,1,0,100,0.1981981981981982,0.5,0,0.0,0.8,0.0,0.0
1265,571989,572971,Sounds fine to me.  I will update in v3 as I posted v2 with a lot of deltas,"Pavel  On 3/8/19 3:06 PM, Pavel Machek wrote:  Sounds fine to me.  I will update in v3 as I posted v2 with a lot of deltas  Dan    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,75,0.17117117117117117,0.5625,0,0.0,0.8,0.0,0.2
1266,571989,573540,"Thank your for the patch. I have some comments below, please take a look. If control_bank was validated in lm3532_parse_node(), then you wouldn't have to take into account this option here. Besides, you don't need this switch statement, but can easily calculate. Ditto. Similarly here. use following macro. mutex_destroy() is missing here. This should go in a separate patch, with DT bindings - see checkpatch.pl complaint.","Hi Dan,  Thank your for the patch.  I have some comments below, please take a look.  On 3/7/19 11:09 PM, Dan Murphy wrote:  If control_bank was validated in lm3532_parse_node(), then you wouldn't have to take into account this option here.  Besides, you don't need this switch statement, but can easily calculate ctrl_en_val:  #define LM3532_GET_CTRL_BANK_EN_VAL(bank_id) BIT(bank_id)   Ditto.   Similarly here. use following macro:  #define LM3532_GET_CTRL_BANK_BRT_REG (bank_id)  BIT((LM3532_REG_CTRL_A_BRT + bank_id*2))   mutex_destroy() is missing here.   This should go in a separate patch, with DT bindings - see checkpatch.pl complaint.   --  Best regards, Jacek Anaszewski",technical,Jacek Anaszewski,jacek.anaszewski@gmail.com,1,0,423,0.7747747747747747,0.625,0,0.4,0.4,0.2,0.0
1267,571989,573917,Thanks for the review.  I have pushed v2 with some code changes but those changes that were made seem to be outside your comments. So I will implement these comments in v3.Ack.  Probably don't even need the macro for this. Same as above Ack. Probably don't need a macro since it is only used once.  Ack I have removed this in v2 of the next patchset.,"Jacek  On 3/10/19 2:49 PM, Jacek Anaszewski wrote:  Thanks for the review.  I have pushed v2 with some code changes but those changes that were made seem to be outside your comments.  So I will implemnt these comments in v3.   Ack.  Probably don't even need the macro for this.  ctrl_en_val = BIT(led_data->control_bank),   Ack  Same as above   Ack. Probably don't need a macro since it is only used once.  brightness_reg = LM3532_REG_CTRL_A_BRT + led->control_bank * 2   Ack   I have removed this in v2 of the next patchset.  Dan    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,350,0.6756756756756757,0.6875,0,0.6,0.4,0.0,0.0
1268,571989,574361,"One more thing I forgot to mention before.[...] Let's have ""//"" comments here.","Dan,  One more thing I forgot to mention before.  On 3/11/19 12:36 PM, Dan Murphy wrote: [...]  Let's have //"" comments here.   --  Best regards, Jacek Anaszewski""",technical,Jacek Anaszewski,jacek.anaszewski@gmail.com,1,0,78,0.18018018018018017,0.75,0,0.6,0.2,0.0,0.0
1269,571989,574414,Awhile ago...commit checkpatch: allow c99 style // comments    Sanitise the lines that contain c99 comments so that the error doesn't    get emitted.,"On Mon, 2019-03-11 at 12:24 -0500, Dan Murphy wrote:  Awhile ago...  commit dadf680de3c2eb4cba9840619991eda0cfe98778 Author: Joe Perches <joe@perches.com> Date:   Tue Aug 2 14:04:33 2016 -0700      checkpatch: allow c99 style // comments          Sanitise the lines that contain c99 comments so that the error doesn't     get emitted.          Link: http://lkml.kernel.org/r/d4d22c34ad7bcc1bceb52f0742f76b7a6d585235.1468368420.git.joe@perches.com     Signed-off-by: Joe Perches <joe@perches.com>     Signed-off-by: Andrew Morton <akpm@linux-foundation.org>     Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",technical,Joe Perches,joe@perches.com,1,0,149,0.24324324324324326,0.875,0,0.6,0.2,0.0,0.0
1270,571989,578061,"The general form uses a space after the //And much more recently commit is specified for various file types, in Documentation so add an appropriate test for    .[chsS] files because many proposed file additions and patches do not use    the correct style.","On Mon, 2019-03-11 at 12:47 -0500, Dan Murphy wrote: []  The general form uses a space after the //  And much more recently:  commit fdf13693d370653013eec3bc7f80a3f535001bf0 Author: Joe Perches <joe@perches.com> Date:   Thu Mar 7 16:28:32 2019 -0800      checkpatch: verify SPDX comment style          Using SPDX commenting style // or /* is specified for various file types     in Documentation/process/license-rules.rst so add an appropriate test for     .[chsS] files because many proposed file additions and patches do not use     the correct style.",technical,Joe Perches,joe@perches.com,1,0,255,0.42342342342342343,1.0,1,1.0,0.0,0.2,0.0
1271,571989,574425,Thanks JoeI guess I was referring to this SPDX warning WARNING: Missing or malformed SPDX-License-Identifier tag in line 1#1.,"Thanks Joe  On 3/11/19 12:30 PM, Joe Perches wrote:  I guess I was referring to this SPDX warning   WARNING: Missing or malformed SPDX-License-Identifier tag in line 1 #1: FILE: drivers/net/can/m_can/m_can.h:1: +//SPDX-License-Identifier: GPL-2.0    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,125,0.2072072072072072,0.9375,0,0.6,0.2,0.0,0.2
1272,571989,574393,Like I said earlier in v2 this header went away. And checkpatch takes issue with // in headers. Unless they have removed that requirement.,"Jacek  On 3/11/19 12:22 PM, Jacek Anaszewski wrote:  Like I said earlier in v2 this header went away.  And checkpatch takes issue with // in headers.  Unless they have removed that requirement.  Dan    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,138,0.24324324324324326,0.8125,0,0.6,0.2,0.0,0.0
1273,572760,573609,iput while holding spinlock is not good I don't understand this comment. no matter the 'retry' parameter is true or not/maybe we should  distinguish them.,"On Sat, Mar 9, 2019 at 12:30 AM Luis Henriques <lhenriques@suse.com> wrote:  iput while holding spinlock is not good   I don't understand this comment. get_quota_realm() unlock snap_rwsem no matter the 'retry' parameter is true or not/    maybe we should  distinguish ‘realm->inode is null' from 'igrab fails'",technical,"Yan, Zheng",ukernel@gmail.com,0,0,154,0.12184873949579832,0.8,0,1.0,0.0,1.0,0.0
1274,572760,573930,"Yep, that's definitely not a good idea.  And maybe this loop could even race with a lookup currently in progress and a new inode could be added to the list after the cleanup.  I didn't verified this race could really occur, because an easy fix is to simply move this loop into ceph_mdsc_destroy() where we don't really need the spinlock anymore.<snip> Right, maybe the parameter name and the comment are a bit misleading. Get_quota_realm may need to do an inode lookup and then retry to get the quota realm.  If this lookup is required, it has to drop the rwsem. However, in ceph_quota_is_same_realm we need to lookup 2 quota realms ""atomically"", i.e. with the rwsem held.  If get_quota_realm needs to drop it, it will do the MDS inode lookup anyway but instead of retrying to get the quota realm it will return -EAGAIN (because 'retry' will be 'false').  This allows for ceph_quota_is_same_realm to restart the operation itself, and retry to get both quota realms without get_quota_realm dropping the rwsem. Does it make sense?  I agree the design isn't great :-/ I tried to describe this behavior in get_quota_realm comment, but it's probably not good enough. Sure, that makes sense.","Yan, Zheng"" <ukernel@gmail.com> writes:   Yep, that's definitely not a good idea.  And maybe this loop could even race with a lookup currently in progress and a new inode could be added to the list after the cleanup.  I didn't verified this race could really occur, because an easy fix is to simply move this loop into ceph_mdsc_destroy() where we don't really need the spinlock anymore.  <snip>   Right, maybe the parameter name and the comment are a bit misleading.  get_quota_realm may need to do an inode lookup and then retry to get the quota realm.  If this lookup is required, it has to drop the rwsem.  However, in ceph_quota_is_same_realm we need to lookup 2 quota realms ""atomically"", i.e. with the rwsem held.  If get_quota_realm needs to drop it, it will do the MDS inode lookup anyway but instead of retrying to get the quota realm it will return -EAGAIN (because 'retry' will be 'false').  This allows for ceph_quota_is_same_realm to restart the operation itself, and retry to get both quota realms without get_quota_realm dropping the rwsem.  Does it make sense?  I agree the design isn't great :-/ I tried to describe this behavior in get_quota_realm comment, but it's probably not good enough.   Sure, that makes sense.  Cheers, --  Luis  """,technical,Luis Henriques,lhenriques@suse.com,0,1,1185,1.0,1.0,1,1.0,0.0,0.0,0.0
1275,573148,589800,Can someone review this patch?,   Can someone review this patch? ,technical,Kangjie Lu,kjlu@umn.edu,0,1,30,1.0,1.0,1,1.0,0.0,1.0,0.0
1276,575419,591072,"Patch looks good to me.Terry, did you have time to review it?","Hi Nicolas,  On Tue, Mar 12, 2019 at 10:37 AM Nicolas Saenz Julienne <nsaenzjulienne@suse.de> wrote:  Patch looks good to me.  Terry, did you have time to review it?  Cheers, Benjamin",technical,Benjamin Tissoires,benjamin.tissoires@redhat.com,1,0,61,0.30434782608695654,0.5,0,1.0,0.0,1.0,0.0
1277,575419,591410,"Actually, the good thing of having a test suite, is that it raises bugs :) You are also missing the computation of the usage in hid_scan_main().This makes the autoloading of hid-multitouch fail, and thus the test suite failing.","On Mon, Mar 25, 2019 at 11:39 AM Benjamin Tissoires <benjamin.tissoires@redhat.com> wrote:  Actually, the good thing of having a test suite, is that it raises bugs :)  You are also missing the computation of the usage in hid_scan_main(). This makes the autoloading of hid-multitouch fail, and thus the test suite failing.  Cheers, Benjamin",technical,Benjamin Tissoires,benjamin.tissoires@redhat.com,1,0,227,1.0,0.75,0,1.0,0.0,0.0,0.0
1278,575419,591412,"Thanks for the review! Ok, I'll look into it and send a follow-up.","Hi Benjamin, Thanks for the review!  On Mon, 2019-03-25 at 16:08 +0100, Benjamin Tissoires wrote:  Ok, I'll look into it and send a follow-up.",technical,Nicolas Saenz Julienne,nsaenzjulienne@suse.de,1,1,66,0.3695652173913043,1.0,1,1.0,0.0,0.0,0.0
1279,576317,576342,"I just checked, this can be removed for this mode. I'll update the patch. Thanks!","Hello Dan,  Am 12.03.19 um 13:12 schrieb Dan Murphy:  ... snip ...   What is m_can_version for the tcan4x5x?  ... snip ...  Wolfgang.",technical,Wolfgang Grandegger,wg@grandegger.com,1,0,81,0.18018018018018017,0.625,0,0.0,0.0,0.0,0.0
1280,576317,576370,I believe it is Bosch M-CAN Revision 3.2.1.1,"Wolfgang  On 3/12/19 7:43 AM, Wolfgang Grandegger wrote:  I believe it is Bosch M-CAN Revision 3.2.1.1    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,44,0.07207207207207207,0.75,0,0.0,0.0,0.0,0.0
1281,576317,576432,"in the previous email you said that it uses:  Bosch M-CAN Revision 3.2.1.1 This means this, hence it's using FIFOs: I think this frees the skb twice! Also, the index depends on version! If > 30, the last used index should be selected. What happens to the ""tx_skb"" if this functions returns with NETDEV_TX_BUSY? Will it be freed? You stop the queue immediately, even if fifos are used. Is this by purpose? .","Hello Dan,  in the previous email you said that the tcan4x5x uses:    Bosch M-CAN Revision 3.2.1.1  This means cdev->version == 32"", hence it's using FIFOs...  Am 12.03.19 um 13:12 schrieb Dan Murphy:  I think this frees the skb twice!  Also, the index depends on ""cdev->version""! If > 30, the last used index should be selected.   What happens to the ""tx_skb"" if this funtions returns with NETDEV_TX_BUSY? Will it be freed?   You stop the queue immediately, even if fifos are used. Is this by purpose?   Wolfgang.""",technical,Wolfgang Grandegger,wg@grandegger.com,1,0,406,0.8018018018018018,0.875,0,0.0,0.0,0.0,0.0
1282,576317,577221,It appears so.  I will fix it up.OK I will need to look into that. I am missing the change I put in to free this. It must have been lost when rebasing the patch setI will add it back and fix this up Good catch.  I should let the work queue handle this.  I see that < 3.0 stops the queue immediately but > 3.0 does not. I don't know what version of core the hi or mcp parts are using it looks to be 3.0. I believe that's where I pulled the stop from. I will remove it.,"Wolfgang  On 3/12/19 9:42 AM, Wolfgang Grandegger wrote:  It appears so.  I will fix it up.   OK I will need to look into that.   I am missing the change I put in to free this. It must have been lost when rebasing the patch set  I will add it back and fix this up   Good catch.  I should let the work queue handle this.  I see that < 3.0 stops the queue immedidately but > 3.0 does not.  I don't know what version of core the hi or mcp parts are using it looks to be 3.0. I believe thats where I pulled the stop from.  I will remove it.    --  ------------------ Dan Murphy",technical,Dan Murphy,dmurphy@ti.com,0,1,467,1.0,1.0,1,0.0,0.0,0.0,0.0
1283,576428,576483,I'm sorry you're going to have to break this up into multiple patches. Probably one for each item on your list.,"On Tue, Mar 12, 2019 at 11:39:13AM -0300, Guilherme T Maeoka wrote:  I'm sorry you're going to have to break this up into multiple patches. Probably one for each item on your list.  regards, dan carpenter",technical,Dan Carpenter,dan.carpenter@oracle.com,0,0,111,0.6756756756756757,0.75,0,0.0,0.0,0.0,0.0
1284,576428,576497,"No problem. In a previous patch I had one for each item, but I thought it could be packed in a single one - and avoid '[PATCH n/m]'. Thanks.","No problem. In a previous patch I had one for each item, but I thought it could be packed in a single one - and avoid '[PATCH n/m]'.  Thanks.  On 3/12/19, Dan Carpenter <dan.carpenter@oracle.com> wrote:",technical,Guilherme M,gui.mspace@gmail.com,0,0,140,1.0,1.0,1,0.0,0.0,0.0,0.0
1285,576524,576515,"Hm, a runtime PM ref is already acquired in nouveau_connector_detect().I'm wondering why that's not sufficient?","On Tue, Mar 12, 2019 at 11:22:43PM +0800, Icenowy Zheng wrote:  Acked-by: Maxime Ripard <maxime.ripard@bootlin.com>  Maxime  --  Maxime Ripard, Bootlin Embedded Linux and Kernel engineering https://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,111,0.21153846153846154,0.48484848484848486,0,0.0,1.0,0.0,0.0
1286,576524,576537,I'm not sure all that remaining is worth it to be honest. It adds a lot of noise for no particular reason (and the same goes for renaming the file itself).,"On Tue, Mar 12, 2019 at 11:22:46PM +0800, Icenowy Zheng wrote:  I'm not sure all that remaining is worth it to be honest. It adds a lot of noise for no particular reason (and the same goes for renaming the file itself).  Maxime  --  Maxime Ripard, Bootlin Embedded Linux and Kernel engineering https://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,155,0.34615384615384615,0.5151515151515151,0,0.0,1.0,0.0,0.0
1287,576524,576545,"Maybe keeping names is okay ""for historical reasons"". In fact I want to keep them.","于 2019年3月12日 GMT+08:00 下午11:36:54, Maxime Ripard <maxime.ripard@bootlin.com> 写到:  Maybe keeping names is okay for historial reasons"".  In fact I want to keep them.   --  使用 K-9 Mail 发送自我的Android设备。""",technical,Icenowy Zheng,icenowy@aosc.io,1,1,82,0.18269230769230768,0.5454545454545454,0,0.0,1.0,0.0,0.23809523809523808
1288,576524,581516,"Thanks for working on this, I was actually close to submitting similar patches for V3 support! I just reviewed the definitions and found a mistakes about the LVDS function (that should be 0x3 instead of 0x2). Otherwise, things look good and match what I had came up with.","Hi Icenowy,  Le mardi 12 mars 2019 à 23:22 +0800, Icenowy Zheng a écrit :  Thanks for working on this, I was actually close to submitting similar patches for V3 support!  I just reviewed the definitions and found a mistakes about the LVDS function (that should be 0x3 instead of 0x2).  Otherwise, things look good and match what I had came up with.  Cheers,  Paul   LVDS should be function 0x3.   Ditto about LVDS.   Ditto about LVDS.   Ditto about LVDS.   Ditto about LVDS.   Ditto about LVDS.   Ditto about LVDS.   Ditto about LVDS.   Ditto about LVDS.   Ditto about LVDS.  --  Paul Kocialkowski, Bootlin Embedded Linux and kernel engineering https://bootlin.com",technical,Paul Kocialkowski,paul.kocialkowski@bootlin.com,1,0,271,0.5288461538461539,0.5757575757575758,0,0.23809523809523808,0.7142857142857143,0.23809523809523808,0.0
1289,576524,581519,"Hi :My two cents about this: kernel development is plagued by the inability to rename and rework things as soon as backward compatibility is involved. I believe that renaming and reworking things is quite a good thing to do when it leads to a situation that is easier to understand and makes more sense. In this case, I don't see any blockers that would prevent us from doing this, so I am strongly in favour of it. I really don't see how increased noise and ""historical reasons"" make up for better clarity.","Hi,  Le mardi 12 mars 2019 à 23:45 +0800, Icenowy Zheng a écrit :  My two cents about this: kernel development is plagued by the unability to rename and rework things as soon as backward compatibility is involved. I believe that renaming and reworking things is quite a good thing to do when it leads to a situation that is easier to understand and makes more sense.  In this case, I don't see any blockers that would prevent us from doing this, so I am strongly in favor of it. I really don't see how increased noise and historical reasons"" make up for better clarity.  Cheers,  Paul  --  Paul Kocialkowski, Bootlin Embedded Linux and kernel engineering https://bootlin.com""",technical,Paul Kocialkowski,paul.kocialkowski@bootlin.com,1,0,507,1.0,0.6060606060606061,0,0.23809523809523808,0.7142857142857143,0.0,0.0
1290,576524,581575,"It simplifies the git history, for once, which has the side effect of reducing conflicts too. A second one is:  Do you prefer to review patches that have some significant value (like a new feature, a bugfix, a new SoC support, etc) or one that renames files and / or symbols ?","On Mon, Mar 18, 2019 at 12:05:12PM +0100, Paul Kocialkowski wrote:  It simplifies the git history, for once, which has the side effect of reducing conflicts too.  A second one is: Do you prefer to review patches that have some significant value (like a new feature, a bugfix, a new SoC support, etc) or one that renames files and / or symbols?  Maxime  -- Maxime Ripard, Bootlin Embedded Linux and Kernel engineering https://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,276,0.5865384615384616,0.6363636363636364,0,0.23809523809523808,0.7142857142857143,0.0,0.0
1291,576524,581615,"Hi: Note that the V3 has a NMI controller at 1c000d0, that is required for handling the AXP209 interrupts IIRQ. I have no idea whether it's also the case on the V3s/S3/S3L though but it would be good to know. Note that I can totally add support for it when adding support for myV3 device that uses the AXP209 this way. Also, could we get proper compatibles and config options for these new SoCs, since they are distinct from the V3S?","Hi,  Le mardi 12 mars 2019 à 23:22 +0800, Icenowy Zheng a écrit :  Note that the V3 has a NMI controller at 1c000d0, that is required for handling the AXP209 interrupts IIRQ. I have no idea whether it's also the case on the V3s/S3/S3L though but it would be good to know.  Note that I can totally add support for it when adding support for my V3 device that uses the AXP209 this way.  Also, could we get proper compatibles and config options for these new SoCs, since they are distinct from the V3S?  Cheers,  Paul  --  Paul Kocialkowski, Bootlin Embedded Linux and kernel engineering https://bootlin.com",technical,Paul Kocialkowski,paul.kocialkowski@bootlin.com,1,0,433,0.8653846153846154,0.6666666666666666,0,0.23809523809523808,0.7142857142857143,0.0,0.0
1292,576524,581774,"It's not mentioned on the datasheet. If it's present, please send a patch.BTW all V3 series chip share the same die (sun8iw8).Thanks","于 2019年3月18日 GMT+08:00 下午8:41:32, Paul Kocialkowski <paul.kocialkowski@bootlin.com> 写到:  It's not mentioned on the datasheet.  If it's present, please send a patch.  BTW all V3 series chip share the same die (sun8iw8).  Thanks   --  使用 K-9 Mail 发送自我的Android设备。",technical,Icenowy Zheng,icenowy@aosc.io,1,1,132,0.27884615384615385,0.696969696969697,0,0.23809523809523808,0.7142857142857143,0.0,0.0
1293,576524,581777,"Hi ,Indeed, it is not documented but the block is definitely there (and it shows up in All winner's kernel source too). I'll send a patch once these series is merged then! Right, so I think it's safe to assume that the controller is there on all of them then.","Hi,  Le lundi 18 mars 2019 à 23:15 +0800, Icenowy Zheng a écrit :  Indeed, it is not documented but the block is definitely there (and it shows up in Allwinner's kernel source too). I'll send a patch once these series is merged then!   Right, so I think it's safe to assume that the controller is there on all of them then.  Cheers,  Paul  --  Paul Kocialkowski, Bootlin Embedded Linux and kernel engineering https://bootlin.com",technical,Paul Kocialkowski,paul.kocialkowski@bootlin.com,1,0,259,0.5865384615384616,0.7272727272727273,0,0.23809523809523808,0.7142857142857143,0.0,0.42857142857142855
1294,576524,596727,Seems like a bunch of duplication for just 2 differences in clocks. Can't you keep the definitions the same and just skip registering the clocks not present? Perhaps reword as 'Clocks only on V3',"On Tue, Mar 12, 2019 at 11:22:50PM +0800, Icenowy Zheng wrote:  Seems like a bunch of duplication for just 2 differences in clocks.  Can't you keep the definitions the same and just skip registering the  clocks not present?   Perhaps reword as 'Clocks only on V3'",technical,Rob Herring,robh@kernel.org,1,0,195,0.36538461538461536,0.8787878787878788,0,0.7142857142857143,0.23809523809523808,0.0,0.0
1295,576524,599521,"I'd rather not, this can lead to access to registers that might not be there when the CCF will read / write that clock","On Thu, Mar 28, 2019 at 08:27:21AM -0500, Rob Herring wrote:  I'd rather not, this can lead to access to registers that might not be there when the CCF will read / write that clock  Maxime  -- Maxime Ripard, Bootlin Embedded Linux and Kernel engineering https://bootlin.com",technical,Maxime Ripard,maxime.ripard@bootlin.com,1,0,118,0.25,0.9696969696969697,0,0.9047619047619048,0.09523809523809523,0.14285714285714285,0.09523809523809523
1296,576524,602579,"I see there is some review comments on this patch set so I am waiting for v2.When we have something Maxime, Rob etc has ACKed, I suggest I merge the pinctrl stuff into an immutable branch in the pinctrl tree so that it can be pulled in to ARM SoC if need be (for DTS files to compile for example).","On Tue, Mar 12, 2019 at 10:24 PM Icenowy Zheng <icenowy@aosc.io> wrote:   I see there is some review comments on this patch set so I am waiting for v2.  When we have something Maxime, Rob etc has ACKed, I suggest I merge the pinctrl stuff into an immutable branch in the pinctrl tree so that it can be pulled in to ARM SoC if need be (for DTS files to compile for example).  Yours, Linus Walleij",technical,Linus Walleij,linus.walleij@linaro.org,1,0,297,0.625,1.0,1,1.0,0.0,0.09523809523809523,0.0
1297,578844,580778,No need to put this in the changelog text at all. Please fix up and resend.,"On Thu, Mar 14, 2019 at 05:31:55PM +0900, Sergey Senozhatsky wrote:  No need to put this in the changelog text at all.  Please fix up and resend.  thanks,  greg k-h",technical,Greg Kroah-Hartman,gregkh@linuxfoundation.org,1,0,75,1.0,0.6666666666666666,0,1.0,0.0,1.0,0.0
1298,579621,581917,Good catch.If we do that we need to set 'err' to an error code here [1] and here[2].  Do you think you could fix this and roll another version of your patch?,"Good day Yue,  On Thu, 14 Mar 2019 at 20:28, Yue Haibing <yuehaibing@huawei.com> wrote:  Good catch.   If we do that we need to set 'err' to an error code here [1] and here [2].  Do you think you could fix this and roll another version of your patch?  [1]. https://elixir.bootlin.com/linux/v5.1-rc1/source/tools/perf/util/cs-etm.c#L1967 [2]. https://elixir.bootlin.com/linux/v5.1-rc1/source/tools/perf/util/cs-etm.c#L1981",technical,Mathieu Poirier,mathieu.poirier@linaro.org,1,0,157,0.9523809523809523,0.25,0,0.75,0.0,0.75,0.0
1299,579621,581921,"Next time please submit two patches, one for the PTR_ERR and another for not throwing away the err = -E!INVAL and returning just -EINVAL, I'm doing it this time.","Em Fri, Mar 15, 2019 at 10:26:49AM +0800, Yue Haibing escreveu:  Next time please submit two patches, one for the PTR_ERR and another for not throwing away the err = -E!INVAL and returning just -EINVAL, I'm doing it this time.    - Arnaldo   --   - Arnaldo",technical,Arnaldo Carvalho de Melo,arnaldo.melo@gmail.com,1,0,161,0.8333333333333334,0.375,0,0.75,0.0,0.0,0.0
1300,579621,582838,Please hold off on that - I've asked for other modifications to be done on this patch.,"On Mon, 18 Mar 2019 at 11:15, Arnaldo Carvalho de Melo <arnaldo.melo@gmail.com> wrote:  Please hold off on that - I've asked for other modifications to be done on this patch.  Mathieu",technical,Mathieu Poirier,mathieu.poirier@linaro.org,1,0,86,0.4523809523809524,0.5,0,1.0,0.0,0.0,0.0
1301,579621,582856,"Do you really think that it is necessary to hold? The fixes are trivial and I already have them split and applied, see them below, I think whatever other changes can be done in further patches, no?","Em Tue, Mar 19, 2019 at 08:38:32AM -0600, Mathieu Poirier escreveu:  Do you really think that it is necessary to hold? The fixes are trivial and I already have them split and applied, see them below, I think whatever other changes can be done in further patches, no?  - Arnaldo",technical,Arnaldo Carvalho de Melo,arnaldo.melo@gmail.com,1,0,197,1.0,0.625,0,1.0,0.0,0.0,0.0
1302,579621,582887,"Proceeding with the patches below would created two new bugs, hence asking Yue for modifications.","On Tue, Mar 19, 2019 at 11:46:31AM -0300, Arnaldo Carvalho de Melo wrote:  Proceeding with the patches below would created two new bugs, hence asking Yue for modifications.  Mathieu",technical,Mathieu Poirier,mathieu.poirier@linaro.org,1,0,97,0.40476190476190477,0.75,0,1.0,0.0,0.0,0.0
1303,579621,582903,"Sorry for late, then I can send a new patch to fix the two new bugs.","On 2019/3/19 22:55, Mathieu Poirier wrote:   Sorry for late, then I can send a new patch to fix the two new bugs.",technical,YueHaibing,yuehaibing@huawei.com,0,0,68,0.42857142857142855,0.875,0,1.0,0.0,0.0,0.0
1304,579621,583048,"Ok, I'll take your word on it then, dropping the patches.","Em Tue, Mar 19, 2019 at 08:55:25AM -0600, Mathieu Poirier escreveu:  Ok, I'll take your word on it then, dropping the patches.  - Arnaldo    --   - Arnaldo",technical,Arnaldo Carvalho de Melo,arnaldo.melo@gmail.com,1,0,57,0.35714285714285715,1.0,1,1.0,0.0,0.0,0.0
1305,580584,580653,Some stray whitespace in here. I cleaned it up.,"On Sat, 16 Mar 2019 12:07:32 -0300 Marcelo Schmitt <marcelo.schmitt1@gmail.com> wrote:  Some stray whitespace in here. I cleaned it up.  Thanks,  Jonathan",technical,Jonathan Cameron,jic23@kernel.org,1,0,47,1.0,1.0,1,0.0,0.0,0.0,0.0
1306,580956,580973,"hostfs now mostly works.  Almost all of the common operations are now    implemented, the main exceptions being mknod and executing files from    a hostfs filesystem. Enough archaeology for today. :-)","Am Montag, 18. Mrz 2019, 00:09:09 CET schrieb Colin King:  commit bf4f804738544a95b8bc8d6a7e2629c3fc0240dd Author: jdike <jdike> Date:   Sat Dec 9 22:52:44 2000 +0000      hostfs now mostly works.  Almost all of the common operations are now     implemented, the main exceptions being mknod and executing files from     a hostfs filesystem.  Enough archaeology for today. :-)  Thanks, //richard",technical,Richard Weinberger,richard@nod.at,1,0,200,0.8780487804878049,0.4,0,0.0,0.0,0.0,0.0
1307,580956,581105,Nice. How do you find these ancient git commits?,"On 17/03/2019 23:49, Richard Weinberger wrote:  Nice. How do you find these ancient git commits?",technical,Colin Ian King,colin.king@canonical.com,0,0,48,0.2682926829268293,0.6,0,0.0,0.0,0.0,0.0
1308,580956,581114,"Hi,  Colin is obviously right with that. But my guess is that the error occurred because the pattern (from, to) is broken here. Also Maybe the maintainer can fix that. just my 2 cents","Hi, Colin is obvously right with that. But my guess is that the error occured because the pattern (from, to) is brocken here. Also   Maybe the maintainer can fix that.  just my 2 cents,  re,  wh   Am 18.03.2019 00:09, schrieb Colin King:",technical,Walter Harms,wharms@bfs.de,0,0,183,1.0,0.8,0,0.0,0.0,0.0,0.0
1309,580956,581376,"This commit is from the old UML cvs tree. I did a import to git some time ago and pushed it to: For classic pre-git stuff, check:","Am Montag, 18. Mrz 2019, 09:41:28 CET schrieb Colin Ian King:  This commit is from the old UML cvs tree. I did a import to git some time ago and pushed it to: https://git.kernel.org/pub/scm/linux/kernel/git/rw/uml-history.git/  For classic pre-git stuff, check: https://git.kernel.org/pub/scm/linux/kernel/git/tglx/history.git/  Thanks, //richard",technical,Richard Weinberger,richard@nod.at,1,0,129,0.7560975609756098,1.0,1,0.0,0.0,0.0,0.0
1310,581567,582204,"Hello,[I put Thierry into To: because some remaining questions depend on his views]The situation here is as follows: The actual period length calculates as. Consider a clk rate of 600 MHz, then the driver maps ""requested period ""to ""actual period"" as follows: There is an obvious rounding issue: If 218452 ns are requested, we should end in the scale = 1 case for sure. (Similar issues exist for the other cases.) And then there are cases that are not that clear: What if 218000 ns are requested? Where should the line be drawn? Thierry? And what about long periods? The longest actually supported period length is around 3.5 seconds. What if a consumer requests 18 seconds? Where should the line be drawn when the driver is supposed to return-EINVAL (or -ERANGE)? Thierry? Starting with this write the new period length might be active with the previous duty cycle. Is this worth a comment? I think the window where this can actually happen should be made as small as possible, so it would be great to first calculate both register values and then write them in two consecutive writels.What about rounding here? I'd say use ""round closest"" instead of ""round down"".If the PWM was configured for this, get_state still returns .duty_cycle =0. Is this acceptable? Space after : please. Also applies to the other error strings. I think it would be more common to call the struct pwm_device pointer ""pwm"" and the struct pwm_sifive_ddata pointer ""ddata"".""dev"" is usually a pointer to a struct device. The other functions need the same adaption of course. Here is another rounding question. Given that the period length can only be modified by factors of two there are cases where the real period is off by a factor of at least 1.4 which has an effect on the duty cycle. Consider again an input clk rate of 600 MHz. We either have to go for real period = 109226 ns (as is currently implemented) or with 218452 ns. Which one should be chosen I already asked above. Here the question is (probably depending on the former question) how should the actual duty_cycle be calculated? This goto is a noop and so can be dropped. Is it a problem when the notifier is called before pwmchip_add was called? Out of interest: Is a real problem addressed here? I.e.: Doe sthe input clock actually change in practise? Also note that pwm_sifive_clock_notifier only adapts the period but not the duty cycle (any more).Given that a clk rate change affects the output, I wonder if the change should be declined if the pwm is running. If the bootloader setup a display with a backlight driven by a PWM it would be ideal to not modify the already running hardware here. Here is another consumer API function call. I think you're leaking a clk_enable here. The probe function does one unconditionally that is never undone.","Hello,  [I put Thierry into To: because some remaining questions depend on his views]  On Mon, Mar 18, 2019 at 05:17:14PM +0530, Yash Shah wrote:  The situation here is as follows: The actual period length calculates as:  	period_length = (0x10000 << scale) / rate  Consider a clk rate of 600 MHz, then the driver maps requested period"" to ""actual period"" as follows:  	if requested_period <= 218453 ns: 		// scale = 0 		actual_period = 109226 ns 	else if requested_period <= 436906 ns: 		// scale = 1 		actual_period = 218452 ns 	else if requested_period <= 873812 ns: 		// scale = 2 		actual_period = 436904 ns 	... 	else if requested_period <= 3579139413 ns: 		// scale = 14 		actual_period = 1789569707 ns 	else: 		//scale = 15 		actual_period = 3579139413 ns  There is an obvious rounding issue: If 218452 ns are requested, we should end in the scale = 1 case for sure. (Similar issues exist for the other cases.)  And then there are cases that are not that clear: What if 218000 ns are requested? Where should the line be drawn? Thierry?  And what about long periods? The longest actually supported period length is around 3.5 seconds. What if a consumer requests 18 seconds? Where should the line be drawn when the driver is supposed to return -EINVAL (or -ERANGE)? Thierry?   Starting with this write the new period length might be active with the previous duty cycle. Is this worth a comment? I think the window where this can actually happen should be made as small as possible, so it would be great to first calculate both register values and then write them in two consecutive writels.   What about rounding here? I'd say use ""round closest"" instead of ""round down"".   If the PWM was configured for { .period = 1000000, .duty_cycle = 1000000, .enabled = false, ... }, .get_state still returns .duty_cycle = 0. Is this acceptable?   Space after : please. Also applies to the other error strings.   I think it would be more common to call the struct pwm_device pointer ""pwm"" and the struct pwm_sifive_ddata pointer ""ddata"". ""dev"" is usually a pointer to a struct device. The other functions need the same adaption of course.   Here is another rounding question. Given that the period length can only be modified by factors of two there are cases where the real period is off by a factor of at least 1.4 which has an effect on the duty cycle. Consider again an input clk rate of 600 MHz, and:  	.duty_cycle = 109226 [ns] 	.period = 152916 [ns]  We either have to go for real period = 109226 ns (as is currently implemented) or with 218452 ns. Which one should be chosen I already asked above. Here the question is (probably depending on the former question) how should the actual duty_cycle be calculated?   This goto is a noop and so can be dropped.   Is it a problem when the notifier is called before pwmchip_add was called? Out of interest: Is a real problem addressed here? I.e.: Does the input clock actually change in practise? Also note that pwm_sifive_clock_notifier only adapts the period but not the duty cycle (any more).  Given that a clk rate change affects the output, I wonder if the change should be declined if the pwm is running.   If the bootloader setup a display with a backlight driven by a PWM it would be ideal to not modify the already running hardware here.   Here is another consumer API function call.   I think you're leaking a clk_enable here. The probe function does one unconditionally that is never undone.   Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |""",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,2792,1.0,0.6666666666666666,0,0.0,0.0,0.0,0.0
1311,581567,582453,"The comment for this scenario has already been mentioned under the limitation on top of this driver. Anyway, I will try to implement your suggestion of consecutive writes Sure Ok. Will change that Sure Ok, will drop this initialization. Will avoid using it.Will use this instead","On Tue, Mar 19, 2019 at 3:16 AM Uwe Kleine-König <u.kleine-koenig@pengutronix.de> wrote:  The comment for this scenario has already been mentioned under the limitation on top of this driver. Anyway, I will try to implement your suggestion of consecutive writes   Sure   Ok. Will change that   Sure   Ok, will drop this initialization.   Will avoid using it.   Will use clk_disable_unprepare instead of clk_unprepare",technical,Yash Shah,yash.shah@sifive.com,1,1,278,0.08818342151675485,0.8333333333333334,0,0.0,0.0,0.0,0.0
1312,581567,582482,"Yeah, the comment at the top is for general information about the shortcomings. The comment here would be to say: The problem occurs *here*.","On Tue, Mar 19, 2019 at 12:52:12PM +0530, Yash Shah wrote:  Yeah, the comment at the top is for general information about the shortcomings. The comment here would be to say: The problem occurs *here*.  Best regards Uwe  --  Pengutronix e.K.                           | Uwe Kleine-Knig            | Industrial Linux Solutions                 | http://www.pengutronix.de/  |",technical,Uwe Kleine-König,u.kleine-koenig@pengutronix.de,1,0,140,0.05291005291005291,1.0,1,0.0,0.0,0.0,0.0
1313,582249,601077,Are you sure this can happen? I don't think this is possible if client->dev.of_node is non-NULL. The 'if (client->dev.of_node)' above should see to that.,"On Mon, 18 Mar 2019, Aditya Pakki wrote:   Are you sure this can happen?  I don't think this is possible if client->dev.of_node is non-NULL.  The 'if (client->dev.of_node)' above should see to that.   --  Lee Jones [李琼斯] Linaro Services Technical Lead Linaro.org │ Open source software for ARM SoCs Follow Linaro: Facebook | Twitter | Blog",technical,Lee Jones,lee.jones@linaro.org,1,0,153,1.0,1.0,1,1.0,0.0,1.0,0.0
1314,582304,584013,"We would not even enter this path without matching compatible, so I think a check here is not really necessary.","On 19/03/2019 01:25, Aditya Pakki wrote:  We would not even enter this path without matching compatible, so I  think a check here is not really necessary.  thanks, srini",technical,Srinivas Kandagatla,srinivas.kandagatla@linaro.org,1,0,111,1.0,1.0,1,1.0,0.0,1.0,0.0
1315,924009,924139,"I'd suggest using the username ""fsgqa2"" so that other tests that want to use a second username can do so using a more logically name username.","On Tue, Dec 12, 2017 at 04:45:11PM -0800, Luis R. Rodriguez wrote:  I'd suggest using the username fsgqa2"" so that other tests that want to use a second username can do so using a more logically name username.         	 		     	- Ted""",technical,Theodore Ts'o,tytso@mit.edu,1,0,142,0.11857707509881422,0.5,0,0.0,1.0,0.0,0.0
1316,924009,1674,"This isn't a ""test names with digits"" test, but ""test names beginning with digits"" test.  Changing the username to not begin with digits invalidates the entire purpose of the test which is to ensure that xfs_quota can differentiate between UIDs and names beginning with numbers....So from that perspective, NAK. IF there are distros not allowing usernames to start with digits, then this test needs a _requires check and to _not run on those systems.","On Tue, Dec 12, 2017 at 04:45:11PM -0800, Luis R. Rodriguez wrote:      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  This isn't a test names with digits"" test, but ""test names begining with digits"" test.  Changing the username to not begin with digits invalidates the entire purpose of the test which is to ensure that xfs_quota can differentiate between UIDs and names beginning with numbers....  So from that perspective, NAK.  IF there are distros not allowing usernames to start with digits, then this test needs a _requires check and to _notrun on those systems.  Cheers,  Dave. --  Dave Chinner david@fromorbit.com""",technical,Dave Chinner,david@fromorbit.com,0,0,450,0.3438735177865613,0.5454545454545454,0,0.0,0.9891304347826086,0.0,0.0
1317,924009,1678,"These should not run automatically if you don't have an external log device configured. Every test should either work with an external logdev or explicitly not run them, so I'm not sure what you're trying to achieve here....This change also looks wrong because This test requires a valid test. this is the external logdev test, and it is a duplicate uuid mount test that has nothing to do with external log devices. And, FWIW, we already have a ""log"" group to indicate tests that exercise the log, and that mostly includes all the tests that use external logs. It would be better to tag all the tests that exercise the log with ""log"" rather than create some new group that doesn't really provide any added benefit","On Tue, Dec 12, 2017 at 04:45:15PM -0800, Luis R. Rodriguez wrote:  These should notrun automatically if you don't have an external log device configured. Every test should either work with an external logdev or explicitly notrun them, so I'm not sure what you're trying to acheive here....   This change also looks wrong because:  xfs/044  [not run] This test requires a valid $SCRATCH_LOGDEV xfs/045 1s ... 1s  xfs/044 is the external logdev test, and xfs/045 is a duplicate uuid mount test that has nothign to do with external log devices.  And, FWIW, we already have a log"" group to indicate tests that exercise the log, and that mostly includes all the tests that use external logs. It would be better to tag all the tests that exercise the log with ""log"" rather than create some new group that doesn't really provide any added benefit....  Cheers,  Dave. --  Dave Chinner david@fromorbit.com""",technical,Dave Chinner,david@fromorbit.com,0,0,713,0.5731225296442688,0.5909090909090909,0,0.0,0.9891304347826086,0.0,0.0
1318,924009,1681,NAK. this should be automatically detected by the tests and they_not_run when support is not available.,"On Tue, Dec 12, 2017 at 04:45:19PM -0800, Luis R. Rodriguez wrote:  NAK. this should be automatically detected by the tests and they _not_run when support is not available.  Cheers,  Dave. --  Dave Chinner david@fromorbit.com",technical,Dave Chinner,david@fromorbit.com,0,0,103,0.07114624505928854,0.6363636363636364,0,0.0,0.9891304347826086,0.0,0.0
1319,924009,1684,This already _not_runs on new systems. we do not want to be adding one-off group descriptors to avoid tests like this - the tests themselves already detect whether they should run or not.xfs/096 2s ... [not run] Requires older mkfs without strict input checks: the last supported version of xfsprogs is 4.5.Why are you trying to add groups to define tests that not_runcorrectly on systems they aren't supported on?,"On Tue, Dec 12, 2017 at 04:45:18PM -0800, Luis R. Rodriguez wrote:  This already _not_runs on new systems. we do not want to be adding one-off group descriptors to avoid tests like this - the tests themselves already detect whether they should run or not.  xfs/096 2s ... [not run] Requires older mkfs without strict input checks: the last supported version of xfsprogs is 4.5.  Why are you trying to add groups to define tests that not_run correctly on systems they aren't supported on?  Cheers,  Dave. --  Dave Chinner david@fromorbit.com",technical,Dave Chinner,david@fromorbit.com,0,0,414,0.2964426877470356,0.6818181818181818,0,0.0,0.9891304347826086,0.0,0.0
1320,924009,1726,"The way I'm splitting up tests is one first run with a basic xfs section on a configuration file, with no external log, which pretty much runs all tests but excludes all which require external or funky configurations. A secondary pass then goes through these extra groups and then runs tests only for the previously excluded groups but with their own respective section. So for instance in this case I have. Automatic detection if the requirements are met is fine, but this doesn't let me easily use say. I see...So for my case would one better goal be to just run check without the external one and one with the external log?","On Thu, Dec 14, 2017 at 08:50:13AM +1100, Dave Chinner wrote:  The way I'm splitting up tests is one first run with a basic xfs section on a configuration file, with no external log, which pretty much runs all tests but excludes all which require external or funky configurations.  A secondary pass then goes through these extra groups and then runs tests only for the previously excluded groups but with their own respective section. So for instance in this case I have:  [xfs] .... [logdev_xfs] ...  Automatic detection if the requirements are met is fine, but this doesn't  let me easily use say:  	./check -s logdev_xfs -g logdev   I see...   So for my case would one better goal be to just run check without the external one and one with the external log?    ./check -s xfs -g log   ./check -s logdev_xfs -g log    Luis",technical,Luis R. Rodriguez,mcgrof@kernel.org,1,1,626,0.4901185770750988,0.7272727272727273,0,0.0,0.9891304347826086,0.0,0.0
1321,924009,1752,"Which seems to me like a misguided attempt to optimise test runtimes. i.e. this doesn't provide test coverage of external log behaviour in all the cases that need to be tested. Data integrity code paths are affected by having an external log. IO ordering changes with external logs, which can expose update/crash recovery problems. external logs can expose data IO race conditions that are masked by interleaved log IO. etc, etc, etc. You can't just run an internal log test then add couple of extra external log tests and say ""external logs work fine"". You can do that if we ignore the fact that a large number of tests need to be run on both internal and external log devices to cover the differences in behaviour between them. See above. Your test coverage assumptions are wrong, so what you are trying to do really doesn't tell you whether external logs work correctly or not. It's worse that not testing external logs at all,  because it gives the false impression that they have been exhaustively tested and work just fine when that really isn't the case.","On Thu, Dec 14, 2017 at 12:00:52AM +0100, Luis R. Rodriguez wrote:  Which seems to me like a misguided attempt to optimise test runtimes. i.e. this doesn't provide test coverage of external log behaviour in all the cases that need to be tested.  Data integrity code paths are affected by having an external log. IO ordering changes with external logs, which can expose update/crash recovery problems. external logs can expose data IO race conditions that are masked by interleaved log IO. etc, etc, etc.  You can't just run an internal log test then add couple of extra external log tests and say external logs work fine"".   You can do that if we ignore the fact that a large number of tests need to be run on both internal and external log devices to cover the differences in behaviour between them.   See above. Your test coverage assumptions are wrong, so what you are trying to do really doesn't tell you whether external logs work correctly or not. It's worse that not testing external logs at all, because it gives the false impression that they have been exhaustively tested and work just fine when that really isn't the case.  Cheers,  Dave. --  Dave Chinner david@fromorbit.com""",technical,Dave Chinner,david@fromorbit.com,0,0,1061,0.8260869565217391,0.7727272727272727,0,0.0,0.9891304347826086,0.0,0.0
1322,924009,1907,"This looks fine to me. The only system I have by hand that have both but not any is openSUSE Tumbleweed. Without this patch, dbtest was not built on openSUSE, and was built successfully with this patch applied. And dbtest is still built on RHEL6/7 and Fedora. BTW, I'll queue patch 3 and this patch for next fstests release, while other patches seem not necessary, I agreed with Dave that groups are not for excluding tests, the required tools and environments should be detected by tests and _not run if not met. (The README change looks fine, but it doesn't apply due to the ""fsgqa-381"" change, so I drop it too for now.)","On Tue, Dec 12, 2017 at 04:45:14PM -0800, Luis R. Rodriguez wrote:                      ^^^^^^ ndbm.h?  This looks fine to me.  The only system I have by hand that have both <gdbm.h> and <ndbm.h> but not any <gdbm/[gn]dbm.h> is openSUSE Tumbleweed. Without this patch, dbtest was not built on openSUSE, and was built successfully with this patch applied. And dbtest is still built on RHEL6/7 and Fedora.  BTW, I'll queue patch 3 and this patch for next fstests release, while other patches seem not necessary, I agreed with Dave that groups are not for excluding tests, the required tools and environments should be detected by tests and _notrun if not met. (The README change looks fine, but it doesn't apply due to the fsgqa-381"" change, so I drop it too for now.)  Thanks, Eryu""",technical,Eryu Guan,eguan@redhat.com,0,0,623,0.525691699604743,0.8181818181818182,0,0.010869565217391304,0.9891304347826086,0.0,0.0
1323,924009,10555,"Makes sense, thanks.","On Thu, Dec 14, 2017 at 10:39:14AM +1100, Dave Chinner wrote:  Makes sense, thanks.    Luis",technical,Luis R. Rodriguez,mcgrof@kernel.org,1,1,20,0.019762845849802372,0.8636363636363636,0,0.010869565217391304,0.9891304347826086,0.0,0.0
1324,924009,10569,"Indeed, openSUSE and SLE releases. Yeap. Feel free to modify the commit log accordingly then. Curious, what packages does Fedora/ RHEL6/7 use for the requirement here? We just have one: I think patch 2 is fine too. Yeah makes sense now. I think we should also document when adding a group makes sense as well. Feel free to modify it, its not a big deal.","On Thu, Dec 14, 2017 at 01:51:02PM +0800, Eryu Guan wrote:  Indeed, openSUSE and SLE releases.   Yeap.   Feel free to modify the commit log accordingly then. Curious, what packages does Fedora/ RHEL6/7 use for the requirement here?  We just have one:  $ rpm -ql gdbm-devel-1.12-1.282.x86_64 /usr/bin/gdbm_dump /usr/bin/gdbm_load /usr/bin/gdbmtool /usr/include/dbm.h /usr/include/gdbm.h /usr/include/ndbm.h /usr/lib64/libgdbm.a /usr/lib64/libgdbm.so /usr/lib64/libgdbm_compat.a /usr/lib64/libgdbm_compat.so /usr/lib64/libndbm.a /usr/lib64/libndbm.so /usr/share/info/gdbm.info.gz /usr/share/man/man1/gdbm_dump.1.gz /usr/share/man/man1/gdbm_load.1.gz /usr/share/man/man1/gdbmtool.1.gz /usr/share/man/man3/gdbm.3.gz   I think patch 2 is fine too.   Yeah makes sense now. I think we should also document when adding a group makes sense as well.   Feel free to modify it, its not a big deal.    Luis",technical,Luis R. Rodriguez,mcgrof@kernel.org,1,1,353,0.30434782608695654,0.9090909090909091,0,0.010869565217391304,0.9891304347826086,0.0,0.0
1325,924009,11015,"gdbm-devel too, but it has this pointing, so there's no such problem and dbtest is building normally OK, I'll modify on commit, thanks!","On Thu, Dec 14, 2017 at 06:55:03PM +0100, Luis R. Rodriguez wrote:  gdbm-devel too, but it has gdbm/[gn]dbm.h pointing to ../[gn]dbm.h, so there's no such problem and dbtest is building normally.  # rpm -ql gdbm-devel /usr/include/dbm.h /usr/include/gdbm /usr/include/gdbm.h /usr/include/gdbm/dbm.h /usr/include/gdbm/gdbm.h /usr/include/gdbm/ndbm.h /usr/include/ndbm.h /usr/lib64/libgdbm.so /usr/lib64/libgdbm_compat.so /usr/share/info/gdbm.info.gz /usr/share/man/man3/gdbm.3.gz   OK, I'll modify on commit, thanks!  Thanks, Eryu",technical,Eryu Guan,eguan@redhat.com,0,0,135,0.11857707509881422,0.9545454545454546,0,0.021739130434782608,0.9782608695652174,0.0,0.9782608695652174
1326,924009,233071,"Hi guys -This change breaks on older releases like SLES 11 where both define datum, so we get build failures.  The failure is new, but not because it used to pass and now doesn't.  It's apparently never built on SLES releases since we ship it and then we not run the test that uses.  Now that we're looking for gdbm.h and find it, we attempt to build src/dbtest and fail. This fix isn't the right solution.  The problem is that we have a couple layers of old cruft that needs to be cleaned out.1) As Luis notes, nothing sets HAVE_GDBM_H.  The thing is that there is no version of gdbm.h that exports the NDBM interface.  Further, looking at the git history, nothing has ever set HAVE_GDBM_H.  It was dead code when it was committed initially as best I can tell.2) openSUSE Tumbleweed doesn't need <gdbm.h> at all.  It needs <ndbm.h>and this fix works because Luis added it to the one sed to check for <ndbm.h> but it was a check for IRIX and the caller was removed ages ago.  It wouldn't matter if it were called anyway since libndbm is an IRIX library.  Linux, IIRC, has never shipped one. I'll post a few patches following this to clean it up and get it working on SLES11.","On 12/14/17 12:51 AM, Eryu Guan wrote:  Hi guys -  This change breaks on older releases like SLES 11 where both <ndbm.h> and <gdbm.h> define datum, so we get build failures.  The failure is new, but not because it used to pass and now doesn't.  It's apparently never built on SLES releases since we ship /usr/include/ndbm.h and then we notrun the test that uses.  Now that we're looking for gdbm.h and find it, we attempt to build src/dbtest and fail.  This fix isn't the right solution.  The problem is that we have a couple layers of old cruft that needs to be cleaned out.  1) As Luis notes, nothing sets HAVE_GDBM_H.  The thing is that there is no version of gdbm.h that exports the NDBM interface.  Further, looking at the git history, nothing has ever set HAVE_GDBM_H.  It was dead code when it was committed initially as best I can tell. 2) openSUSE Tumbleweed doesn't need <gdbm.h> at all.  It needs <ndbm.h> and this fix works because Luis added it to the HAVE_GDBM_H stanza. 3) AC_PACKAGE_WANT_NDBM used to check for <ndbm.h> but it was a check for IRIX and the caller was removed ages ago.  It wouldn't matter if it were called anyway since libndbm is an IRIX library.  Linux, IIRC, has never shipped a libndbm.  I'll post a few patches following this to clean it up and get it working on SLES11.  -Jeff  --  Jeff Mahoney SUSE Labs",technical,Jeff Mahoney,jeffm@suse.com,0,0,1174,1.0,1.0,1,1.0,0.0,0.9782608695652174,0.0
